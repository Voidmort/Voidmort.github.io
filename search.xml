<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Aerial Cactus Identification</title>
    <url>/2020/01/10/AerialCactusIdentification/</url>
    <content><![CDATA[<h1 id="仙人掌识别"><a href="#仙人掌识别" class="headerlink" title="仙人掌识别"></a>仙人掌识别</h1><p>To assess the impact of climate change on Earth’s flora and fauna, it is vital to quantify how human activities such as logging, mining, and agriculture are impacting our protected natural areas. Researchers in Mexico have created the VIGIA project, which aims to build a system for autonomous surveillance of protected areas. A first step in such an effort is the ability to recognize the vegetation inside the protected areas. In this competition, you are tasked with creation of an algorithm that can identify a specific type of cactus in aerial imagery.</p>
<p><a href="https://www.kaggle.com/c/aerial-cactus-identification" target="_blank" rel="noopener">https://www.kaggle.com/c/aerial-cactus-identification</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># This Python 3 environment comes with many helpful analytics libraries installed</span></span><br><span class="line"><span class="comment"># It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python</span></span><br><span class="line"><span class="comment"># For example, here's several helpful packages to load in </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># linear algebra</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># data processing, CSV file I/O (e.g. pd.read_csv)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Input data files are available in the "../input/" directory.</span></span><br><span class="line"><span class="comment"># For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">print(os.listdir(<span class="string">"../input"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Any results you write to the current directory are saved as output.</span></span><br></pre></td></tr></table></figure>
<pre><code>[&apos;aerial-cactus-identification&apos;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm, tqdm_notebook</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, load_model</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> layer_utils</span><br><span class="line"><span class="keyword">from</span> keras.utils.data_utils <span class="keyword">import</span> get_file</span><br><span class="line"><span class="keyword">from</span> keras.applications.imagenet_utils <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br></pre></td></tr></table></figure>
<h1 id="imge-预处理"><a href="#imge-预处理" class="headerlink" title="imge 预处理"></a>imge 预处理</h1><h2 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h2><p>首先解压图片数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line">Dataset = <span class="string">"aerial-cactus-identification"</span></span><br><span class="line"><span class="keyword">with</span> zipfile.ZipFile(<span class="string">"../input/"</span>+Dataset+<span class="string">"/train.zip"</span>,<span class="string">"r"</span>) <span class="keyword">as</span> z:</span><br><span class="line">    z.extractall(<span class="string">"."</span>)</span><br><span class="line"><span class="keyword">with</span> zipfile.ZipFile(<span class="string">"../input/"</span>+Dataset+<span class="string">"/test.zip"</span>,<span class="string">"r"</span>) <span class="keyword">as</span> z:</span><br><span class="line">    z.extractall(<span class="string">"."</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(os.listdir(<span class="string">"./"</span>))</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;train&apos;, &apos;test&apos;, &apos;__notebook_source__.ipynb&apos;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_path = <span class="string">'./train/'</span></span><br><span class="line">test_path = <span class="string">"./test/"</span></span><br><span class="line">train_csv = pd.read_csv(<span class="string">'../input/aerial-cactus-identification/train.csv'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="把图片转换为矩阵"><a href="#把图片转换为矩阵" class="headerlink" title="把图片转换为矩阵"></a>把图片转换为矩阵</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">labels = []</span><br><span class="line">images = []</span><br><span class="line">image_name = train_csv[<span class="string">'id'</span>].values</span><br><span class="line"><span class="keyword">for</span> img_id <span class="keyword">in</span> tqdm_notebook(image_name):</span><br><span class="line">    imdata = cv2.imread(train_path + img_id)</span><br><span class="line">    images.append(imdata)</span><br><span class="line">    has_cactus = train_csv[train_csv[<span class="string">'id'</span>] == img_id][<span class="string">'has_cactus'</span>].values</span><br><span class="line">    labels.append(has_cactus)</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(IntProgress(value=0, max=17500), HTML(value=&apos;&apos;)))
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">labels[<span class="number">0</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[array([1]),
 array([1]),
 array([1]),
 array([1]),
 array([1]),
 array([1]),
 array([0]),
 array([0]),
 array([1]),
 array([1])]
</code></pre><p>画出图片观察一下，第1，2个图片有仙人掌，第7个图片没有。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">ax[<span class="number">0</span>].imshow(images[<span class="number">0</span>])</span><br><span class="line">ax[<span class="number">1</span>].imshow(images[<span class="number">1</span>])</span><br><span class="line">ax[<span class="number">2</span>].imshow(images[<span class="number">6</span>])</span><br></pre></td></tr></table></figure>
<p><img src="kernelCactus_14_1.png" alt="png"></p>
<p>数值归一化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">images = np.asarray(images)</span><br><span class="line">images = images.astype(<span class="string">'float32'</span>)</span><br><span class="line">images /= <span class="number">255</span></span><br><span class="line">labels = np.asarray(labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">images.shape, labels.shape</span><br></pre></td></tr></table></figure>
<pre><code>((17500, 32, 32, 3), (17500, 1))
</code></pre><p>画出有无仙人掌图片条形图</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.countplot(np.squeeze(labels))</span><br></pre></td></tr></table></figure>
<p><img src="kernelCactus_19_1.png" alt="png"></p>
<h1 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: identity_block</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity_block</span><span class="params">(X, f, filters, stage, block)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the identity block as defined in Figure 3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    f -- integer, specifying the shape of the middle CONV's window for the main path</span></span><br><span class="line"><span class="string">    filters -- python list of integers, defining the number of filters in the CONV layers of the main path</span></span><br><span class="line"><span class="string">    stage -- integer, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    block -- string/character, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">'res'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    bn_name_base = <span class="string">'bn'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save the input value. You'll need this later to add back to the main path. </span></span><br><span class="line">    X_shortcut = X</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First component of main path</span></span><br><span class="line">    X = Conv2D(filters = F1, kernel_size = (<span class="number">1</span>, <span class="number">1</span>), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">'valid'</span>, name = conv_name_base + <span class="string">'2a'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2a'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Second component of main path (≈3 lines)</span></span><br><span class="line">    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (<span class="number">1</span>, <span class="number">1</span>), padding = <span class="string">"same"</span>, name = conv_name_base + <span class="string">"2b"</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">"2b"</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">"relu"</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Third component of main path (≈2 lines)</span></span><br><span class="line">    X = Conv2D(filters = F3, kernel_size = (<span class="number">1</span>, <span class="number">1</span>), strides = (<span class="number">1</span>, <span class="number">1</span>), padding = <span class="string">"valid"</span>, name = conv_name_base + <span class="string">"2c"</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">"2c"</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class="line">    X = Add()([X, X_shortcut])</span><br><span class="line">    X = Activation(<span class="string">"relu"</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: convolutional_block</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolutional_block</span><span class="params">(X, f, filters, stage, block, s = <span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the convolutional block as defined in Figure 4</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    f -- integer, specifying the shape of the middle CONV's window for the main path</span></span><br><span class="line"><span class="string">    filters -- python list of integers, defining the number of filters in the CONV layers of the main path</span></span><br><span class="line"><span class="string">    stage -- integer, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    block -- string/character, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    s -- Integer, specifying the stride to be used</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">'res'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    bn_name_base = <span class="string">'bn'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save the input value</span></span><br><span class="line">    X_shortcut = X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">##### MAIN PATH #####</span></span><br><span class="line">    <span class="comment"># First component of main path </span></span><br><span class="line">    X = Conv2D(F1, (<span class="number">1</span>, <span class="number">1</span>), strides = (s,s), name = conv_name_base + <span class="string">'2a'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2a'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Second component of main path (≈3 lines)</span></span><br><span class="line">    X = Conv2D(F2, (f, f), strides = (<span class="number">1</span>, <span class="number">1</span>), name = conv_name_base + <span class="string">"2b"</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>), padding = <span class="string">"same"</span>)(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">"2b"</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">"relu"</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Third component of main path (≈2 lines)</span></span><br><span class="line">    X = Conv2D(F3, (<span class="number">1</span>, <span class="number">1</span>), strides = (<span class="number">1</span>, <span class="number">1</span>), name = conv_name_base + <span class="string">"2c"</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">"2c"</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment">##### SHORTCUT PATH #### (≈2 lines)</span></span><br><span class="line">    X_shortcut = Conv2D(F3, (<span class="number">1</span>, <span class="number">1</span>), strides = (s, s), name = conv_name_base + <span class="string">"1"</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X_shortcut)</span><br><span class="line">    X_shortcut = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">"1"</span>)(X_shortcut)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class="line">    X = Add()([X, X_shortcut])</span><br><span class="line">    X = Activation(<span class="string">"relu"</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: ResNet50</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet50</span><span class="params">(input_shape = <span class="params">(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)</span>, classes = <span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the popular ResNet50 the following architecture:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; BATCHNORM -&gt; RELU -&gt; MAXPOOL -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; CONVBLOCK -&gt; IDBLOCK*3</span></span><br><span class="line"><span class="string">    -&gt; CONVBLOCK -&gt; IDBLOCK*5 -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; AVGPOOL -&gt; TOPLAYER</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the images of the dataset</span></span><br><span class="line"><span class="string">    classes -- integer, number of classes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a Model() instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the input as a tensor with shape input_shape</span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Zero-Padding</span></span><br><span class="line">    X = ZeroPadding2D((<span class="number">3</span>, <span class="number">3</span>))(X_input)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Stage 1</span></span><br><span class="line">    X = Conv2D(<span class="number">32</span>, (<span class="number">7</span>, <span class="number">7</span>), strides = (<span class="number">2</span>, <span class="number">2</span>), name = <span class="string">'conv1'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = <span class="string">'bn_conv1'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = MaxPooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 2</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">32</span>, <span class="number">32</span>, <span class="number">256</span>], stage = <span class="number">2</span>, block=<span class="string">'a'</span>, s = <span class="number">1</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">32</span>, <span class="number">32</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 3 (≈4 lines)</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">64</span>, <span class="number">64</span>, <span class="number">512</span>], stage = <span class="number">3</span>, block = <span class="string">"a"</span>, s = <span class="number">2</span>)</span><br><span class="line">    X = identity_block(X, f = <span class="number">3</span>, filters = [<span class="number">64</span>, <span class="number">64</span>, <span class="number">512</span>], stage = <span class="number">3</span>, block = <span class="string">"b"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 4 (≈6 lines)</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">128</span>, <span class="number">128</span>, <span class="number">1024</span>], stage = <span class="number">4</span>, block = <span class="string">"a"</span>, s = <span class="number">2</span>)</span><br><span class="line">    X = identity_block(X, f = <span class="number">3</span>, filters = [<span class="number">128</span>, <span class="number">128</span>, <span class="number">1024</span>], stage = <span class="number">4</span>, block = <span class="string">"b"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 5 (≈3 lines)</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">256</span>, <span class="number">256</span>, <span class="number">2048</span>], stage = <span class="number">5</span>, block = <span class="string">"a"</span>, s = <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># AVGPOOL (≈1 line). Use "X = AveragePooling2D(...)(X)"</span></span><br><span class="line">    <span class="comment">#X = AveragePooling2D((2, 2), name = "ave_pool")(X)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># output layer</span></span><br><span class="line">    X = Flatten()(X)</span><br><span class="line">    X = Dense(classes, activation=<span class="string">'softmax'</span>, name=<span class="string">'fc'</span> + str(classes), kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create model</span></span><br><span class="line">    model = Model(inputs = X_input, outputs = X, name=<span class="string">'ResNet50'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = ResNet50()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 30, 30, 32)        896       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 28, 28, 32)        9248      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 12, 12, 64)        18496     
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 10, 10, 64)        36928     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 1600)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               819712    
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513       
=================================================================
Total params: 885,793
Trainable params: 885,793
Non-trainable params: 0
_________________________________________________________________
</code></pre><h1 id="开始预测"><a href="#开始预测" class="headerlink" title="开始预测"></a>开始预测</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,      </span><br><span class="line">              optimizer=optimizers.RMSprop(lr=<span class="number">1e-4</span>),         </span><br><span class="line">              metrics=[<span class="string">'acc'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hist = model.fit(images, labels,</span><br><span class="line">                validation_split=<span class="number">0.2</span>,</span><br><span class="line">                batch_size=<span class="number">100</span>,</span><br><span class="line">                epochs = <span class="number">20</span>,</span><br><span class="line">                )</span><br></pre></td></tr></table></figure>
<pre><code>Train on 14000 samples, validate on 3500 samples
Epoch 1/20
14000/14000 [==============================] - 4s 316us/step - loss: 0.3205 - acc: 0.8562 - val_loss: 0.2419 - val_acc: 0.9023
Epoch 2/20
14000/14000 [==============================] - 1s 74us/step - loss: 0.1864 - acc: 0.9275 - val_loss: 0.1531 - val_acc: 0.9431
Epoch 3/20
14000/14000 [==============================] - 1s 74us/step - loss: 0.1672 - acc: 0.9336 - val_loss: 0.1459 - val_acc: 0.9429
Epoch 4/20
14000/14000 [==============================] - 1s 74us/step - loss: 0.1520 - acc: 0.9414 - val_loss: 0.1198 - val_acc: 0.9563
Epoch 5/20
14000/14000 [==============================] - 1s 74us/step - loss: 0.1383 - acc: 0.9474 - val_loss: 0.1515 - val_acc: 0.9437
Epoch 6/20
14000/14000 [==============================] - 1s 75us/step - loss: 0.1271 - acc: 0.9528 - val_loss: 0.1026 - val_acc: 0.9649
Epoch 7/20
14000/14000 [==============================] - 1s 74us/step - loss: 0.1176 - acc: 0.9560 - val_loss: 0.1102 - val_acc: 0.9611
Epoch 8/20
14000/14000 [==============================] - 1s 76us/step - loss: 0.1113 - acc: 0.9574 - val_loss: 0.0971 - val_acc: 0.9663
Epoch 9/20
14000/14000 [==============================] - 1s 75us/step - loss: 0.1046 - acc: 0.9626 - val_loss: 0.0826 - val_acc: 0.9729
Epoch 10/20
14000/14000 [==============================] - 1s 75us/step - loss: 0.1002 - acc: 0.9626 - val_loss: 0.0854 - val_acc: 0.9700
Epoch 11/20
14000/14000 [==============================] - 1s 75us/step - loss: 0.0928 - acc: 0.9646 - val_loss: 0.0860 - val_acc: 0.9700
Epoch 12/20
14000/14000 [==============================] - 1s 75us/step - loss: 0.0869 - acc: 0.9683 - val_loss: 0.2181 - val_acc: 0.9163
Epoch 13/20
14000/14000 [==============================] - 1s 75us/step - loss: 0.0829 - acc: 0.9694 - val_loss: 0.0929 - val_acc: 0.9663
Epoch 14/20
14000/14000 [==============================] - 1s 75us/step - loss: 0.0793 - acc: 0.9705 - val_loss: 0.0618 - val_acc: 0.9814
Epoch 15/20
14000/14000 [==============================] - 1s 74us/step - loss: 0.0737 - acc: 0.9731 - val_loss: 0.0687 - val_acc: 0.9777
Epoch 16/20
14000/14000 [==============================] - 1s 74us/step - loss: 0.0718 - acc: 0.9738 - val_loss: 0.0578 - val_acc: 0.9823
Epoch 17/20
14000/14000 [==============================] - 1s 74us/step - loss: 0.0671 - acc: 0.9750 - val_loss: 0.0616 - val_acc: 0.9786
Epoch 18/20
14000/14000 [==============================] - 1s 74us/step - loss: 0.0620 - acc: 0.9784 - val_loss: 0.1049 - val_acc: 0.9626
Epoch 19/20
14000/14000 [==============================] - 1s 75us/step - loss: 0.0612 - acc: 0.9771 - val_loss: 0.0627 - val_acc: 0.9791
Epoch 20/20
14000/14000 [==============================] - 1s 74us/step - loss: 0.0592 - acc: 0.9780 - val_loss: 0.0547 - val_acc: 0.9834
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hist.history.keys()</span><br></pre></td></tr></table></figure>
<pre><code>dict_keys([&apos;val_loss&apos;, &apos;val_acc&apos;, &apos;loss&apos;, &apos;acc&apos;])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># summarize history for accuracy</span></span><br><span class="line">plt.plot(hist.history[<span class="string">'acc'</span>])</span><br><span class="line">plt.plot(hist.history[<span class="string">'val_acc'</span>])</span><br><span class="line">plt.title(<span class="string">'model accuracy'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'epoch'</span>)</span><br><span class="line">plt.legend([<span class="string">'train'</span>, <span class="string">'test'</span>], loc=<span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># summarize history for loss</span></span><br><span class="line">plt.plot(hist.history[<span class="string">'loss'</span>])</span><br><span class="line">plt.plot(hist.history[<span class="string">'val_loss'</span>])</span><br><span class="line">plt.title(<span class="string">'model loss'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'epoch'</span>)</span><br><span class="line">plt.legend([<span class="string">'train'</span>, <span class="string">'test'</span>], loc=<span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="kernelcactus-identifiaction-cnn_21_0.png" alt="png"></p>
<p><img src="kernelcactus-identifiaction-cnn_21_1.png" alt="png"></p>
<h1 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test = []</span><br><span class="line">test_id = []</span><br><span class="line"><span class="keyword">for</span> img_id <span class="keyword">in</span> tqdm_notebook(os.listdir(test_path)):</span><br><span class="line">    test.append(cv2.imread(test_path + img_id))     </span><br><span class="line">    test_id.append(img_id)</span><br><span class="line">test = np.asarray(test)</span><br><span class="line">test = test.astype(<span class="string">'float32'</span>)</span><br><span class="line">test /= <span class="number">255</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_predictions = model.predict(test)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sub_df = pd.DataFrame(test_predictions, columns=[<span class="string">'has_cactus'</span>])</span><br><span class="line">sub_df[<span class="string">'has_cactus'</span>] = sub_df[<span class="string">'has_cactus'</span>].apply(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x &gt; <span class="number">0.75</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line">sub_df[<span class="string">'id'</span>] = <span class="string">''</span></span><br><span class="line">cols = sub_df.columns.tolist()</span><br><span class="line">cols = cols[<span class="number">-1</span>:] + cols[:<span class="number">-1</span>]</span><br><span class="line">sub_df=sub_df[cols]</span><br><span class="line"><span class="keyword">for</span> i, img <span class="keyword">in</span> enumerate(test_id):</span><br><span class="line">    sub_df.set_value(i,<span class="string">'id'</span>,img)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.countplot(sub_df[<span class="string">'has_cactus'</span>])</span><br></pre></td></tr></table></figure>
<p><img src="kernelcactus-identifiaction-cnn_25_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sub_df.to_csv(<span class="string">'submission.csv'</span>,index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<h1 id="得分"><a href="#得分" class="headerlink" title="得分"></a>得分</h1><p>Public Score</p>
<p>0.9583</p>
<h1 id="简单的模型"><a href="#简单的模型" class="headerlink" title="简单的模型"></a>简单的模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#model</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line">model = models.Sequential() </span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>,                     </span><br><span class="line">                                    input_shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>))) </span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>))) </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">model.add(layers.Flatten()) </span><br><span class="line">model.add(layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>)) </span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Kaggle</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title>CodeForces978C Letters</title>
    <url>/2019/06/10/CodeForces978C%20Letters/</url>
    <content><![CDATA[<p>There are  dormitories in Berland State University, they are numbered with integers from  to . Each dormitory consists of rooms, there are  rooms in -th dormitory. The rooms in -th dormitory are numbered from  to .</p>
<p>A postman delivers letters. Sometimes there is no specific dormitory and room number in it on an envelope. Instead of it only a room number among all rooms of all dormitories is written on an envelope. In this case, assume that all the rooms are numbered from  to  and the rooms of the first dormitory go first, the rooms of the second dormitory go after them and so on.</p>
<p>For example, in case ,  and  an envelope can have any integer from to  written on it. If the number  is written on an envelope, it means that the letter should be delivered to the room number  of the second dormitory.</p>
<p>For each of  letters by the room number among all  dormitories, determine the particular dormitory and the room number in a dormitory where this letter should be delivered.</p>
<p>Input<br>The first line contains two integers  and   — the number of dormitories and the number of letters.</p>
<p>The second line contains a sequence  , where  equals to the number of rooms in the -th dormitory. The third line contains a sequence  , where  equals to the room number (among all rooms of all dormitories) for the -th letter. All  are given in increasing order.</p>
<p>Output<br>Print  lines. For each letter print two integers  and  — the dormitory number  and the room number  in this dormitory  to deliver the letter.</p>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line">Examples</span><br><span class="line">Input</span><br><span class="line"><span class="number">3</span> <span class="number">6</span></span><br><span class="line"><span class="number">10</span> <span class="number">15</span> <span class="number">12</span></span><br><span class="line"><span class="number">1</span> <span class="number">9</span> <span class="number">12</span> <span class="number">23</span> <span class="number">26</span> <span class="number">37</span></span><br><span class="line">Output</span><br><span class="line"><span class="number">1</span> <span class="number">1</span></span><br><span class="line"><span class="number">1</span> <span class="number">9</span></span><br><span class="line"><span class="number">2</span> <span class="number">2</span></span><br><span class="line"><span class="number">2</span> <span class="number">13</span></span><br><span class="line"><span class="number">3</span> <span class="number">1</span></span><br><span class="line"><span class="number">3</span> <span class="number">12</span></span><br><span class="line">Input</span><br><span class="line"><span class="number">2</span> <span class="number">3</span></span><br><span class="line"><span class="number">5</span> <span class="number">10000000000</span></span><br><span class="line"><span class="number">5</span> <span class="number">6</span> <span class="number">9999999999</span></span><br><span class="line">Output</span><br><span class="line"><span class="number">1</span> <span class="number">5</span></span><br><span class="line"><span class="number">2</span> <span class="number">1</span></span><br><span class="line"><span class="number">2</span> <span class="number">9999999994</span></span><br><span class="line">Note</span><br><span class="line">In <span class="keyword">the</span> <span class="keyword">first</span> example letters should be delivered <span class="keyword">in</span> <span class="keyword">the</span> following order:</span><br><span class="line"></span><br><span class="line"><span class="keyword">the</span> <span class="keyword">first</span> letter <span class="keyword">in</span> room  <span class="keyword">of</span> <span class="keyword">the</span> <span class="keyword">first</span> dormitory</span><br><span class="line"><span class="keyword">the</span> <span class="keyword">second</span> letter <span class="keyword">in</span> room  <span class="keyword">of</span> <span class="keyword">the</span> <span class="keyword">first</span> dormitory</span><br><span class="line"><span class="keyword">the</span> <span class="keyword">third</span> letter <span class="keyword">in</span> room  <span class="keyword">of</span> <span class="keyword">the</span> <span class="keyword">second</span> dormitory</span><br><span class="line"><span class="keyword">the</span> <span class="keyword">fourth</span> letter <span class="keyword">in</span> room  <span class="keyword">of</span> <span class="keyword">the</span> <span class="keyword">second</span> dormitory</span><br><span class="line"><span class="keyword">the</span> <span class="keyword">fifth</span> letter <span class="keyword">in</span> room  <span class="keyword">of</span> <span class="keyword">the</span> <span class="keyword">third</span> dormitory</span><br><span class="line"><span class="keyword">the</span> <span class="keyword">sixth</span> letter <span class="keyword">in</span> room  <span class="keyword">of</span> <span class="keyword">the</span> <span class="keyword">third</span> dormitory</span><br></pre></td></tr></table></figure>
<p>解析：</p>
<pre><code>题意: 查询m个数分别在一个有n个元素的递增序列中的相对位置。

两种解法：
    暴力解法为两层嵌套循环，时间复杂度为O(m×n)；
    二分法查找，时间复杂度为O(m×log2n)
</code></pre><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAXN 2048 <span class="comment">//定义一个数组的大小</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 暴力求解</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">letters</span><span class="params">(<span class="keyword">int</span>* a, <span class="keyword">int</span>* b, <span class="keyword">int</span> n, <span class="keyword">int</span> m)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i, j, t, sum=<span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; m; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            sum += a[i];</span><br><span class="line">            <span class="keyword">if</span>(b[j] &lt;= sum)</span><br><span class="line">            &#123;</span><br><span class="line">                t = sum - a[i];</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"%d %d\n"</span>, i+<span class="number">1</span>, b[j] - t);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 二分法</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">binary_letters</span><span class="params">(<span class="keyword">int</span>* a, <span class="keyword">int</span>* b, <span class="keyword">int</span> n, <span class="keyword">int</span> m)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum[MAXN] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="keyword">int</span> i, j, r, l, mid;</span><br><span class="line">    </span><br><span class="line">    sum[<span class="number">0</span>] = a[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 构建ai序列</span></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt; n; i++)</span><br><span class="line">        sum[i] = sum[i<span class="number">-1</span>] + a[i];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; m; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(b[j] &gt; sum[i<span class="number">-1</span>])</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%d index out of bounds.\n"</span>, b[j]);</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span>(r = n - <span class="number">1</span>, l = <span class="number">0</span>; r - l &gt; <span class="number">1</span>; )</span><br><span class="line">        &#123;</span><br><span class="line">            mid = (r + l) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(b[j] &gt; sum[mid])</span><br><span class="line">                l = mid;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                r = mid;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(b[j] &gt; sum[l])</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%d %d\n"</span>, l+<span class="number">2</span>, b[j]-sum[l]);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%d %d\n"</span>, l+<span class="number">1</span>, b[j]);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a[] = &#123;<span class="number">10</span>, <span class="number">15</span>, <span class="number">12</span>, <span class="number">12</span>, <span class="number">12</span>&#125;;</span><br><span class="line">    <span class="keyword">int</span> b[] = &#123;<span class="number">1</span>, <span class="number">9</span>, <span class="number">12</span>, <span class="number">23</span>, <span class="number">26</span>, <span class="number">37</span>, <span class="number">50</span>, <span class="number">62</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> n = <span class="keyword">sizeof</span>(a) / <span class="keyword">sizeof</span>(<span class="keyword">int</span>);</span><br><span class="line">    <span class="keyword">int</span> m = <span class="keyword">sizeof</span>(b) / <span class="keyword">sizeof</span>(<span class="keyword">int</span>);</span><br><span class="line"></span><br><span class="line">    letters(a, b, n, m);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"================\n"</span>);</span><br><span class="line">    binary_letters(a, b, n, m);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>out put</p>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span> <span class="number">1</span></span><br><span class="line"><span class="number">1</span> <span class="number">9</span></span><br><span class="line"><span class="number">2</span> <span class="number">2</span></span><br><span class="line"><span class="number">2</span> <span class="number">13</span></span><br><span class="line"><span class="number">3</span> <span class="number">1</span></span><br><span class="line"><span class="number">3</span> <span class="number">12</span></span><br><span class="line"><span class="number">5</span> <span class="number">1</span></span><br><span class="line">================</span><br><span class="line"><span class="number">1</span> <span class="number">1</span></span><br><span class="line"><span class="number">1</span> <span class="number">9</span></span><br><span class="line"><span class="number">2</span> <span class="number">2</span></span><br><span class="line"><span class="number">2</span> <span class="number">13</span></span><br><span class="line"><span class="number">3</span> <span class="number">1</span></span><br><span class="line"><span class="number">3</span> <span class="number">12</span></span><br><span class="line"><span class="number">5</span> <span class="number">1</span></span><br><span class="line"><span class="number">62</span> index out of bounds.</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>算法练习</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>c</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>Django 部署(Apache) 趟过的坑</title>
    <url>/2019/01/27/Django%20%E9%83%A8%E7%BD%B2(Apache)/</url>
    <content><![CDATA[<p>Django是一个，由Python写成的开放源代码的Web应用框架，在使用apache部署的时候走了好多坑这里记录下。</p>
<p>参考：</p>
<p><a href="http://www.runoob.com/django/django-tutorial.html" target="_blank" rel="noopener">Django教程</a></p>
<p><a href="https://code.ziqiangxuetang.com/django/django-deploy.html" target="_blank" rel="noopener">apache部署</a></p>
<h1 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h1><ol>
<li>一个服务器，我使用的是阿里云服务器。</li>
<li>推荐使用ubuntu镜像，因为软件集成度高（就是简单，傻瓜也会玩）。</li>
<li>已经使用Django搭建好web服务，如何搭建看<a href="http://www.runoob.com/django/django-tutorial.html" target="_blank" rel="noopener">Django教程</a>。</li>
</ol>
<p>这里只记录部署apache的坑了，其他上面都有详细讲解，就略了。</p>
<h1 id="安装apache2和mod-wsgi"><a href="#安装apache2和mod-wsgi" class="headerlink" title="安装apache2和mod_wsgi"></a>安装apache2和mod_wsgi</h1><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo apt-get install apache2</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> Python 2</span></span><br><span class="line">sudo apt-get install libapache2-mod-wsgi</span><br><span class="line"> </span><br><span class="line"><span class="meta">#</span><span class="bash"> Python 3</span></span><br><span class="line">sudo apt-get install libapache2-mod-wsgi-py3</span><br></pre></td></tr></table></figure>
<p><strong>看版本！！！（非常重要）</strong></p>
<p>版本不同在配置上有区别，推荐使用比较新的版本，也就是2.4以上，如果是1，下面的配置是不一样的！！！<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">apachectl -v</span><br><span class="line"></span><br><span class="line">Server version: Apache/2.4.18 (Ubuntu)</span><br><span class="line">Server built:   2018-06-07T19:43:03</span><br></pre></td></tr></table></figure></p>
<p>先别急着配置，看看能不能正常启动</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">sudo<span class="built_in"> service </span>apache2 restart</span><br></pre></td></tr></table></figure>
<p>这时候正常情况会启动默认配置，使用浏览器访问你服务器的外网IP，如果正常会显示下图：</p>
<p><img src="1548558426.jpg" alt="image"></p>
<p>无法访问请检查阿里云的防火墙设置，看端口是否允许通过，浏览器默认是80，这里顺便加一个8080，供下面测试。</p>
<p><img src="1548558134.jpg" alt="image"></p>
<p><strong>Apache2 Ubuntu Default Page 页面可以正常访问</strong>代表我们的apache安装成功，下面开始修改配置文件。</p>
<h1 id="设置目录和文件的权限"><a href="#设置目录和文件的权限" class="headerlink" title="设置目录和文件的权限"></a>设置目录和文件的权限</h1><p>一般目录权限设置为 755，文件权限设置为 644 </p>
<p>假如项目位置在 /home/user/WebService （在WebService 下面有一个 manage.py，WebService 是项目名称）</p>
<figure class="highlight excel"><table><tr><td class="code"><pre><span class="line">cd /home/user/</span><br><span class="line">sudo chmod -R <span class="number">644</span> <span class="built_in">WebService</span></span><br><span class="line">sudo <span class="built_in">find</span> <span class="built_in">WebService</span> -<span class="built_in">type</span> d | xargs chmod <span class="number">755</span></span><br></pre></td></tr></table></figure>
<h1 id="Django-的-settings-py-要设置清楚"><a href="#Django-的-settings-py-要设置清楚" class="headerlink" title="Django 的 settings.py 要设置清楚"></a>Django 的 settings.py 要设置清楚</h1><p>media 文件夹一般用来存放用户上传文件，static 一般用来放自己网站的js，css，图片等，在settings.py中的相关设置</p>
<p>STATIC_URL 为静态文件的网址 STATIC_ROOT 为静态文件的根目录，</p>
<p>MEDIA_URL 为用户上传文件夹的根目录，MEDIA_URL为对应的访问网址</p>
<p>需要media的 要给media目录单独设置写的权限</p>
<p>ALLOWED_HOSTS是为了限定请求中的host值，以防止黑客构造包来发送请求。只有在列表中的host才能访问。</p>
<p><strong><em>注意：在这里本人强烈建议不要使用</em>通配符去配置，另外当DEBUG设置为False的时候必须配置这个配置。否则会抛出异常。*</strong><br><figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="attr">ALLOWED_HOSTS</span> = [<span class="string">'*'</span>]</span><br></pre></td></tr></table></figure></p>
<p>这里先写个*等全部调通了再改。。。</p>
<h1 id="apache的配置文件"><a href="#apache的配置文件" class="headerlink" title="apache的配置文件"></a>apache的配置文件</h1><figure class="highlight vim"><table><tr><td class="code"><pre><span class="line"><span class="keyword">cd</span> /etc/apache2/sites-available</span><br><span class="line">sudo <span class="keyword">vim</span> mysite.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure>
<p>在这里我们自己写个配置<br><figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="section">&lt;VirtualHost *:8080&gt;</span></span><br><span class="line">    <span class="attribute"><span class="nomarkup">ServerName</span></span> www.yourdomain.com</span><br><span class="line">	<span class="attribute">ServerAdmin</span> youremail@mail.com</span><br><span class="line">	<span class="attribute">ErrorLog</span> <span class="variable">$&#123;APACHE_LOG_DIR&#125;</span>/error.log</span><br><span class="line">	<span class="attribute">CustomLog</span> <span class="variable">$&#123;APACHE_LOG_DIR&#125;</span>/access.log combined</span><br><span class="line">	<span class="attribute">Alias</span> /static/ /home/user//WebService/static/</span><br><span class="line">	<span class="section">&lt;Directory /home/user/WebService/static&gt;</span></span><br><span class="line">	   <span class="attribute"><span class="nomarkup">Options</span></span> Indexes FollowSymLinks</span><br><span class="line">	   <span class="attribute">AllowOverride</span> None</span><br><span class="line">	   <span class="attribute">Require</span> <span class="literal">all</span> granted</span><br><span class="line">	<span class="section">&lt;/Directory&gt;</span></span><br><span class="line">	<span class="attribute">WSGIScriptAlias</span> / /home/user/WebService/WebService/wsgi.py</span><br><span class="line">	<span class="section">&lt;Directory /home/user/WebService/WebService&gt;</span></span><br><span class="line">	<span class="section">&lt;Files wsgi.py&gt;</span></span><br><span class="line">	   <span class="attribute"><span class="nomarkup">Options</span></span> Indexes FollowSymLinks</span><br><span class="line">	   <span class="attribute">AllowOverride</span> None</span><br><span class="line">	   <span class="attribute">Require</span> <span class="literal">all</span> granted</span><br><span class="line">	<span class="section">&lt;/Files&gt;</span></span><br><span class="line">	<span class="section">&lt;/Directory&gt;</span></span><br><span class="line"><span class="section">&lt;/VirtualHost&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>根据自己的情况改改，要注意目录要对，并且也写下面这个配置，apache版本不同配置是不同的！！！</p>
<p><strong>Options Indexes FollowSymLinks</strong></p>
<p><strong>AllowOverride None</strong></p>
<p><strong>Require all granted</strong></p>
<p>因为我们刚配置里写的是8080的端口，所以要把它加到监听列表里<br><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/apache2/ports.conf</span><br><span class="line"></span><br><span class="line">Listen <span class="number">80</span></span><br><span class="line"></span><br><span class="line">加一句</span><br><span class="line"></span><br><span class="line">Listen <span class="number">80</span></span><br><span class="line">Listen <span class="number">8080</span></span><br></pre></td></tr></table></figure></p>
<h1 id="wsgi-py文件修改"><a href="#wsgi-py文件修改" class="headerlink" title="wsgi.py文件修改"></a>wsgi.py文件修改</h1><p>修改这个文件的目的就是把apache2和你的网站project联系起来</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> join,dirname,abspath <span class="comment"># +</span></span><br><span class="line"><span class="keyword">from</span> django.core.wsgi <span class="keyword">import</span> get_wsgi_application</span><br><span class="line"></span><br><span class="line">PROJECT_DIR = dirname(dirname(abspath(__file__))) <span class="comment"># +</span></span><br><span class="line"><span class="keyword">import</span> sys <span class="comment"># +</span></span><br><span class="line">sys.path.insert(<span class="number">0</span>,PROJECT_DIR) <span class="comment"># +</span></span><br><span class="line"></span><br><span class="line">os.environ.setdefault(<span class="string">"DJANGO_SETTINGS_MODULE"</span>, <span class="string">"WebService.settings"</span>)</span><br><span class="line"><span class="keyword">from</span> django.core.wsgi <span class="keyword">import</span> get_wsgi_application</span><br><span class="line">application = get_wsgi_application()</span><br></pre></td></tr></table></figure>
<h1 id="激活新配"><a href="#激活新配" class="headerlink" title="激活新配"></a>激活新配</h1><p><strong>这里不用写路径</strong></p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">sudo</span> <span class="selector-tag">a2ensite</span> <span class="selector-tag">mysite</span> 或 <span class="selector-tag">sudo</span> <span class="selector-tag">a2ensite</span> <span class="selector-tag">mysite</span><span class="selector-class">.conf</span></span><br></pre></td></tr></table></figure>
<p>重启apach<br><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">sudo<span class="built_in"> service </span>apache2 restart</span><br></pre></td></tr></table></figure></p>
<p>访问 你的网站，记得加端口号 0.0.0.0:8080</p>
<h1 id="出错看log"><a href="#出错看log" class="headerlink" title="出错看log"></a>出错看log</h1><figure class="highlight stata"><table><tr><td class="code"><pre><span class="line"><span class="keyword">cat</span> /<span class="keyword">var</span>/<span class="keyword">log</span>/apache2/<span class="keyword">error</span>.<span class="built_in">log</span></span><br></pre></td></tr></table></figure>
<h1 id="虚拟环境部署"><a href="#虚拟环境部署" class="headerlink" title="虚拟环境部署"></a>虚拟环境部署</h1><p>通常我们的系统中会有多个python环境，使用virtualenv管理</p>
<h2 id="源代码安装python3-7"><a href="#源代码安装python3-7" class="headerlink" title="源代码安装python3.7"></a>源代码安装python3.7</h2><p>上<a href="https://www.python.org/" target="_blank" rel="noopener">Python官网</a>下载最新版本的source包，解压后进入安装目录，配置makefile，编译，安装。<br><figure class="highlight vala"><table><tr><td class="code"><pre><span class="line"><span class="meta"># ./configure --prefix=/usr/local --enable-shared --with-ssl</span></span><br><span class="line"><span class="meta"># make</span></span><br><span class="line"><span class="meta"># make install</span></span><br></pre></td></tr></table></figure></p>
<p>–prefix=/usr/local –enable-shared 的意思是创建共享链接，以便其他软件编译时调用</p>
<p>–with-ssl 的意思是允许ssl，pip安装的时候会用到</p>
<h2 id="安装虚拟环境"><a href="#安装虚拟环境" class="headerlink" title="安装虚拟环境"></a>安装虚拟环境</h2><p>Python 虚拟环境用于将软件包安装与系统隔离开来。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">创建一个新的虚拟环境，方法是选择 Python 解释器并创建一个 ./venv 目录来存放它：</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> virtualenv --system-site-packages -p python3.7 ./venv</span></span><br><span class="line"></span><br><span class="line">使用特定于 shell 的命令激活该虚拟环境：</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> ./venv/bin/activate  <span class="comment"># sh, bash, ksh, or zsh</span></span></span><br><span class="line"></span><br><span class="line">当 virtualenv 处于有效状态时，shell 提示符带有 (venv) 前缀。</span><br><span class="line"></span><br><span class="line">在不影响主机系统设置的情况下，在虚拟环境中安装软件包。首先升级 pip：</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install --upgrade pip</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip list  <span class="comment"># show packages installed within the virtual environment</span></span></span><br><span class="line"></span><br><span class="line">之后要退出 virtualenv，请使用以下命令：</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> deactivate  <span class="comment"># don't exit until you're done using TensorFlow</span></span></span><br></pre></td></tr></table></figure>
<h2 id="编译mod-wsgi"><a href="#编译mod-wsgi" class="headerlink" title="编译mod_wsgi"></a>编译mod_wsgi</h2><p><a href="https://modwsgi.readthedocs.io/en/develop/user-guides/virtual-environments.html" target="_blank" rel="noopener">mod_wsgi官网</a></p>
<p>mod_wsgi是一个apache的模块，用来把python web和apache连接起来，说实话，不咋好用，一定要下载最新版本，老版本会有不少问题<br><a href="https://github.com/GrahamDumpleton/mod_wsgi/releases" target="_blank" rel="noopener">下载地址</a></p>
<p>解压后进入安装目录，配置makefile，编译，安装。</p>
<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">./configure --with-apxs=/usr/bin/apxs2 --with-<span class="keyword">python</span>=<span class="keyword">py3</span>.<span class="number">7</span>env/venv/bin/<span class="keyword">python3</span>.<span class="number">7</span></span><br><span class="line"><span class="keyword">make</span></span><br><span class="line"><span class="keyword">make</span> install</span><br></pre></td></tr></table></figure>
<p>apxs2没有的话以防万一就装一下，–with-python指的是我们想要链接的python目标</p>
<h2 id="配置Apache"><a href="#配置Apache" class="headerlink" title="配置Apache"></a>配置Apache</h2><p>在上面的配置基础上加两行<br><figure class="highlight crystal"><table><tr><td class="code"><pre><span class="line">WSGIScriptAlias / <span class="regexp">/home/user</span><span class="regexp">/WebService/</span>WebService/wsgi.py <span class="comment"># 在这行下加俩</span></span><br><span class="line">WSGIDaemonProcess yourdomain.com python-path=<span class="regexp">/home/user</span><span class="regexp">/WebService:/home</span><span class="regexp">/user/py</span>3.<span class="number">7</span>env/venv/<span class="class"><span class="keyword">lib</span>/<span class="title">python3</span>.7/<span class="title">site</span>-<span class="title">packages</span></span></span><br><span class="line">WSGIProcessGroup yourdomain.com</span><br></pre></td></tr></table></figure></p>
<p>WSGIDaemonProcess 你的域名 python-path=刚才用virtualenv创建的python包的路径</p>
<p>重启apache</p>
<p>$ service apache2 restart</p>
<p>看log我们的apache成功的链接到python3.7：</p>
<p>Apache/2.4.18 (Ubuntu) mod_wsgi/4.6.5 Python/3.7 configured – resuming normal operations</p>
<h2 id="如果出错"><a href="#如果出错" class="headerlink" title="如果出错"></a>如果出错</h2><p>看错误代码慢慢查，我研究了一天才成功，首先就是编译的时候configure后一定要带对参数，缺少的库也要全手动安装，还有靠一部分运气才能成功。</p>
]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title>Git学习笔记</title>
    <url>/2018/08/20/Git%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>Git是一个开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。<br>Git 是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。<br>Git 与常用的版本控制工具 CVS, Subversion 等不同，它采用了分布式版本库的方式，不必服务器端软件支持。<br>Git不同于SVN之前写过 <a href="http://t.cn/RnkXgR2" title="百度文库地址" target="_blank" rel="noopener">SVN使用手册</a> ，搭建过SVN服务，现在复习一下Git</p>
<h1 id="配置Git"><a href="#配置Git" class="headerlink" title="配置Git"></a>配置Git</h1><p>首先创建SSH key：</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">ssh-keygen</span> <span class="selector-tag">-t</span> <span class="selector-tag">rsa</span> <span class="selector-tag">-C</span> "<span class="selector-tag">email</span>@<span class="keyword">email</span>.<span class="keyword">com</span>"</span><br></pre></td></tr></table></figure>
<p>在~/目录下生成.ssh文件夹，打开id_rsa.pub，复制里面的key。<br>在github上，进入 Account Settings（账户配置），左边选择SSH Keys，Add SSH<br>Key,title随便填，粘贴在电脑上生成的key。<br>验证是否成功<br><figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">ssh</span> <span class="selector-tag">-T</span> <span class="selector-tag">git</span>@<span class="keyword">github</span>.<span class="keyword">com</span></span><br></pre></td></tr></table></figure></p>
<p>如果是第一次的会提示是否continue，输入yes就会看到：You’ve successfully authenticated, but GitHub does not provide shell access 。这就表示已成功连上github。<br>接下来我们要做的就是把本地仓库传到github上去，在此之前还需要设置username和email，因为github每次commit都会记录他们。<br><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">git<span class="built_in"> config </span>--global user.name <span class="string">"your name"</span></span><br><span class="line">git<span class="built_in"> config </span>--global user.eamil <span class="string">"your_eamail@email.com"</span></span><br></pre></td></tr></table></figure></p>
<p>上传远程仓库，需要添加远程地址，仓库需要在github上先建立好<br><figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line">git remote <span class="keyword">add</span><span class="bash"> origin git@github.com:yourName/yourRepo.git</span></span><br></pre></td></tr></table></figure></p>
<h1 id="检出仓库"><a href="#检出仓库" class="headerlink" title="检出仓库"></a>检出仓库</h1><p>克隆本地仓库：</p>
<pre><code>git clone /path/to/repository 
</code></pre><p>克隆远程仓库：</p>
<pre><code>git clone username@host:/path/to/repository
</code></pre><h1 id="推送流程"><a href="#推送流程" class="headerlink" title="推送流程"></a>推送流程</h1><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="variable">$mkdir</span> test  #创建一个测试目录</span><br><span class="line"><span class="variable">$cd</span> test/    #进入test目录</span><br><span class="line"><span class="variable">$echo</span> <span class="string">"#git test file"</span> &gt;&gt; README<span class="selector-class">.md</span> #给readme文件写入内容</span><br><span class="line"><span class="variable">$ls</span></span><br><span class="line">README.md</span><br><span class="line"><span class="variable">$git</span> init   #初始化init</span><br><span class="line"><span class="variable">$git</span> add README<span class="selector-class">.md</span>  #添加文件</span><br><span class="line">[master (root-commit) <span class="number">0205</span>aab] 添加 README<span class="selector-class">.md</span> 文件</span><br><span class="line"> <span class="number">1</span> file changed, <span class="number">1</span> insertion(+)</span><br><span class="line"> create mode <span class="number">100644</span> README.md</span><br><span class="line"><span class="variable">$git</span> commit -m <span class="string">"add readme.md file"</span>     #提交备注信息</span><br><span class="line"></span><br><span class="line">#提交到github</span><br><span class="line">$ git remote add origin git@github<span class="selector-class">.com</span>:usename/Repositoryname.git</span><br><span class="line">$ git push -u origin master</span><br></pre></td></tr></table></figure>
<h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">#初始化git</span></span><br><span class="line">$git init</span><br><span class="line"></span><br><span class="line"><span class="comment">#拷贝一个仓库到本地</span></span><br><span class="line">$git clone [url]</span><br><span class="line"></span><br><span class="line"><span class="comment">#添加文件到缓存</span></span><br><span class="line">$git add</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看当前项目状态</span></span><br><span class="line">$git status -s</span><br><span class="line"><span class="comment">#A 加入缓存 M有改动 </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看修改</span></span><br><span class="line">执行 git diff 来查看执行 git status 的结果的详细信息。</span><br><span class="line">git diff 命令显示已写入缓存与已修改但尚未写入缓存的改动的区别。git diff 有两个主要的应用场景。</span><br><span class="line">尚未缓存的改动：git diff</span><br><span class="line">查看已缓存的改动： git diff <span class="comment">--cached</span></span><br><span class="line">查看已缓存的与未缓存的所有改动：git diff HEAD</span><br><span class="line">显示摘要而非整个 diff：git diff <span class="comment">--stat</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#添加到仓库</span></span><br><span class="line">$git <span class="keyword">commit</span> -m <span class="string">"描述"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#取消缓存内容</span></span><br><span class="line">$git <span class="keyword">reset</span> <span class="keyword">HEAD</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#删除文件</span></span><br><span class="line">如果只是简单地从工作目录中手工删除文件，运行 git <span class="keyword">status</span> 时就会在 Changes <span class="keyword">not</span> staged <span class="keyword">for</span> <span class="keyword">commit</span> 的提示。</span><br><span class="line">要从 Git 中移除某个文件，就必须要从已跟踪文件清</span><br><span class="line">单中移除，然后提交。可以用以下命令完成此项工作</span><br><span class="line"></span><br><span class="line">$git rm &lt;<span class="keyword">file</span>&gt;</span><br><span class="line"></span><br><span class="line">如果删除之前修改过并且已经放到暂存区域的话，则必须要用强制删除选项 -f</span><br><span class="line"></span><br><span class="line">$git rm -f &lt;<span class="keyword">file</span>&gt;</span><br><span class="line"></span><br><span class="line">如果把文件从暂存区域移除，但仍然希望保留在当前工作目录中，换句话说，仅是从跟踪清单中删除，使用 <span class="comment">--cached 选项即可</span></span><br><span class="line"></span><br><span class="line">$git rm <span class="comment">--cached &lt;file&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#重命名</span></span><br><span class="line">$git mv oldname newname</span><br><span class="line"></span><br><span class="line"><span class="comment">#push到远程分支</span></span><br><span class="line">git push origin 本地分支名字:远程分支名</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看所有分支</span></span><br><span class="line">git branch -a</span><br><span class="line"></span><br><span class="line">    * <span class="keyword">master</span></span><br><span class="line">      remotes/origin/<span class="keyword">HEAD</span> -&gt; origin/<span class="keyword">master</span></span><br><span class="line">      </span><br><span class="line"><span class="comment">#创建并切换分支 </span></span><br><span class="line">git checkout -b 本地分支名 origin/远程分支名</span><br></pre></td></tr></table></figure>
<p>查看当前远程仓库：<br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">$ git remote</span><br><span class="line">origin</span><br><span class="line">$ git remote -v</span><br><span class="line">origin  git@github<span class="selector-class">.com</span>:Voidmort/blogs<span class="selector-class">.git</span> (fetch)</span><br><span class="line">origin  git@github<span class="selector-class">.com</span>:Voidmort/blogs<span class="selector-class">.git</span> (push)</span><br></pre></td></tr></table></figure></p>
<p>Git 有两个命令用来提取远程仓库的更新。<br>1、从远程仓库下载新分支与数据：</p>
<pre><code>git fetch
</code></pre><p>该命令执行完后需要执行git merge 远程分支到你所在的分支。<br>2、从远端仓库提取数据并尝试合并到当前分支：</p>
<pre><code>git merge
</code></pre><p>该命令就是在执行 git fetch 之后紧接着执行 git merge 远程分支到你所在的任意分支。</p>
<h1 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h1><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">$ git remote -v</span><br><span class="line">origin  git@github<span class="selector-class">.com</span>:Voidmort/blogs<span class="selector-class">.git</span> (fetch)</span><br><span class="line">origin  git@github<span class="selector-class">.com</span>:Voidmort/blogs<span class="selector-class">.git</span> (push)</span><br><span class="line"></span><br><span class="line">#添加仓库<span class="number">2</span></span><br><span class="line">$ git remote add origin2 git@github<span class="selector-class">.com</span>:Voidmort/blogs.git</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ git remote -v</span><br><span class="line">origin  git@github<span class="selector-class">.com</span>:Voidmort/blogs<span class="selector-class">.git</span> (fetch)</span><br><span class="line">origin  git@github<span class="selector-class">.com</span>:Voidmort/blogs<span class="selector-class">.git</span> (push)</span><br><span class="line">origin2 git@github<span class="selector-class">.com</span>:Voidmort/blogs<span class="selector-class">.git</span> (fetch)</span><br><span class="line">origin2 git@github<span class="selector-class">.com</span>:Voidmort/blogs<span class="selector-class">.git</span> (push)</span><br><span class="line"></span><br><span class="line">#删除仓库<span class="number">2</span></span><br><span class="line">$ git remote rm origin2</span><br><span class="line"></span><br><span class="line">$ git remote -v</span><br><span class="line">origin  git@github<span class="selector-class">.com</span>:Voidmort/blogs<span class="selector-class">.git</span> (fetch)</span><br><span class="line">origin  git@github<span class="selector-class">.com</span>:Voidmort/blogs<span class="selector-class">.git</span> (push)</span><br></pre></td></tr></table></figure>
<p>git在终端不能识别中文</p>
<figure class="highlight taggerscript"><table><tr><td class="code"><pre><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line">Your branch is up to date with 'origin/master'.</span><br><span class="line"></span><br><span class="line">Changes not staged for commit:</span><br><span class="line">  (use "git add &lt;file&gt;..." to update what will be committed)</span><br><span class="line">  (use "git checkout -- &lt;file&gt;..." to discard changes in working directory)</span><br><span class="line"></span><br><span class="line">        modified:   "<span class="symbol">\3</span>46<span class="symbol">\2</span>34<span class="symbol">\2</span>72<span class="symbol">\3</span>45<span class="symbol">\2</span>31<span class="symbol">\2</span>50<span class="symbol">\3</span>45<span class="symbol">\2</span>55<span class="symbol">\2</span>46<span class="symbol">\3</span>44<span class="symbol">\2</span>71<span class="symbol">\2</span>40<span class="symbol">\3</span>45<span class="symbol">\2</span>56<span class="symbol">\2</span>36<span class="symbol">\3</span>46<span class="symbol">\2</span>10<span class="symbol">\2</span>30<span class="symbol">\3</span>57<span class="symbol">\2</span>74<span class="symbol">\2</span>10<span class="symbol">\3</span>45<span class="symbol">\2</span>15<span class="symbol">\2</span>01<span class="symbol">\3</span>57<span class="symbol">\2</span>74<span class="symbol">\2</span>11.ipynb"</span><br></pre></td></tr></table></figure>
<p>core.quotepath设为false的话，就不会对0x80以上的字符进行quote。中文显示正常。</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">git<span class="built_in"> config </span>--global core.quotepath <span class="literal">false</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Django</title>
    <url>/2020/02/05/Django/</url>
    <content><![CDATA[<p>Python下有许多款不同的 Web 框架。Django是重量级选手中最有代表性的一位。许多成功的网站和APP都基于Django。</p>
<p>Django是一个开放源代码的Web应用框架，由Python写成。</p>
<p>Django采用了MVC的软件设计模式，即模型M，视图V和控制器C。</p>
<h1 id="Django的安装"><a href="#Django的安装" class="headerlink" title="Django的安装"></a>Django的安装</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install Django</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> django</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>django.VERSION</span><br><span class="line">(<span class="number">2</span>, <span class="number">1</span>, <span class="number">7</span>, <span class="string">'final'</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h1 id="创建工程"><a href="#创建工程" class="headerlink" title="创建工程"></a>创建工程</h1><p>安装完django后我们会有一个 <code>django-admin</code>的管理工具，我们使用此工具来创建工程。</p>
<figure class="highlight gherkin"><table><tr><td class="code"><pre><span class="line">django-admin startproject projectname</span><br><span class="line">$ cd projectname/</span><br><span class="line">$ tree</span><br><span class="line">.</span><br><span class="line">|<span class="string">-- projectname</span></span><br><span class="line">|<span class="string">   </span>|<span class="string">-- __init__.py</span></span><br><span class="line">|<span class="string">   </span>|<span class="string">-- settings.py</span></span><br><span class="line">|<span class="string">   </span>|<span class="string">-- urls.py</span></span><br><span class="line">|<span class="string">   `-- wsgi.py</span></span><br><span class="line"><span class="string">`-- manage.py</span></span><br></pre></td></tr></table></figure>
<p>目录说明：</p>
<pre><code>projectname: 项目的容器。
manage.py: 一个实用的命令行工具，可让你以各种方式与该 Django 项目进行交互。
__init__.py: 一个空文件，告诉 Python 该目录是一个 Python 包。
settings.py: 该 Django 项目的设置/配置。
urls.py: 该 Django 项目的 URL 声明; 一份由 Django 驱动的网站&quot;目录&quot;。
wsgi.py: 一个 WSGI 兼容的 Web 服务器的入口，以便运行你的项目。
</code></pre><p>接下来我们进入 projectname 目录输入以下命令，启动服务器：</p>
<p><code>python manage.py runserver 0.0.0.0:8000</code></p>
<p>0.0.0.0 让其它电脑可连接到开发服务器，8000 为端口号。如果不说明，那么端口号默认为 8000。</p>
<p>在浏览器输入你服务器的 ip（这里我们输入本机 IP 地址： 127.0.0.1:8000） 及端口号，如果正常启动，会打开django默认页面。</p>
<h1 id="创建-APP"><a href="#创建-APP" class="headerlink" title="创建 APP"></a>创建 APP</h1><p>python manage.py startapp 应用名</p>
<p>新建一个应用文件appname，它的里面也创建了一些py文件和包：</p>
<figure class="highlight gherkin"><table><tr><td class="code"><pre><span class="line">python manage.py startapp appname</span><br><span class="line">.</span><br><span class="line">|<span class="string">-- appname</span></span><br><span class="line">|<span class="string">   </span>|<span class="string">-- __init__.py</span></span><br><span class="line">|<span class="string">   </span>|<span class="string">-- admin.py</span></span><br><span class="line">|<span class="string">   </span>|<span class="string">-- apps.py</span></span><br><span class="line">|<span class="string">   </span>|<span class="string">-- migrations</span></span><br><span class="line">|<span class="string">   </span>|<span class="string">-- models.py</span></span><br><span class="line">|<span class="string">   </span>|<span class="string">-- test.py</span></span><br><span class="line">|<span class="string">   </span>|<span class="string">-- views.py</span></span><br><span class="line">|<span class="string">   </span>|<span class="string">-- urls.py</span></span><br></pre></td></tr></table></figure>
<p>下面介绍这些文件都是什么：</p>
<pre><code>admin.py：管理站点模型的声明文件，默认为空。
apps.py：应用信息定义文件。在其中生成了类Appconfig，类用于定义应用名等Meta数据。
migrations: 用于在之后定义引用迁移功能。
models.py: 添加模型层数据类的文件。
test.py：测试代码文件。
views.py：定义URL响应函数。
urls.py：需要自己创建，作为子路由。
</code></pre><p>创建好app后在<code>settings.py</code>中的<code>INSTALLED_APPS</code>添加<code>appname</code></p>
<figure class="highlight sml"><table><tr><td class="code"><pre><span class="line"><span class="type">INSTALLED_APPS</span> = [</span><br><span class="line">    <span class="symbol">'django</span>.contrib.admin',</span><br><span class="line">    <span class="symbol">'django</span>.contrib.auth',</span><br><span class="line">    <span class="symbol">'django</span>.contrib.contenttypes',</span><br><span class="line">    <span class="symbol">'django</span>.contrib.sessions',</span><br><span class="line">    <span class="symbol">'django</span>.contrib.messages',</span><br><span class="line">    <span class="symbol">'django</span>.contrib.staticfiles',</span><br><span class="line">    <span class="symbol">'appname'</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h1 id="网页和静态文件"><a href="#网页和静态文件" class="headerlink" title="网页和静态文件"></a>网页和静态文件</h1><p>在<code>appname</code>中新建一个<code>templates</code>文件夹存放html</p>
<p>新建<code>static</code>文件夹存放js，image，css等</p>
<p>并且在<code>settings.py</code>中添加静态文件目录<br><figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line">STATIC_URL = '/static/'</span><br><span class="line">STATICFILES_DIRS = [</span><br><span class="line">    os.path.join(BASE_DIR, <span class="string">"static"</span>),</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p>
<p>注意：每一个app中的static访问路径都是相同的。</p>
<p>在html中的使用：</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line">首先创建了一个css文件</span><br><span class="line"></span><br><span class="line">appname/static/js/style.css</span><br><span class="line"></span><br><span class="line">如何使用，appname/templates/index.html里添加下面代码：</span><br><span class="line"></span><br><span class="line">&#123;% load staticfiles %&#125;</span><br><span class="line">    <span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">type</span>=<span class="string">"text/css"</span> <span class="attr">href</span>=<span class="string">"&#123;% static 'js/style.css' %&#125;"</span> /&gt;</span></span><br></pre></td></tr></table></figure>
<h1 id="添加脚本目录"><a href="#添加脚本目录" class="headerlink" title="添加脚本目录"></a>添加脚本目录</h1><p>我们有一些脚本写在另一个文件夹，我们希望引入它们，在<code>settings.py</code>中添加<br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">import sys</span><br><span class="line">sys<span class="selector-class">.path</span><span class="selector-class">.insert</span>(<span class="number">0</span>, os<span class="selector-class">.path</span><span class="selector-class">.join</span>(BASE_DIR, <span class="string">'/home/Scripts'</span>))</span><br></pre></td></tr></table></figure></p>
<h1 id="数据库相关"><a href="#数据库相关" class="headerlink" title="数据库相关"></a>数据库相关</h1><h1 id="部署静态文件"><a href="#部署静态文件" class="headerlink" title="部署静态文件"></a>部署静态文件</h1><h2 id="收集静态文件"><a href="#收集静态文件" class="headerlink" title="收集静态文件"></a>收集静态文件</h2><p>python manage.py collectstatic</p>
<p>这一句话就会把以前放在app下static中的静态文件全部拷贝到 settings.py 中设置的 STATIC_ROOT 文件夹中</p>
<h2 id="用UWSGI部署"><a href="#用UWSGI部署" class="headerlink" title="用UWSGI部署"></a>用UWSGI部署</h2><p>uWSGI是一个Web服务器，它实现了WSGI协议、uwsgi、http等协议。Nginx中HttpUwsgiModule的作用是与uWSGI服务器进行交换。</p>
<p>要注意 WSGI / uwsgi / uWSGI 这三个概念的区分。</p>
<p>WSGI是一种通信协议。</p>
<p>uwsgi是一种线路协议而不是通信协议，在此常用于在uWSGI服务器与其他网络服务器的数据通信。</p>
<p>而uWSGI是实现了uwsgi和WSGI两种协议的Web服务器。</p>
<p>uwsgi协议是一个uWSGI服务器自有的协议，它用于定义传输信息的类型（type of information），每一个uwsgi packet前4byte为传输信息类型描述，它与WSGI相比是两样东西。</p>
<p>uwsgi安装命令</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sudo pip <span class="keyword">install</span> uwsgi <span class="comment">--upgrade</span></span><br></pre></td></tr></table></figure>
<p>uwsgi.ini配置文件编写</p>
<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="section">[uwsgi]</span></span><br><span class="line"><span class="attr">http-socket</span> = <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">80</span>  # IP地址与端口号</span><br><span class="line"><span class="attr">chdir</span> = /home/user/myProject/  # Django工程的目录</span><br><span class="line"><span class="attr">virtualenv</span> = /home/user/pyvenv/env3.<span class="number">7</span> # python虚拟环境目录</span><br><span class="line"><span class="attr">wsgi-file</span> = myProject/wsgi.py # wsgi.py的位置，接在chdir后面</span><br><span class="line"><span class="attr">static-map</span> = /static=/home/user/myProject/static # 静态文件的位置</span><br><span class="line"><span class="attr">enable-threads</span> = <span class="literal">true</span>   # 允许在django创建线程</span><br><span class="line"><span class="attr">auto-procname</span> = <span class="literal">true</span>    # 自动给进程起名字</span><br><span class="line"><span class="attr">daemonize</span> = /home/user/log/uwsgi.log    # log地址</span><br><span class="line"><span class="attr">pidfile</span> = /home/user/myProject/uwsgi.pid    # 保存主进程pid的文件</span><br><span class="line"><span class="attr">disable-logging</span> = <span class="literal">true</span>  # log只记录错误信息</span><br><span class="line"><span class="attr">buffer-size</span> = <span class="number">51200</span> # 允许传输的字节数</span><br><span class="line"><span class="attr">processes</span> = <span class="number">4</span>   # 进程数</span><br><span class="line"><span class="attr">threads</span> = <span class="number">2</span> # 线程数</span><br><span class="line"><span class="attr">vacuum</span> = <span class="literal">true</span>   # 允许主进程</span><br></pre></td></tr></table></figure>
<p>关闭只能关闭pidfile里记录的pid号，不能kill其它三个进程</p>
<figure class="highlight dsconfig"><table><tr><td class="code"><pre><span class="line"><span class="string">uwsgi </span><span class="built_in">--ini</span> <span class="string">uwsgi.</span><span class="string">ini </span>  <span class="comment"># 启动</span></span><br><span class="line"><span class="string">uwsgi </span><span class="built_in">--reload</span> <span class="string">uwsgi.</span><span class="string">pid </span>   <span class="comment"># 重启</span></span><br><span class="line"><span class="string">uwsgi </span><span class="built_in">--stop</span> <span class="string">uwsgi.</span><span class="string">pid </span> <span class="comment"># 关闭</span></span><br></pre></td></tr></table></figure>
<h2 id="部署到服务器"><a href="#部署到服务器" class="headerlink" title="部署到服务器"></a>部署到服务器</h2><p>用 apache2 或 nginx 示例代码</p>
<p>apache2配置文件<br><figure class="highlight dts"><table><tr><td class="code"><pre><span class="line">Alias <span class="meta-keyword">/static/</span> <span class="meta-keyword">/path/</span>to/collected_static/</span><br><span class="line"> </span><br><span class="line"><span class="params">&lt;Directory /path/to/collected_static&gt;</span></span><br><span class="line">    Require all granted</span><br><span class="line"><span class="params">&lt;/Directory&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>nginx 示例代码：<br><figure class="highlight puppet"><table><tr><td class="code"><pre><span class="line">location /<span class="keyword">media</span>  &#123;</span><br><span class="line">    <span class="literal">alias</span> /<span class="built_in">path</span>/to/<span class="literal">project</span>/media;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">location</span> /<span class="keyword">static</span> &#123;</span><br><span class="line">    <span class="literal">alias</span> /<span class="built_in">path</span>/to/<span class="literal">project</span>/collected_static;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title>DragonBoard-410C开发环境搭建</title>
    <url>/2018/04/19/DragonBoard-410C%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="初识DragonBoard-410c"><a href="#初识DragonBoard-410c" class="headerlink" title="初识DragonBoard 410c"></a>初识DragonBoard 410c</h1><p>DragonBoard 410c是一款搭载Qualcomm® Snapdragon™ 410（64位的四核处理器）的开发板，它功能齐全，具有强大的处理能力，内置 8GB eMMC (支持标准microSD卡槽)，并且还内置wifi、蓝牙、和GPS模块，具有HDMI 输出及USB 接口 (3个)。所有这些集成到只有信用卡大小的一块板子上，售价仅为75美元。所有的这一切使得DragonBoard 410c成为嵌入式计算以及物联网（IoT）产品的理想选择，例如下一代的机器人，摄像头，医疗设备，自动售货机，智能建筑，数字标牌，赌场游戏机等等。<br>DragonBoard 410c<br>　　DragonBoard 410c 兼容96Boards消费版（CE）规范，该规范由Linaro社区委员会组织维护，定位移动、嵌入及数字家庭领域。<br>　　DragonBoard 410c目前已经可以运行Android5.1、Ubuntu以及Windows 10 IoT Core等系统，并且是首批取得微软认证的设备之一，认证后可支持Azure IoT SDK，可随时用于物联网应用。</p>
<p><img src="1.png" alt=""># 开发环境的搭建<br>本次开发环境的搭建都是在windows下完成</p>
<h2 id="刷为Linux系统"><a href="#刷为Linux系统" class="headerlink" title="刷为Linux系统"></a>刷为Linux系统</h2><p>410C开发板自带安卓系统，通过HDMI连接显示器即可显示，通过USB连接键盘鼠标进行操作。<br>此次开发环境要求是Linux，所以要重刷系统，官方提供的是debian深度修改的系统，高通起名叫linaro。Linux内核为4.140.</p>
<p><img src="2.png" alt=""><br><code>&gt;&gt;&gt;</code><a href="https://releases.linaro.org/96boards/dragonboard410c/linaro/debian" target="_blank" rel="noopener">镜像下载连接点这里</a><br><code>&gt;&gt;&gt;</code><a href="http://sourceforge.net/projects/win32diskimager/?source=typ_redirect" target="_blank" rel="noopener">Win32DiskImager卡刷工具点这里</a></p>
<p>这里我选择了最新的18.01的卡刷<a href="https://publishing-ie-linaro-org.s3.amazonaws.com/releases/96boards/dragonboard410c/linaro/debian/18.01/dragonboard-410c-sdcard-installer-buster-359.zip?Signature=XHu0W3gjMDOsrZqTJ407ixon0SU%3D&amp;Expires=1524017445&amp;AWSAccessKeyId=AKIAIELXV2RYNAHFUP7A" target="_blank" rel="noopener">img</a></p>
<p>写入Linux镜像到SD卡操作步骤如下：<br><figure class="highlight arduino"><table><tr><td class="code"><pre><span class="line">● 下载Win32DiskImager和卡刷镜像</span><br><span class="line">● 打开Disklmager工具</span><br><span class="line">● 选择镜像文件路径</span><br><span class="line">● 选择电脑映射的<span class="built_in">SD</span>卡盘符</span><br><span class="line">● 点击 Write 把镜像写入<span class="built_in">SD</span>卡</span><br></pre></td></tr></table></figure></p>
<p><img src="3.png" alt=""><br>使开发板从SD卡启动：<br><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">● 在开发板上插入写好镜像的sd卡</span><br><span class="line">● 一个鼠标和键盘连接到<span class="number">410</span>C上</span><br><span class="line">● 显示器通过HDMI连接到<span class="number">410</span>C上</span><br><span class="line">● 设置启动开关S6 - <span class="number">0100</span>(从sd卡启动)</span><br><span class="line">● 接入电源</span><br><span class="line">● 开发板应该会启动并显示一个对话框,选择要安装的操作系统</span><br><span class="line">● 选择显示的操作系统(Linux Linaro)并点击“Install”。</span><br><span class="line">● 如果一切都成功进入下一步</span><br></pre></td></tr></table></figure></p>
<p><img src="4.png" alt=""><br>重启开发板：<br><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">● 拔掉电源线</span><br><span class="line">● 拆下sd卡</span><br><span class="line">● 复位启动开关调到<span class="number">0000</span></span><br><span class="line">● 重启后应该会引导进入新的系统</span><br></pre></td></tr></table></figure></p>
<h2 id="通过Visual-Studio编译调试410C的程序"><a href="#通过Visual-Studio编译调试410C的程序" class="headerlink" title="通过Visual Studio编译调试410C的程序"></a>通过Visual Studio编译调试410C的程序</h2><p>个人更喜欢使用VS而不是Eclipse，好在这款开发板支持使用VS2013Pro交叉编译，首先安装VS2013或2012，不支持更高版本。</p>
<figure class="highlight dns"><table><tr><td class="code"><pre><span class="line">VS2013旗舰版/专业版/高级版产品密钥</span><br><span class="line">Visual Studio Ultimate <span class="number">2013</span> <span class="keyword">KEY</span>（VS2013旗舰版密钥）：BWG7X-J98B3-W34RT-<span class="number">33</span>B3R-JVYW9</span><br><span class="line"></span><br><span class="line">Visual Studio Premium <span class="number">2013</span> <span class="keyword">KEY</span>（VS2013高级版密钥）：FBJVC-<span class="number">3</span>CMTX-D8DVP-RTQCT-<span class="number">92494</span></span><br><span class="line"></span><br><span class="line">Visual Studio Professional <span class="number">2013</span> <span class="keyword">KEY</span>（VS2013专业版密钥）： XDM3T-W3T3V-MGJWK-<span class="number">8</span>BFVD-GVPKY</span><br><span class="line"></span><br><span class="line">Team Foundation Server <span class="number">2013</span> <span class="keyword">KEY</span>（密钥）：MHG9J-HHHX9-WWPQP-D8T7H-<span class="number">7</span>KCQG</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">VS2013官方中文专业版（Visual Studio Professional <span class="number">2013</span>）安装激活方法</span><br><span class="line"><span class="number">1</span>、下载后得到的是ISO文件，直接解压缩或用虚拟光驱加载运行都可以</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>、无所不藏在这里直接解压，然后双击“vs_ultimate.exe”开始安装；</span><br><span class="line"></span><br><span class="line"><span class="number">3</span>、设置好安装路径后，点击“我同意许可条款和条件”点击“下一步”继续；</span><br><span class="line"></span><br><span class="line"><span class="number">4</span>、选择您要安装的Visual Studio <span class="number">2013</span>选项，根据自身需要勾选安装；</span><br><span class="line"></span><br><span class="line"><span class="number">5</span>、接下来就是有点漫长的安装过程，这时候就是拼电脑配置的时候了；</span><br><span class="line"></span><br><span class="line"><span class="number">6</span>、成功安装后打开软件，设置好熟悉的环境启动（包括vb、vc、vf等多个开发环境）</span><br><span class="line"></span><br><span class="line"><span class="number">7</span>、第一次运行软件会有点慢，再点击“帮助”–“注册软件”–可以看到软件有<span class="number">30</span>天试用期，点击“更改我的产品许可证”；</span><br><span class="line"></span><br><span class="line"><span class="number">10</span>、输入visual studio <span class="number">2013</span>专业版密钥【XDM3T-W3T3V-MGJWK-<span class="number">8</span>BFVD-GVPKY】</span><br><span class="line"></span><br><span class="line"><span class="number">11</span>、到这步就已成功激活visual studio <span class="number">2013</span>专业版了，现在您可以无限制免费使用VS2013。</span><br><span class="line"></span><br><span class="line">VS2013官方中文专业版（Visual Studio Professional <span class="number">2013</span>）下载地址</span><br><span class="line">Visual Studio Professional <span class="number">2013</span> with Update <span class="number">5</span> (x86) 官方专业版下载地址：</span><br><span class="line">ed2k://|file|cn_visual_studio_professional_2013_with_update_5_x86_dvd_<span class="number">6815749</span>.iso|<span class="number">5517246464</span>|DEA9BB85B73F6A1F23E638DFE06CEF07|/</span><br></pre></td></tr></table></figure>
<p>安装好visual studio后安装<a href="https://developer.qualcomm.com/qfile/28678/snapdragondebuggerforvsinstaller.zip" target="_blank" rel="noopener">snapdragondebuggerforvsinstaller</a>，解压后安装即可。</p>
<p>什么是 Snapdragon Debugger for Visual Studio？</p>
<p>Snapdragon Debugger for Visual Studio 是微软 Visual Studio IDE 的一款插件工具，针对目前搭载骁龙处理器的设备，可调试各种 API。</p>
<p>目前这款工具可用于在 Microsoft Visual Studio 环境中创建并调试 Android NDK应用。仅可调试原生 C/C++ 代码，不支持调试 Java 代码。</p>
<p>关于更多请看<a href="https://developer.qualcomm.com/software/snapdragon-debugger-visual-studio/faq" target="_blank" rel="noopener">Snapdragon Debugger for Visual Studio 快速入门指南</a></p>
<p>这个插件安装需要许多东西，SDK,NDK，而我只是需要编译C，不想装太多不用的东西，但是又想用visual studio，可以用VS来远程调试Linux程序</p>
<h1 id="用VS2015开发Linux程序"><a href="#用VS2015开发Linux程序" class="headerlink" title="用VS2015开发Linux程序"></a>用VS2015开发Linux程序</h1><p>vs2017自带Linux开发环境，可惜我只装了2015，所以尝试用2015来开发Linux<br>需要一个插件：</p>
<p><a href="https://marketplace.visualstudio.com/items?itemName=VisualCppDevLabs.VisualCforLinuxDevelopment" target="_blank" rel="noopener">Visual C++ for Linux Development(VC_Linux.exe) </a></p>
<p>通过远程SSH协议既可以调试了</p>
<p>因为中美贸易战，高通可能不给中国供货了，所以公司改用NXP的芯片，这个项目就搁浅了，话说NXP不也被高通收购了吗</p>
<h1 id="发现问题："><a href="#发现问题：" class="headerlink" title="发现问题："></a>发现问题：</h1><p>我选择的410C刷的img是debian-283，有个问题，WiFi连接后会时不时自己断开但是桌面右下角的wifi图标显示正常，查看官网更新日志，在359已经修复这个问题，但是因为中美贸易战的原因无法从高通官网下载镜像了，所以没有验证是否还存在次BUG。</p>
]]></content>
      <categories>
        <category>嵌入式开发</category>
      </categories>
      <tags>
        <tag>嵌入式</tag>
        <tag>410C</tag>
      </tags>
  </entry>
  <entry>
    <title>Jetson Nano学习笔记</title>
    <url>/2021/04/30/Jetson%20Nano%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>JetsonNano是Nvidia的一个嵌入式开发板，具备Maxwell128核心的GPU和4核心 ARM A57的CPU，可运行Ubuntu(Linux for Tegra，L4T)，浮点运算能力为472GFLOPS(FP16)，官方给出的功率为10W。<br>JetsonNano可以用来搞些机器学习相关内容。</p>
<ol>
<li>JetsonNano平台只支持Python3.6的TensorFlow。</li>
<li>JetsonNano镜像版本：JetPack4.4</li>
<li>TensroFlow API版本：r2.1.0</li>
</ol>
<p>以上信息为当前时间更新。</p>
<h1 id="关于Jetson-Nano-Developer-Kit"><a href="#关于Jetson-Nano-Developer-Kit" class="headerlink" title="关于Jetson Nano Developer Kit"></a>关于Jetson Nano Developer Kit</h1><table>
<thead>
<tr>
<th>硬件</th>
<th>配置</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPU</td>
<td>NVIDIA Maxwell™ 架构，配备 128 个 NVIDIA CUDA® 核心</td>
</tr>
<tr>
<td>CPU</td>
<td>四核 ARM® Cortex®-A57 MPCore 处理器</td>
</tr>
<tr>
<td>内存</td>
<td>4 GB 64 位 LPDDR4</td>
</tr>
<tr>
<td>存储</td>
<td>Micro SD 卡卡槽（需要另购16G以上SD卡接入）</td>
</tr>
</tbody>
</table>
<p><img src="https://www.waveshare.net/photo/development-board/Jetson-Nano-Developer-Kit/Jetson-Nano-Developer-Kit-intro1.jpg" alt="image"></p>
<ol>
<li>Micro SD 卡卡槽: 可接入TF卡（16G以上），烧写系统镜像</li>
<li>40PIN GPIO扩展接口（兼容树莓派40PIN接口）</li>
<li>Micro  USB接口：用于5V电源输入或者USB数据传输</li>
<li>千兆以太网口: 10/100/1000Base-T 自适应以太网端口</li>
<li>USB3.0接口：4个USB3.0接口</li>
<li>HDMI高清接口：用于外接HDMI屏幕</li>
<li>DisplayPort接口：用于外接DP屏幕</li>
<li>DC电源接口：用于外接5V电源（外径5.5， 内径2.1）</li>
<li>MIPS CSI 摄像头接口：兼容树莓派摄像头接口</li>
</ol>
<h1 id="烧写镜像"><a href="#烧写镜像" class="headerlink" title="烧写镜像"></a>烧写镜像</h1><p><a href="https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit" target="_blank" rel="noopener">官方教程</a></p>
<p>推荐使用<a href="https://www.balena.io/etcher/" target="_blank" rel="noopener">Etcher</a>烧写</p>
<p>镜像很大，有14G，烧写很慢，但安装系统很快。</p>
<p>系统版本为 Ubuntu 18.04.4 LTS (GNU/Linux 4.9.140-tegra aarch64)</p>
<h1 id="安装tensorflow"><a href="#安装tensorflow" class="headerlink" title="安装tensorflow"></a>安装tensorflow</h1><p>安装方法为官方给出</p>
<p><a href="https://forums.developer.nvidia.com/t/official-tensorflow-for-jetson-nano/71770" target="_blank" rel="noopener">link</a></p>
<p>安装版本为 <strong>Python 3.6 + JetPack4.4</strong></p>
<figure class="highlight q"><table><tr><td class="code"><pre><span class="line">$ sudo apt-<span class="built_in">get</span> <span class="keyword">update</span></span><br><span class="line"></span><br><span class="line">$ sudo apt-<span class="built_in">get</span> install libhdf5-serial-<span class="built_in">dev</span> hdf5-tools libhdf5-<span class="built_in">dev</span> zlib1g-<span class="built_in">dev</span> zip libjpeg8-<span class="built_in">dev</span> liblapack-<span class="built_in">dev</span> libblas-<span class="built_in">dev</span> gfortran</span><br><span class="line"></span><br><span class="line">$ sudo apt-<span class="built_in">get</span> install python3-pip</span><br><span class="line"></span><br><span class="line">$ sudo pip3 install -U pip</span><br></pre></td></tr></table></figure>
<p>安装h5py会出错，尝试重启单独安装，这个有点玄学，我重刷了两次，单独安装才安装成功</p>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">$ sudo pip3 install -U pip testresources setuptools numpy==<span class="number">1.16</span><span class="number">.1</span> future==<span class="number">0.17</span><span class="number">.1</span> mock==<span class="number">3.0</span><span class="number">.5</span> h5py==<span class="number">2.9</span><span class="number">.0</span> keras_preprocessing==<span class="number">1.0</span><span class="number">.5</span> keras_applications==<span class="number">1.0</span><span class="number">.8</span> gast==<span class="number">0.2</span><span class="number">.2</span> futures protobuf pybind11</span><br></pre></td></tr></table></figure>
<p>接下来是漫长的等待</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line"><span class="comment"># TF-2.x</span></span><br><span class="line">$ sudo pip3 install --pre --extra-index-url https:<span class="regexp">//</span>developer.download.nvidia.com<span class="regexp">/compute/</span>redist<span class="regexp">/jp/</span>v44 tensorflow==<span class="number">2.2</span>.<span class="number">0</span>+nv20.<span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># TF-1.15</span></span><br><span class="line">$ sudo pip3 install --pre --extra-index-url https:<span class="regexp">//</span>developer.download.nvidia.com<span class="regexp">/compute/</span>redist<span class="regexp">/jp/</span>v44 ‘tensorflow&lt;<span class="number">2</span>’</span><br></pre></td></tr></table></figure>
<figure class="highlight x86asm"><table><tr><td class="code"><pre><span class="line">Successfully installed absl-py-<span class="number">0.9</span><span class="meta">.0</span> astunparse-<span class="number">1.6</span><span class="meta">.3</span> cachetools-<span class="number">4.1</span><span class="meta">.1</span> gast-<span class="number">0.3</span><span class="meta">.3</span> google-auth-<span class="number">1.19</span><span class="meta">.1</span> google-auth-oauthlib-<span class="number">0.4</span><span class="meta">.1</span> google-pasta-<span class="number">0.2</span><span class="meta">.0</span> grpcio-<span class="number">1.30</span><span class="meta">.0</span> h5py-<span class="number">2.10</span><span class="meta">.0</span> importlib-metadata-<span class="number">1.7</span><span class="meta">.0</span> keras-preprocessing-<span class="number">1.1</span><span class="meta">.2</span> markdown-<span class="number">3.2</span><span class="meta">.2</span> oauthlib-<span class="number">3.1</span><span class="meta">.0</span> opt-einsum-<span class="number">3.2</span><span class="meta">.1</span> pyasn1-<span class="number">0.4</span><span class="meta">.8</span> pyasn1-modules-<span class="number">0.2</span><span class="meta">.8</span> requests-<span class="number">2.24</span><span class="meta">.0</span> requests-oauthlib-<span class="number">1.3</span><span class="meta">.0</span> rsa-<span class="number">4.6</span> scipy-<span class="number">1.4</span><span class="meta">.1</span> six-<span class="number">1.15</span><span class="meta">.0</span> tensorboard-<span class="number">2.2</span><span class="meta">.2</span> tensorboard-plugin-wit-<span class="number">1.7</span><span class="meta">.0</span> tensorflow-<span class="number">2.2</span><span class="meta">.0</span>+nv20<span class="meta">.6</span> tensorflow-estimator-<span class="number">2.2</span><span class="meta">.0</span> termcolor-<span class="number">1.1</span><span class="meta">.0</span> werkzeug-<span class="number">1.0</span><span class="meta">.1</span> wrapt-<span class="number">1.12</span><span class="meta">.1</span> zipp-<span class="number">3.1</span><span class="meta">.0</span></span><br><span class="line"></span><br><span class="line"><span class="symbol">nano@jetson:</span>~$ python3</span><br><span class="line">Python <span class="number">3.6</span><span class="meta">.9</span> (<span class="meta">default</span>, Apr <span class="number">18</span> <span class="number">2020</span>, <span class="number">01</span>:<span class="number">56</span>:<span class="number">04</span>)</span><br><span class="line">[GCC <span class="number">8.4</span><span class="meta">.0</span>] on linux</span><br><span class="line">Type <span class="string">"help"</span>, <span class="string">"copyright"</span>, <span class="string">"credits"</span> <span class="keyword">or</span> <span class="string">"license"</span> for more information.</span><br><span class="line">&gt;&gt;&gt; import tensorflow</span><br><span class="line"><span class="number">2020</span>-<span class="number">07</span>-<span class="number">16</span> <span class="number">08</span>:<span class="number">53</span>:<span class="number">15.536130</span>: I tensorflow/stream_executor/platform/<span class="meta">default</span>/dso_loader.cc:<span class="number">48</span>] Successfully opened dynamic library libcudart.so<span class="meta">.10</span>.</span><br></pre></td></tr></table></figure>
<h1 id="安装jupyter"><a href="#安装jupyter" class="headerlink" title="安装jupyter"></a>安装jupyter</h1><p>能否安装成功也有一定的玄学，遇到的问题有，pyzmq编译出错，远程登录无法识别密码。</p>
<p><a href="https://github.com/NVIDIA-AI-IOT/jetbot/wiki/Create-SD-Card-Image-From-Scratch" target="_blank" rel="noopener">安装方法同样来自官方</a></p>
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><span class="line">安装nodejs</span><br><span class="line">$ sudo apt <span class="keyword">install </span>nodejs npm</span><br><span class="line"></span><br><span class="line">安装<span class="keyword">jupyter</span></span><br><span class="line"><span class="keyword">$ </span>sudo pip3 <span class="keyword">install </span><span class="keyword">jupyter </span><span class="keyword">jupyterlab</span></span><br><span class="line"><span class="keyword"></span></span><br><span class="line"><span class="keyword">$ </span>sudo <span class="keyword">jupyter </span>labextension <span class="keyword">install </span>@<span class="keyword">jupyter-widgets/jupyterlab-manager</span></span><br><span class="line"><span class="keyword"></span></span><br><span class="line"><span class="keyword">$ </span>sudo <span class="keyword">jupyter </span>labextension <span class="keyword">install </span>@<span class="keyword">jupyterlab/statusbar</span></span><br><span class="line"><span class="keyword"></span></span><br><span class="line"><span class="keyword">生成jupyter配置文件</span></span><br><span class="line"><span class="keyword">$ </span><span class="keyword">jupyter </span>lab --generate-<span class="built_in">config</span></span><br><span class="line"></span><br><span class="line">$ <span class="keyword">jupyter </span>notebook password</span><br><span class="line">输入登陆密码</span><br><span class="line">生成<span class="keyword">jupyter_notebook_config.json文件</span></span><br></pre></td></tr></table></figure>
<p>修改jupyter_notebook_config.py的配置，添加以下内容</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">c.NotebookApp.ip = <span class="string">'*'</span> <span class="comment"># 就是设置所有ip皆可访问</span></span><br><span class="line">c.NotebookApp.password = <span class="string">u'sha1:0fb67bb71f8f:9525f730807d01c04ea963492b0e3340de7b9d67'</span> <span class="comment">#jupyter_notebook_config.json里的sha1密文</span></span><br><span class="line">c.NotebookApp.open_browser = <span class="keyword">False</span> <span class="comment"># 禁止自动打开浏览器</span></span><br><span class="line">c.NotebookApp.port = <span class="number">8888</span> <span class="comment">#指定为NAT端口映射的端口号</span></span><br></pre></td></tr></table></figure>
<p>启动<br><figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line"><span class="variable">$ </span>jupyter-lab</span><br></pre></td></tr></table></figure></p>
<h1 id="安装openCV"><a href="#安装openCV" class="headerlink" title="安装openCV"></a>安装openCV</h1><p>自带opencv，存在一个bug，如果先导入tensorflow会出错，但先导入cv2，再导入tensorflow则没有错误。</p>
<figure class="highlight groovy"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; <span class="keyword">import</span> tensorflow</span><br><span class="line"><span class="number">2020</span><span class="number">-07</span><span class="number">-16</span> <span class="number">08</span>:<span class="number">53</span>:<span class="number">15.536130</span>: I tensorflow<span class="regexp">/stream_executor/</span>platform<span class="regexp">/default/</span>dso_loader.<span class="string">cc:</span><span class="number">48</span>] Successfully opened dynamic library libcudart.so<span class="number">.10</span><span class="number">.2</span></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&gt;&gt;&gt; <span class="keyword">import</span> cv2</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">  File <span class="string">"/usr/lib/python3.6/dist-packages/cv2/__init__.py"</span>, line <span class="number">89</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    bootstrap()</span><br><span class="line">  File <span class="string">"/usr/lib/python3.6/dist-packages/cv2/__init__.py"</span>, line <span class="number">79</span>, <span class="keyword">in</span> bootstrap</span><br><span class="line">    <span class="keyword">import</span> cv2</span><br><span class="line"><span class="string">ImportError:</span> <span class="regexp">/usr/</span>lib<span class="regexp">/aarch64-linux-gnu/</span>libgomp.so<span class="number">.1</span>: cannot allocate memory <span class="keyword">in</span> <span class="keyword">static</span> TLS block</span><br><span class="line"></span><br><span class="line">先导入cv2则不会出错</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; <span class="keyword">import</span> cv2</span><br><span class="line">&gt;&gt;&gt; <span class="keyword">import</span> tensorflow</span><br><span class="line"><span class="number">2020</span><span class="number">-07</span><span class="number">-16</span> <span class="number">08</span>:<span class="number">56</span>:<span class="number">07.842807</span>: I tensorflow<span class="regexp">/stream_executor/</span>platform<span class="regexp">/default/</span>dso_loader.<span class="string">cc:</span><span class="number">48</span>] Successfully opened dynamic library libcudart.so<span class="number">.10</span><span class="number">.2</span></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<h1 id="测试Tensorflow"><a href="#测试Tensorflow" class="headerlink" title="测试Tensorflow"></a>测试Tensorflow</h1><p>首先导入tensorflow<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure></p>
<p>看下版本<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.__version__</span><br></pre></td></tr></table></figure></p>
<pre><code>&apos;2.2.0&apos;
</code></pre><p>载入并准备好 <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST数据集</a>。将样本从整数转换为浮点数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line"></span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line">x_train, x_test = x_train / <span class="number">255.0</span>, x_test / <span class="number">255.0</span></span><br></pre></td></tr></table></figure>
<pre><code>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 1s 0us/step
</code></pre><p>用tf.keras.Sequential构建一个模型。<br>为模型选择优化器和损失函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">  tf.keras.layers.Flatten(input_shape=(<span class="number">28</span>, <span class="number">28</span>)),</span><br><span class="line">  tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">  tf.keras.layers.Dropout(<span class="number">0.2</span>),</span><br><span class="line">  tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>,</span><br><span class="line">              loss=<span class="string">'sparse_categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<p>开始训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.fit(x_train, y_train, epochs=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/5
1875/1875 [==============================] - 26s 14ms/step - loss: 0.2933 - accuracy: 0.9154
Epoch 2/5
1875/1875 [==============================] - 26s 14ms/step - loss: 0.1426 - accuracy: 0.9577
Epoch 3/5
1875/1875 [==============================] - 26s 14ms/step - loss: 0.1062 - accuracy: 0.9676
Epoch 4/5
1875/1875 [==============================] - 26s 14ms/step - loss: 0.0870 - accuracy: 0.9731
Epoch 5/5
1875/1875 [==============================] - 25s 14ms/step - loss: 0.0734 - accuracy: 0.9771

&lt;tensorflow.python.keras.callbacks.History at 0x7f4c402710&gt;
</code></pre><p>速度不是太慢，最后看下准确度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.evaluate(x_test,  y_test, verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>313/313 - 2s - loss: 0.0704 - accuracy: 0.9803
[0.07044795155525208, 0.9803000092506409]
</code></pre><p>98%的准确度 jetson nano 完美运行tensorflow2</p>
<h1 id="坑"><a href="#坑" class="headerlink" title="坑"></a>坑</h1><p>我想通过ssh远程运行图形程序，如何尝试都不行，直到在<strong>NVIDIA Metropolis Documentation</strong>发现了这句话。</p>
<p><a href="https://docs.nvidia.com/metropolis/deepstream/DeepStream_5.0_Release_Notes.pdf" target="_blank" rel="noopener">ReleaseNotes</a> 2.0 LIMITATIONS</p>
<figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line">On Jetson, <span class="built_in">running</span> a DeepStream <span class="built_in">application</span> <span class="keyword">over</span> SSH (via putty) <span class="keyword">with</span> X11 forwarding</span><br><span class="line"><span class="keyword">does</span> <span class="keyword">not</span> work</span><br></pre></td></tr></table></figure>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这个东西还是存在着许多bug，系统也不是非常稳定，开发也很麻烦，如果是想玩玩还是推荐树莓派。</p>
]]></content>
      <categories>
        <category>嵌入式</category>
      </categories>
      <tags>
        <tag>Jetson Nano</tag>
      </tags>
  </entry>
  <entry>
    <title>文本转Emoji</title>
    <url>/2020/06/01/EmojiAnalysis/</url>
    <content><![CDATA[<h1 id="分析微博中的emoji"><a href="#分析微博中的emoji" class="headerlink" title="分析微博中的emoji"></a>分析微博中的emoji</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据分析库</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> emoji</span><br></pre></td></tr></table></figure>
<h1 id="处理收集到到微博评论"><a href="#处理收集到到微博评论" class="headerlink" title="处理收集到到微博评论"></a>处理收集到到微博评论</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">all_path</span><span class="params">(dirname)</span>:</span></span><br><span class="line">    result = []<span class="comment">#所有的文件</span></span><br><span class="line">    <span class="keyword">for</span> maindir, subdir, file_name_list <span class="keyword">in</span> os.walk(dirname):</span><br><span class="line">        <span class="keyword">for</span> filename <span class="keyword">in</span> file_name_list:</span><br><span class="line">            apath = os.path.join(maindir, filename)<span class="comment">#合并成一个完整路径</span></span><br><span class="line">            result.append(apath)</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">filenames = all_path(<span class="string">'WeiboData'</span>)</span><br><span class="line">filenames = filter(<span class="keyword">lambda</span> x: <span class="string">'WeiBoDataRet'</span> <span class="keyword">in</span> x, filenames)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_list = []</span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> filenames:</span><br><span class="line">    tmp = pd.read_csv(filename, sep=<span class="string">'|'</span>, encoding=<span class="string">"utf-8"</span>, names=[<span class="string">'text'</span>, <span class="string">'emoji'</span>])</span><br><span class="line">    data_list.append(tmp)</span><br><span class="line">data = pd.concat(data_list) <span class="comment"># 把所有数据拼接起来</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>emoji</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>heartbroken</td>
      <td>伤心</td>
    </tr>
    <tr>
      <th>1</th>
      <td>走好</td>
      <td>悲伤</td>
    </tr>
    <tr>
      <th>2</th>
      <td>走好</td>
      <td>心</td>
    </tr>
    <tr>
      <th>3</th>
      <td>走好</td>
      <td>心</td>
    </tr>
    <tr>
      <th>4</th>
      <td>走好</td>
      <td>心</td>
    </tr>
  </tbody>
</table>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>emoji</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1674858</td>
      <td>1673955</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>220772</td>
      <td>1780</td>
    </tr>
    <tr>
      <th>top</th>
      <td></td>
      <td>心</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>228436</td>
      <td>194223</td>
    </tr>
  </tbody>
</table>
</div>

<h1 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h1><p>去重</p>
<p>去除 空格</p>
<p>去除非emoji</p>
<p>构建emoji字典</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = data.drop_duplicates()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data.text = data.text.apply(<span class="keyword">lambda</span> x: x.strip())</span><br><span class="line">data.emoji = data.emoji.apply(<span class="keyword">lambda</span> x: str(x).strip())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GetEmojiDict</span><span class="params">()</span>:</span></span><br><span class="line">    fr = open(<span class="string">'ImgName.txt'</span>, <span class="string">'r'</span>)</span><br><span class="line">    jsondata = fr.read()</span><br><span class="line">    <span class="keyword">return</span> json.loads(jsondata)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">emojiDict = GetEmojiDict()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">len(emojiDict)</span><br></pre></td></tr></table></figure>
<pre><code>149
</code></pre><h2 id="去除非中文text"><a href="#去除非中文text" class="headerlink" title="去除非中文text"></a>去除非中文text</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">zhPattern=re.compile(<span class="string">u'[\u4e00-\u9fa5]+'</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">FilterEmoji</span><span class="params">(contents)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> contents <span class="keyword">in</span> emojiDict:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> zhPattern.search(contents):</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">True</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data.info()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;
Int64Index: 287976 entries, 0 to 703321
Data columns (total 2 columns):
 #   Column  Non-Null Count   Dtype 
---  ------  --------------   ----- 
 0   text    287976 non-null  object
 1   emoji   287976 non-null  object
dtypes: object(2)
memory usage: 6.6+ MB
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">filterReg = data.emoji.apply(<span class="keyword">lambda</span> x: FilterEmoji(x))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">filterIndex = filterReg[filterReg==<span class="number">0</span>].index</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">filterIndex</span><br></pre></td></tr></table></figure>
<pre><code>Int64Index([    61,    374,    653,    816,   1428,   1503,   1772,   2093,
              2382,   2697,
            ...
            701750, 701882, 701894, 702185, 702186, 702380, 702501, 703077,
            703095, 703096],
           dtype=&apos;int64&apos;, length=4556)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = data.drop(filterIndex)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">filterReg = data.text.apply(<span class="keyword">lambda</span> x: FilterEmoji(x))</span><br><span class="line">filterIndex = filterReg[filterReg==<span class="number">1</span>].index</span><br><span class="line">filterIndex</span><br></pre></td></tr></table></figure>
<pre><code>Int64Index([     0,     47,     56,     65,    144,    152,    159,    168,
               174,    175,
            ...
            700205, 700206, 700207, 700266, 700352, 702191, 702192, 702193,
            702194, 702195],
           dtype=&apos;int64&apos;, length=3475)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">filterIndex</span><br></pre></td></tr></table></figure>
<pre><code>Int64Index([     0,     47,     56,     65,    144,    152,    159,    168,
               174,    175,
            ...
            700205, 700206, 700207, 700266, 700352, 702191, 702192, 702193,
            702194, 702195],
           dtype=&apos;int64&apos;, length=3475)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = data.drop(filterIndex)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>emoji</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>277826</td>
      <td>277826</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>215349</td>
      <td>1339</td>
    </tr>
    <tr>
      <th>top</th>
      <td>[加油]</td>
      <td>允悲</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>72</td>
      <td>22486</td>
    </tr>
  </tbody>
</table>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">emoji_counts = data.emoji.value_counts()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">emoji_counts.describe()</span><br></pre></td></tr></table></figure>
<pre><code>count     1339.000000
mean       207.487677
std       1196.566008
min          1.000000
25%          3.000000
50%          9.000000
75%         41.000000
max      22486.000000
Name: emoji, dtype: float64
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>emoji</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>走好</td>
      <td>悲伤</td>
    </tr>
    <tr>
      <th>2</th>
      <td>走好</td>
      <td>心</td>
    </tr>
    <tr>
      <th>5</th>
      <td>走好</td>
      <td>蜡烛</td>
    </tr>
    <tr>
      <th>6</th>
      <td>舒服了，着尼哥终于死了</td>
      <td>笑哈哈</td>
    </tr>
    <tr>
      <th>7</th>
      <td>舒服了，着尼哥终于死了</td>
      <td>给力</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>703299</th>
      <td>期待蜜桃第二季，期待邓伦</td>
      <td>:peach:</td>
    </tr>
    <tr>
      <th>703303</th>
      <td>期待伦伦</td>
      <td>心</td>
    </tr>
    <tr>
      <th>703315</th>
      <td>川西真的是随便一个地方都是风景</td>
      <td>鼓掌</td>
    </tr>
    <tr>
      <th>703320</th>
      <td>お疲れ様でした</td>
      <td>跪了</td>
    </tr>
    <tr>
      <th>703321</th>
      <td>太快了！！！</td>
      <td>赞</td>
    </tr>
  </tbody>
</table>
<p>277826 rows × 2 columns</p>
</div>

<h2 id="删除重复的text"><a href="#删除重复的text" class="headerlink" title="删除重复的text"></a>删除重复的text</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dupdata = data.text.duplicated()</span><br><span class="line">dupIndex = dupdata[dupdata==<span class="number">1</span>].index</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">len(dupIndex)</span><br></pre></td></tr></table></figure>
<pre><code>62477
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = data.drop(dupIndex)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>emoji</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>走好</td>
      <td>悲伤</td>
    </tr>
    <tr>
      <th>6</th>
      <td>舒服了，着尼哥终于死了</td>
      <td>笑哈哈</td>
    </tr>
    <tr>
      <th>9</th>
      <td>老人家值得所有人尊重</td>
      <td>赞</td>
    </tr>
    <tr>
      <th>12</th>
      <td>老百姓真好</td>
      <td>心</td>
    </tr>
    <tr>
      <th>14</th>
      <td>虽然行为点赞，但是还是衷心希望老人，把自己过好了，有能力，再去帮助别人。</td>
      <td>摊手</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>703298</th>
      <td>期待蜜桃第二季，期待邓伦</td>
      <td>心</td>
    </tr>
    <tr>
      <th>703303</th>
      <td>期待伦伦</td>
      <td>心</td>
    </tr>
    <tr>
      <th>703315</th>
      <td>川西真的是随便一个地方都是风景</td>
      <td>鼓掌</td>
    </tr>
    <tr>
      <th>703320</th>
      <td>お疲れ様でした</td>
      <td>跪了</td>
    </tr>
    <tr>
      <th>703321</th>
      <td>太快了！！！</td>
      <td>赞</td>
    </tr>
  </tbody>
</table>
<p>203185 rows × 2 columns</p>
</div>

<h1 id="单词向量"><a href="#单词向量" class="headerlink" title="单词向量"></a>单词向量</h1><p>下面这段code的功能是解析词组对应的向量的三个字典</p>
<pre><code>word_to_index 单词查索引
index_to_word 索引查单词
word_to_vec_map 单词查向量
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_word_vecs</span><span class="params">(file)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(file, <span class="string">'r'</span>, encoding=<span class="string">"utf-8"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        words = set()</span><br><span class="line">        word_to_vec_map = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            line = line.strip().split()</span><br><span class="line">            curr_word = line[<span class="number">0</span>]</span><br><span class="line">            words.add(curr_word)</span><br><span class="line">            word_to_vec_map[curr_word] = np.array(line[<span class="number">1</span>:], dtype=np.float64)</span><br><span class="line">        </span><br><span class="line">        i = <span class="number">1</span></span><br><span class="line">        words_to_index = &#123;&#125;</span><br><span class="line">        index_to_words = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> sorted(words):</span><br><span class="line">            words_to_index[w] = i</span><br><span class="line">            index_to_words[i] = w</span><br><span class="line">            i = i + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> words_to_index, index_to_words, word_to_vec_map</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_to_index, index_to_word, word_to_vec_map = read_word_vecs(<span class="string">'sgns.weibo.word'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_to_vec_map[<span class="string">'他们'</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([-0.058985, -0.095981,  0.213523,  0.078806,  0.090758,  0.303698,
       -0.080819, -0.070496,  0.100697, -0.014494,  0.105789, -0.081538,
       -0.220132, -0.001089, -0.010554,  0.045872,  0.020978, -0.078958,
        0.310522,  0.026538,  0.116843, -0.012077,  0.091833,  0.199016,
       -0.253477,  0.105833, -0.079761, -0.114635,  0.437327,  0.003209,
       -0.191938, -0.292937, -0.042434, -0.092598, -0.031424,  0.232141,
        0.175102, -0.028322, -0.182089, -0.127304, -0.105405, -0.0155  ,
       -0.105409,  0.128716,  0.271304, -0.258079,  0.294854, -0.225564,
        0.041693,  0.122313, -0.10642 ,  0.218218, -0.061122,  0.032375,
        0.061754,  0.060876,  0.177719,  0.080874,  0.040064,  0.028098,
        0.181363, -0.073601, -0.009067, -0.031534,  0.190581,  0.175827,
       -0.003394, -0.120093,  0.136633,  0.22353 , -0.286703, -0.083716,
        0.07307 ,  0.290753, -0.073568, -0.146416,  0.287048,  0.177982,
        0.159483,  0.033554, -0.113645,  0.086506,  0.182751,  0.222543,
        0.069108, -0.005411, -0.117244,  0.278492,  0.292221, -0.277547,
        0.035062,  0.05546 ,  0.043035,  0.118464, -0.03085 ,  0.163017,
        0.032309,  0.238069,  0.164545, -0.162392, -0.093865,  0.358772,
       -0.138829, -0.27499 , -0.190523, -0.198303, -0.228555,  0.02823 ,
        0.12706 , -0.017478,  0.279601, -0.130354,  0.376413,  0.107592,
        0.501358, -0.392651,  0.167826,  0.030806, -0.047537,  0.0542  ,
       -0.027822, -0.177908,  0.436953, -0.139909, -0.205398, -0.069401,
        0.210465, -0.09408 , -0.030155, -0.186514, -0.408763,  0.209337,
        0.154496, -0.155053, -0.073264, -0.208221,  0.031705,  0.007868,
        0.105028, -0.313043, -0.030095,  0.32314 ,  0.039472,  0.056924,
       -0.029449, -0.18332 ,  0.329696, -0.20353 ,  0.079724,  0.005614,
        0.033271,  0.129164,  0.06442 , -0.093268, -0.26306 ,  0.042632,
       -0.066531, -0.25593 , -0.082908, -0.211791,  0.269096, -0.231714,
        0.498682,  0.171995, -0.188561, -0.254678,  0.127424,  0.490121,
       -0.002619, -0.270687, -0.062654, -0.009806,  0.068663, -0.131597,
        0.157276, -0.118741,  0.362313, -0.107524, -0.043709,  0.051271,
        0.016886, -0.303519, -0.131623, -0.103483,  0.090379,  0.071147,
        0.132338, -0.146149, -0.366627, -0.351044, -0.063839,  0.082302,
        0.385776,  0.158985,  0.224325, -0.116336, -0.247472, -0.500043,
       -0.054399, -0.51975 , -0.165844,  0.067776, -0.311503,  0.160354,
        0.310949, -0.158256, -0.13147 , -0.046553, -0.132425, -0.174187,
        0.137154,  0.128941,  0.077095,  0.086764, -0.085013, -0.076975,
        0.116672, -0.234487, -0.029225, -0.297913,  0.03733 ,  0.07142 ,
       -0.333047,  0.250342,  0.071834, -0.360994,  0.160254, -0.085961,
       -0.244442, -0.00217 ,  0.016221, -0.25117 ,  0.102826, -0.190794,
       -0.163422,  0.067348, -0.066799, -0.105879,  0.281125, -0.092643,
        0.014463, -0.040031, -0.047755, -0.192767,  0.166827, -0.210013,
       -0.126185,  0.228651,  0.28803 ,  0.045921,  0.15332 ,  0.014357,
       -0.149424, -0.235598, -0.137925, -0.333645,  0.114881,  0.25207 ,
        0.046461,  0.00136 ,  0.089115, -0.182189, -0.200544,  0.175124,
        0.069565, -0.055904,  0.05993 ,  0.067038,  0.119123,  0.143849,
       -0.182774,  0.354611, -0.137333,  0.157642,  0.028673, -0.504065,
       -0.006483, -0.056175,  0.131101, -0.106961, -0.07638 ,  0.294719,
        0.003378,  0.096714, -0.157428, -0.032374, -0.244506,  0.012603,
        0.202828,  0.080087,  0.06369 , -0.315489, -0.087886,  0.172018,
       -0.135227, -0.168902,  0.25539 , -0.265512, -0.209118,  0.003291])
</code></pre><h1 id="文本分词"><a href="#文本分词" class="headerlink" title="文本分词"></a>文本分词</h1><p>使用jieba对text分词</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">words = data.text.apply(<span class="keyword">lambda</span> x: list(jieba.cut(x)))</span><br></pre></td></tr></table></figure>
<pre><code>Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.805 seconds.
Prefix dict has been built successfully.
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">words</span><br></pre></td></tr></table></figure>
<pre><code>1                                                    [走, 好]
6                                 [舒服, 了, ，, 着尼哥, 终于, 死, 了]
9                                        [老人家, 值得, 所有人, 尊重]
12                                              [老百姓, 真, 好]
14        [虽然, 行为, 点赞, ，, 但是, 还是, 衷心希望, 老人, ，, 把, 自己, 过,...
                                ...                        
703298                             [期待, 蜜桃, 第二季, ，, 期待, 邓伦]
703303                                             [期待, 伦伦]
703315                    [川西, 真的, 是, 随便, 一个, 地方, 都, 是, 风景]
703320                                [お, 疲, れ, 様, で, し, た]
703321                                     [太快, 了, ！, ！, ！]
Name: text, Length: 203185, dtype: object
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data[<span class="string">'words'</span>] = words</span><br></pre></td></tr></table></figure>
<pre><code>&lt;ipython-input-41-21ad536f0372&gt;:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  data[&apos;words&apos;] = words
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>emoji</th>
      <th>words</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>走好</td>
      <td>悲伤</td>
      <td>[走, 好]</td>
    </tr>
    <tr>
      <th>6</th>
      <td>舒服了，着尼哥终于死了</td>
      <td>笑哈哈</td>
      <td>[舒服, 了, ，, 着尼哥, 终于, 死, 了]</td>
    </tr>
    <tr>
      <th>9</th>
      <td>老人家值得所有人尊重</td>
      <td>赞</td>
      <td>[老人家, 值得, 所有人, 尊重]</td>
    </tr>
    <tr>
      <th>12</th>
      <td>老百姓真好</td>
      <td>心</td>
      <td>[老百姓, 真, 好]</td>
    </tr>
    <tr>
      <th>14</th>
      <td>虽然行为点赞，但是还是衷心希望老人，把自己过好了，有能力，再去帮助别人。</td>
      <td>摊手</td>
      <td>[虽然, 行为, 点赞, ，, 但是, 还是, 衷心希望, 老人, ，, 把, 自己, 过,...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>703298</th>
      <td>期待蜜桃第二季，期待邓伦</td>
      <td>心</td>
      <td>[期待, 蜜桃, 第二季, ，, 期待, 邓伦]</td>
    </tr>
    <tr>
      <th>703303</th>
      <td>期待伦伦</td>
      <td>心</td>
      <td>[期待, 伦伦]</td>
    </tr>
    <tr>
      <th>703315</th>
      <td>川西真的是随便一个地方都是风景</td>
      <td>鼓掌</td>
      <td>[川西, 真的, 是, 随便, 一个, 地方, 都, 是, 风景]</td>
    </tr>
    <tr>
      <th>703320</th>
      <td>お疲れ様でした</td>
      <td>跪了</td>
      <td>[お, 疲, れ, 様, で, し, た]</td>
    </tr>
    <tr>
      <th>703321</th>
      <td>太快了！！！</td>
      <td>赞</td>
      <td>[太快, 了, ！, ！, ！]</td>
    </tr>
  </tbody>
</table>
<p>203185 rows × 3 columns</p>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ret_words_vector</span><span class="params">(words)</span>:</span></span><br><span class="line">    vector = np.zeros(<span class="number">300</span>)</span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        v = word_to_vec_map.get(word, <span class="keyword">None</span>)</span><br><span class="line">        <span class="keyword">if</span> type(v) != type(<span class="keyword">None</span>):</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">            vector += v</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> vector.all():</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">return</span> vector/n</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vectors = data.words.apply(ret_words_vector)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vectors</span><br></pre></td></tr></table></figure>
<pre><code>1        [-0.047105999999999995, 0.23246850000000002, 0...
6        [-0.027774333333333328, -0.114836, 0.062758, 0...
9        [-0.05153150000000001, 0.103688, 0.23323525, 0...
12       [-0.16598533333333335, 0.16545333333333334, 0....
13       [-0.045179, 0.132102, 0.237468, 0.255355, -0.0...
                               ...                        
53656    [-0.06892586111111111, -0.040252222222222224, ...
53666    [-0.124204, -0.0818, -0.02715266666666667, 0.0...
53669    [0.049611333333333334, 0.011862, 0.10510733333...
53683    [-0.01563454166666667, -0.008308624999999997, ...
53685    [-0.363667, -0.25396850000000004, 0.011313, -0...
Name: words, Length: 6638, dtype: object
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data[<span class="string">'vectors'</span>] = vectors</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>emoji</th>
      <th>words</th>
      <th>vectors</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>走好</td>
      <td>悲伤</td>
      <td>[走, 好]</td>
      <td>[-0.047105999999999995, 0.23246850000000002, 0...</td>
    </tr>
    <tr>
      <th>6</th>
      <td>舒服了，着尼哥终于死了</td>
      <td>笑哈哈</td>
      <td>[舒服, 了, ，, 着尼哥, 终于, 死, 了]</td>
      <td>[-0.027774333333333328, -0.114836, 0.062758, 0...</td>
    </tr>
    <tr>
      <th>9</th>
      <td>老人家值得所有人尊重</td>
      <td>赞</td>
      <td>[老人家, 值得, 所有人, 尊重]</td>
      <td>[-0.05153150000000001, 0.103688, 0.23323525, 0...</td>
    </tr>
    <tr>
      <th>12</th>
      <td>老百姓真好</td>
      <td>心</td>
      <td>[老百姓, 真, 好]</td>
      <td>[-0.16598533333333335, 0.16545333333333334, 0....</td>
    </tr>
    <tr>
      <th>13</th>
      <td>好人</td>
      <td>赞</td>
      <td>[好人]</td>
      <td>[-0.045179, 0.132102, 0.237468, 0.255355, -0.0...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>53656</th>
      <td>还有我们sf9的李达渊他让我们有钱的话先孝敬父母然后有多余的钱再买专辑如果还有多余的钱再来看...</td>
      <td>泪</td>
      <td>[还有, 我们, sf9, 的, 李达渊, 他, 让, 我们, 有钱, 的话, 先, 孝敬父...</td>
      <td>[-0.06892586111111111, -0.040252222222222224, ...</td>
    </tr>
    <tr>
      <th>53666</th>
      <td>全球的挑战</td>
      <td>允悲</td>
      <td>[全球, 的, 挑战]</td>
      <td>[-0.124204, -0.0818, -0.02715266666666667, 0.0...</td>
    </tr>
    <tr>
      <th>53669</th>
      <td>[加油]</td>
      <td>鲜花</td>
      <td>[[, 加油, ]]</td>
      <td>[0.049611333333333334, 0.011862, 0.10510733333...</td>
    </tr>
    <tr>
      <th>53683</th>
      <td>假设不离婚！各过各的！她的业务不再给他凭他的能力早晚被辞退然后没了生计！房子爱住你就住没人管...</td>
      <td>doge</td>
      <td>[假设, 不, 离婚, ！, 各过, 各, 的, ！, 她, 的, 业务, 不再, 给, 他...</td>
      <td>[-0.01563454166666667, -0.008308624999999997, ...</td>
    </tr>
    <tr>
      <th>53685</th>
      <td>我不配</td>
      <td>伤心</td>
      <td>[我, 不配]</td>
      <td>[-0.363667, -0.25396850000000004, 0.011313, -0...</td>
    </tr>
  </tbody>
</table>
<p>6638 rows × 4 columns</p>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = data.dropna()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>emoji</th>
      <th>words</th>
      <th>vectors</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>走好</td>
      <td>悲伤</td>
      <td>[走, 好]</td>
      <td>[-0.047105999999999995, 0.23246850000000002, 0...</td>
    </tr>
    <tr>
      <th>6</th>
      <td>舒服了，着尼哥终于死了</td>
      <td>笑哈哈</td>
      <td>[舒服, 了, ，, 着尼哥, 终于, 死, 了]</td>
      <td>[-0.027774333333333328, -0.114836, 0.062758, 0...</td>
    </tr>
    <tr>
      <th>9</th>
      <td>老人家值得所有人尊重</td>
      <td>赞</td>
      <td>[老人家, 值得, 所有人, 尊重]</td>
      <td>[-0.05153150000000001, 0.103688, 0.23323525, 0...</td>
    </tr>
    <tr>
      <th>12</th>
      <td>老百姓真好</td>
      <td>心</td>
      <td>[老百姓, 真, 好]</td>
      <td>[-0.16598533333333335, 0.16545333333333334, 0....</td>
    </tr>
    <tr>
      <th>13</th>
      <td>好人</td>
      <td>赞</td>
      <td>[好人]</td>
      <td>[-0.045179, 0.132102, 0.237468, 0.255355, -0.0...</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>53656</th>
      <td>还有我们sf9的李达渊他让我们有钱的话先孝敬父母然后有多余的钱再买专辑如果还有多余的钱再来看...</td>
      <td>泪</td>
      <td>[还有, 我们, sf9, 的, 李达渊, 他, 让, 我们, 有钱, 的话, 先, 孝敬父...</td>
      <td>[-0.06892586111111111, -0.040252222222222224, ...</td>
    </tr>
    <tr>
      <th>53666</th>
      <td>全球的挑战</td>
      <td>允悲</td>
      <td>[全球, 的, 挑战]</td>
      <td>[-0.124204, -0.0818, -0.02715266666666667, 0.0...</td>
    </tr>
    <tr>
      <th>53669</th>
      <td>[加油]</td>
      <td>鲜花</td>
      <td>[[, 加油, ]]</td>
      <td>[0.049611333333333334, 0.011862, 0.10510733333...</td>
    </tr>
    <tr>
      <th>53683</th>
      <td>假设不离婚！各过各的！她的业务不再给他凭他的能力早晚被辞退然后没了生计！房子爱住你就住没人管...</td>
      <td>doge</td>
      <td>[假设, 不, 离婚, ！, 各过, 各, 的, ！, 她, 的, 业务, 不再, 给, 他...</td>
      <td>[-0.01563454166666667, -0.008308624999999997, ...</td>
    </tr>
    <tr>
      <th>53685</th>
      <td>我不配</td>
      <td>伤心</td>
      <td>[我, 不配]</td>
      <td>[-0.363667, -0.25396850000000004, 0.011313, -0...</td>
    </tr>
  </tbody>
</table>
<p>6580 rows × 4 columns</p>
</div>

<h1 id="emoji映射为数字"><a href="#emoji映射为数字" class="headerlink" title="emoji映射为数字"></a>emoji映射为数字</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NpEncoder</span><span class="params">(json.JSONEncoder)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">default</span><span class="params">(self, obj)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(obj, np.integer):</span><br><span class="line">            <span class="keyword">return</span> int(obj)</span><br><span class="line">        <span class="keyword">elif</span> isinstance(obj, np.floating):</span><br><span class="line">            <span class="keyword">return</span> float(obj)</span><br><span class="line">        <span class="keyword">elif</span> isinstance(obj, np.ndarray):</span><br><span class="line">            <span class="keyword">return</span> obj.tolist()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> super(NpEncoder, self).default(obj)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmojiMap</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">(emoji_data)</span>:</span></span><br><span class="line">        labelencoder = LabelEncoder()</span><br><span class="line">        emoji_set = list(set(data.emoji))</span><br><span class="line">        x = labelencoder.fit_transform(emoji_set)</span><br><span class="line">        emoji_to_index = dict(zip(emoji_set, x))</span><br><span class="line">        index_to_emoji = dict(zip(x, emoji_set))</span><br><span class="line">        <span class="keyword">return</span> emoji_to_index, index_to_emoji</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(emoji_dictionary)</span>:</span></span><br><span class="line">        fw = open(<span class="string">'emoji_dictionary.json'</span>, <span class="string">'w'</span>)</span><br><span class="line">        data = json.dumps(emoji_dictionary, cls=NpEncoder)</span><br><span class="line">        fw.write(data)</span><br><span class="line">        fw.close()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(filename)</span>:</span></span><br><span class="line">        fr = open(<span class="string">'emoji_dictionary.json'</span>, <span class="string">'r'</span>)</span><br><span class="line">        data = fr.read()</span><br><span class="line">        emoji_dictionary = json.loads(data)</span><br><span class="line">        fr.close()</span><br><span class="line">        <span class="keyword">return</span> emoji_dictionary</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">emoji_to_index, index_to_emoji = EmojiMap.generate(data.emoji)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">len(emoji_to_index)</span><br></pre></td></tr></table></figure>
<pre><code>1043
</code></pre><h2 id="保存emoji-dictionary"><a href="#保存emoji-dictionary" class="headerlink" title="保存emoji_dictionary"></a>保存emoji_dictionary</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#EmojiMap.save(emoji_to_index)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">emoji_vector = data.emoji.apply(<span class="keyword">lambda</span> x: emoji_to_index[x])</span><br><span class="line">data[<span class="string">'emoji_vector'</span>] = emoji_vector</span><br></pre></td></tr></table></figure>
<pre><code>&lt;ipython-input-52-2ca0af0b144e&gt;:2: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  data[&apos;emoji_vector&apos;] = emoji_vector
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>emoji</th>
      <th>words</th>
      <th>emoji_vector</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>走好</td>
      <td>悲伤</td>
      <td>[走, 好]</td>
      <td>967</td>
    </tr>
    <tr>
      <th>6</th>
      <td>舒服了，着尼哥终于死了</td>
      <td>笑哈哈</td>
      <td>[舒服, 了, ，, 着尼哥, 终于, 死, 了]</td>
      <td>1005</td>
    </tr>
    <tr>
      <th>9</th>
      <td>老人家值得所有人尊重</td>
      <td>赞</td>
      <td>[老人家, 值得, 所有人, 尊重]</td>
      <td>1023</td>
    </tr>
    <tr>
      <th>12</th>
      <td>老百姓真好</td>
      <td>心</td>
      <td>[老百姓, 真, 好]</td>
      <td>963</td>
    </tr>
    <tr>
      <th>14</th>
      <td>虽然行为点赞，但是还是衷心希望老人，把自己过好了，有能力，再去帮助别人。</td>
      <td>摊手</td>
      <td>[虽然, 行为, 点赞, ，, 但是, 还是, 衷心希望, 老人, ，, 把, 自己, 过,...</td>
      <td>977</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>703298</th>
      <td>期待蜜桃第二季，期待邓伦</td>
      <td>心</td>
      <td>[期待, 蜜桃, 第二季, ，, 期待, 邓伦]</td>
      <td>963</td>
    </tr>
    <tr>
      <th>703303</th>
      <td>期待伦伦</td>
      <td>心</td>
      <td>[期待, 伦伦]</td>
      <td>963</td>
    </tr>
    <tr>
      <th>703315</th>
      <td>川西真的是随便一个地方都是风景</td>
      <td>鼓掌</td>
      <td>[川西, 真的, 是, 随便, 一个, 地方, 都, 是, 风景]</td>
      <td>1041</td>
    </tr>
    <tr>
      <th>703320</th>
      <td>お疲れ様でした</td>
      <td>跪了</td>
      <td>[お, 疲, れ, 様, で, し, た]</td>
      <td>1025</td>
    </tr>
    <tr>
      <th>703321</th>
      <td>太快了！！！</td>
      <td>赞</td>
      <td>[太快, 了, ！, ！, ！]</td>
      <td>1023</td>
    </tr>
  </tbody>
</table>
<p>203185 rows × 4 columns</p>
</div>

<h1 id="文本转向量与预测"><a href="#文本转向量与预测" class="headerlink" title="文本转向量与预测"></a>文本转向量与预测</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_to_vector</span><span class="params">(txt)</span>:</span></span><br><span class="line">    words = jieba.cut(txt)</span><br><span class="line">    <span class="keyword">return</span> ret_words_vector(words)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_emoji</span><span class="params">(txt, alg)</span>:</span></span><br><span class="line">    X_test = text_to_vector(txt)</span><br><span class="line">    X_test = np.array([X_test])</span><br><span class="line">    Y_pred = alg.predict(X_test)</span><br><span class="line">    Y_pred = int(Y_pred)</span><br><span class="line">    <span class="keyword">return</span> index_to_emoji[Y_pred]</span><br></pre></td></tr></table></figure>
<h1 id="构建训练集"><a href="#构建训练集" class="headerlink" title="构建训练集"></a>构建训练集</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># shuffle数据</span></span><br><span class="line">data = data.sample(frac=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data.info()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;
Int64Index: 6580 entries, 30383 to 19525
Data columns (total 5 columns):
 #   Column        Non-Null Count  Dtype 
---  ------        --------------  ----- 
 0   text          6580 non-null   object
 1   emoji         6580 non-null   object
 2   words         6580 non-null   object
 3   vectors       6580 non-null   object
 4   emoji_vector  6580 non-null   int64 
dtypes: int64(1), object(4)
memory usage: 308.4+ KB
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train = data.vectors.iloc[<span class="number">0</span>: ]</span><br><span class="line">Y_train = np.array(data.emoji_vector.iloc[<span class="number">0</span>: ])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train.shape, Y_train.shape</span><br></pre></td></tr></table></figure>
<pre><code>((6580,), (6580,))
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> X_train:</span><br><span class="line">    a.append(i)</span><br><span class="line">a = np.array(a)</span><br><span class="line">X_train = a</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> Y_train:</span><br><span class="line">    a.append([i])</span><br><span class="line">a = np.array(a)</span><br><span class="line">Y_train = a</span><br></pre></td></tr></table></figure>
<h1 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC, LinearSVC</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">svm = SVC(C=<span class="number">100</span>, gamma=<span class="number">10</span>, probability=<span class="keyword">True</span>)</span><br><span class="line">svm.fit(X_train, Y_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predict_emoji(<span class="string">'虽然行为点赞，但是还是衷心希望老人，把自己过好了，有能力，再去帮助别人。'</span>, svm)</span><br></pre></td></tr></table></figure>
<pre><code>&apos;心&apos;
</code></pre><h1 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Random Forest</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">random_forest = RandomForestClassifier(max_depth=<span class="number">50</span>)</span><br><span class="line">random_forest.fit(X_train, Y_train)</span><br><span class="line">random_forest.score(X_train, Y_train)</span><br><span class="line">acc_random_forest = round(random_forest.score(X_train, Y_train) * <span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">acc_random_forest</span><br></pre></td></tr></table></figure>
<pre><code>99.47
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predict_emoji(<span class="string">'你是不是傻'</span>, random_forest)</span><br></pre></td></tr></table></figure>
<pre><code>&apos;doge&apos;
</code></pre><h1 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line"></span><br><span class="line">joblib.dump(random_forest, <span class="string">'random_forest.model'</span>)</span><br><span class="line">svm2 = joblib.load(<span class="string">'random_forest.model'</span>)</span><br></pre></td></tr></table></figure>
<h1 id="使用keras搭建STML模型"><a href="#使用keras搭建STML模型" class="headerlink" title="使用keras搭建STML模型"></a>使用keras搭建STML模型</h1><h2 id="构建Feature和Labe"><a href="#构建Feature和Labe" class="headerlink" title="构建Feature和Labe"></a>构建Feature和Labe</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>emoji</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>走好</td>
      <td>悲伤</td>
    </tr>
    <tr>
      <th>6</th>
      <td>舒服了，着尼哥终于死了</td>
      <td>笑哈哈</td>
    </tr>
    <tr>
      <th>9</th>
      <td>老人家值得所有人尊重</td>
      <td>赞</td>
    </tr>
    <tr>
      <th>12</th>
      <td>老百姓真好</td>
      <td>心</td>
    </tr>
    <tr>
      <th>14</th>
      <td>虽然行为点赞，但是还是衷心希望老人，把自己过好了，有能力，再去帮助别人。</td>
      <td>摊手</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>703298</th>
      <td>期待蜜桃第二季，期待邓伦</td>
      <td>心</td>
    </tr>
    <tr>
      <th>703303</th>
      <td>期待伦伦</td>
      <td>心</td>
    </tr>
    <tr>
      <th>703315</th>
      <td>川西真的是随便一个地方都是风景</td>
      <td>鼓掌</td>
    </tr>
    <tr>
      <th>703320</th>
      <td>お疲れ様でした</td>
      <td>跪了</td>
    </tr>
    <tr>
      <th>703321</th>
      <td>太快了！！！</td>
      <td>赞</td>
    </tr>
  </tbody>
</table>
<p>203185 rows × 2 columns</p>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = data</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_len = df.words.map(<span class="keyword">lambda</span> x: len(x)).max()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_len</span><br></pre></td></tr></table></figure>
<pre><code>215
</code></pre><p>将一组句子(字符串)转换为与句子中的单词对应的索引数组。输出形状应该是这样的，它可以提供给’ Embedding() ‘</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentences_to_indices</span><span class="params">(X, word_to_index, max_len)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.</span></span><br><span class="line"><span class="string">    The output shape should be such that it can be given to `Embedding()`</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- array of sentences (strings), of shape (m, 1)</span></span><br><span class="line"><span class="string">    word_to_index -- a dictionary containing the each word mapped to its index</span></span><br><span class="line"><span class="string">    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">0</span>]                                   <span class="comment"># number of training examples</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)</span></span><br><span class="line">    X_indices = np.zeros((m, max_len))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                               <span class="comment"># loop over training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert the ith training sentence in lower case and split is into words. You should get a list of words.</span></span><br><span class="line">        sentence_words = X[i]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize j to 0</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loop over the words of sentence_words</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> sentence_words:</span><br><span class="line">            <span class="comment"># Set the (i,j)th entry of X_indices to the index of the correct word.</span></span><br><span class="line">            X_indices[i, j] = word_to_index.get(w, <span class="number">0</span>)</span><br><span class="line">            <span class="comment"># Increment j to j + 1</span></span><br><span class="line">            j = j + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X_indices</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X1_indices = sentences_to_indices(np.array(df.words), word_to_index, max_len)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train = X1_indices</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Y_train = np.array(df.emoji_vector)</span><br></pre></td></tr></table></figure>
<p>模型结构如下图</p>
<p><img src="emojifier-model.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Input, Dropout, LSTM, Activation</span><br><span class="line"><span class="keyword">from</span> keras.layers.embeddings <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br></pre></td></tr></table></figure>
<pre><code>Using TensorFlow backend.
</code></pre><p>实现pretrained_embedding_layer ()需要执行以下步骤:</p>
<ol>
<li>将嵌入矩阵初始化为具有正确形状的零数组。</li>
<li>用从’ word_to_vec_map ‘中提取的所有单词嵌入填充嵌入矩阵。</li>
<li>定义Keras嵌入层。使用嵌入()(<a href="https://keras.io/layers/embeddings/)。通过在调用“Embedding()”时设置“trainable" target="_blank" rel="noopener">https://keras.io/layers/embeddings/)。通过在调用“Embedding()”时设置“trainable</a> = False”，确保这个层是不可训练的。如果您设置’ trainable = True ‘，那么它将允许优化算法修改单词embeddings的值。</li>
<li>设嵌入权重等于嵌入矩阵</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: 预处理一个embedding层</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretrained_embedding_layer</span><span class="params">(word_to_vec_map, word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    创建一个 Keras Embedding() 层</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    embedding_layer -- pretrained layer Keras instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    vocab_len = len(word_to_index) + <span class="number">1</span>                  <span class="comment"># 增加一层</span></span><br><span class="line">    emb_dim = word_to_vec_map[<span class="string">"我"</span>].shape[<span class="number">0</span>]      <span class="comment"># 默认的向量维度</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将嵌入矩阵初始化为一个形状为零的numpy数组(vocab_len，单词向量的维数= emb_dim)</span></span><br><span class="line">    emb_matrix = np.zeros((vocab_len, emb_dim))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将嵌入矩阵的每一行“index”设为词汇表中“index”第四个单词的单词向量表示</span></span><br><span class="line">    <span class="keyword">for</span> word, index <span class="keyword">in</span> word_to_index.items():</span><br><span class="line">        emb_matrix[index, :] = word_to_vec_map[word]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义Keras嵌入层与正确的输出/输入大小，使其可训练。使用嵌入(…)。设置trainable=False。</span></span><br><span class="line">    embedding_layer = Embedding(vocab_len, emb_dim, trainable=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建嵌入层，在设置嵌入层权重之前需要。不要修改“None”。</span></span><br><span class="line">    embedding_layer.build((<span class="keyword">None</span>,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将嵌入层的权重设置为嵌入矩阵。现在是预先训练好的。</span></span><br><span class="line">    embedding_layer.set_weights([emb_matrix])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> embedding_layer</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)</span><br><span class="line">print(<span class="string">"weights[0][1][3] ="</span>, embedding_layer.get_weights()[<span class="number">0</span>][<span class="number">1</span>][<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<pre><code>weights[0][1][3] = 0.475248
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Emojify</span><span class="params">(input_shape, word_to_vec_map, word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Function creating the Emojify-v2 model's graph.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the input, usually (max_len,)</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a model instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).</span></span><br><span class="line">    sentence_indices = Input(shape=input_shape, dtype=<span class="string">"int32"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create the embedding layer pretrained with GloVe Vectors (≈1 line)</span></span><br><span class="line">    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Propagate sentence_indices through your embedding layer, you get back the embeddings</span></span><br><span class="line">    embeddings = embedding_layer(sentence_indices)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Propagate the embeddings through an LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a batch of sequences.</span></span><br><span class="line">    X = LSTM(units=<span class="number">128</span>, return_sequences=<span class="keyword">True</span>)(embeddings)</span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    X = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># Propagate X trough another LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a single hidden state, not a batch of sequences.</span></span><br><span class="line">    X = LSTM(units=<span class="number">128</span>, return_sequences=<span class="keyword">False</span>)(X)</span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    X = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.</span></span><br><span class="line">    X = Dense(<span class="number">1</span>)(X)</span><br><span class="line">    <span class="comment"># Add a softmax activation</span></span><br><span class="line">    X = Activation(<span class="string">"softmax"</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create Model instance which converts sentence_indices into X.</span></span><br><span class="line">    model = Model(inputs=sentence_indices, outputs=X)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">maxLen = len(max(X_train, key=len))</span><br><span class="line">model = Emojify((maxLen,), word_to_vec_map, word_to_index)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train[<span class="number">0</span>], Y_train[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>(array([170081.,  75835.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.,      0.,      0.,
             0.,      0.,      0.,      0.,      0.]),
 967)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.fit(X_train, Y_train, epochs = <span class="number">5</span>, batch_size = <span class="number">32</span>, shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/5
 33632/203185 [===&gt;..........................] - ETA: 32:43 - loss: -13765.9092 - accuracy: 0.0000e+00
</code></pre><h1 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = list(jieba.cut(<span class="string">'今天天气不错'</span>))</span><br><span class="line">x = np.array([x])</span><br><span class="line">x = sentences_to_indices(x, word_to_index, maxLen)</span><br><span class="line">p = model.predict(x)</span><br><span class="line">index_to_emoji[int(p)]</span><br></pre></td></tr></table></figure>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>随机森林效果最好，但是训练速度最慢，占用内存太大，SVM稍快，占用空间小。</p>
]]></content>
      <categories>
        <category>LSTM</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>Keras学习笔记</title>
    <url>/2020/08/01/Keras%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow  <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br></pre></td></tr></table></figure>
<h1 id="使用Sequential模型"><a href="#使用Sequential模型" class="headerlink" title="使用Sequential模型"></a>使用Sequential模型</h1><p>一个Sequential模型适用于简单的层堆叠， 其中每一层正好有一个输入张量和一个输出张量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = keras.Sequential(</span><br><span class="line">    [</span><br><span class="line">        layers.Dense(<span class="number">2</span>, activation=<span class="string">"relu"</span>, name=<span class="string">"layer1"</span>),</span><br><span class="line">        layers.Dense(<span class="number">3</span>, activation=<span class="string">"relu"</span>, name=<span class="string">"layer2"</span>),</span><br><span class="line">        layers.Dense(<span class="number">4</span>, name=<span class="string">"layer3"</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = tf.ones((<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">x</span><br></pre></td></tr></table></figure>
<pre><code>&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 1., 1.],
       [1., 1., 1.],
       [1., 1., 1.]], dtype=float32)&gt;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = model(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y</span><br></pre></td></tr></table></figure>
<pre><code>&lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=
array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.]], dtype=float32)&gt;
</code></pre><p>上述代码等效于一下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create 3 layers</span></span><br><span class="line">layer1 = layers.Dense(<span class="number">2</span>, activation=<span class="string">"relu"</span>, name=<span class="string">"layer1"</span>)</span><br><span class="line">layer2 = layers.Dense(<span class="number">3</span>, activation=<span class="string">"relu"</span>, name=<span class="string">"layer2"</span>)</span><br><span class="line">layer3 = layers.Dense(<span class="number">4</span>, name=<span class="string">"layer3"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Call layers on a test input</span></span><br><span class="line">x = tf.ones((<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">y = layer3(layer2(layer1(x)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y</span><br></pre></td></tr></table></figure>
<pre><code>&lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=
array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.]], dtype=float32)&gt;
</code></pre><p>Sequential不适用于以下情况：</p>
<pre><code>模型有多个输入或多个输出
任何一层都有多个输入或多个输出
需要进行图层共享
需要非线性拓扑（例如，残余连接，多分支模型）
</code></pre><p>可通过以下layers属性访问其图层</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.layers</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;tensorflow.python.keras.layers.core.Dense at 0x1ee679d3cf8&gt;,
 &lt;tensorflow.python.keras.layers.core.Dense at 0x1ee679f3a90&gt;,
 &lt;tensorflow.python.keras.layers.core.Dense at 0x1ee67a0c208&gt;]
</code></pre><p>还可以通过以下add()方法逐步创建一个顺序模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = keras.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">2</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">3</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<p>还有一种相应的pop()方法可以删除图层：顺序模型的行为非常类似于图层列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(len(model.layers))</span><br></pre></td></tr></table></figure>
<pre><code>3
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.pop()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(len(model.layers))</span><br></pre></td></tr></table></figure>
<pre><code>2
</code></pre><p>Sequential构造函数接受name参数，就像Keras中的任何层或模型一样。这对于用语义上有意义的名称注释TensorBoard图很有用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = keras.Sequential(name=<span class="string">"my_sequential"</span>)</span><br><span class="line">model.add(layers.Dense(<span class="number">2</span>, activation=<span class="string">"relu"</span>, name=<span class="string">"layer1"</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">3</span>, activation=<span class="string">"relu"</span>, name=<span class="string">"layer2"</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">4</span>, name=<span class="string">"layer3"</span>))</span><br></pre></td></tr></table></figure>
<h1 id="预先指定输入形状"><a href="#预先指定输入形状" class="headerlink" title="预先指定输入形状"></a>预先指定输入形状</h1><p>Keras中的所有图层都需要知道其输入的形状，以便能够创建其权重。因此，当创建这样的图层时，最初没有权重：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">layer = layers.Dense(<span class="number">3</span>)</span><br><span class="line">layer.weights</span><br></pre></td></tr></table></figure>
<pre><code>[]
</code></pre><p>由于权重的形状取决于输入的形状，因此会在首次调用输入时创建其权重：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = tf.ones((<span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">y = layer(x)</span><br><span class="line">layer.weights</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;tf.Variable &apos;dense_3/kernel:0&apos; shape=(4, 3) dtype=float32, numpy=
 array([[-0.23496091, -0.42415935, -0.38969237],
        [ 0.47878957,  0.6321573 ,  0.53070235],
        [-0.57678986,  0.5862113 , -0.5439472 ],
        [-0.8276289 ,  0.88936853, -0.6267946 ]], dtype=float32)&gt;,
 &lt;tf.Variable &apos;dense_3/bias:0&apos; shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)&gt;]
</code></pre><p>这也适用于顺序模型。当实例化没有输入形状的顺序模型时，它不是“构建”的：它没有权重（并且调用 model.weights结果仅说明了这一点）。权重是在模型首次看到一些输入数据时创建的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = keras.Sequential(</span><br><span class="line">    [</span><br><span class="line">        layers.Dense(<span class="number">2</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        layers.Dense(<span class="number">3</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        layers.Dense(<span class="number">4</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">x = tf.ones((<span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">y = model(x)</span><br><span class="line">print(<span class="string">"Number of weights after calling the model:"</span>, len(model.weights))  <span class="comment"># 6</span></span><br></pre></td></tr></table></figure>
<pre><code>Number of weights after calling the model: 6
</code></pre><p>一旦“构建”了模型，就可以调用其summary()方法以显示其内容：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_6&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_4 (Dense)              (1, 2)                    10        
_________________________________________________________________
dense_5 (Dense)              (1, 3)                    9         
_________________________________________________________________
dense_6 (Dense)              (1, 4)                    16        
=================================================================
Total params: 35
Trainable params: 35
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>但是，当逐步构建顺序模型时，能够显示到目前为止的模型摘要（包括当前输出形状）非常有用。在这种情况下，应该通过将一个Input 对象传递给模型来启动模型，以使它从一开始就知道其输入形状：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = keras.Sequential()</span><br><span class="line">model.add(keras.Input(shape=(<span class="number">4</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">2</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_8&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_10 (Dense)             (None, 2)                 10        
=================================================================
Total params: 10
Trainable params: 10
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>由于该Input对象model.layers不是图层，因此不会显示为的一部分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.layers</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;tensorflow.python.keras.layers.core.Dense at 0x1ee68ade4e0&gt;]
</code></pre><p>一个简单的替代方法是将一个input_shape参数传递给第一层：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = keras.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">2</span>, activation=<span class="string">"relu"</span>, input_shape=(<span class="number">4</span>,)))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_9&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_11 (Dense)             (None, 2)                 10        
=================================================================
Total params: 10
Trainable params: 10
Non-trainable params: 0
_________________________________________________________________
</code></pre><p>使用这样的预定义输入形状构建的模型始终具有权重（甚至在查看任何数据之前），并且始终具有定义的输出形状。</p>
<p>通常，建议的最佳做法是始终事先指定顺序模型的输入形状（如果预先知道它是什么）。</p>
<h1 id="常见的调试工作流程：add-summary"><a href="#常见的调试工作流程：add-summary" class="headerlink" title="常见的调试工作流程：add()+summary()"></a>常见的调试工作流程：add()+summary()</h1><p>在构建新的顺序体系结构时，以渐进方式堆叠层add()并经常打印模型摘要很有用。例如，可以监视堆栈Conv2D和MaxPooling2D图层如何对图像特征贴图进行下采样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = keras.Sequential()</span><br><span class="line">model.add(keras.Input(shape=(<span class="number">250</span>, <span class="number">250</span>, <span class="number">3</span>)))  <span class="comment"># 250x250 RGB images</span></span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, <span class="number">5</span>, strides=<span class="number">2</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.MaxPooling2D(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># (40, 40, 32)</span></span><br><span class="line"></span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.MaxPooling2D(<span class="number">3</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(layers.MaxPooling2D(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在我们有了4x4的特征图，是时候应用MaxPooling了。</span></span><br><span class="line">model.add(layers.GlobalMaxPooling2D())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后，我们添加一个分类层。</span></span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential_11&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_6 (Conv2D)            (None, 123, 123, 32)      2432      
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 121, 121, 32)      9248      
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 40, 40, 32)        0         
=================================================================
Total params: 11,680
Trainable params: 11,680
Non-trainable params: 0
_________________________________________________________________
Model: &quot;sequential_11&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_6 (Conv2D)            (None, 123, 123, 32)      2432      
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 121, 121, 32)      9248      
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 40, 40, 32)        0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 38, 38, 32)        9248      
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 36, 36, 32)        9248      
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 12, 12, 32)        0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 10, 10, 32)        9248      
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 8, 8, 32)          9248      
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 4, 4, 32)          0         
=================================================================
Total params: 48,672
Trainable params: 48,672
Non-trainable params: 0
_________________________________________________________________
</code></pre><h1 id="拥有模型后该怎么办"><a href="#拥有模型后该怎么办" class="headerlink" title="拥有模型后该怎么办"></a>拥有模型后该怎么办</h1><p>一旦模型架构准备就绪，将需要：</p>
<pre><code>训练模型，评估模型并进行推理。
将模型保存到磁盘并还原。
通过利用多个GPU来加速模型训练。
</code></pre><h1 id="使用顺序模型进行特征提取"><a href="#使用顺序模型进行特征提取" class="headerlink" title="使用顺序模型进行特征提取"></a>使用顺序模型进行特征提取</h1><p>一旦建立了顺序模型，它的行为就类似于功能API模型。这意味着每个层都有一个input and output属性。这些属性可以做一些事情，例如快速创建一个模型，以提取顺序模型中所有中间层的输出：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">initial_model = keras.Sequential(</span><br><span class="line">    [</span><br><span class="line">        keras.Input(shape=(<span class="number">250</span>, <span class="number">250</span>, <span class="number">3</span>)),</span><br><span class="line">        layers.Conv2D(<span class="number">32</span>, <span class="number">5</span>, strides=<span class="number">2</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">feature_extractor = keras.Model(</span><br><span class="line">    inputs=initial_model.inputs,</span><br><span class="line">    outputs=[layer.output <span class="keyword">for</span> layer <span class="keyword">in</span> initial_model.layers],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Call feature extractor on test input.</span></span><br><span class="line">x = tf.ones((<span class="number">1</span>, <span class="number">250</span>, <span class="number">250</span>, <span class="number">3</span>))</span><br><span class="line">features = feature_extractor(x)</span><br></pre></td></tr></table></figure>
<p>这是一个类似的示例，仅从一层中提取要素：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">initial_model = keras.Sequential(</span><br><span class="line">    [</span><br><span class="line">        keras.Input(shape=(<span class="number">250</span>, <span class="number">250</span>, <span class="number">3</span>)),</span><br><span class="line">        layers.Conv2D(<span class="number">32</span>, <span class="number">5</span>, strides=<span class="number">2</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">        layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">"relu"</span>, name=<span class="string">"my_intermediate_layer"</span>),</span><br><span class="line">        layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">feature_extractor = keras.Model(</span><br><span class="line">    inputs=initial_model.inputs,</span><br><span class="line">    outputs=initial_model.get_layer(name=<span class="string">"my_intermediate_layer"</span>).output,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># Call feature extractor on test input.</span></span><br><span class="line">x = tf.ones((<span class="number">1</span>, <span class="number">250</span>, <span class="number">250</span>, <span class="number">3</span>))</span><br><span class="line">features = feature_extractor(x)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>tensorflow2</category>
      </categories>
  </entry>
  <entry>
    <title>LeetCode 16. 3Sum Closest</title>
    <url>/2019/01/07/LeetCode%2016.%203Sum%20Closest/</url>
    <content><![CDATA[<p>好久没写C的，写个C复习一下，写个简单的leetcode吧</p>
<ol start="16">
<li>3Sum Closest<br>Given an array nums of n integers and an integer target, find three integers in nums such that the sum is closest to target. Return the sum of the three integers. You may assume that each input would have exactly one solution.</li>
</ol>
<p>Example:</p>
<p>Given array nums = [-1, 2, 1, -4], and target = 1.</p>
<p>The sum that is closest to the target is 2. (-1 + 2 + 1 = 2).</p>
<p>这个问题比较简单，我先写了个暴力解法<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">threeSumClosest</span><span class="params">(<span class="keyword">int</span>* nums, <span class="keyword">int</span> numsSize, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> result=nums[<span class="number">0</span>] + nums[<span class="number">1</span>] + nums[<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">int</span> i, j, k;</span><br><span class="line">    <span class="keyword">int</span> sum;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(i=<span class="number">0</span>; i&lt;numsSize<span class="number">-2</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(j=i+<span class="number">1</span>; j&lt;numsSize<span class="number">-1</span>; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(k=j+<span class="number">1</span>; k&lt;numsSize; k++)</span><br><span class="line">            &#123;</span><br><span class="line">                sum = nums[i] + nums[j] + nums[k];</span><br><span class="line">                <span class="keyword">if</span>(<span class="built_in">abs</span>(sum-target) &lt;= <span class="built_in">abs</span>(result-target))</span><br><span class="line">                    result = sum;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>runtime为172 ms可想而知是很低的。</p>
<p>看了下别人的solution，先排序再算要快好多，于是我先尝试了</p>
<p>冒泡排序</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bubble_sort</span><span class="params">(<span class="keyword">int</span> arr[], <span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i, j, temp;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; len - <span class="number">1</span>; i++)</span><br><span class="line">        <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; len - <span class="number">1</span> - i; j++)</span><br><span class="line">            <span class="keyword">if</span> (arr[j] &gt; arr[j + <span class="number">1</span>]) &#123;</span><br><span class="line">                temp = arr[j];</span><br><span class="line">                arr[j] = arr[j + <span class="number">1</span>];</span><br><span class="line">                arr[j + <span class="number">1</span>] = temp;</span><br><span class="line">            &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Runtime 为8 ms，还是差点，仔细看了下，因该是我的排序算法慢了，看到别人的solution用c中的快排<code>qsort()</code>，可惜我之前没学过这个函数,所以没想到，赶紧恶补下。</p>
<p><a href="https://www.cnblogs.com/laizhenghong2012/p/8442270.html/" target="_blank" rel="noopener">原文地址</a></p>
<figure class="highlight cs"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">qsort</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span> *<span class="keyword">base</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    size_t nmemb,</span></span></span><br><span class="line"><span class="function"><span class="params">    size_t size,</span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">int</span> (*compar</span>)(<span class="params"><span class="keyword">const</span> <span class="keyword">void</span> *, <span class="keyword">const</span> <span class="keyword">void</span> *</span>)</span></span><br><span class="line"><span class="function">    )</span>;</span><br><span class="line">    </span><br><span class="line">函数功能：qsort()函数的功能是对数组进行排序，数组有nmemb个元素，每个元素大小为size。</span><br><span class="line"></span><br><span class="line">参数<span class="keyword">base</span>       - <span class="keyword">base</span>指向数组的起始地址，通常该位置传入的是一个数组名</span><br><span class="line">参数nmemb  - nmemb表示该数组的元素个数</span><br><span class="line">参数size        - size表示该数组中每个元素的大小（字节数）</span><br><span class="line">参数(*compar)(<span class="keyword">const</span> <span class="keyword">void</span> *, <span class="keyword">const</span> <span class="keyword">void</span> *) - 此为指向比较函数的函数指针，决定了排序的顺序。</span><br><span class="line"></span><br><span class="line">函数返回值：无</span><br><span class="line"></span><br><span class="line">注意：如果两个元素的值是相同的，那么它们的前后顺序是不确定的。也就是说qsort()是一个不稳定的排序算法。</span><br><span class="line"></span><br><span class="line">compar参数</span><br><span class="line">compar参数指向一个比较两个元素的函数。比较函数的原型应该像下面这样。注意两个形参必须是<span class="keyword">const</span> <span class="keyword">void</span> *型，同时在调用compar 函数（compar实质为函数指针，这里称它所指向的函数也为compar）时，传入的实参也必须转换成<span class="keyword">const</span> <span class="keyword">void</span> *型。在compar函数内部会将<span class="keyword">const</span> <span class="keyword">void</span> *型转换成实际类型，见下文。</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">compar</span>(<span class="params"><span class="keyword">const</span> <span class="keyword">void</span> *p1, <span class="keyword">const</span> <span class="keyword">void</span> *p2</span>)</span>;</span><br><span class="line">如果compar返回值小于<span class="number">0</span>（&lt; <span class="number">0</span>），那么p1所指向元素会被排在p2所指向元素的前面</span><br><span class="line">如果compar返回值等于<span class="number">0</span>（= <span class="number">0</span>），那么p1所指向元素与p2所指向元素的顺序不确定</span><br><span class="line">如果compar返回值大于<span class="number">0</span>（&gt; <span class="number">0</span>），那么p1所指向元素会被排在p2所指向元素的后面</span><br><span class="line">因此，如果想让qsort()进行从小到大（升序）排序，那么一个通用的compar函数可以写成这样：</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">compareMyType</span> (<span class="params"><span class="keyword">const</span> <span class="keyword">void</span> * a, <span class="keyword">const</span> <span class="keyword">void</span> * b</span>)</span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="keyword">if</span> ( *(MyType*)a &lt;  *(MyType*)b ) <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">   <span class="keyword">if</span> ( *(MyType*)a == *(MyType*)b ) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">   <span class="keyword">if</span> ( *(MyType*)a &gt;  *(MyType*)b ) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">注意：你要将MyType换成实际数组元素的类型。</span><br></pre></td></tr></table></figure>
<p>使用快排后</p>
<p>Runtime: 4 ms, faster than 100.00% of C online submissions for 3Sum Closest.</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">comp</span><span class="params">(<span class="keyword">const</span> <span class="keyword">void</span>*a,<span class="keyword">const</span> <span class="keyword">void</span>*b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> *(<span class="keyword">int</span>*)a-*(<span class="keyword">int</span>*)b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">threeSumClosest</span><span class="params">(<span class="keyword">int</span>* nums, <span class="keyword">int</span> numsSize, <span class="keyword">int</span> target)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> result=nums[<span class="number">0</span>] + nums[<span class="number">1</span>] + nums[<span class="number">2</span>];</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(numsSize &lt;= <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> i, j, k;</span><br><span class="line">    <span class="keyword">int</span> sum;</span><br><span class="line">    </span><br><span class="line">    qsort(nums, numsSize, <span class="keyword">sizeof</span>(<span class="keyword">int</span>), comp);</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span>(i=<span class="number">0</span>; i&lt;numsSize<span class="number">-2</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        j = i + <span class="number">1</span>;</span><br><span class="line">        k = numsSize - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(j &lt; k)</span><br><span class="line">        &#123;</span><br><span class="line">            sum = nums[i] + nums[j] + nums[k];</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">abs</span>(sum-target) &lt;= <span class="built_in">abs</span>(result-target))</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(sum == target)</span><br><span class="line">                    <span class="keyword">return</span> sum;</span><br><span class="line">                result = sum;   </span><br><span class="line">            &#125;</span><br><span class="line">            (sum &gt; target)? k--: j++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>算法练习</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>c</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>LeetCode 28.Implement strStr().</title>
    <url>/2018/09/14/LeetCode%2028.Implement%20strStr()./</url>
    <content><![CDATA[<p>28.Implement strStr().</p>
<p>Return the index of the first occurrence of needle in haystack, or -1 if needle is not part of haystack.</p>
<p>Example 1:</p>
<p>Input: haystack = “hello”, needle = “ll”</p>
<p>Output: 2</p>
<p>Example 2:</p>
<p>Input: haystack = “aaaaa”, needle = “bba”</p>
<p>Output: -1<br>Clarification:</p>
<p>What should we return when needle is an empty string? This is a great question to ask during an interview.</p>
<p>For the purpose of this problem, we will return 0 when needle is an empty string. This is consistent to C’s strstr() and Java’s indexOf().</p>
<p>三种解法，首先是暴力解法，本来我先写了这个解法试试看，结果leetcode没通过，在倒数第二个case时超时了，在处理aaaa…ab, aa…ab(非常多个a)这种情况时，时间复杂度时n*m，显然不是一个很好的算法，使用KMP算法只有O(n+m)。</p>
<p>感谢UP主 <a href="https://www.bilibili.com/video/av22409335?t=318" target="_blank" rel="noopener">[KMP算法]NEXT数列手算演示</a></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">暴力破解法：</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">strStr</span><span class="params">(<span class="keyword">char</span>* haystack, <span class="keyword">char</span>* needle)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>, j = <span class="number">0</span>, n = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">strlen</span>(haystack) &lt; <span class="built_in">strlen</span>(needle))</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">strlen</span>(needle) == <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(i &lt;= <span class="built_in">strlen</span>(haystack))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(haystack[i] == needle[j])</span><br><span class="line">            <span class="keyword">for</span>(j = <span class="number">0</span>, n = i; j &lt; <span class="built_in">strlen</span>(needle) &amp;&amp; haystack[n] == needle[j]; j++, n++)</span><br><span class="line">                ;</span><br><span class="line">        <span class="keyword">if</span>(j == <span class="built_in">strlen</span>(needle))</span><br><span class="line">            <span class="keyword">return</span> i;</span><br><span class="line">        i++;</span><br><span class="line">        j = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">KMP算法：</span><br><span class="line"><span class="function"><span class="keyword">int</span>* <span class="title">next_arr</span><span class="params">(<span class="keyword">char</span>* ptr)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ptr_num = <span class="built_in">strlen</span>(ptr);</span><br><span class="line">    <span class="keyword">int</span>* next;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>, j = <span class="number">1</span>;</span><br><span class="line">    next = <span class="built_in">calloc</span>(ptr_num, <span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>, j = <span class="number">0</span>; i &lt; ptr_num;)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(ptr[i] == ptr[j])</span><br><span class="line">            next[i++] = ++j;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(j)</span><br><span class="line">            j = next[j - <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            next[i++] = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> next;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">strStr_kmp</span><span class="params">(<span class="keyword">char</span>* haystack, <span class="keyword">char</span>* needle)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> m = <span class="built_in">strlen</span>(haystack), n = <span class="built_in">strlen</span>(needle);</span><br><span class="line">    <span class="keyword">int</span> i, j;</span><br><span class="line">    <span class="keyword">int</span>* next = next_arr(needle);</span><br><span class="line">    <span class="keyword">if</span> (!n)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>, j = <span class="number">0</span>; i &lt; m;)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (haystack[i] == needle[j])</span><br><span class="line">        &#123;</span><br><span class="line">            i++;</span><br><span class="line">            j++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(j == n)</span><br><span class="line">            <span class="keyword">return</span> i - j;</span><br><span class="line">        <span class="keyword">if</span> (i &lt; m &amp;&amp; haystack[i] != needle[j])</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (j)</span><br><span class="line">                j = next[j - <span class="number">1</span>];</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                 i++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">free</span>(next);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用KMP算法运算速率成功的打败了100%的人，然后点了<br>sample 0 ms submission，看到了别人的算法。。。<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">strStr</span><span class="params">(<span class="keyword">char</span>* haystack, <span class="keyword">char</span>* needle)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a = <span class="built_in">strlen</span>(needle);</span><br><span class="line">    <span class="keyword">int</span> b = <span class="built_in">strlen</span>(haystack);</span><br><span class="line">    <span class="keyword">if</span>(a==<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;b-a+<span class="number">1</span>;i++)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j&lt;b; j++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(haystack[i+j] == needle[j])&#123;</span><br><span class="line">                <span class="keyword">if</span>(j == a<span class="number">-1</span>)</span><br><span class="line">                    <span class="keyword">return</span> i;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>算法练习</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>c</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown学习笔记</title>
    <url>/2018/03/30/Markdown%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>初次写博客，从搭建github hexo走了不少坑，稍后记录搭建博客过程，首先学习使用markdown<br>教程来源：<a href="http://www.markdown.cn/" target="_blank" rel="noopener">http://www.markdown.cn/</a><br>markdown是一个HTML的转换工具，是HTML的书写格式</p>
<h1 id="标题语法"><a href="#标题语法" class="headerlink" title="标题语法"></a>标题语法</h1><p>类Atx形式在首行插入1个到6个#，对应到标题1到6<br>一定要# + 空格 + 标题<br>加空格！！！！！！<br><figure class="highlight clean"><table><tr><td class="code"><pre><span class="line"># 一级标题</span><br><span class="line">## 二级标题</span><br><span class="line">###### 六级标题</span><br></pre></td></tr></table></figure></p>
<h1 id="区块引用"><a href="#区块引用" class="headerlink" title="区块引用"></a>区块引用</h1><p>段首使用 &gt; 作为引用，引用部分也支持markdown语法<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">hello word!</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"><span class="comment"># 标题一</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">代码提示：</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">    <span class="built_in">return</span> Null;</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>hello word!</p>
<h1 id="标题一"><a href="#标题一" class="headerlink" title="标题一"></a>标题一</h1><p>代码提示：<br>   return Null;</p>
</blockquote>
<h1 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h1><p>markdown支持有序和无序列表<br>无序列表标记符号可以是“* + -”，有序符号是数字加英文点“1. ”</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line"><span class="bullet">* </span>red</span><br><span class="line"><span class="bullet">* </span>green</span><br><span class="line"><span class="bullet">* </span>blue</span><br><span class="line"><span class="bullet">+ </span>red</span><br><span class="line"><span class="bullet">+ </span>green</span><br><span class="line"><span class="bullet">- </span>red</span><br><span class="line">1.good</span><br><span class="line">2.name</span><br><span class="line">3.learn</span><br></pre></td></tr></table></figure>
<ul>
<li>red</li>
<li>green</li>
<li>blue</li>
</ul>
<ul>
<li>red</li>
<li>green</li>
</ul>
<ul>
<li>red</li>
</ul>
<ol>
<li>good</li>
<li>name</li>
<li>learn</li>
</ol>
<h1 id="代码区块"><a href="#代码区块" class="headerlink" title="代码区块"></a>代码区块</h1><p>要在 Markdown 中建立代码区块很简单，只要简单地缩进 4 个空格或是 1 个制表符就可以，例如，下面的输入：<br>这是一个普通段落：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">   <span class="comment">//这是一个代码区块。</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	print(<span class="string">"Hello word!\n"</span>);</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h1><p>三个连续的<em> - _<br>    <strong><strong><strong><strong><em>**</em></strong></strong></strong></strong></em></p>
<hr>
<h1 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h1><p>链接文字用[方括号]标记<br>要建立一个内行式链接，只要在方括号后面紧接着元括号并插入网址链接</p>
<pre><code>This is [an example](http://example.com/ &quot;Title&quot;) inline link.
[This link](http://example.net/) has no title attribute.
</code></pre><p><a href="http://www.baidu.com/" title="百度" target="_blank" rel="noopener">百度一下</a><br>This is <a href="http://example.com/" title="Title" target="_blank" rel="noopener">an example</a> inline link.<br><a href="http://example.net/" target="_blank" rel="noopener">This link</a> has no title attribute.</p>
<h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>标记段行内代码，用反引号包起来（’）</p>
<pre><code>use the &apos;printf()&apos; function
</code></pre><p>Use the <code>printf()</code> function.</p>
<h1 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h1><p>和链接一样<br><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">![<span class="string">Alt text</span>](<span class="link">/path/to/img.jpg</span>)</span><br><span class="line">![<span class="string">Alt text</span>](<span class="link">/path/to/img.jpg "Optional title"</span>)</span><br></pre></td></tr></table></figure></p>
<p><a href="/path/to/img.jpg">Alt text</a></p>
<p>详细叙述如下：</p>
<p>一个惊叹号 !<br>接着一个方括号，里面放上图片的替代文字<br>接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上 选择性的 ‘title’ 文字。</p>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="反斜杠"><a href="#反斜杠" class="headerlink" title="反斜杠"></a>反斜杠</h2><p>在一下符号前面加入反斜杠来插入普通符号：</p>
<figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">\   反斜线</span><br><span class="line">`   反引号</span><br><span class="line"><span class="bullet">*   </span>星号</span><br><span class="line">_   底线</span><br><span class="line">&#123;&#125;  花括号</span><br><span class="line">[]  方括号</span><br><span class="line">()  括弧</span><br><span class="line"><span class="section">#   井字号</span></span><br><span class="line"><span class="bullet">+   </span>加号</span><br><span class="line"><span class="bullet">-   </span>减号</span><br><span class="line">.   英文句点</span><br><span class="line">!   惊叹号</span><br></pre></td></tr></table></figure>
<h2 id="自动链接"><a href="#自动链接" class="headerlink" title="自动链接"></a>自动链接</h2><p>处理短链接用&lt;&gt;括住能够自动转换成链接<br><a href="http://www.baidu.com" target="_blank" rel="noopener">http://www.baidu.com</a><br><a href="mailto:&#119;&#x78;&#106;&#53;&#54;&#x35;&#x38;&#x40;&#104;&#x6f;&#x74;&#x6d;&#97;&#105;&#46;&#99;&#111;&#x6d;" target="_blank" rel="noopener">&#119;&#x78;&#106;&#53;&#54;&#x35;&#x38;&#x40;&#104;&#x6f;&#x74;&#x6d;&#97;&#105;&#46;&#99;&#111;&#x6d;</a></p>
<h1 id="编辑软件"><a href="#编辑软件" class="headerlink" title="编辑软件"></a>编辑软件</h1><p>windows平台</p>
<ul>
<li><a href="http://markdownpad.com/" title="markdownpad" target="_blank" rel="noopener">Markdownpad</a></li>
</ul>
<p>激活：</p>
<figure class="highlight groovy"><table><tr><td class="code"><pre><span class="line">注册信息</span><br><span class="line"></span><br><span class="line">邮箱地址：</span><br><span class="line"></span><br><span class="line">Soar360<span class="meta">@live</span>.com</span><br><span class="line"></span><br><span class="line">授权密钥：</span><br><span class="line">GBPduHjWfJU1mZqcPM3BikjYKF6xKhlKIys3i1MU2eJHqWGImDHzWdD6xhMNLGVpbP2M5SN6bnxn2kSE8qHqNY5QaaRxmO3YSMHxlv2EYpjdwLcPwfeTG7kUdnhKE0vVy4RidP6Y2wZ0q74f47fzsZo45JE2hfQBFi2O9Jldjp1mW8HUpTtLA2a5<span class="regexp">/sQytXJUQl/</span>QKO0jUQY4pa5CCx20sV1ClOTZtAGngSOJtIOFXK599sBr5aIEFyH0K7H4BoNMiiDMnxt1rD8Vb<span class="regexp">/ikJdhGMMQr0R4B+L3nWU97eaVPTRKfWGDE8/</span>eAgKzpGwrQQoDh+nzX1xoVQ8NAuH+s4UcSeQ==</span><br></pre></td></tr></table></figure>
<p>在windows 10 系统下，windows10 MarkdownPad html会产生一个 渲染错误 awesomium（ This view has crashed ），此时就需要下载一个 HTML UI ENGINE（awesomium_v1.6.6_sdk_win）去解决该错误，该组件的下载地址：<br> <a href="http://markdownpad.com/download/awesomium_v1.6.6_sdk_win.exe" target="_blank" rel="noopener">http://markdownpad.com/download/awesomium_v1.6.6_sdk_win.exe</a></p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas 简介</title>
    <url>/2018/12/23/Pandas%20%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<p>这个是学习tensorflow前的准备，</p>
<p><a href="http://pandas.pydata.org/" target="_blank" rel="noopener">pandas</a> 是一种列存数据分析 API。它是用于处理和分析输入数据的强大工具，很多机器学习框架都支持将 pandas 数据结构作为输入。 虽然全方位介绍 pandas API 会占据很长篇幅，但它的核心概念非常简单，我们会在下文中进行说明。有关更完整的参考，请访问 <a href="http://pandas.pydata.org/pandas-docs/stable/index.html" target="_blank" rel="noopener">pandas 文档网站</a>，其中包含丰富的文档和教程资源。</p>
<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><p>导入pandas 并输出版本<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">print(pd.__version__)</span><br></pre></td></tr></table></figure></p>
<p>0.23.4</p>
<p>pandas中的主要数据结构被时限为一下两类：</p>
<pre><code>DataFrame： 一个关系型数据表格，其中包含多行和已命名的列，就像excel一样
Series：它是单独的一列，DataFrame中包含一个或多个Series，每个Series都有一个名称。就像我们写个表格在第一列写上每一行代表什么一样。
</code></pre><p>数据框架是用于数据操控的一种常用抽象实现形式，spark中的rdd，数据库中的table 类似。</p>
<p>创建Series的一种方法是构建Series对象。列入：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.Series([<span class="string">'Beijing'</span>, <span class="string">'Shanghai'</span>, <span class="string">'Shenzhen'</span>])</span><br></pre></td></tr></table></figure></p>
<p>你可以将映射string列名称的dict传递到它们各自的Series，从而创建DataFrame对象。如果Series在长度上不一致，系统会用特殊的NA值填充缺失的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">city_names = pd.Series([<span class="string">'Beijing'</span>, <span class="string">'Shanghai'</span>, <span class="string">'Shenzhen'</span>])</span><br><span class="line">population = pd.Series([<span class="number">21534678</span>, <span class="number">23541023</span>, <span class="number">120456</span>])</span><br><span class="line"></span><br><span class="line">data = pd.DataFrame(&#123;<span class="string">'City name'</span>: city_names, <span class="string">'Population'</span>: population&#125;)</span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">  City name  Population</span><br><span class="line"><span class="number">0</span>   Beijing    <span class="number">21534678</span></span><br><span class="line"><span class="number">1</span>  Shanghai    <span class="number">23541023</span></span><br><span class="line"><span class="number">2</span>  Shenzhen      <span class="number">120456</span></span><br></pre></td></tr></table></figure>
<p>大多数情况下，我们需要把整个文件加载到DataFrame中，下面我们加载一个包含加利福尼亚州住房的数据文件。并创建特征定义，通过head方法浏览DataFrame前几个纪录</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">california_housing_dataframe = pd.read_csv(<span class="string">"https://download.mlcc.google.cn/mledu-datasets/california_housing_train.csv"</span>, sep=<span class="string">","</span>)</span><br><span class="line">california_housing_dataframe.describe()</span><br><span class="line">print(california_housing_dataframe.head())</span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">   longitude  latitude         ...          median_income  median_house_value</span><br><span class="line"><span class="number">0</span>    <span class="number">-114.31</span>     <span class="number">34.19</span>         ...                 <span class="number">1.4936</span>             <span class="number">66900.0</span></span><br><span class="line"><span class="number">1</span>    <span class="number">-114.47</span>     <span class="number">34.40</span>         ...                 <span class="number">1.8200</span>             <span class="number">80100.0</span></span><br><span class="line"><span class="number">2</span>    <span class="number">-114.56</span>     <span class="number">33.69</span>         ...                 <span class="number">1.6509</span>             <span class="number">85700.0</span></span><br><span class="line"><span class="number">3</span>    <span class="number">-114.57</span>     <span class="number">33.64</span>         ...                 <span class="number">3.1917</span>             <span class="number">73400.0</span></span><br><span class="line"><span class="number">4</span>    <span class="number">-114.57</span>     <span class="number">33.57</span>         ...                 <span class="number">1.9250</span>             <span class="number">65500.0</span></span><br><span class="line"></span><br><span class="line">[<span class="number">5</span> rows x <span class="number">9</span> columns]</span><br></pre></td></tr></table></figure>
<p>pandas的另一个强大的功能是绘图制表，借助DataFrame.hist，可以快速了解一个列中值的分布。pandas使用的画图库是matplotlib所以我们也可以使用这个库中的方法来操作图表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">hist = california_housing_dataframe.hist(<span class="string">'housing_median_age'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Figure_1.png" alt="image"></p>
<h1 id="访问数据"><a href="#访问数据" class="headerlink" title="访问数据"></a>访问数据</h1><p>可以使用 dict 或list 的方法来访问DataFrame数据<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cities = pd.DataFrame(&#123;<span class="string">'City name'</span>: city_names, <span class="string">'Population'</span>: population&#125;)</span><br><span class="line">print(type(cities[<span class="string">'City name'</span>]))</span><br><span class="line">print(cities[<span class="string">'City name'</span>])</span><br><span class="line"></span><br><span class="line">print(type(cities[<span class="string">'City name'</span>][<span class="number">1</span>]))</span><br><span class="line">print(cities[<span class="string">'City name'</span>][<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">print(type(cities[<span class="number">0</span>:<span class="number">2</span>]))</span><br><span class="line">print(cities[<span class="number">0</span>:<span class="number">2</span>])</span><br></pre></td></tr></table></figure></p>
<figure class="highlight delphi"><table><tr><td class="code"><pre><span class="line">&lt;<span class="keyword">class</span> <span class="string">'pandas.core.series.Series'</span>&gt;</span><br><span class="line"><span class="number">0</span>     Beijing</span><br><span class="line"><span class="number">1</span>    Shanghai</span><br><span class="line"><span class="number">2</span>    Shenzhen</span><br><span class="line"><span class="keyword">Name</span>: City <span class="keyword">name</span>, dtype: <span class="keyword">object</span></span><br><span class="line"></span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">'str'</span>&gt;</span><br><span class="line">Shanghai</span><br><span class="line"></span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">'pandas.core.frame.DataFrame'</span>&gt;</span><br><span class="line">  City <span class="keyword">name</span>  Population</span><br><span class="line"><span class="number">0</span>   Beijing    <span class="number">21534678</span></span><br><span class="line"><span class="number">1</span>  Shanghai    <span class="number">23541023</span></span><br></pre></td></tr></table></figure>
<h1 id="操控数据"><a href="#操控数据" class="headerlink" title="操控数据"></a>操控数据</h1><p>可以向series应用Python的基本用算指令。<br><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">population / <span class="number">1000</span></span><br><span class="line"><span class="number">0</span>    <span class="number">21534.678</span></span><br><span class="line"><span class="number">1</span>    <span class="number">23541.023</span></span><br><span class="line"><span class="number">2</span>      <span class="number">120.456</span></span><br></pre></td></tr></table></figure></p>
<p>NumPy是一个用于科学计算的常用工具包。pandas series可作用大多数NumPy函数的参数。<br><figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">import</span> <span class="selector-tag">numpy</span> <span class="selector-tag">as</span> <span class="selector-tag">np</span></span><br><span class="line"><span class="selector-tag">np</span><span class="selector-class">.log</span>(<span class="selector-tag">population</span>)</span><br><span class="line"></span><br><span class="line">0    13<span class="selector-class">.655892</span></span><br><span class="line">1    13<span class="selector-class">.831172</span></span><br><span class="line">2    13<span class="selector-class">.092314</span></span><br><span class="line"><span class="selector-tag">dtype</span>: <span class="selector-tag">float64</span></span><br></pre></td></tr></table></figure></p>
<p>对于更加复杂的单列转换，可以使用Series.apply。像Python映射函数一样，Series.apply将以参数形式接受lambda函数，而该函数会应用与每个值，下面的例子是创建一个population是否超过一定数值的series。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(population.apply(<span class="keyword">lambda</span> val: val &gt; <span class="number">1000000</span>))</span><br><span class="line"></span><br><span class="line"><span class="number">0</span>     <span class="keyword">True</span></span><br><span class="line"><span class="number">1</span>     <span class="keyword">True</span></span><br><span class="line"><span class="number">2</span>    <span class="keyword">False</span></span><br><span class="line">dtype: bool</span><br></pre></td></tr></table></figure>
<p>DataFrames的修改方式也非常简单。例如，一下代码向现有的DataFrame添加了两个Series。<br><figure class="highlight clean"><table><tr><td class="code"><pre><span class="line">cities[<span class="string">'Area square miles'</span>] = pd.Series([<span class="number">98.87</span>, <span class="number">176.53</span>, <span class="number">46.92</span>])  # 随便写的数</span><br><span class="line">cities[<span class="string">'Population density'</span>] = cities[<span class="string">'Population'</span>] / cities[<span class="string">'Area square miles'</span>]</span><br><span class="line">print(cities)</span><br><span class="line"></span><br><span class="line">  City name         ...          Population density</span><br><span class="line"><span class="number">0</span>   Beijing         ...               <span class="number">217808.010519</span></span><br><span class="line"><span class="number">1</span>  Shanghai         ...               <span class="number">133354.234408</span></span><br><span class="line"><span class="number">2</span>  Shenzhen         ...                 <span class="number">2567.263427</span></span><br></pre></td></tr></table></figure></p>
<h1 id="练习1"><a href="#练习1" class="headerlink" title="练习1"></a>练习1</h1><p>通过添加一个新的布尔值列，修改cities表格</p>
<pre><code>城市以sh开头
城市面积大于50 （上面数都是我随便写的）
</code></pre><p>注意：布尔值 Series 1 辑与时，应使用 &amp;，而不是 and。<br><figure class="highlight sqf"><table><tr><td class="code"><pre><span class="line">cities[<span class="string">'Is wide and has Sh name'</span>] = (cities[<span class="string">'Area square miles'</span>] &gt; <span class="number">50</span>) &amp; cities[<span class="string">'City name'</span>].<span class="built_in">apply</span>(lambda <span class="built_in">name</span>: <span class="built_in">name</span>.startswith(<span class="string">'Sh'</span>))</span><br><span class="line"></span><br><span class="line">print(cities)</span><br><span class="line"></span><br><span class="line">  City <span class="built_in">name</span>           ...             Is wide <span class="built_in">and</span> has Sh <span class="built_in">name</span></span><br><span class="line"><span class="number">0</span>   Beijing           ...                               <span class="literal">False</span></span><br><span class="line"><span class="number">1</span>  Shanghai           ...                                <span class="literal">True</span></span><br><span class="line"><span class="number">2</span>  Shenzhen           ...                               <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">[<span class="number">3</span> rows x <span class="number">5</span> columns]</span><br></pre></td></tr></table></figure></p>
<h1 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h1><p>Series和DataFrame对象也定义了index属性，改属性向每个Series项或DataFrame行赋一个标识符值。默认情况下，在构造时，pandas会赋可反应数据源数据顺序的索引值。索引值在创建后时稳定的；也就是说，他们不会因为数据重新排序而发生改变。</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="builtin-name">print</span>(city_names.index)</span><br><span class="line">RangeIndex(<span class="attribute">start</span>=0, <span class="attribute">stop</span>=3, <span class="attribute">step</span>=1)</span><br><span class="line"></span><br><span class="line"><span class="builtin-name">print</span>(cities.index)</span><br><span class="line">RangeIndex(<span class="attribute">start</span>=0, <span class="attribute">stop</span>=3, <span class="attribute">step</span>=1)</span><br><span class="line"></span><br><span class="line"><span class="builtin-name">print</span>(cities.reindex([2, 0, 1]))</span><br><span class="line">  City name           <span class="built_in">..</span>.             Is wide <span class="keyword">and</span> has Sh name</span><br><span class="line">2  Shenzhen           <span class="built_in">..</span>.                               <span class="literal">False</span></span><br><span class="line">0   Beijing           <span class="built_in">..</span>.                               <span class="literal">False</span></span><br><span class="line">1  Shanghai           <span class="built_in">..</span>.                                <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">[3 rows x 5 columns]</span><br><span class="line"></span><br><span class="line"><span class="builtin-name">print</span>(cities.reindex(np.random.permutation(cities.index)))</span><br><span class="line">  City name           <span class="built_in">..</span>.             Is wide <span class="keyword">and</span> has Sh name</span><br><span class="line">0   Beijing           <span class="built_in">..</span>.                               <span class="literal">False</span></span><br><span class="line">2  Shanghai           <span class="built_in">..</span>.                                <span class="literal">True</span></span><br><span class="line">1  Shenzhen           <span class="built_in">..</span>.                               <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">[3 rows x 5 columns]</span><br></pre></td></tr></table></figure>
<h1 id="练习2"><a href="#练习2" class="headerlink" title="练习2"></a>练习2</h1><p>reindex方法允许使用未包含在原始DataFrame索引值中的索引值。请示一下，看看如果使用此类值会发生什么。</p>
<p>如果reindex输入数组包含原始DataFrame索引值中没有的值，reindex会为此类“丢失的”索引添加新行，并在所有对应列中填充NaN值</p>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">cities.reindex([<span class="number">0</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">2</span>])</span><br><span class="line">  City name           ...             Is wide and has Sh name</span><br><span class="line"><span class="number">0</span>   Beijing           ...                               False</span><br><span class="line"><span class="number">4</span>       NaN           ...                                 NaN</span><br><span class="line"><span class="number">5</span>       NaN           ...                                 NaN</span><br><span class="line"><span class="number">2</span>  Shenzhen           ...                               False</span><br><span class="line"></span><br><span class="line">[<span class="number">4</span> rows x <span class="number">5</span> columns]</span><br></pre></td></tr></table></figure>
<p>这种行为是可取的，因为索引通常是从实际数据中提取的字符串，在这种情况下，如果容许出现“丢失的”索引，将可以轻松的使用外部列表重建索引，因为我们不必担心将输入清理掉。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>Python 进度条 tqdm</title>
    <url>/2019/07/26/Python%20%E8%BF%9B%E5%BA%A6%E6%9D%A1%20tqdm/</url>
    <content><![CDATA[<p>tqdm是Python中专门用于进度条美化的模块，通过在非while的循环体内嵌入tqdm，可以得到一个能更好展现程序运行过程的提示进度条，本文就将针对tqdm的基本用法进行介绍。</p>
<h1 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h1><p>tqdm()的使用非常简单，只要传入一个迭代器就可以了，例如range()。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> tqdm([<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>, <span class="string">'e'</span>]):</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>100%|██████████| 5/5 [00:05&lt;00:00,  1.00s/it]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> tqdm(range(<span class="number">10</span>)):</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>100%|██████████| 10/10 [00:10&lt;00:00,  1.00s/it]
</code></pre><p>tqdm 还提供了tqdm(range())的简单版本，trange()</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> trange</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> trange(<span class="number">10</span>):</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>100%|██████████| 10/10 [00:10&lt;00:00,  1.00s/it]
</code></pre><p>tqdm为jupyter提供了一个效果更好的进度条，在jupyter里我们可以使用这个效果更好。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm_notebook</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm_notebook(range(<span class="number">100</span>),desc=<span class="string">'demo：'</span>):</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><img src="1.png" alt="image"></p>
<p>ok，结束。</p>
]]></content>
      <tags>
        <tag>python库</tag>
      </tags>
  </entry>
  <entry>
    <title>Python实现异常检测算法</title>
    <url>/2018/08/27/Python%E5%AE%9E%E7%8E%B0%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>应用高斯分布开发异常检测算法，这个比较简单，高斯分布也叫做正态分布，高中就学过，如果我们的数据符合高斯分布或者比较像高斯分布的时候可以使用这个算法，通过训练集计算高斯分布函数，与交叉验证集比较设置合适的Σ，当测试数据小于Σ时则为异常</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"></span><br><span class="line">data = sio.loadmat(<span class="string">'./data/ex8data1.mat'</span>);</span><br><span class="line">X = data[<span class="string">'X'</span>] <span class="comment"># 训练集</span></span><br><span class="line">Xval = data[<span class="string">'Xval'</span>] <span class="comment"># 交叉验证集</span></span><br><span class="line">Yval = data[<span class="string">'yval'</span>]</span><br><span class="line">X1 = X[:, [<span class="number">0</span>]]</span><br><span class="line">X2 = X[:, [<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax.scatter(X1, X2)</span><br><span class="line"><span class="comment"># plt.plot(Xval[:, [0]], Xval[:, [1]], '.', markerfacecolor='g', markeredgecolor="k", markersize=14)</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算平均数 𝜇 和 方差 𝜎2</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aveandvar</span><span class="params">(x)</span>:</span></span><br><span class="line">    sum = np.array([<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> x: sum = sum + i</span><br><span class="line">    ave = sum/len(x)</span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> x: sum += (i - ave)*(i - ave)</span><br><span class="line">    var = sum/len(x)</span><br><span class="line">    <span class="keyword">return</span> ave, var</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算概率密度p(x); 特征集, 平均值, 平方差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian_distribution</span><span class="params">(x, u, s)</span>:</span></span><br><span class="line">    px = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> x:</span><br><span class="line">        p = <span class="number">1</span>/(np.sqrt((<span class="number">2</span> * np.pi * s))) * np.exp(-((i - u) * (i - u))/(<span class="number">2</span> * s))</span><br><span class="line">        px.append(p)</span><br><span class="line">    px = np.array(px)</span><br><span class="line">    <span class="keyword">return</span> px</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择阈值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_threshold</span><span class="params">(pval, yval)</span>:</span></span><br><span class="line">    best_epsilon = <span class="number">0</span></span><br><span class="line">    best_f1 = <span class="number">0</span></span><br><span class="line">    f1 = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    step = (pval.max() - pval.min()) / <span class="number">10000</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epsilon <span class="keyword">in</span> np.arange(pval.min(), pval.max(), step):</span><br><span class="line">        preds = pval &lt; epsilon</span><br><span class="line"></span><br><span class="line">        tp = np.sum(np.logical_and(preds == <span class="number">1</span>, yval == <span class="number">1</span>)).astype(float)</span><br><span class="line">        fp = np.sum(np.logical_and(preds == <span class="number">1</span>, yval == <span class="number">0</span>)).astype(float)</span><br><span class="line">        fn = np.sum(np.logical_and(preds == <span class="number">0</span>, yval == <span class="number">1</span>)).astype(float)</span><br><span class="line"></span><br><span class="line">        precision = tp / (tp + fp)</span><br><span class="line">        recall = tp / (tp + fn)</span><br><span class="line">        f1 = (<span class="number">2</span> * precision * recall) / (precision + recall)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> f1 &gt; best_f1:</span><br><span class="line">            best_f1 = f1</span><br><span class="line">            best_epsilon = epsilon</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> best_epsilon, best_f1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 画一下这两个特征值的高斯曲线 首先要排下序</span></span><br><span class="line">X1.T.sort()</span><br><span class="line">X2.T.sort()</span><br><span class="line">u,s = aveandvar(X1)</span><br><span class="line">px1 = gaussian_distribution(X1, u, s)</span><br><span class="line">u,s = aveandvar(X2)</span><br><span class="line">px2 = gaussian_distribution(X2, u, s)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">211</span>)</span><br><span class="line">plt.title(<span class="string">'X1'</span>)</span><br><span class="line">plt.plot(X1, px1 )</span><br><span class="line">plt.subplot(<span class="number">212</span>)</span><br><span class="line">plt.title(<span class="string">'X2'</span>)</span><br><span class="line">plt.plot(X2, px2 )</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算训练集</span></span><br><span class="line">u,s = aveandvar(X)</span><br><span class="line">px = gaussian_distribution(X, u, s)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算测试集</span></span><br><span class="line">u,s = aveandvar(Xval)</span><br><span class="line">tpx = gaussian_distribution(X, u, s)</span><br><span class="line"></span><br><span class="line">epsilon, f1 = select_threshold(tpx, Yval)</span><br><span class="line">print(epsilon, f1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标记出异常数据</span></span><br><span class="line">outliers = np.where(px &lt; epsilon)</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>])</span><br><span class="line">ax.scatter(X[outliers[<span class="number">0</span>],<span class="number">0</span>], X[outliers[<span class="number">0</span>],<span class="number">1</span>], s=<span class="number">50</span>, color=<span class="string">'r'</span>, marker=<span class="string">'o'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调库验证</span></span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line">px = np.zeros((X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>]))</span><br><span class="line">px[:,<span class="number">0</span>] = stats.norm(u[<span class="number">0</span>], s[<span class="number">0</span>]).pdf(X[:,<span class="number">0</span>])</span><br><span class="line">px[:,<span class="number">1</span>] = stats.norm(u[<span class="number">1</span>], s[<span class="number">1</span>]).pdf(X[:,<span class="number">1</span>])</span><br><span class="line">outliers = np.where(px &lt; epsilon)</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>])</span><br><span class="line">ax.scatter(X[outliers[<span class="number">0</span>],<span class="number">0</span>], X[outliers[<span class="number">0</span>],<span class="number">1</span>], s=<span class="number">50</span>, color=<span class="string">'r'</span>, marker=<span class="string">'o'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>这是我们的训练集合，明显有六个是异常数据</p>
<p><img src="Figure_1.png" alt="image"></p>
<p>画出连个特征的高斯函数，比较像高斯分布</p>
<p><img src="Figure_1-1.png" alt="image"></p>
<p>通过我自己写的高斯密度函数计算，有些过拟合，多拟合到了两个点，不知道为什么。</p>
<p><img src="Figure_1-2.png" alt="image"></p>
<p>调用scipy的高斯函数库计算后完美的检测到了异常数据</p>
<p><img src="Figure_1-3.png" alt="image"></p>
]]></content>
      <categories>
        <category>机器学习代码实现</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python实现PCA降维算法</title>
    <url>/2018/08/21/Python%E5%AE%9E%E7%8E%B0PCA%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>这是第二个无监督学习的算法，是一个降维算法，可以把多个特征进行压缩，我在压缩后计算了与原数据的偏差，当我把四个特征压缩为三个时偏差只有0.5%，压缩为一个特征时偏差也只有7%，当只有一个特征时把数据展开也可以轻易的分为三类，所以这是一个非常优秀的算法。</p>
<p>值得注意的点是在计算奇异矩阵时遇到的问题，首先我们有一个m×n（m个数据，n个特征）的矩阵X，我们希望得到一个m×k的矩阵Z，具体降维过程分三步：</p>
<pre><code>·第一步：均值归一化，就是把每一个数都减去总数的平均值，得到的一个和平均数差距的新矩阵Xj。
·第二部：计算协方差矩阵，在这里要注意的时，Xj(i)是一个n×1的矩阵，Xj(i)的转置是一个1×n的矩阵，所以他俩相乘得到一个n×n的矩阵Σ，其实就是的到一个奇异矩阵，因为只有奇异矩阵才可能有特征值。
·第三部：奇异值分解，计算∑的特征值，使用svd()函数分解出U,S,V三个向量，U也是一个n×n的矩阵，在U中选取k个向量，获得一个n×k的矩阵Ureduce，新的特征矩阵z就等于Ureduce的转置(k×n)乘以X(n×m)结果得到一个k×m的新矩阵
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MYPCA</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data, k)</span>:</span></span><br><span class="line">        self.m = len(data)  <span class="comment"># 训练数据个数</span></span><br><span class="line">        self.n = len(data[<span class="number">0</span>])  <span class="comment"># 现在的特征数</span></span><br><span class="line">        self.k = k  <span class="comment"># 优化后的特征数</span></span><br><span class="line">        self.X = data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第一步是均值归一化。我们需要计算出所有特征的均值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">data_preprocess</span><span class="params">(self)</span>:</span></span><br><span class="line">        sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> self.X:</span><br><span class="line">            sum += i</span><br><span class="line">        u = sum/self.m</span><br><span class="line">        self.newX = np.empty([<span class="number">0</span>, self.n])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> self.X:</span><br><span class="line">            self.newX = np.row_stack((self.newX, i - u))</span><br><span class="line">        <span class="keyword">return</span> self.newX</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二步计算协方差矩阵 传入均值归一化后的矩阵 Σ=1𝑚Σ(𝑥(𝑖))𝑛𝑖=1(𝑥(𝑖))𝑇</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">covariance_matrix</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> X:</span><br><span class="line">            i = i[np.newaxis, :]</span><br><span class="line">            sum += np.dot(i.T, i)</span><br><span class="line">        sigma = sum/self.m</span><br><span class="line">        <span class="keyword">return</span> sigma</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算新的特征向量Z</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_z</span><span class="params">(self, U, X)</span>:</span></span><br><span class="line">        z = np.empty([self.k, <span class="number">0</span>])</span><br><span class="line">        Ureduce = U[...,<span class="number">0</span>:self.k]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> X:</span><br><span class="line">            i = i[np.newaxis, :]</span><br><span class="line">            t = np.dot(Ureduce.T, i.T)</span><br><span class="line">            z = np.column_stack((z, t))</span><br><span class="line">        <span class="keyword">return</span> z</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算训练集误差</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">error_analysis</span><span class="params">(self)</span>:</span></span><br><span class="line">        S = self.S</span><br><span class="line">        sigmaK = <span class="number">0</span></span><br><span class="line">        sigmaN = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n):</span><br><span class="line">            <span class="keyword">if</span> i &lt; self.k:</span><br><span class="line">                sigmaK += S[i]</span><br><span class="line">            <span class="keyword">if</span> i &lt; self.n:</span><br><span class="line">                sigmaN += S[i]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> - sigmaK/sigmaN</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 恢复到之前维度</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rovecor_dimensional</span><span class="params">(self)</span>:</span></span><br><span class="line">        Ureduce = self.U[..., <span class="number">0</span>:self.k]</span><br><span class="line">        Xappox = np.dot(Ureduce, self.z)</span><br><span class="line">        <span class="keyword">return</span> Xappox</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self)</span>:</span></span><br><span class="line">        newX = self.data_preprocess()</span><br><span class="line">        sigma = self.covariance_matrix(newX)</span><br><span class="line">        self.U, self.S, self.V = np.linalg.svd(sigma)</span><br><span class="line">        <span class="comment"># 这里使用均值归一化后的X和原X对结果没有影响</span></span><br><span class="line">        <span class="comment">#self.z = self.get_z(self.U, self.X)</span></span><br><span class="line">        self.z = self.get_z(self.U, newX)</span><br><span class="line">        <span class="keyword">return</span> self.z.T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造训练集：引入鸢尾花数据集来作为训练集, 具有四个特征,分三类</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">data = iris.data</span><br><span class="line">data = np.array(data[:])</span><br><span class="line">m = len(data)</span><br><span class="line"><span class="comment">#np.random.shuffle(data)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把四个特征压缩为三个</span></span><br><span class="line">irispca = MYPCA(data, <span class="number">3</span>)</span><br><span class="line">z = irispca.train()</span><br><span class="line">error = irispca.error_analysis()</span><br><span class="line">print(error)</span><br><span class="line">x1 = z[:, [<span class="number">0</span>]]</span><br><span class="line">x2 = z[:, [<span class="number">1</span>]]</span><br><span class="line">x3 = z[:, [<span class="number">2</span>]]</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>)</span><br><span class="line">ax.scatter(x1, x2, x3, c=<span class="string">'r'</span>, marker=<span class="string">'*'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'x1 Label'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'x2 Label'</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">'x3 Label'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把四个特征压缩为一个</span></span><br><span class="line">irispca = MYPCA(data, <span class="number">1</span>)</span><br><span class="line">z = irispca.train()</span><br><span class="line">plt.plot(z, <span class="string">'.'</span>)</span><br><span class="line">error = irispca.error_analysis()</span><br><span class="line">print(error)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Kmeans的算法验证一下是否还可以正确分类</span></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">3</span>, random_state=<span class="number">0</span>).fit(z)</span><br><span class="line">kmeans_u = kmeans.cluster_centers_</span><br><span class="line">u = np.transpose(kmeans_u)</span><br><span class="line">plt.plot([m/<span class="number">6</span>, m/<span class="number">2</span>, <span class="number">5</span>*m/<span class="number">6</span>], u[<span class="number">0</span>], <span class="string">'*'</span>, markerfacecolor=<span class="string">'g'</span>, markeredgecolor=<span class="string">"k"</span>, markersize=<span class="number">14</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="PCAFigure_1.png" alt="image"></p>
<p><img src="PCAFigure_2.png" alt="image"></p>
]]></content>
      <categories>
        <category>机器学习代码实现</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python实现梯度下降</title>
    <url>/2018/04/19/Python%E5%AE%9E%E7%8E%B0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
    <content><![CDATA[<p>看了Andrew Ng的关于机器学习中梯度下降的学习，用最简单粗暴的解法实现下</p>
<p>注意的地方就是<code>$\theta_0,\theta_1$</code>是同时更新的，所以用一个临时变量接了下</p>
<p>收敛条件的判断：可以让函数迭代指定的次数后退出，也可以认为n次迭代的结果和n-1次的结果非常接近时就代表下降到谷底，退出函数</p>
<p>步数alpha的设置和epsilon的选择，这个例子我尝试步数为0.0025时就会振荡无法收敛，epsilon等于于0.001时有时会产生局部最优解，建议是<code>$10^{-4}$</code></p>
<p>明天继续尝试最小二乘法，这个代码写得比较loser，先放这，学完再优化，记录下现在和以后的思考有什么区别<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> math   <span class="comment"># This will import math module</span></span><br><span class="line"><span class="comment"># 构造训练集</span></span><br><span class="line"><span class="comment"># x 特征值</span></span><br><span class="line"><span class="comment"># y 实际结果</span></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">50</span>, <span class="number">1</span>)</span><br><span class="line">m = len(x)</span><br><span class="line">y = x/<span class="number">2</span> + np.random.randn(m) <span class="number">-5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 终止条件</span></span><br><span class="line">loop_max = <span class="number">100000</span>  <span class="comment"># 最大迭代次数(防止死循环)</span></span><br><span class="line">epsilon = <span class="number">1e-4</span>   <span class="comment"># 精确度</span></span><br><span class="line"></span><br><span class="line">alpha = <span class="number">0.002</span>  <span class="comment"># 步长(注意取值过大会导致振荡即不收敛,过小收敛速度变慢)</span></span><br><span class="line">count = <span class="number">0</span>  <span class="comment"># 循环次数</span></span><br><span class="line">finish = <span class="number">0</span>  <span class="comment"># 终止标志</span></span><br><span class="line">theta = np.random.randn(<span class="number">2</span>)<span class="comment"># 初始化theta</span></span><br><span class="line"><span class="comment">#theta = [0.5,-0.5]</span></span><br><span class="line">temp = np.zeros(<span class="number">2</span>)</span><br><span class="line">error = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> count &lt; loop_max:</span><br><span class="line">    count+=<span class="number">1</span></span><br><span class="line">    sum = np.zeros(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        sum[<span class="number">0</span>] = sum[<span class="number">0</span>] + (theta[<span class="number">0</span>] + theta[<span class="number">1</span>] * x[i] - y[i])</span><br><span class="line">    temp0 = theta[<span class="number">0</span>] - alpha * sum[<span class="number">0</span>] / m</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        sum[<span class="number">1</span>] = sum[<span class="number">1</span>]+ (theta[<span class="number">0</span>] + theta[<span class="number">1</span>] * x[i] - y[i]) * x[i]</span><br><span class="line">    temp1 = theta[<span class="number">1</span>] - alpha * sum[<span class="number">1</span>] / m</span><br><span class="line"></span><br><span class="line">    theta[<span class="number">0</span>] = temp0</span><br><span class="line">    theta[<span class="number">1</span>] = temp1</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 判断是否已收敛</span></span><br><span class="line">    <span class="keyword">if</span> abs((sum[<span class="number">1</span>]+ sum[<span class="number">0</span>] - error)) &lt; epsilon:</span><br><span class="line">        finish = <span class="number">1</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        error = sum[<span class="number">1</span>]+ sum[<span class="number">0</span>]</span><br><span class="line">    print(<span class="string">'intercept = %s slope = %s'</span> % (theta[<span class="number">0</span>], theta[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x, y)</span></span><br><span class="line"><span class="comment">#print('intercept = %s slope = %s' % (intercept, slope))</span></span><br><span class="line">print(<span class="string">'loop count = %d\n'</span> % count, theta)</span><br><span class="line">plt.plot(x, y, <span class="string">'r*'</span>)</span><br><span class="line">plt.plot(x, theta[<span class="number">1</span>] * x + theta[<span class="number">0</span>], <span class="string">'g'</span>)</span><br><span class="line"><span class="comment">#plt.plot(x, slope * x + intercept, 'b')</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>偷懒了两天后用normal equation方法实现了，结果和stats.linregress的结果完全一样，注意矩阵需要垂直排列，记录俩函数用来修改矩阵堆叠方式<br><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">vstack()函数 </span><br><span class="line">函数原型：vstack(tup) ，参数tup可以是元组，列表，或者numpy数组，返回结果为numpy的数组 </span><br><span class="line">作用：在垂直方向把元素堆叠起来</span><br><span class="line">&gt;&gt;&gt;import numpy as np</span><br><span class="line">&gt;&gt;&gt;a=[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]]</span><br><span class="line">&gt;&gt;&gt;b=[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]]</span><br><span class="line">&gt;&gt;&gt;c=[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]]</span><br><span class="line">&gt;&gt;&gt;d=[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]]</span><br><span class="line">&gt;&gt;&gt;print(np.vstack((a,b,c,d)))</span><br><span class="line">[[<span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span>]</span><br><span class="line"> [<span class="number">3</span>]</span><br><span class="line"> [<span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span>]</span><br><span class="line"> [<span class="number">3</span>]</span><br><span class="line"> [<span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span>]</span><br><span class="line"> [<span class="number">3</span>]</span><br><span class="line"> [<span class="number">1</span>]</span><br><span class="line"> [<span class="number">2</span>]</span><br><span class="line"> [<span class="number">3</span>]]</span><br><span class="line"> </span><br><span class="line"> stack函数原型为：stack(arrays, axis=<span class="number">0</span>)</span><br><span class="line"> import numpy as np</span><br><span class="line">a=[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">   [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line">print(<span class="string">"列表a如下："</span>)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"增加一维，新维度的下标为0"</span>)</span><br><span class="line">c=np.stack(a,axis=<span class="number">0</span>)</span><br><span class="line">print(c)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"增加一维，新维度的下标为1"</span>)</span><br><span class="line">c=np.stack(a,axis=<span class="number">1</span>)</span><br><span class="line">print(c)</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">列表a如下：</span><br><span class="line">[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">增加一维，新维度下标为<span class="number">0</span></span><br><span class="line">[[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]</span><br><span class="line"> [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>]]</span><br><span class="line">增加一维，新维度下标为<span class="number">1</span></span><br><span class="line">[[<span class="number">1</span> <span class="number">4</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">5</span>]</span><br><span class="line"> [<span class="number">3</span> <span class="number">6</span>]]</span><br></pre></td></tr></table></figure></p>
<p><a href="https://blog.csdn.net/csdn15698845876/article/details/73380803" target="_blank" rel="noopener"> Numpy中stack()，hstack()，vstack()函数详解</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造训练集</span></span><br><span class="line"><span class="comment"># x 特征值</span></span><br><span class="line"><span class="comment"># y 实际结果</span></span><br><span class="line">x1 = np.arange(<span class="number">0</span>, <span class="number">50</span>, <span class="number">1</span>) + np.random.randn(<span class="number">50</span>) <span class="number">-5</span></span><br><span class="line">m = len(x1)</span><br><span class="line">x0 = np.full(m, <span class="number">1.0</span>)</span><br><span class="line">y = x1/<span class="number">2</span> + np.random.randn(m) <span class="number">-5</span></span><br><span class="line">target_data = np.vstack(y)      <span class="comment"># 将结果矩阵修改为垂直方向</span></span><br><span class="line">x = np.stack((x0, x1), axis=<span class="number">1</span>)  <span class="comment"># 构建X矩阵</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#print(x,y)</span></span><br><span class="line">theta = np.dot(np.dot(np.linalg.inv(np.dot(x.T, x)), x.T), target_data)</span><br><span class="line"></span><br><span class="line">print(theta)</span><br><span class="line">slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x1, y)</span><br><span class="line">print(<span class="string">'intercept = %s slope = %s'</span> % (intercept, slope))</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">得到的结果和stats.linregress函数完全一样，猜测这个函数也是如此实现的</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">plt.plot(x1, y, <span class="string">'*'</span>)</span><br><span class="line">plt.plot(x, slope * x + intercept, <span class="string">'b'</span>)</span><br><span class="line">plt.plot(x, theta[<span class="number">1</span>] * x + theta[<span class="number">0</span>], <span class="string">'r'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="1.png" alt=""><br>通过学习别人的代码和修改完成了最终版本，要注意步长和终止条件，步长alpha通过多次尝试最后选取适合值<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造训练集</span></span><br><span class="line"><span class="comment"># x 特征值</span></span><br><span class="line"><span class="comment"># y 实际结果</span></span><br><span class="line">x1 = np.arange(<span class="number">0</span>, <span class="number">50</span>, <span class="number">1</span>) + np.random.randn(<span class="number">50</span>)</span><br><span class="line">m = len(x1)</span><br><span class="line">x0 = np.full(m, <span class="number">1.0</span>)</span><br><span class="line">x = np.vstack([x0, x1]).T</span><br><span class="line">y = x1/<span class="number">2</span> + np.random.randn(m) <span class="number">-5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 两种终止条件</span></span><br><span class="line">loop_max = <span class="number">10000</span>  <span class="comment"># 最大迭代次数(防止死循环)</span></span><br><span class="line">epsilon = <span class="number">1e-4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化权值</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">theta = np.random.randn(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">alpha = <span class="number">0.002</span>  <span class="comment"># 步长(注意取值过大会导致振荡即不收敛,过小收敛速度变慢) 大于0.002会不收敛</span></span><br><span class="line">error = np.zeros(<span class="number">2</span>)</span><br><span class="line">count = <span class="number">0</span>  <span class="comment"># 循环次数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> count &lt; loop_max:</span><br><span class="line">    count += <span class="number">1</span></span><br><span class="line">    delta = np.zeros(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        delta = delta + (np.dot(theta, x[i]) - y[i]) * x[i]/m</span><br><span class="line">    theta = theta - alpha * delta</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 判断是否已收敛</span></span><br><span class="line">    <span class="keyword">if</span> np.linalg.norm(theta - error) &lt; epsilon: <span class="comment"># np.linalg.norm 求范类：平方和，开方</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        error = theta</span><br><span class="line">    print(theta)</span><br><span class="line"></span><br><span class="line">print(theta,count)</span><br><span class="line">slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x1, y)</span><br><span class="line">print(<span class="string">'intercept = %s slope = %s'</span> % (intercept, slope))</span><br><span class="line">plt.plot(x1, y, <span class="string">'g*'</span>)</span><br><span class="line">plt.plot(x, theta[<span class="number">1</span>] * x + theta[<span class="number">0</span>], <span class="string">'r'</span>)</span><br><span class="line">plt.plot(x, slope * x + intercept, <span class="string">'b'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>机器学习代码实现</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python实现K-均值算法</title>
    <url>/2018/08/16/Python%E5%AE%9E%E7%8E%B0K-%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>第一个无监督学习算法，K-均值，这是一个非常普及的聚类算法，实现起来也比较简单，学习了Andrew Ng的视频讲解，直接纪录一下重点吧。</p>
<p>首先训练集合选取了sklearn自带的多类单标签数据集make_blobs</p>
<p>初始化变量有m:训练集的个数，Feature:训练集的维度，K：要分成几类，u：一个K*Feature维度的数组，储存聚类中心，c:储存每次迭代的分类结果，uDict:储存分类结果的字典</p>
<p>总结：因为数据量比较少，根据观察畸变函的结果数，基本迭代三次就分类成功了，说明这是一个非常优秀的算法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建单标签数据集</span></span><br><span class="line">center = [[<span class="number">1</span>,<span class="number">1</span>],[<span class="number">-1</span>,<span class="number">-1</span>],[<span class="number">1</span>,<span class="number">-1</span>]]</span><br><span class="line">cluster_std = <span class="number">0.3</span></span><br><span class="line">X,labels = make_blobs(n_samples=<span class="number">200</span>, centers=center, n_features=<span class="number">2</span>, cluster_std=cluster_std, random_state=<span class="number">0</span>)</span><br><span class="line">unique_lables = set(labels)</span><br><span class="line">colors=plt.cm.Spectral(np.linspace(<span class="number">0</span>, <span class="number">1</span>, len(unique_lables)))</span><br><span class="line"><span class="keyword">for</span> k,col <span class="keyword">in</span> zip(unique_lables, colors):</span><br><span class="line">    x_k=X[labels==k]</span><br><span class="line">    plt.plot(x_k[:,<span class="number">0</span>], x_k[:,<span class="number">1</span>], <span class="string">'o'</span>, markerfacecolor=col, markeredgecolor=<span class="string">"k"</span>, markersize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##########</span></span><br><span class="line">Feature = <span class="number">2</span>  <span class="comment"># 特征数</span></span><br><span class="line">m = <span class="number">200</span>  <span class="comment"># 训练数据个数</span></span><br><span class="line"><span class="comment"># 初始化K 和 聚类中心u</span></span><br><span class="line">K = <span class="number">3</span></span><br><span class="line">u = np.empty([K, Feature])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(K):</span><br><span class="line">    u[i] = random.choice(X)</span><br><span class="line"></span><br><span class="line"><span class="comment">#  c 储存分类结果和距离</span></span><br><span class="line">c = np.zeros([m,<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出初始聚类中心</span></span><br><span class="line">t = np.transpose(u)</span><br><span class="line">plt.plot(t[<span class="number">0</span>], t[<span class="number">1</span>], <span class="string">'+'</span>, markerfacecolor=<span class="string">'g'</span>, markeredgecolor=<span class="string">"k"</span>, markersize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 储存分类结果的字典</span></span><br><span class="line">uDict = &#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 移动聚类中心</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MoveK</span><span class="params">(c)</span>:</span></span><br><span class="line">    u = np.empty([K, Feature])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(K):</span><br><span class="line">        uDict[i] = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(K):</span><br><span class="line">            <span class="keyword">if</span>(c[i][<span class="number">0</span>] == j):</span><br><span class="line">                uDict[j].append(X[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(K):</span><br><span class="line">        sum = np.zeros([<span class="number">1</span>, Feature])</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> uDict[i]:</span><br><span class="line">            sum = np.add(sum, j)</span><br><span class="line">        u[i] = sum/len(uDict[i])</span><br><span class="line">    <span class="keyword">return</span> u</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 畸变函数 Distortion function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Distortion</span><span class="params">(u)</span>:</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> uDict.keys():</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> uDict[i]:</span><br><span class="line">            dis = np.linalg.norm(j - u[i])</span><br><span class="line">            sum += dis * dis</span><br><span class="line">    <span class="keyword">return</span> sum/m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始迭代</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    <span class="comment"># 我希望找到 c[i](代表第i个数据) 距离 u[k]（聚类中心） 最小</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        flag = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(K):</span><br><span class="line">            dis = np.linalg.norm(X[i]-u[j])</span><br><span class="line">            <span class="keyword">if</span>(flag <span class="keyword">or</span> dis &lt; c[i][<span class="number">1</span>]):</span><br><span class="line">                flag = <span class="keyword">False</span></span><br><span class="line">                c[i][<span class="number">0</span>] = j</span><br><span class="line">                c[i][<span class="number">1</span>] = dis</span><br><span class="line">    u = MoveK(c)</span><br><span class="line">    print(Distortion(u))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证结果</span></span><br><span class="line">print(<span class="string">"my kemans cluster enters:"</span>, u)</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">3</span>, random_state=<span class="number">0</span>).fit(X)</span><br><span class="line">kmeans_u = kmeans.cluster_centers_</span><br><span class="line">print(<span class="string">"sklearn kemans cluster enters:"</span>, kmeans_u)</span><br><span class="line"></span><br><span class="line">t = np.transpose(u)</span><br><span class="line">plt.plot(t[<span class="number">0</span>], t[<span class="number">1</span>], <span class="string">'*'</span>, markerfacecolor=<span class="string">'blue'</span>, markeredgecolor=<span class="string">"k"</span>, markersize=<span class="number">14</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>执行结果：</p>
<p>0.8093064467708514<br>0.2770795968584342<br>0.17288024424551154<br>0.17288024424551154<br>0.17288024424551154</p>
<p>my kemans cluster enters: [[ 0.95712283 -1.02057236]<br> [ 1.01281413  1.06595402]<br> [-1.03507066 -1.03233287]]</p>
<p>sklearn kemans cluster enters: [[ 0.95712283 -1.02057236]<br> [ 1.01281413  1.06595402]<br> [-1.03507066 -1.03233287]]</p>
<p> <img src="20180816195610.png" alt="image"></p>
<p>算法成功的从+号的位置移动到五角星的位置。</p>
]]></content>
      <categories>
        <category>机器学习代码实现</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python虚拟环境的搭建</title>
    <url>/2020/02/19/Python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E7%9A%84%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<p>我使用的Ubuntu18已经自带了pyhon3.6，现在我想用pip安装一些其它的应用的版本和现有的有冲突，为了防止冲突，我需要另一个python环境。</p>
<h1 id="python的安装"><a href="#python的安装" class="headerlink" title="python的安装"></a>python的安装</h1><p>这里因为是在Linux系统上，所以使用源码安装。<a href="https://www.python.org/downloads/release/python-381/" target="_blank" rel="noopener">PythonSource</a>下载ptyhon3.8的源码</p>
<p>解压后进入Python-3.8.1文件夹，执行命令<br><figure class="highlight stata"><table><tr><td class="code"><pre><span class="line">$ ./configure</span><br><span class="line">checking build system <span class="keyword">type</span>... x86_64-pc-linux-gnu</span><br><span class="line">checking host system <span class="keyword">type</span>... x86_64-pc-linux-gnu</span><br><span class="line">checking <span class="keyword">for</span> python3.8... <span class="keyword">no</span></span><br><span class="line">checking <span class="keyword">for</span> python3... python3</span><br><span class="line">checking <span class="keyword">for</span> --enable-universalsdk... <span class="keyword">no</span></span><br><span class="line">checking <span class="keyword">for</span> --with-universal-archs... <span class="keyword">no</span></span><br><span class="line">checking MACHDEP... <span class="string">"linux"</span></span><br><span class="line">checking <span class="keyword">for</span> gcc... <span class="keyword">no</span></span><br><span class="line">checking <span class="keyword">for</span> <span class="keyword">cc</span>... <span class="keyword">no</span></span><br><span class="line">checking <span class="keyword">for</span> <span class="keyword">cl</span>.exe... <span class="keyword">no</span></span><br><span class="line">configure: <span class="keyword">error</span>: <span class="keyword">in</span> `/home/void/Python-3.8.1':</span><br><span class="line">configure: <span class="keyword">error</span>: <span class="keyword">no</span> acceptable C compiler found <span class="keyword">in</span> <span class="variable">$PATH</span></span><br><span class="line">See `config.<span class="keyword">log</span>' <span class="keyword">for</span> <span class="keyword">more</span> details</span><br></pre></td></tr></table></figure></p>
<p>出错了，我们竟然没有gcc，cc和cl也没有。</p>
<h1 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h1><p>安装python前先安装需要的依赖软件，如果已经安装则不需要再安装</p>
<p>如果是新安装的系统先更新下apt：</p>
<figure class="highlight q"><table><tr><td class="code"><pre><span class="line">$ sudo apt-<span class="built_in">get</span> <span class="keyword">update</span></span><br></pre></td></tr></table></figure>
<p>先安装gcc：</p>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">$ sudo apt install gcc</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">$ gcc --version</span><br><span class="line">gcc (Ubuntu <span class="number">7.4</span><span class="number">.0</span><span class="number">-1</span>ubuntu1~<span class="number">18.04</span><span class="number">.1</span>) <span class="number">7.4</span><span class="number">.0</span></span><br></pre></td></tr></table></figure>
<p>安装g++</p>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">$ sudo apt install g++</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">$ g++ --version</span><br><span class="line">g++ (Ubuntu <span class="number">7.4</span><span class="number">.0</span><span class="number">-1</span>ubuntu1~<span class="number">18.04</span><span class="number">.1</span>) <span class="number">7.4</span><span class="number">.0</span></span><br></pre></td></tr></table></figure>
<p>连make也没有，安装make</p>
<figure class="highlight clean"><table><tr><td class="code"><pre><span class="line">$ sudo apt install make</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">$ make -v</span><br><span class="line">GNU Make <span class="number">4.1</span></span><br></pre></td></tr></table></figure>
<p>安装其它依赖：<br><figure class="highlight q"><table><tr><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span> install -y make build-essential libssl-<span class="built_in">dev</span> zlib1g-<span class="built_in">dev</span> libbz2-<span class="built_in">dev</span> libreadline-<span class="built_in">dev</span> libsqlite3-<span class="built_in">dev</span> wget curl llvm libncurses5-<span class="built_in">dev</span> libncursesw5-<span class="built_in">dev</span> xz-utils tk-<span class="built_in">dev</span> libffi-<span class="built_in">dev</span> liblzma-<span class="built_in">dev</span> python-openssl</span><br></pre></td></tr></table></figure></p>
<h1 id="构建Python"><a href="#构建Python" class="headerlink" title="构建Python"></a>构建Python</h1><figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line">$ ./configure</span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line">creating Modules/Setup.local</span><br><span class="line">creating Makefile</span><br><span class="line"></span><br><span class="line">$ make</span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line">Python build finished successfully!</span><br><span class="line"></span><br><span class="line">$ sudo make altinstall </span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line">Installing collected packages: setuptools, pip</span><br><span class="line">Successfully installed pip-19.2.3 setuptools-41.2.0</span><br></pre></td></tr></table></figure>
<p><strong>If you want a release build with all stable optimizations active (PGO, etc),<br>please run ./configure –enable-optimizations</strong></p>
<p><strong>警告 make install 可以覆盖或伪装 python3 二进制文件。因此，建议使用 make altinstall 而不是 make install ，因为后者只安装了 exec_prefix/bin/pythonversion 。</strong></p>
<p>检查安装是否成功</p>
<figure class="highlight crystal"><table><tr><td class="code"><pre><span class="line">$ python3.<span class="number">8</span> --version</span><br><span class="line">Python <span class="number">3.8</span>.<span class="number">1</span></span><br><span class="line">$ pip3.<span class="number">8</span> --version</span><br><span class="line">pip <span class="number">19.2</span>.<span class="number">3</span> from /usr/local/<span class="class"><span class="keyword">lib</span>/<span class="title">python3</span>.8/<span class="title">site</span>-<span class="title">packages</span>/<span class="title">pip</span> (<span class="title">python</span> 3.8)</span></span><br></pre></td></tr></table></figure>
<p>顺便更新下pip<br><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">$ sudo pip3.8 <span class="keyword">install</span> <span class="comment">--upgrade pip</span></span><br></pre></td></tr></table></figure></p>
<h1 id="安装虚拟环境"><a href="#安装虚拟环境" class="headerlink" title="安装虚拟环境"></a>安装虚拟环境</h1><p>virtualenv是用来创建一个单独的Python运行环境的工具。</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">$ sudo pip3.<span class="number">8</span> <span class="keyword">install</span> virtualenv</span><br></pre></td></tr></table></figure>
<p>安装virtualenv好后就可以创建一个单独python环境了<br><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">先创建一个存放python环境的文件夹</span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir pyvenv</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> pyenv</span></span><br><span class="line">创建环境</span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo virtualenv env</span></span><br></pre></td></tr></table></figure></p>
<p>新建一个名为env的虚拟环境，并在当前目录下新建同名文件夹</p>
<h1 id="虚拟环境的使用"><a href="#虚拟环境的使用" class="headerlink" title="虚拟环境的使用"></a>虚拟环境的使用</h1><p>使用source命令执行虚拟环境目录中bin/activate文件，将激活虚拟环境，命令行前出现（环境名）表示已在虚拟环境中</p>
<p>激活并使用虚拟环境<br><figure class="highlight mel"><table><tr><td class="code"><pre><span class="line">$ <span class="keyword">source</span> <span class="keyword">env</span>/bin/activate</span><br><span class="line">(<span class="keyword">env</span>) []$ deactive</span><br></pre></td></tr></table></figure></p>
<p>退出环境<br>执行命令deactivate退出虚拟环境</p>
]]></content>
      <categories>
        <category>virtualenv</category>
      </categories>
  </entry>
  <entry>
    <title>Python实现逻辑回归</title>
    <url>/2018/05/04/Python%E5%AE%9E%E7%8E%B0%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<p>首先总结一下学习，虽然叫回归但是和回归没有任何关系，刚尝试写代码时，思考分类问题陷入了线性回归的思路，纠结了好久，已经求出weights但不会拟合直线，后来用笔画了下立刻明白思考偏离了，所以就算有了电脑还是应该用笔在纸上画一画。</p>
<p>写代码首先第一步是要知道做什么：我需要画一个直线，直线公式为 θ0 <em> x0 + θ1 </em> x1 + θ2 * x2 = 0  其中x0 = 1。想要画出这条直线我需要知道三个θ的值，通过吴大大的机器学习视频我知道的把θ的转置乘以x的带入逻辑函数g(z)就能求出预测函数h(x),然后通过梯度下降的方式更新θ，最终得到θ的近似值。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 逻辑函数(Logistic function)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gfunc</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造训练集：引入了鸢尾花数据集来作为训练集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">data = iris.data</span><br><span class="line">target = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取前一百行的第一列和第三列做特征值</span></span><br><span class="line">X = data[<span class="number">0</span>:<span class="number">100</span>, [<span class="number">0</span>, <span class="number">2</span>]]</span><br><span class="line">y = target[<span class="number">0</span>:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出训练集的散点图</span></span><br><span class="line">label = np.array(y)</span><br><span class="line">index_0 = np.where(label == <span class="number">0</span>)</span><br><span class="line">plt.scatter(X[index_0, <span class="number">0</span>], X[index_0, <span class="number">1</span>], marker=<span class="string">'x'</span>, color=<span class="string">'b'</span>, label=<span class="string">'0'</span>, s=<span class="number">15</span>)</span><br><span class="line">index_1 = np.where(label == <span class="number">1</span>)</span><br><span class="line">plt.scatter(X[index_1, <span class="number">0</span>], X[index_1, <span class="number">1</span>], marker=<span class="string">'o'</span>, color=<span class="string">'r'</span>, label=<span class="string">'1'</span>, s=<span class="number">15</span>)</span><br><span class="line">plt.xlabel(<span class="string">'X1'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'X2'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">########################################################</span></span><br><span class="line"><span class="comment"># 训练集构建完成后判断边界，我猜边界是一条直线</span></span><br><span class="line"><span class="comment"># 直线的公式：θ0 * x0 + θ1 * x1 + θ2 * x2 = 0  其中x0 = 1</span></span><br><span class="line"><span class="comment"># 因为这个问题里是一个二维分类，所以边界是有三个θ决定的</span></span><br><span class="line"><span class="comment">########################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集的个数m</span></span><br><span class="line">m = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新构建了X向量 加上了x0=1</span></span><br><span class="line">x0 = np.full(m, <span class="number">1.0</span>)</span><br><span class="line">x0 = np.vstack(x0)</span><br><span class="line">x = np.column_stack((x0, X))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机设置三个theta值</span></span><br><span class="line">theta = np.random.randn(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两种终止条件</span></span><br><span class="line">loop_max = <span class="number">10000</span>  <span class="comment"># 最大迭代次数(防止死循环)</span></span><br><span class="line">epsilon = <span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line">error = np.zeros(<span class="number">3</span>)</span><br><span class="line">count = <span class="number">0</span></span><br><span class="line">alpha = <span class="number">0.001</span>  <span class="comment"># 步长</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> count &lt; loop_max:</span><br><span class="line">    delta = np.zeros(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> i  <span class="keyword">in</span> range(m):</span><br><span class="line">        delta = delta + (gfunc(np.dot(theta, x[i])) - y[i]) * x[i]</span><br><span class="line">    theta = theta - alpha * delta</span><br><span class="line">    <span class="comment"># 判断是否已收敛</span></span><br><span class="line">    <span class="keyword">if</span> np.linalg.norm(theta - error) &lt; epsilon:</span><br><span class="line">        finish = <span class="number">1</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        error = theta</span><br><span class="line">    count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"The number of iterations = "</span>, count)</span><br><span class="line">print(theta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># x0 = 1</span></span><br><span class="line"><span class="comment"># 已经求得theta参数，给出x1的值，根据theta计算x2，画出直线</span></span><br><span class="line">x1 = np.arange(<span class="number">4</span>, <span class="number">7.5</span>, <span class="number">0.5</span>)</span><br><span class="line">x2 = (- theta[<span class="number">0</span>] - theta[<span class="number">1</span>] * x1) / theta[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(x1, x2, color=<span class="string">'black'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="1.png" alt=""><br>后来我通过学习他人的逻辑回归函数，修改步长，观察损失图，发现了些有趣的事，我把代码重构了，更便于可视化</p>
<p><a href="https://github.com/Voidmort/Machine-Learning-Python-Code/blob/master/logistic%20regression2.py" target="_blank" rel="noopener">&gt;逻辑回归源代码&lt;</a></p>
<p>首先我把步长设置为0.001，然后画出loss图：</p>
<p><img src="2.png" alt=""><br><img src="3.png" alt=""><br>0.001的步数大概迭代2500多次达到低谷，从图中中观察到loss损失相当平滑，没有出现震荡</p>
<p>然后我修改了步数为0.01，只通过800次迭代就下降到低谷，但是出现震荡，如果在线性回归中出现震荡则不会收敛，但是在逻辑回归问题中，尽管出现了震荡，但最终还是收敛。<br><img src="4.png" alt=""><br><img src="5.png" alt=""><br>但如果我把步数设置的更大0.02时，就会每1800次后出现震荡的情况，最终无法收敛。</p>
<p><img src="6.png" alt=""></p>
]]></content>
      <categories>
        <category>机器学习代码实现</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow 1</title>
    <url>/2018/12/26/TensorFlow_1/</url>
    <content><![CDATA[<h1 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h1><p>当tensorflow的环境搭建好后我们就可以尝试run下了，先写个hello world看看吧。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    tf.contrib.eager.enable_eager_execution()</span><br><span class="line"><span class="keyword">except</span> ValueError:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">tensor = tf.constant(<span class="string">'Hello, world!'</span>)</span><br><span class="line">tensor_value = tensor.numpy()</span><br><span class="line">print(tensor_value)</span><br></pre></td></tr></table></figure>
<pre><code>b&apos;Hello, world!&apos;
</code></pre><h1 id="TensorFlow-编程概念"><a href="#TensorFlow-编程概念" class="headerlink" title="TensorFlow 编程概念"></a>TensorFlow 编程概念</h1><p>TensorFlow的名称源自张量，张量是任意维度的数组。借助TensorFlow，可以操作具有很大维度的张量。</p>
<pre><code>标量是零维数组（零阶张量）。例如：&apos;hi&apos; 或 3
矢量是一维数组（一阶张量）。例如：[2, 3, 5, 7, 11] 或 [3]
矩阵是二维数组（二阶张量）。例如：[[3.1, 8.2, 5.9][4.3, -2.7, 6.5]]
</code></pre><p>TensorFlow指令会创建，销毁和控制张量。典型TensorFlow程序中的大多数代码都是指令。<br>TensorFlow图（也叫 计算图 或 数据流图）是一种图数据结构。很多TensorFlow程序由单个图构成，但是TensorFlow程序可以选择创建多个图。图的节点是指令；图的边是张量。张量流经图，在每个节点由一个指令操控。一个指令的输出张量通常会变成后续指令的输入张量。TensorFlow会实现延迟执行模型，意味着系统仅会根据相关节点的需求在需要时计算节点。</p>
<p>张量可以作为常量或变量储存在图中。常量储存的是值是不会发生更改的张量，而变量储存的值是会发生更改的张量。常量和变量都只是图中的一种指令。常量是始终会返回同一张量值得指令。变量是会返回分配给得任何张量的指令。</p>
<p>要定义常量，使用tf.constant指令，并传入它的值。例如：</p>
<pre><code>x = tf.constant([1.2])
</code></pre><p>同样，可以创建变量：</p>
<pre><code>y = tf.Variable([3])

改变值： y = y.assign([1])
</code></pre><p>创建好变量或常量后，可以对它们使用其他指令（如tf.add）。</p>
<p>图必须在TensorFlow会话中运行，会话储存了它所运行的图的状态：</p>
<pre><code>将 tf.Session()作为会话：
    initialization = tf.global_variables_initializer()
    print(y.eval())
</code></pre><p>在使用tf.Variable时可以调用tf.global_variables_initializer，以明确初始化这些变量。</p>
<p>注意：会话可以将图分发到多个机器上执行（假设程序在某个分布式计算框架上运行）。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>TensorFlow编程有两个流程：</p>
<pre><code>1.将常量，变量和指令整合到一个图中。
2.在一个会话中评估这些常量，变量和指令。
</code></pre><h2 id="创建一个简单的-TensorFlow-程序"><a href="#创建一个简单的-TensorFlow-程序" class="headerlink" title="创建一个简单的 TensorFlow 程序"></a>创建一个简单的 TensorFlow 程序</h2><p>我们来看看如何编写一个将两个常量相加的简单 TensorFlow 程序。</p>
<h3 id="添加-import-语句"><a href="#添加-import-语句" class="headerlink" title="添加 import 语句"></a>添加 import 语句</h3><p>想要运行tensorflow程序，必须添加这句：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<p>其他常见的import语句包括：</p>
<pre><code>import matplotlib.pyplot as plt # 数据可视化
import numpy as np              # 较低级的数学python库
import pandas as pd             # 较高级的数学python库
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个图</span></span><br><span class="line">g = tf.Graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    <span class="comment"># 创建三个量，</span></span><br><span class="line">    x = tf.constant(<span class="number">8</span>, name=<span class="string">"x_const"</span>)</span><br><span class="line">    y = tf.constant(<span class="number">5</span>, name=<span class="string">"y_const"</span>)</span><br><span class="line">    sum = tf.add(x, y, name=<span class="string">"x_y_sum"</span>)</span><br><span class="line">    python</span><br><span class="line">    <span class="comment"># 创建一个会话，将会执行默认图</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        print(sum.eval())</span><br></pre></td></tr></table></figure>
<pre><code>TF already imported with eager execution!
13
</code></pre>]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow 10</title>
    <url>/2019/02/25/TensorFlow_10/</url>
    <content><![CDATA[<h1 id="稀疏数据和嵌入简介"><a href="#稀疏数据和嵌入简介" class="headerlink" title="稀疏数据和嵌入简介"></a>稀疏数据和嵌入简介</h1><ul>
<li>将影评字符串数据转换为稀疏特征矢量</li>
<li>使用稀疏特征矢量实现情感分析线性模型</li>
<li>通过将数据投射到二维空间的嵌入来实现情感分析 DNN 模型</li>
<li>将嵌入可视化，以便查看模型学到的词语之间的关系</li>
</ul>
<p>在此练习中，我们将探讨稀疏数据，并使用影评文本数据（来自 <a href="http://ai.stanford.edu/~amaas/data/sentiment/" target="_blank" rel="noopener">ACL 2011 IMDB 数据集</a>）进行嵌入。这些数据已被处理成 <code>tf.Example</code> 格式。</p>
<h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2><p>我们导入依赖项并下载训练数据和测试数据。<a href="https://www.tensorflow.org/api_docs/python/tf/keras" target="_blank" rel="noopener"><code>tf.keras</code></a> 中包含一个文件下载和缓存工具，我们可以用它来检索数据集。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line">tf.logging.set_verbosity(tf.logging.ERROR)</span><br><span class="line">train_url = <span class="string">'https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/train.tfrecord'</span></span><br><span class="line">train_path = tf.keras.utils.get_file(train_url.split(<span class="string">'/'</span>)[<span class="number">-1</span>], train_url)</span><br><span class="line">test_url = <span class="string">'https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/test.tfrecord'</span></span><br><span class="line">test_path = tf.keras.utils.get_file(test_url.split(<span class="string">'/'</span>)[<span class="number">-1</span>], test_url)</span><br></pre></td></tr></table></figure>
<pre><code>Downloading data from https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/train.tfrecord
41631744/41625533 [==============================] - 0s 0us/step
Downloading data from https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/test.tfrecord
40689664/40688441 [==============================] - 0s 0us/step
</code></pre><h2 id="构建情感分析模型"><a href="#构建情感分析模型" class="headerlink" title="构建情感分析模型"></a>构建情感分析模型</h2><p>  我们根据这些数据训练一个情感分析模型，以预测某条评价总体上是<em>好评</em>（标签为 1）还是<em>差评</em>（标签为 0）。</p>
<p>为此，我们会使用<em>词汇表</em>（即我们预计将在数据中看到的每个术语的列表），将字符串值 <code>terms</code> 转换为特征矢量。在本练习中，我们创建了侧重于一组有限术语的小型词汇表。其中的大多数术语明确表示是<em>好评</em>或<em>差评</em>，但有些只是因为有趣而被添加进来。</p>
<p>词汇表中的每个术语都与特征矢量中的一个坐标相对应。为了将样本的字符串值 <code>terms</code> 转换为这种矢量格式，我们按以下方式处理字符串值：如果该术语没有出现在样本字符串中，则坐标值将为 0；如果出现在样本字符串中，则值为 1。未出现在该词汇表中的样本中的术语将被弃用。</p>
<p> <strong>注意</strong>：<em>我们当然可以使用更大的词汇表，而且有创建此类词汇表的专用工具。此外，我们可以添加少量的 OOV（未收录词汇）分桶，您可以在其中对词汇表中未包含的术语进行哈希处理，而不仅仅是弃用这些术语。我们还可以使用<strong>特征哈希</strong>法对每个术语进行哈希处理，而不是创建显式词汇表。这在实践中很有效，但却不具备可解读性（这对本练习非常实用）。如需了解处理此类词汇表的工具，请参阅 tf.feature_column 模块。</em></p>
<h2 id="构建输入管道"><a href="#构建输入管道" class="headerlink" title="构建输入管道"></a>构建输入管道</h2><p>  首先，我们来配置输入管道，以将数据导入 TensorFlow 模型中。我们可以使用以下函数来解析训练数据和测试数据（格式为 <a href="https://www.tensorflow.org/guide/datasets#consuming_tfrecord_data" target="_blank" rel="noopener">TFRecord</a>），然后返回一个由特征和相应标签组成的字典。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_parse_function</span><span class="params">(record)</span>:</span></span><br><span class="line">  <span class="string">"""Extracts features and labels.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    record: File path to a TFRecord file    </span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A `tuple` `(labels, features)`:</span></span><br><span class="line"><span class="string">      features: A dict of tensors representing the features</span></span><br><span class="line"><span class="string">      labels: A tensor with the corresponding labels.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  features = &#123;</span><br><span class="line">    <span class="string">"terms"</span>: tf.VarLenFeature(dtype=tf.string), <span class="comment"># terms are strings of varying lengths</span></span><br><span class="line">    <span class="string">"labels"</span>: tf.FixedLenFeature(shape=[<span class="number">1</span>], dtype=tf.float32) <span class="comment"># labels are 0 or 1</span></span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  parsed_features = tf.parse_single_example(record, features)</span><br><span class="line">  </span><br><span class="line">  terms = parsed_features[<span class="string">'terms'</span>].values</span><br><span class="line">  labels = parsed_features[<span class="string">'labels'</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span>  &#123;<span class="string">'terms'</span>:terms&#125;, labels</span><br></pre></td></tr></table></figure>
<p> 为了确认函数是否能正常运行，我们为训练数据构建一个 <code>TFRecordDataset</code>，并使用上述函数将数据映射到特征和标签。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create the Dataset object.</span></span><br><span class="line">ds = tf.data.TFRecordDataset(train_path)</span><br><span class="line"><span class="comment"># Map features and labels with the parse function.</span></span><br><span class="line">ds = ds.map(_parse_function)</span><br><span class="line"></span><br><span class="line">ds</span><br></pre></td></tr></table></figure>
<pre><code>&lt;DatasetV1Adapter shapes: ({terms: (?,)}, (1,)), types: ({terms: tf.string}, tf.float32)&gt;
</code></pre><p> 运行以下单元，以从训练数据集中获取第一个样本。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n = ds.make_one_shot_iterator().get_next()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(n)</span><br></pre></td></tr></table></figure>
<pre><code>({&apos;terms&apos;: array([b&apos;but&apos;, b&apos;it&apos;, b&apos;does&apos;, b&apos;have&apos;, b&apos;some&apos;, b&apos;good&apos;, b&apos;action&apos;,
         b&apos;and&apos;, b&apos;a&apos;, b&apos;plot&apos;, b&apos;that&apos;, b&apos;is&apos;, b&apos;somewhat&apos;, b&apos;interesting&apos;,
         b&apos;.&apos;, b&apos;nevsky&apos;, b&apos;acts&apos;, b&apos;like&apos;, b&apos;a&apos;, b&apos;body&apos;, b&apos;builder&apos;,
         b&apos;and&apos;, b&apos;he&apos;, b&apos;isn&apos;, b&quot;&apos;&quot;, b&apos;t&apos;, b&apos;all&apos;, b&apos;that&apos;, b&apos;attractive&apos;,
         b&apos;,&apos;, b&apos;in&apos;, b&apos;fact&apos;, b&apos;,&apos;, b&apos;imo&apos;, b&apos;,&apos;, b&apos;he&apos;, b&apos;is&apos;, b&apos;ugly&apos;,
         b&apos;.&apos;, b&apos;(&apos;, b&apos;his&apos;, b&apos;acting&apos;, b&apos;skills&apos;, b&apos;lack&apos;, b&apos;everything&apos;,
         b&apos;!&apos;, b&apos;)&apos;, b&apos;sascha&apos;, b&apos;is&apos;, b&apos;played&apos;, b&apos;very&apos;, b&apos;well&apos;, b&apos;by&apos;,
         b&apos;joanna&apos;, b&apos;pacula&apos;, b&apos;,&apos;, b&apos;but&apos;, b&apos;she&apos;, b&apos;needed&apos;, b&apos;more&apos;,
         b&apos;lines&apos;, b&apos;than&apos;, b&apos;she&apos;, b&apos;was&apos;, b&apos;given&apos;, b&apos;,&apos;, b&apos;her&apos;,
         b&apos;character&apos;, b&apos;needed&apos;, b&apos;to&apos;, b&apos;be&apos;, b&apos;developed&apos;, b&apos;.&apos;,
         b&apos;there&apos;, b&apos;are&apos;, b&apos;way&apos;, b&apos;too&apos;, b&apos;many&apos;, b&apos;men&apos;, b&apos;in&apos;, b&apos;this&apos;,
         b&apos;story&apos;, b&apos;,&apos;, b&apos;there&apos;, b&apos;is&apos;, b&apos;zero&apos;, b&apos;romance&apos;, b&apos;,&apos;, b&apos;too&apos;,
         b&apos;much&apos;, b&apos;action&apos;, b&apos;,&apos;, b&apos;and&apos;, b&apos;way&apos;, b&apos;too&apos;, b&apos;dumb&apos;, b&apos;of&apos;,
         b&apos;an&apos;, b&apos;ending&apos;, b&apos;.&apos;, b&apos;it&apos;, b&apos;is&apos;, b&apos;very&apos;, b&apos;violent&apos;, b&apos;.&apos;,
         b&apos;i&apos;, b&apos;did&apos;, b&apos;however&apos;, b&apos;love&apos;, b&apos;the&apos;, b&apos;scenery&apos;, b&apos;,&apos;,
         b&apos;this&apos;, b&apos;movie&apos;, b&apos;takes&apos;, b&apos;you&apos;, b&apos;all&apos;, b&apos;over&apos;, b&apos;the&apos;,
         b&apos;world&apos;, b&apos;,&apos;, b&apos;and&apos;, b&apos;that&apos;, b&apos;is&apos;, b&apos;a&apos;, b&apos;bonus&apos;, b&apos;.&apos;, b&apos;i&apos;,
         b&apos;also&apos;, b&apos;liked&apos;, b&apos;how&apos;, b&apos;it&apos;, b&apos;had&apos;, b&apos;some&apos;, b&apos;stuff&apos;,
         b&apos;about&apos;, b&apos;the&apos;, b&apos;mafia&apos;, b&apos;in&apos;, b&apos;it&apos;, b&apos;,&apos;, b&apos;not&apos;, b&apos;too&apos;,
         b&apos;much&apos;, b&apos;or&apos;, b&apos;too&apos;, b&apos;little&apos;, b&apos;,&apos;, b&apos;but&apos;, b&apos;enough&apos;,
         b&apos;that&apos;, b&apos;it&apos;, b&apos;got&apos;, b&apos;my&apos;, b&apos;attention&apos;, b&apos;.&apos;, b&apos;the&apos;,
         b&apos;actors&apos;, b&apos;needed&apos;, b&apos;to&apos;, b&apos;be&apos;, b&apos;more&apos;, b&apos;handsome&apos;, b&apos;.&apos;,
         b&apos;.&apos;, b&apos;.&apos;, b&apos;the&apos;, b&apos;biggest&apos;, b&apos;problem&apos;, b&apos;i&apos;, b&apos;had&apos;, b&apos;was&apos;,
         b&apos;that&apos;, b&apos;nevsky&apos;, b&apos;was&apos;, b&apos;just&apos;, b&apos;too&apos;, b&apos;normal&apos;, b&apos;,&apos;,
         b&apos;not&apos;, b&apos;sexy&apos;, b&apos;enough&apos;, b&apos;.&apos;, b&apos;i&apos;, b&apos;think&apos;, b&apos;for&apos;, b&apos;most&apos;,
         b&apos;guys&apos;, b&apos;,&apos;, b&apos;sascha&apos;, b&apos;will&apos;, b&apos;be&apos;, b&apos;hot&apos;, b&apos;enough&apos;, b&apos;,&apos;,
         b&apos;but&apos;, b&apos;for&apos;, b&apos;us&apos;, b&apos;ladies&apos;, b&apos;that&apos;, b&apos;are&apos;, b&apos;fans&apos;, b&apos;of&apos;,
         b&apos;action&apos;, b&apos;,&apos;, b&apos;nevsky&apos;, b&apos;just&apos;, b&apos;doesn&apos;, b&quot;&apos;&quot;, b&apos;t&apos;, b&apos;cut&apos;,
         b&apos;it&apos;, b&apos;.&apos;, b&apos;overall&apos;, b&apos;,&apos;, b&apos;this&apos;, b&apos;movie&apos;, b&apos;was&apos;, b&apos;fine&apos;,
         b&apos;,&apos;, b&apos;i&apos;, b&apos;didn&apos;, b&quot;&apos;&quot;, b&apos;t&apos;, b&apos;love&apos;, b&apos;it&apos;, b&apos;nor&apos;, b&apos;did&apos;,
         b&apos;i&apos;, b&apos;hate&apos;, b&apos;it&apos;, b&apos;,&apos;, b&apos;just&apos;, b&apos;found&apos;, b&apos;it&apos;, b&apos;to&apos;, b&apos;be&apos;,
         b&apos;another&apos;, b&apos;normal&apos;, b&apos;action&apos;, b&apos;flick&apos;, b&apos;.&apos;], dtype=object)},
 array([0.], dtype=float32))
</code></pre><p> 现在，我们构建一个正式的输入函数，可以将其传递给 TensorFlow Estimator 对象的 <code>train()</code> 方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create an input_fn that parses the tf.Examples from the given files,</span></span><br><span class="line"><span class="comment"># and split them into features and targets.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_input_fn</span><span class="params">(input_filenames, num_epochs=None, shuffle=True)</span>:</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Same code as above; create a dataset and map features and labels.</span></span><br><span class="line">  ds = tf.data.TFRecordDataset(input_filenames)</span><br><span class="line">  ds = ds.map(_parse_function)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> shuffle:</span><br><span class="line">    ds = ds.shuffle(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Our feature data is variable-length, so we pad and batch</span></span><br><span class="line">  <span class="comment"># each field of the dataset structure to whatever size is necessary.     </span></span><br><span class="line">  ds = ds.padded_batch(<span class="number">25</span>, ds.output_shapes)</span><br><span class="line">  </span><br><span class="line">  ds = ds.repeat(num_epochs)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Return the next batch of data.</span></span><br><span class="line">  features, labels = ds.make_one_shot_iterator().get_next()</span><br><span class="line">  <span class="keyword">return</span> features, labels</span><br></pre></td></tr></table></figure>
<h2 id="使用具有稀疏输入和显式词汇表的线性模型"><a href="#使用具有稀疏输入和显式词汇表的线性模型" class="headerlink" title="使用具有稀疏输入和显式词汇表的线性模型"></a>使用具有稀疏输入和显式词汇表的线性模型</h2><p>对于我们的第一个模型，我们将使用 50 个信息性术语来构建 <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/LinearClassifier" target="_blank" rel="noopener"><code>LinearClassifier</code></a> 模型；始终从简单入手！</p>
<p>以下代码将为我们的术语构建特征列。<a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list" target="_blank" rel="noopener"><code>categorical_column_with_vocabulary_list</code></a> 函数可使用“字符串-特征矢量”映射来创建特征列。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 50 informative terms that compose our model vocabulary. </span></span><br><span class="line">informative_terms = (<span class="string">"bad"</span>, <span class="string">"great"</span>, <span class="string">"best"</span>, <span class="string">"worst"</span>, <span class="string">"fun"</span>, <span class="string">"beautiful"</span>,</span><br><span class="line">                     <span class="string">"excellent"</span>, <span class="string">"poor"</span>, <span class="string">"boring"</span>, <span class="string">"awful"</span>, <span class="string">"terrible"</span>,</span><br><span class="line">                     <span class="string">"definitely"</span>, <span class="string">"perfect"</span>, <span class="string">"liked"</span>, <span class="string">"worse"</span>, <span class="string">"waste"</span>,</span><br><span class="line">                     <span class="string">"entertaining"</span>, <span class="string">"loved"</span>, <span class="string">"unfortunately"</span>, <span class="string">"amazing"</span>,</span><br><span class="line">                     <span class="string">"enjoyed"</span>, <span class="string">"favorite"</span>, <span class="string">"horrible"</span>, <span class="string">"brilliant"</span>, <span class="string">"highly"</span>,</span><br><span class="line">                     <span class="string">"simple"</span>, <span class="string">"annoying"</span>, <span class="string">"today"</span>, <span class="string">"hilarious"</span>, <span class="string">"enjoyable"</span>,</span><br><span class="line">                     <span class="string">"dull"</span>, <span class="string">"fantastic"</span>, <span class="string">"poorly"</span>, <span class="string">"fails"</span>, <span class="string">"disappointing"</span>,</span><br><span class="line">                     <span class="string">"disappointment"</span>, <span class="string">"not"</span>, <span class="string">"him"</span>, <span class="string">"her"</span>, <span class="string">"good"</span>, <span class="string">"time"</span>,</span><br><span class="line">                     <span class="string">"?"</span>, <span class="string">"."</span>, <span class="string">"!"</span>, <span class="string">"movie"</span>, <span class="string">"film"</span>, <span class="string">"action"</span>, <span class="string">"comedy"</span>,</span><br><span class="line">                     <span class="string">"drama"</span>, <span class="string">"family"</span>)</span><br><span class="line"></span><br><span class="line">terms_feature_column = tf.feature_column.categorical_column_with_vocabulary_list(key=<span class="string">"terms"</span>, vocabulary_list=informative_terms)</span><br></pre></td></tr></table></figure>
<p> 接下来，我们将构建 <code>LinearClassifier</code>，在训练集中训练该模型，并在评估集中对其进行评估。阅读上述代码后，运行该模型以了解其效果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_optimizer = tf.train.AdagradOptimizer(learning_rate=<span class="number">0.1</span>)</span><br><span class="line">my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line"></span><br><span class="line">feature_columns = [ terms_feature_column ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">classifier = tf.estimator.LinearClassifier(</span><br><span class="line">  feature_columns=feature_columns,</span><br><span class="line">  optimizer=my_optimizer,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">classifier.train(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([train_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">evaluation_metrics = classifier.evaluate(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([train_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line">print(<span class="string">"Training set metrics:"</span>)</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> evaluation_metrics:</span><br><span class="line">  print(m, evaluation_metrics[m])</span><br><span class="line">print(<span class="string">"---"</span>)</span><br><span class="line"></span><br><span class="line">evaluation_metrics = classifier.evaluate(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([test_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test set metrics:"</span>)</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> evaluation_metrics:</span><br><span class="line">  print(m, evaluation_metrics[m])</span><br><span class="line">print(<span class="string">"---"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Training set metrics:
accuracy 0.78928
accuracy_baseline 0.5
auc 0.87206453
auc_precision_recall 0.8640158
average_loss 0.45088252
label/mean 0.5
loss 11.272063
precision 0.77057767
prediction/mean 0.4956976
recall 0.82384
global_step 1000
---
Test set metrics:
accuracy 0.78504
accuracy_baseline 0.5
auc 0.86939275
auc_precision_recall 0.8610384
average_loss 0.4532239
label/mean 0.5
loss 11.330598
precision 0.7680963
prediction/mean 0.49426404
recall 0.81664
global_step 1000
---
</code></pre><h2 id="使用深度神经网络-DNN-模型"><a href="#使用深度神经网络-DNN-模型" class="headerlink" title="使用深度神经网络 (DNN) 模型"></a>使用深度神经网络 (DNN) 模型</h2><p>上述模型是一个线性模型，效果非常好。但是，我们可以使用 DNN 模型实现更好的效果吗？</p>
<p>我们将 <code>LinearClassifier</code> 切换为 <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier" target="_blank" rel="noopener"><code>DNNClassifier</code></a>。运行以下单元，看看您的模型效果如何。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">##################### Here's what we changed ##################################</span></span><br><span class="line">classifier = tf.estimator.DNNClassifier(                                      <span class="comment">#</span></span><br><span class="line">  feature_columns=[tf.feature_column.indicator_column(terms_feature_column)], <span class="comment">#</span></span><br><span class="line">  hidden_units=[<span class="number">20</span>,<span class="number">20</span>],                                                       <span class="comment">#</span></span><br><span class="line">  optimizer=my_optimizer,                                                     <span class="comment">#</span></span><br><span class="line">)                                                                             <span class="comment">#</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  classifier.train(</span><br><span class="line">    input_fn=<span class="keyword">lambda</span>: _input_fn([train_path]),</span><br><span class="line">    steps=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">  evaluation_metrics = classifier.evaluate(</span><br><span class="line">    input_fn=<span class="keyword">lambda</span>: _input_fn([train_path]),</span><br><span class="line">    steps=<span class="number">1</span>)</span><br><span class="line">  print(<span class="string">"Training set metrics:"</span>)</span><br><span class="line">  <span class="keyword">for</span> m <span class="keyword">in</span> evaluation_metrics:</span><br><span class="line">    print(m, evaluation_metrics[m])</span><br><span class="line">  print(<span class="string">"---"</span>)</span><br><span class="line"></span><br><span class="line">  evaluation_metrics = classifier.evaluate(</span><br><span class="line">    input_fn=<span class="keyword">lambda</span>: _input_fn([test_path]),</span><br><span class="line">    steps=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  print(<span class="string">"Test set metrics:"</span>)</span><br><span class="line">  <span class="keyword">for</span> m <span class="keyword">in</span> evaluation_metrics:</span><br><span class="line">    print(m, evaluation_metrics[m])</span><br><span class="line">  print(<span class="string">"---"</span>)</span><br><span class="line"><span class="keyword">except</span> ValueError <span class="keyword">as</span> err:</span><br><span class="line">  print(err)</span><br></pre></td></tr></table></figure>
<pre><code>Training set metrics:
accuracy 0.92
accuracy_baseline 0.68
auc 0.9705881
auc_precision_recall 0.9885154
average_loss 0.33181748
label/mean 0.68
loss 8.295437
precision 1.0
prediction/mean 0.52073544
recall 0.88235295
global_step 1000
---
Test set metrics:
accuracy 0.8
accuracy_baseline 0.56
auc 0.75974023
auc_precision_recall 0.66889143
average_loss 0.7257034
label/mean 0.56
loss 18.142586
precision 0.84615386
prediction/mean 0.4270074
recall 0.78571427
global_step 1000
---
</code></pre><h2 id="在-DNN-模型中使用嵌入"><a href="#在-DNN-模型中使用嵌入" class="headerlink" title="在 DNN 模型中使用嵌入"></a>在 DNN 模型中使用嵌入</h2><p>在此任务中，我们将使用嵌入列来实现 DNN 模型。嵌入列会将稀疏数据作为输入，并返回一个低维度密集矢量作为输出。</p>
<p> <strong>注意</strong>：<em>从计算方面而言，embedding_column 通常是用于在稀疏数据中训练模型最有效的选项。在此练习末尾的<a href="#scrollTo=XDMlGgRfKSVz">可选部分</a>，我们将更深入地讨论使用 <code>embedding_column</code> 与 <code>indicator_column</code> 之间的实现差异，以及如何在这两者之间做出权衡。</em></p>
<p> 在下面的代码中，执行以下操作：</p>
<ul>
<li>通过将数据投射到二维空间的 <code>embedding_column</code> 来为模型定义特征列（如需详细了解 <code>embedding_column</code> 的函数签名，请参阅相关 <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column" target="_blank" rel="noopener">TF 文档</a>）。</li>
<li><p>定义符合以下规范的 <code>DNNClassifier</code>：</p>
<ul>
<li>具有两个隐藏层，每个包含 20 个单元</li>
<li>采用学习速率为 0.1 的 AdaGrad 优化方法</li>
<li><code>gradient_clip_norm 值为 5.0</code></li>
</ul>
<p><strong>注意</strong>：<em>在实践中，我们可能会将数据投射到 2 维以上（比如 50 或 100）的空间中。但就目前而言，2 维是比较容易可视化的维数。</em></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">########################## SOLUTION CODE ########################################</span></span><br><span class="line">terms_embedding_column = tf.feature_column.embedding_column(terms_feature_column, dimension=<span class="number">2</span>)</span><br><span class="line">feature_columns = [ terms_embedding_column ]</span><br><span class="line"></span><br><span class="line">my_optimizer = tf.train.AdagradOptimizer(learning_rate=<span class="number">0.1</span>)</span><br><span class="line">my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line"></span><br><span class="line">classifier = tf.estimator.DNNClassifier(</span><br><span class="line">  feature_columns=feature_columns,</span><br><span class="line">  hidden_units=[<span class="number">20</span>,<span class="number">20</span>],</span><br><span class="line">  optimizer=my_optimizer</span><br><span class="line">)</span><br><span class="line"><span class="comment">#################################################################################</span></span><br><span class="line"></span><br><span class="line">classifier.train(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([train_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">evaluation_metrics = classifier.evaluate(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([train_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line">print(<span class="string">"Training set metrics:"</span>)</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> evaluation_metrics:</span><br><span class="line">  print(m, evaluation_metrics[m])</span><br><span class="line">print(<span class="string">"---"</span>)</span><br><span class="line"></span><br><span class="line">evaluation_metrics = classifier.evaluate(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([test_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test set metrics:"</span>)</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> evaluation_metrics:</span><br><span class="line">  print(m, evaluation_metrics[m])</span><br><span class="line">print(<span class="string">"---"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Training set metrics:
accuracy 0.78516
accuracy_baseline 0.5
auc 0.8685013
auc_precision_recall 0.8568284
average_loss 0.45557868
label/mean 0.5
loss 11.389467
precision 0.7566789
prediction/mean 0.52443045
recall 0.84064
global_step 1000
---
Test set metrics:
accuracy 0.78168
accuracy_baseline 0.5
auc 0.8668425
auc_precision_recall 0.85428405
average_loss 0.45733798
label/mean 0.5
loss 11.433449
precision 0.7556637
prediction/mean 0.52328736
recall 0.83256
global_step 1000
---
</code></pre><h2 id="确信模型中确实存在嵌入"><a href="#确信模型中确实存在嵌入" class="headerlink" title="确信模型中确实存在嵌入"></a>确信模型中确实存在嵌入</h2><p>上述模型使用了 <code>embedding_column</code>，而且似乎很有效，但这并没有让我们了解到内部发生的情形。我们如何检查该模型确实在内部使用了嵌入？</p>
<p>首先，我们来看看该模型中的张量：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classifier.get_variable_names()</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;dnn/hiddenlayer_0/bias&apos;,
 &apos;dnn/hiddenlayer_0/bias/t_0/Adagrad&apos;,
 &apos;dnn/hiddenlayer_0/kernel&apos;,
 &apos;dnn/hiddenlayer_0/kernel/t_0/Adagrad&apos;,
 &apos;dnn/hiddenlayer_1/bias&apos;,
 &apos;dnn/hiddenlayer_1/bias/t_0/Adagrad&apos;,
 &apos;dnn/hiddenlayer_1/kernel&apos;,
 &apos;dnn/hiddenlayer_1/kernel/t_0/Adagrad&apos;,
 &apos;dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights&apos;,
 &apos;dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights/t_0/Adagrad&apos;,
 &apos;dnn/logits/bias&apos;,
 &apos;dnn/logits/bias/t_0/Adagrad&apos;,
 &apos;dnn/logits/kernel&apos;,
 &apos;dnn/logits/kernel/t_0/Adagrad&apos;,
 &apos;global_step&apos;]
</code></pre><p> 好的，我们可以看到这里有一个嵌入层：<code>&#39;dnn/input_from_feature_columns/input_layer/terms_embedding/...&#39;</code>。（顺便说一下，有趣的是，该层可以与模型的其他层一起训练，就像所有隐藏层一样。）</p>
<p>嵌入层的形状是否正确？请运行以下代码来查明。</p>
<p> <strong>注意</strong>：<em>在我们的示例中，嵌入是一个矩阵，可让我们将一个 50 维矢量投射到 2 维空间。</em></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> neure <span class="keyword">in</span> classifier.get_variable_names():</span><br><span class="line">    print(classifier.get_variable_value(neure).shape, <span class="string">": "</span> + neure)</span><br></pre></td></tr></table></figure>
<pre><code>(20,) : dnn/hiddenlayer_0/bias
(20,) : dnn/hiddenlayer_0/bias/t_0/Adagrad
(2, 20) : dnn/hiddenlayer_0/kernel
(2, 20) : dnn/hiddenlayer_0/kernel/t_0/Adagrad
(20,) : dnn/hiddenlayer_1/bias
(20,) : dnn/hiddenlayer_1/bias/t_0/Adagrad
(20, 20) : dnn/hiddenlayer_1/kernel
(20, 20) : dnn/hiddenlayer_1/kernel/t_0/Adagrad
(50, 2) : dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights
(50, 2) : dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights/t_0/Adagrad
(1,) : dnn/logits/bias
(1,) : dnn/logits/bias/t_0/Adagrad
(20, 1) : dnn/logits/kernel
(20, 1) : dnn/logits/kernel/t_0/Adagrad
() : global_step
</code></pre><p>花些时间来手动检查各个层及其形状，以确保一切都按照您预期的方式互相连接。</p>
<h2 id="检查嵌入"><a href="#检查嵌入" class="headerlink" title="检查嵌入"></a>检查嵌入</h2><p>现在，我们来看看实际嵌入空间，并了解术语最终所在的位置。请执行以下操作：</p>
<ol>
<li><p>运行以下代码来查看我们在训练的嵌入。一切最终是否如您所预期的那样？</p>
</li>
<li><p>重新运行<strong>在 DNN 模型中使用嵌 </strong> 中的代码来重新训练该模型，然后再次运行下面的嵌入可视化。哪些保持不变？哪些发生了变化？</p>
</li>
<li><p>最后，仅使用 10 步来重新训练该模型（这将产生一个糟糕的模型）。再次运行下面的嵌入可视化。您现在看到了什么？为什么？</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">embedding_matrix = classifier.get_variable_value(<span class="string">'dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> term_index <span class="keyword">in</span> range(len(informative_terms)):</span><br><span class="line">  <span class="comment"># Create a one-hot encoding for our term.  It has 0s everywhere, except for</span></span><br><span class="line">  <span class="comment"># a single 1 in the coordinate that corresponds to that term.</span></span><br><span class="line">  term_vector = np.zeros(len(informative_terms))</span><br><span class="line">  term_vector[term_index] = <span class="number">1</span></span><br><span class="line">  <span class="comment"># We'll now project that one-hot vector into the embedding space.</span></span><br><span class="line">  embedding_xy = np.matmul(term_vector, embedding_matrix)</span><br><span class="line">  plt.text(embedding_xy[<span class="number">0</span>],</span><br><span class="line">           embedding_xy[<span class="number">1</span>],</span><br><span class="line">           informative_terms[term_index])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do a little setup to make sure the plot displays nicely.</span></span><br><span class="line">plt.rcParams[<span class="string">"figure.figsize"</span>] = (<span class="number">15</span>, <span class="number">15</span>)</span><br><span class="line">plt.xlim(<span class="number">1.2</span> * embedding_matrix.min(), <span class="number">1.2</span> * embedding_matrix.max())</span><br><span class="line">plt.ylim(<span class="number">1.2</span> * embedding_matrix.min(), <span class="number">1.2</span> * embedding_matrix.max())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="TensorFlow_10_30_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">embedding_matrix = classifier.get_variable_value(<span class="string">'dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> term_index <span class="keyword">in</span> range(len(informative_terms)):</span><br><span class="line">  <span class="comment"># Create a one-hot encoding for our term.  It has 0s everywhere, except for</span></span><br><span class="line">  <span class="comment"># a single 1 in the coordinate that corresponds to that term.</span></span><br><span class="line">  term_vector = np.zeros(len(informative_terms))</span><br><span class="line">  term_vector[term_index] = <span class="number">1</span></span><br><span class="line">  <span class="comment"># We'll now project that one-hot vector into the embedding space.</span></span><br><span class="line">  embedding_xy = np.matmul(term_vector, embedding_matrix)</span><br><span class="line">  plt.text(embedding_xy[<span class="number">0</span>],</span><br><span class="line">           embedding_xy[<span class="number">1</span>],</span><br><span class="line">           informative_terms[term_index])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do a little setup to make sure the plot displays nicely.</span></span><br><span class="line">plt.rcParams[<span class="string">"figure.figsize"</span>] = (<span class="number">15</span>, <span class="number">15</span>)</span><br><span class="line">plt.xlim(<span class="number">1.2</span> * embedding_matrix.min(), <span class="number">1.2</span> * embedding_matrix.max())</span><br><span class="line">plt.ylim(<span class="number">1.2</span> * embedding_matrix.min(), <span class="number">1.2</span> * embedding_matrix.max())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="TensorFlow_10_31_0.png" alt="png"></p>
<h2 id="任务-6：尝试改进模型的效果"><a href="#任务-6：尝试改进模型的效果" class="headerlink" title="任务 6：尝试改进模型的效果"></a>任务 6：尝试改进模型的效果</h2><p>看看您能否优化该模型以改进其效果。您可以尝试以下几种做法：</p>
<ul>
<li><strong>更改超参数</strong>或<strong>使用其他优化工具</strong>，比如 Adam（通过遵循这些策略，您的准确率可能只会提高一两个百分点）。</li>
<li><strong>向 <code>informative_terms</code> 中添加其他术语。</strong>此数据集有一个完整的词汇表文件，其中包含 30716 个术语，您可以在以下位置找到该文件：<a href="https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/terms.txt" target="_blank" rel="noopener">https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/terms.txt</a> 您可以从该词汇表文件中挑选出其他术语，也可以通过 <code>categorical_column_with_vocabulary_file</code> 特征列使用整个词汇表文件。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Download the vocabulary file.</span></span><br><span class="line">terms_url = <span class="string">'https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/terms.txt'</span></span><br><span class="line">terms_path = tf.keras.utils.get_file(terms_url.split(<span class="string">'/'</span>)[<span class="number">-1</span>], terms_url)</span><br></pre></td></tr></table></figure>
<pre><code>Downloading data from https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/terms.txt
253952/253538 [==============================] - 0s 0us/step
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create a feature column from "terms", using a full vocabulary file.</span></span><br><span class="line">informative_terms = <span class="keyword">None</span></span><br><span class="line"><span class="keyword">with</span> io.open(terms_path, <span class="string">'r'</span>, encoding=<span class="string">'utf8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">  <span class="comment"># Convert it to a set first to remove duplicates.</span></span><br><span class="line">  informative_terms = list(set(f.read().split()))</span><br><span class="line">  </span><br><span class="line">terms_feature_column = tf.feature_column.categorical_column_with_vocabulary_list(key=<span class="string">"terms"</span>, </span><br><span class="line">                                                                                 vocabulary_list=informative_terms)</span><br><span class="line"></span><br><span class="line">terms_embedding_column = tf.feature_column.embedding_column(terms_feature_column, dimension=<span class="number">2</span>)</span><br><span class="line">feature_columns = [ terms_embedding_column ]</span><br><span class="line"></span><br><span class="line">my_optimizer = tf.train.AdagradOptimizer(learning_rate=<span class="number">0.1</span>)</span><br><span class="line">my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line"></span><br><span class="line">classifier = tf.estimator.DNNClassifier(</span><br><span class="line">  feature_columns=feature_columns,</span><br><span class="line">  hidden_units=[<span class="number">10</span>,<span class="number">10</span>],</span><br><span class="line">  optimizer=my_optimizer</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">classifier.train(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([train_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">evaluation_metrics = classifier.evaluate(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([train_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line">print(<span class="string">"Training set metrics:"</span>)</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> evaluation_metrics:</span><br><span class="line">  print(m, evaluation_metrics[m])</span><br><span class="line">print(<span class="string">"---"</span>)</span><br><span class="line"></span><br><span class="line">evaluation_metrics = classifier.evaluate(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([test_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test set metrics:"</span>)</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> evaluation_metrics:</span><br><span class="line">  print(m, evaluation_metrics[m])</span><br><span class="line">print(<span class="string">"---"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Training set metrics:
accuracy 0.82
accuracy_baseline 0.5
auc 0.89789164
auc_precision_recall 0.8937925
average_loss 0.4075714
label/mean 0.5
loss 10.189285
precision 0.83664364
prediction/mean 0.4748372
recall 0.79528
global_step 1000
---
Test set metrics:
accuracy 0.8048
accuracy_baseline 0.5
auc 0.88663244
auc_precision_recall 0.8821298
average_loss 0.42734343
label/mean 0.5
loss 10.683586
precision 0.8235394
prediction/mean 0.47395515
recall 0.77584
global_step 1000
---
</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我们可能获得了比我们原来的线性模型更好且具有嵌入的 DNN 解决方案，但线性模型也相当不错，而且训练速度快得多。线性模型的训练速度之所以更快，是因为它们没有太多要更新的参数或要反向传播的层。</p>
<p>在有些应用中，线性模型的速度可能非常关键，或者从质量的角度来看，线性模型可能完全够用。在其他领域，DNN 提供的额外模型复杂性和能力可能更重要。在定义模型架构时，请记得要充分探讨您的问题，以便知道自己所处的情形。</p>
<h3 id="可选内容：在-embedding-column-与-indicator-column-之间进行权衡"><a href="#可选内容：在-embedding-column-与-indicator-column-之间进行权衡" class="headerlink" title="可选内容：在 embedding_column 与 indicator_column 之间进行权衡"></a><em>可选内容：</em>在 <code>embedding_column</code> 与 <code>indicator_column</code> 之间进行权衡</h3><p>从概念上讲，在训练 <code>LinearClassifier</code> 或 <code>DNNClassifier</code> 时，需要根据实际情况使用稀疏列。TF 提供了两个选项：<code>embedding_column</code> 或 <code>indicator_column</code>。</p>
<p>在训练 LinearClassifier（如<strong>使用具有稀疏输入和显式词汇表的线性模型</strong> 中所示）时，系统在后台使用了 <code>embedding_column</code>。正如<strong>使用深度神经网络 (DNN) 模型</strong> 中所示，在训练 <code>DNNClassifier</code> 时，您必须明确选择 <code>embedding_column</code> 或 <code>indicator_column</code>。本部分通过一个简单的示例讨论了这两者之间的区别，以及如何在二者之间进行权衡。</p>
<p> 假设我们的稀疏数据包含 <code>&quot;great&quot;</code>、<code>&quot;beautiful&quot;</code> 和 <code>&quot;excellent&quot;</code> 这几个值。由于我们在此处使用的词汇表大小为 $V = 50$，因此第一层中的每个单元（神经元）的权重将为 50。我们用 $s$ 表示稀疏输入中的项数。对于此示例稀疏数据，$s = 3$。对于具有 $V$ 个可能值的输入层，带有 $d$ 个单元的隐藏层需要运行一次“矢量 - 矩阵”乘法运算：$(1 \times V) <em> (V \times d)$。此运算会产生 $O(V </em> d)$ 的计算成本。请注意，此成本与隐藏层中的权重数成正比，而与 $s$ 无关。</p>
<p>如果输入使用 <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/indicator_column" target="_blank" rel="noopener"><code>indicator_column</code></a> 进行了独热编码（长度为 $V$ 的布尔型矢量，存在用 1 表示，其余则为 0），这表示很多零进行了相乘和相加运算。</p>
<p> 当我们通过使用大小为 $d$ 的 <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column" target="_blank" rel="noopener"><code>embedding_column</code></a> 获得完全相同的结果时，我们将仅查询与示例输入中存在的 3 个特征 <code>&quot;great&quot;</code>、<code>&quot;beautiful&quot;</code> 和 <code>&quot;excellent&quot;</code> 相对应的嵌入并将这三个嵌入相加：$(1 \times d) + (1 \times d) + (1 \times d)$。由于不存在的特征的权重在“矢量-矩阵”乘法中与 0 相乘，因此对结果没有任何影响；而存在的特征的权重在“矢量-矩阵”乘法中与 1 相乘。因此，将通过嵌入查询获得的权重相加会获得与“矢量-矩阵”乘法相同的结果。</p>
<p>当使用嵌入时，计算嵌入查询是一个 $O(s <em> d)$ 计算；从计算方面而言，它比稀疏数据中的 <code>indicator_column</code> 的 $O(V </em> d)$ 更具成本效益，因为 $s$ 远远小于 $V$。（请注意，这些嵌入是临时学习的结果。在任何指定的训练迭代中，都是当前查询的权重。</p>
<p> 正如我们在<strong>在 DNN 模型中使用嵌入</strong> 中看到的，通过在训练 <code>DNNClassifier</code> 过程中使用 <code>embedding_column</code>，我们的模型学习了特征的低维度表示法，其中点积定义了一个针对目标任务的相似性指标。在本例中，影评中使用的相似术语（例如 <code>&quot;great&quot;</code> 和 <code>&quot;excellent&quot;</code>）在嵌入空间中彼此之间距离较近（即具有较大的点积），而相异的术语（例如 <code>&quot;great&quot;</code> 和 <code>&quot;bad&quot;</code>）在嵌入空间中彼此之间距离较远（即具有较小的点积）。</p>
]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow 2</title>
    <url>/2018/12/28/TensorFlow_2/</url>
    <content><![CDATA[<h1 id="创建和控制张量"><a href="#创建和控制张量" class="headerlink" title="创建和控制张量"></a>创建和控制张量</h1><h2 id="矢量加法"><a href="#矢量加法" class="headerlink" title="矢量加法"></a>矢量加法</h2><p>可以对张量执行金典的数学运算，试着创建一些矢量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  tf.contrib.eager.enable_eager_execution()</span><br><span class="line">  print(<span class="string">"TF imported with eager execution!"</span>)</span><br><span class="line"><span class="keyword">except</span> ValueError:</span><br><span class="line">  print(<span class="string">"TF already imported with eager execution!"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个包含质数的‘primes’矢量</span></span><br><span class="line">primes = tf.constant([<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">11</span> ,<span class="number">13</span>], dtype=tf.int32)</span><br><span class="line">print(<span class="string">"primes:"</span>, primes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个值全为 1 的 ones 矢量</span></span><br><span class="line">ones = tf.ones([<span class="number">6</span>], dtype=tf.int32)</span><br><span class="line">print(ones)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个通过对前两个矢量执行元素级加法而创建的矢量。</span></span><br><span class="line">just_beyond_primes = tf.add(primes, ones)</span><br><span class="line">print(just_beyond_primes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把primes中的元素乘二</span></span><br><span class="line">twos = tf.constant([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], dtype=tf.int32)</span><br><span class="line">primes_doubled = primes * twos</span><br><span class="line">print(primes_doubled)</span><br></pre></td></tr></table></figure>
<pre><code>TF imported with eager execution!
primes: tf.Tensor([ 2  3  5  7 11 13], shape=(6,), dtype=int32)
tf.Tensor([1 1 1 1 1 1], shape=(6,), dtype=int32)
tf.Tensor([ 3  4  6  8 12 14], shape=(6,), dtype=int32)
tf.Tensor([ 4  6 10 14 22 26], shape=(6,), dtype=int32)
</code></pre><p>输出的张量不仅会返回值，还会返回形状shape，以及储存在张量中的值的类型。调用numpy方法会以NumPy数组的形式返回。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">some_matrix = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=tf.int32)</span><br><span class="line">print(some_matrix)</span><br><span class="line">print(<span class="string">"\nnumpy matrix:\n"</span>, some_matrix.numpy())</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(
[[1 2 3]
 [4 5 6]], shape=(2, 3), dtype=int32)

numpy matrix:
 [[1 2 3]
 [4 5 6]]
</code></pre><h2 id="张量形状"><a href="#张量形状" class="headerlink" title="张量形状"></a>张量形状</h2><p>shape 是用来描述张量维度大小和数量。张量的形状表示为list，其中第 <code>i</code> 个元素表示维度 <code>i</code> 的大小。列表的长度表示张量的阶（即维数）。</p>
<p>如果是二维的则shape=(行数， 列数)</p>
<p>例如shape=(n1, n2, n3, …, x, y)<br>则说明 一共有 (n1 x n2 x n3 x ….)个x行y列的数组构成。</p>
<p>例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 一个标量</span></span><br><span class="line">scalar = tf.zeros([])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个有三个元素的向量</span></span><br><span class="line">vector = tf.zeros([<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个两行三列的矩阵</span></span><br><span class="line">matrix = tf.zeros([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">matrix2 = tf.zeros([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">print(<span class="string">'scalar has shape'</span>, scalar.get_shape(), <span class="string">'and value:\n'</span>, scalar.numpy())</span><br><span class="line">print(<span class="string">'vector has shape'</span>, vector.get_shape(), <span class="string">'and value:\n'</span>, vector.numpy())</span><br><span class="line">print(<span class="string">'matrix has shape'</span>, matrix.get_shape(), <span class="string">'and value:\n'</span>, matrix.numpy())</span><br><span class="line">print(<span class="string">'matrix2 has shape'</span>, matrix2.get_shape(), <span class="string">'and value:\n'</span>, matrix2.numpy())</span><br></pre></td></tr></table></figure>
<pre><code>scalar has shape () and value:
 0.0
vector has shape (3,) and value:
 [0. 0. 0.]
matrix has shape (2, 3) and value:
 [[0. 0. 0.]
 [0. 0. 0.]]
matrix2 has shape (2, 3, 4, 5) and value:
 [[[[0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]]

  [[0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]]

  [[0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]]]


 [[[0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]]

  [[0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]]

  [[0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]
   [0. 0. 0. 0. 0.]]]]
</code></pre><h2 id="广播"><a href="#广播" class="headerlink" title="广播"></a>广播</h2><p>在数学中，您只能对形状相同的张量执行元素级运算（例如，<em>相加</em>和<em>等于</em>）。不过，在 TensorFlow 中，您可以对张量执行传统意义上不可行的运算。TensorFlow 支持<strong>广播</strong>（一种借鉴自 NumPy 的概念）。利用广播，元素级运算中的较小数组会增大到与较大数组具有相同的形状。例如，通过广播：</p>
<ul>
<li>如果运算需要大小为 <code>[6]</code> 的张量，则大小为 <code>[1]</code> 或 <code>[]</code> 的张量可以作为运算数。</li>
<li>如果运算需要大小为 <code>[4, 6]</code> 的张量，则以下任何大小的张量都可以作为运算数：<ul>
<li><code>[1, 6]</code></li>
<li><code>[6]</code></li>
<li><code>[]</code></li>
</ul>
</li>
<li><p>如果运算需要大小为 <code>[3, 5, 6]</code> 的张量，则以下任何大小的张量都可以作为运算数：</p>
<ul>
<li><code>[1, 5, 6]</code></li>
<li><code>[3, 1, 6]</code></li>
<li><code>[3, 5, 1]</code></li>
<li><code>[1, 1, 1]</code></li>
<li><code>[5, 6]</code></li>
<li><code>[1, 6]</code></li>
<li><code>[6]</code></li>
<li><code>[1]</code></li>
<li><code>[]</code></li>
</ul>
</li>
</ul>
<p><strong>注意：</strong>当张量被广播时，从概念上来说，系统会<strong>复制</strong>其条目（出于性能考虑，实际并不复制。广播专为实现性能优化而设计）。</p>
<p>有关完整的广播规则集，请参阅简单易懂的 <a href="http://docs.scipy.org/doc/numpy-1.10.1/user/basics.broadcasting.html" target="_blank" rel="noopener">NumPy 广播文档</a>。</p>
<p>以下代码执行了与之前一样的张量运算，不过使用的是标量值（而不是全包含 <code>1</code> 或全包含 <code>2</code> 的矢量）和广播。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">primes = tf.constant([<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">13</span>], dtype=tf.int32)</span><br><span class="line">print(<span class="string">"primes:"</span>, primes)</span><br><span class="line"></span><br><span class="line">one = tf.constant(<span class="number">1</span>, dtype=tf.int32)</span><br><span class="line">print(<span class="string">"one:"</span>, one)</span><br><span class="line"></span><br><span class="line">just_beyond_primes = tf.add(primes, one)</span><br><span class="line">print(<span class="string">"just_beyond_primes:"</span>, just_beyond_primes)</span><br><span class="line"></span><br><span class="line">two = tf.constant(<span class="number">2</span>, dtype=tf.int32)</span><br><span class="line">primes_doubled = primes * two</span><br><span class="line">print(primes_doubled)</span><br></pre></td></tr></table></figure>
<pre><code>primes: tf.Tensor([ 2  3  5  7 11 13], shape=(6,), dtype=int32)
one: tf.Tensor(1, shape=(), dtype=int32)
just_beyond_primes: tf.Tensor([ 3  4  6  8 12 14], shape=(6,), dtype=int32)
tf.Tensor([ 4  6 10 14 22 26], shape=(6,), dtype=int32)
</code></pre><h3 id="练习-1：矢量运算。"><a href="#练习-1：矢量运算。" class="headerlink" title="练习 1：矢量运算。"></a>练习 1：矢量运算。</h3><p>执行矢量运算以创建一个“just_under_primes_squared”矢量，其中第 <code>i</code> 个元素等于 <code>primes</code> 中第 <code>i</code> 个元素的平方减 1。例如，第二个元素为 <code>3 * 3 - 1 = 8</code>。</p>
<p>使用 <code>tf.multiply</code> 或 <code>tf.pow</code> 操作可求得 <code>primes</code> 矢量中每个元素值的平方。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">solution</span><span class="params">(primes)</span>:</span></span><br><span class="line">    primes_squared = tf.pow(primes, <span class="number">2</span>)  <span class="comment"># or tf.multiply(primes, primes)</span></span><br><span class="line">    one = tf.constant(<span class="number">1</span>, dtype=tf.int32)</span><br><span class="line">    just_under_primes_squared = tf.subtract(primes_squared, one)</span><br><span class="line">    <span class="keyword">return</span> just_under_primes_squared</span><br><span class="line"></span><br><span class="line">primes = tf.constant([<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">13</span>], dtype=tf.int32)</span><br><span class="line">just_under_primes_squared = solution(primes)</span><br><span class="line">print(just_under_primes_squared)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor([  3   8  24  48 120 168], shape=(6,), dtype=int32)
</code></pre><h2 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h2><p>在线性代数中，当两个矩阵相乘时，第一个矩阵的<em>列</em>数必须等于第二个矩阵的<em>行</em>数。</p>
<ul>
<li><code>3x4</code> 矩阵乘以 <code>4x2</code> 矩阵是 <strong>_有效_</strong> 的，可以得出一个 <code>3x2</code> 矩阵。</li>
<li><code>4x2</code> 矩阵乘以 <code>3x4</code> 矩阵是 <strong>_无效_</strong> 的。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 一个3x4的矩阵</span></span><br><span class="line">x = tf.constant([[<span class="number">5</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">5</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">-2</span>], [<span class="number">-1</span>, <span class="number">3</span>, <span class="number">-1</span>, <span class="number">-2</span>]],</span><br><span class="line">                dtype=tf.int32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个4x2的矩阵</span></span><br><span class="line">y = tf.constant([[<span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">5</span>], [<span class="number">4</span>, <span class="number">5</span>], [<span class="number">1</span>, <span class="number">6</span>]], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果是一个3x2的矩阵</span></span><br><span class="line">matrix_multiply_result = tf.matmul(x, y)</span><br><span class="line"></span><br><span class="line">print(matrix_multiply_result)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(
[[35 58]
 [35 33]
 [ 1 -4]], shape=(3, 2), dtype=int32)
</code></pre><h2 id="张量变形"><a href="#张量变形" class="headerlink" title="张量变形"></a>张量变形</h2><p>由于张量加法和矩阵乘法均对运算数施加了限制条件，TensorFlow 编程者需要频繁改变张量的形状。</p>
<p>您可以使用 <code>tf.reshape</code> 方法改变张量的形状。<br>例如，您可以将 8x2 张量变形为 2x8 张量或 4x4 张量(改变形状形成的新矩阵元素数和之前必须一样)：</p>
<p>此外，您还可以使用 <code>tf.reshape</code> 更改张量的维数（“阶”）。<br>例如，您可以将 8x2 张量变形为三维 2x2x4 张量或一维 16 元素张量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建一个8x2的矩阵</span></span><br><span class="line">matrix = tf.constant(</span><br><span class="line">    [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>], [<span class="number">11</span>, <span class="number">12</span>], [<span class="number">13</span>, <span class="number">14</span>], [<span class="number">15</span>, <span class="number">16</span>]],</span><br><span class="line">    dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">reshaped_2x8_matrix = tf.reshape(matrix, [<span class="number">2</span>, <span class="number">8</span>])</span><br><span class="line">reshaped_4x4_matrix = tf.reshape(matrix, [<span class="number">4</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Original matrix (8x2):"</span>)</span><br><span class="line">print(matrix.numpy())</span><br><span class="line">print(<span class="string">"Reshaped matrix (2x8):"</span>)</span><br><span class="line">print(reshaped_2x8_matrix.numpy())</span><br><span class="line">print(<span class="string">"Reshaped matrix (4x4):"</span>)</span><br><span class="line">print(reshaped_4x4_matrix.numpy())</span><br><span class="line"></span><br><span class="line">reshaped_2x2x4_tensor = tf.reshape(matrix, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line">one_dimensional_vector = tf.reshape(matrix, [<span class="number">16</span>])</span><br><span class="line">print(<span class="string">"Reshaped 3-D tensor (2x2x4):"</span>)</span><br><span class="line">print(reshaped_2x2x4_tensor.numpy())</span><br><span class="line">print(<span class="string">"1-D vector:"</span>)</span><br><span class="line">print(one_dimensional_vector.numpy())</span><br></pre></td></tr></table></figure>
<pre><code>Original matrix (8x2):
[[ 1  2]
 [ 3  4]
 [ 5  6]
 [ 7  8]
 [ 9 10]
 [11 12]
 [13 14]
 [15 16]]
Reshaped matrix (2x8):
[[ 1  2  3  4  5  6  7  8]
 [ 9 10 11 12 13 14 15 16]]
Reshaped matrix (4x4):
[[ 1  2  3  4]
 [ 5  6  7  8]
 [ 9 10 11 12]
 [13 14 15 16]]
Reshaped 3-D tensor (2x2x4):
[[[ 1  2  3  4]
  [ 5  6  7  8]]

 [[ 9 10 11 12]
  [13 14 15 16]]]
1-D vector:
[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]
</code></pre><h3 id="练习-2：改变两个张量的形状，使其能够相乘。"><a href="#练习-2：改变两个张量的形状，使其能够相乘。" class="headerlink" title="练习 2：改变两个张量的形状，使其能够相乘。"></a>练习 2：改变两个张量的形状，使其能够相乘。</h3><p>下面两个矢量无法进行矩阵乘法运算：</p>
<ul>
<li><code>a = tf.constant([5, 3, 2, 7, 1, 4])</code></li>
<li><code>b = tf.constant([4, 6, 3])</code></li>
</ul>
<p>请改变这两个矢量的形状，使其成为可以进行矩阵乘法运算的运算数。<br>然后，对变形后的张量调用矩阵乘法运算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([<span class="number">5</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">b = tf.constant([<span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">reshaped_a = tf.reshape(a, [<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">reshaped_b = tf.reshape(b, [<span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">c = tf.matmul(reshaped_a, reshaped_b)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"reshaped_a (2x3):"</span>)</span><br><span class="line">print(reshaped_a.numpy())</span><br><span class="line">print(<span class="string">"reshaped_b (3x1):"</span>)</span><br><span class="line">print(reshaped_b.numpy())</span><br><span class="line">print(<span class="string">"reshaped_a x reshaped_b (2x1):"</span>)</span><br><span class="line">print(c.numpy())</span><br></pre></td></tr></table></figure>
<pre><code>reshaped_a (2x3):
[[5 3 2]
 [7 1 4]]
reshaped_b (3x1):
[[4]
 [6]
 [3]]
reshaped_a x reshaped_b (2x1):
[[44]
 [46]]
</code></pre><h2 id="变量、初始化和赋值"><a href="#变量、初始化和赋值" class="headerlink" title="变量、初始化和赋值"></a>变量、初始化和赋值</h2><p>到目前为止，我们执行的所有运算都针对的是静态值 (<code>tf.constant</code>)；调用 <code>numpy()</code> 始终返回同一结果。在 TensorFlow 中可以定义 <code>Variable</code> 对象，它的值是可以更改的。</p>
<p>创建变量时，您可以明确设置一个初始值，也可以使用初始化程序（例如分布）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建初始值为3的标量变量</span></span><br><span class="line">v = tf.contrib.eager.Variable([<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个形状为[1,4]的矢量变量，其初始值为随机的</span></span><br><span class="line"><span class="comment"># 从均值为1，标准差为0.35的正态分布中取样</span></span><br><span class="line">w = tf.contrib.eager.Variable(tf.random_normal([<span class="number">1</span>, <span class="number">4</span>], mean=<span class="number">1.0</span>, stddev=<span class="number">0.35</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"v:"</span>, v.numpy())</span><br><span class="line">print(<span class="string">"w:"</span>, w.numpy())</span><br></pre></td></tr></table></figure>
<pre><code>v: [3]
w: [[0.7752843 1.516361  1.1726708 0.9872638]]
</code></pre><p>要更改变量的值，请使用 <code>assign</code> 操作，并且向变量赋予新值时，其形状必须和之前的形状一致。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v = tf.contrib.eager.Variable([<span class="number">3</span>])</span><br><span class="line">print(v.numpy())</span><br><span class="line"></span><br><span class="line">tf.assign(v, [<span class="number">7</span>])</span><br><span class="line">print(v.numpy())</span><br><span class="line"></span><br><span class="line">v.assign([<span class="number">5</span>])</span><br><span class="line">print(v.numpy())</span><br><span class="line"></span><br><span class="line">v = tf.contrib.eager.Variable([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">print(v.numpy())</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  print(<span class="string">"Assigning [7, 8, 9] to v"</span>)</span><br><span class="line">  v.assign([<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"><span class="keyword">except</span> ValueError <span class="keyword">as</span> e:</span><br><span class="line">  print(<span class="string">"Exception:"</span>, e)</span><br></pre></td></tr></table></figure>
<pre><code>[3]
[7]
[5]
[[1 2 3]
 [4 5 6]]
Assigning [7, 8, 9] to v
Exception: Shapes (2, 3) and (3,) are incompatible
</code></pre><h3 id="练习-3：模拟投掷两个骰子-10-次。"><a href="#练习-3：模拟投掷两个骰子-10-次。" class="headerlink" title="练习 3：模拟投掷两个骰子 10 次。"></a>练习 3：模拟投掷两个骰子 10 次。</h3><p>创建一个骰子模拟，在模拟中生成一个 <code>10x3</code> 二维张量，其中：</p>
<ul>
<li>列 <code>1</code> 和 <code>2</code> 均存储一个六面骰子（值为 1-6）的一次投掷值。</li>
<li>列 <code>3</code> 存储同一行中列 <code>1</code> 和 <code>2</code> 的值的总和。</li>
</ul>
<p>例如，第一行中可能会包含以下值：</p>
<ul>
<li>列 <code>1</code> 存储 <code>4</code></li>
<li>列 <code>2</code> 存储 <code>3</code></li>
<li>列 <code>3</code> 存储 <code>7</code></li>
</ul>
<p>要完成此任务，您需要浏览 <a href="https://www.tensorflow.org/api_guides/python/array_ops" target="_blank" rel="noopener">TensorFlow 文档</a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">die1 = tf.contrib.eager.Variable(</span><br><span class="line">    tf.random_uniform([<span class="number">10</span>, <span class="number">1</span>], minval=<span class="number">1</span>, maxval=<span class="number">7</span>, dtype=tf.int32))</span><br><span class="line">die2 = tf.contrib.eager.Variable(</span><br><span class="line">    tf.random_uniform([<span class="number">10</span>, <span class="number">1</span>], minval=<span class="number">1</span>, maxval=<span class="number">7</span>, dtype=tf.int32))</span><br><span class="line"></span><br><span class="line">dice_sum = tf.add(die1, die2)</span><br><span class="line">resulting_matrix = tf.concat(values=[die1, die2, dice_sum], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(resulting_matrix)</span><br></pre></td></tr></table></figure>
<pre><code>tf.Tensor(
[[ 2  3  5]
 [ 5  6 11]
 [ 3  3  6]
 [ 5  6 11]
 [ 2  1  3]
 [ 6  5 11]
 [ 1  4  5]
 [ 1  1  2]
 [ 4  6 10]
 [ 5  4  9]], shape=(10, 3), dtype=int32)
</code></pre>]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow 3</title>
    <url>/2018/12/29/TensorFlow_3/</url>
    <content><![CDATA[<h1 id="使用TensorFlow的基本步骤"><a href="#使用TensorFlow的基本步骤" class="headerlink" title="使用TensorFlow的基本步骤"></a>使用TensorFlow的基本步骤</h1><h1 id="添加必要的库"><a href="#添加必要的库" class="headerlink" title="添加必要的库"></a>添加必要的库</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> gridspec</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line">tf.logging.set_verbosity(tf.logging.ERROR)</span><br><span class="line">pd.options.display.max_rows = <span class="number">10</span>  <span class="comment"># 最大显示行数</span></span><br><span class="line">pd.options.display.float_format = <span class="string">'&#123;:.1f&#125;'</span>.format  <span class="comment"># 精确度 保留一位小数</span></span><br></pre></td></tr></table></figure>
<h1 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h1><p>加载的数据集，数据基于加利福尼亚州1990年的人口普查数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">california_housing_dataframe = pd.read_csv(<span class="string">"https://download.mlcc.google.cn/mledu-datasets/california_housing_train.csv"</span>, sep=<span class="string">","</span>)</span><br></pre></td></tr></table></figure>
<p>初始化数据集，对数据集进行随机化处理，以确保不会出现损害随机梯度下降的效果。此外，我们会将 <code>median_house_value</code> 调整为以千为单位，这样，模型就能够以常用范围内的学习速率较为轻松地学习这些数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">california_housing_dataframe = california_housing_dataframe.reindex(</span><br><span class="line">    np.random.permutation(california_housing_dataframe.index))</span><br><span class="line">california_housing_dataframe[<span class="string">"median_house_value"</span>] /= <span class="number">1000.0</span></span><br><span class="line">california_housing_dataframe</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>median_house_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>13098</th>
      <td>-121.9</td>
      <td>37.6</td>
      <td>20.0</td>
      <td>1309.0</td>
      <td>184.0</td>
      <td>514.0</td>
      <td>172.0</td>
      <td>11.0</td>
      <td>475.8</td>
    </tr>
    <tr>
      <th>6576</th>
      <td>-118.3</td>
      <td>34.1</td>
      <td>52.0</td>
      <td>1261.0</td>
      <td>616.0</td>
      <td>2309.0</td>
      <td>581.0</td>
      <td>1.6</td>
      <td>225.0</td>
    </tr>
    <tr>
      <th>12732</th>
      <td>-121.8</td>
      <td>37.7</td>
      <td>17.0</td>
      <td>3112.0</td>
      <td>872.0</td>
      <td>1392.0</td>
      <td>680.0</td>
      <td>3.0</td>
      <td>172.5</td>
    </tr>
    <tr>
      <th>6505</th>
      <td>-118.3</td>
      <td>34.0</td>
      <td>34.0</td>
      <td>1462.0</td>
      <td>394.0</td>
      <td>1310.0</td>
      <td>351.0</td>
      <td>1.2</td>
      <td>90.1</td>
    </tr>
    <tr>
      <th>339</th>
      <td>-116.9</td>
      <td>32.7</td>
      <td>9.0</td>
      <td>2652.0</td>
      <td>393.0</td>
      <td>1355.0</td>
      <td>362.0</td>
      <td>6.3</td>
      <td>293.1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>6741</th>
      <td>-118.3</td>
      <td>33.9</td>
      <td>41.0</td>
      <td>896.0</td>
      <td>198.0</td>
      <td>605.0</td>
      <td>168.0</td>
      <td>2.3</td>
      <td>128.1</td>
    </tr>
    <tr>
      <th>496</th>
      <td>-117.0</td>
      <td>33.7</td>
      <td>13.0</td>
      <td>16148.0</td>
      <td>3474.0</td>
      <td>6159.0</td>
      <td>3232.0</td>
      <td>2.0</td>
      <td>97.8</td>
    </tr>
    <tr>
      <th>9140</th>
      <td>-119.0</td>
      <td>35.4</td>
      <td>30.0</td>
      <td>227.0</td>
      <td>75.0</td>
      <td>169.0</td>
      <td>101.0</td>
      <td>1.4</td>
      <td>60.0</td>
    </tr>
    <tr>
      <th>2610</th>
      <td>-117.7</td>
      <td>34.1</td>
      <td>33.0</td>
      <td>2081.0</td>
      <td>409.0</td>
      <td>1008.0</td>
      <td>375.0</td>
      <td>2.6</td>
      <td>138.1</td>
    </tr>
    <tr>
      <th>6827</th>
      <td>-118.3</td>
      <td>34.0</td>
      <td>35.0</td>
      <td>1090.0</td>
      <td>345.0</td>
      <td>1605.0</td>
      <td>330.0</td>
      <td>2.2</td>
      <td>152.8</td>
    </tr>
  </tbody>
</table>
<p>17000 rows × 9 columns</p>
</div>

<h1 id="检查数据"><a href="#检查数据" class="headerlink" title="检查数据"></a>检查数据</h1><p>建议在使用数据之前，先对它有一个初步的了解。</p>
<p>输出关于各列的一些实用统计信息快速摘要：样本数，均值，标准偏差，最大值，最小值和各种分位数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">california_housing_dataframe.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>median_house_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>17000.0</td>
      <td>17000.0</td>
      <td>17000.0</td>
      <td>17000.0</td>
      <td>17000.0</td>
      <td>17000.0</td>
      <td>17000.0</td>
      <td>17000.0</td>
      <td>17000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>-119.6</td>
      <td>35.6</td>
      <td>28.6</td>
      <td>2643.7</td>
      <td>539.4</td>
      <td>1429.6</td>
      <td>501.2</td>
      <td>3.9</td>
      <td>207.3</td>
    </tr>
    <tr>
      <th>std</th>
      <td>2.0</td>
      <td>2.1</td>
      <td>12.6</td>
      <td>2179.9</td>
      <td>421.5</td>
      <td>1147.9</td>
      <td>384.5</td>
      <td>1.9</td>
      <td>116.0</td>
    </tr>
    <tr>
      <th>min</th>
      <td>-124.3</td>
      <td>32.5</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.5</td>
      <td>15.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>-121.8</td>
      <td>33.9</td>
      <td>18.0</td>
      <td>1462.0</td>
      <td>297.0</td>
      <td>790.0</td>
      <td>282.0</td>
      <td>2.6</td>
      <td>119.4</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>-118.5</td>
      <td>34.2</td>
      <td>29.0</td>
      <td>2127.0</td>
      <td>434.0</td>
      <td>1167.0</td>
      <td>409.0</td>
      <td>3.5</td>
      <td>180.4</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>-118.0</td>
      <td>37.7</td>
      <td>37.0</td>
      <td>3151.2</td>
      <td>648.2</td>
      <td>1721.0</td>
      <td>605.2</td>
      <td>4.8</td>
      <td>265.0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>-114.3</td>
      <td>42.0</td>
      <td>52.0</td>
      <td>37937.0</td>
      <td>6445.0</td>
      <td>35682.0</td>
      <td>6082.0</td>
      <td>15.0</td>
      <td>500.0</td>
    </tr>
  </tbody>
</table>
</div>

<h1 id="构建第一个模型"><a href="#构建第一个模型" class="headerlink" title="构建第一个模型"></a>构建第一个模型</h1><p>尝试预测median_house_value，它将是我们的标签，也称为目标。我们将使用total_rooms作为输入特征。</p>
<p><strong>注意</strong>：我们使用的是城市街区级别的数据，因此该特征表示相应街区的房间总数。</p>
<p>为了训练模型，我们将使用 TensorFlow <a href="https://www.tensorflow.org/get_started/estimator" target="_blank" rel="noopener">Estimator</a> API 提供的 <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor" target="_blank" rel="noopener">LinearRegressor</a> 接口。此 API 负责处理大量低级别模型搭建工作，并会提供执行模型训练、评估和推理的便利方法。</p>
<h2 id="第一步：定义特征并配置特征列"><a href="#第一步：定义特征并配置特征列" class="headerlink" title="第一步：定义特征并配置特征列"></a>第一步：定义特征并配置特征列</h2><p>为了将我们的训练数据导入TensorFlow，我们需要指定每个特征包含的数据类型。在练习中，主要使用一下两类数据：</p>
<pre><code>分类数据： 一种文字数据。
数值数据：一种数字（整数或浮点数）数据以及希望是为数字的数据。
</code></pre><p>在 TenssorFlow中，使用“特征列”的结构来表示特征的数据类型。特征列仅储存对特征数据的描述；不包含特征数据本身。</p>
<p>一开始，只使用一个数值输入特征total_rooms。以下代码会从 california_housing_dataframe 中提取 total_rooms 数据，并使用 numeric_column 定义特征列，这样会将其数据指定为数值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义输入特征 total_rooms.</span></span><br><span class="line">my_feature = california_housing_dataframe[[<span class="string">"total_rooms"</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为total_rooms配置一个由数字构成的feature column</span></span><br><span class="line">feature_columns = [tf.feature_column.numeric_column(<span class="string">"total_rooms"</span>)]</span><br></pre></td></tr></table></figure>
<p> <strong>注意</strong>：<code>total_rooms</code> 数据的形状是一维数组（每个街区的房间总数列表）。这是 <code>numeric_column</code> 的默认形状，因此我们不必将其作为参数传递。</p>
<h2 id="第二步：定义目标"><a href="#第二步：定义目标" class="headerlink" title="第二步：定义目标"></a>第二步：定义目标</h2><p>接下来，我们将定义目标，也就是 <code>median_house_value</code>。同样，我们可以从 <code>california_housing_dataframe</code> 中提取它：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义目标</span></span><br><span class="line">targets = california_housing_dataframe[<span class="string">"median_house_value"</span>]</span><br></pre></td></tr></table></figure>
<h2 id="第三步：配置LinearRegressor"><a href="#第三步：配置LinearRegressor" class="headerlink" title="第三步：配置LinearRegressor"></a>第三步：配置LinearRegressor</h2><p> 接下来，我们将使用 LinearRegressor 配置线性回归模型，并使用 <code>GradientDescentOptimizer</code>（它会实现小批量随机梯度下降法 (SGD)）训练该模型。<code>learning_rate</code> 参数可控制梯度步长的大小。</p>
<p><strong>注意</strong>：为了安全起见，我们还会通过 <code>clip_gradients_by_norm</code> 将<a href="https://developers.google.com/machine-learning/glossary/#gradient_clipping" target="_blank" rel="noopener">梯度裁剪</a>应用到我们的优化器。梯度裁剪可确保梯度大小在训练期间不会变得过大，梯度过大会导致梯度下降法失败。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用梯度下降作为训练模型的优化器</span></span><br><span class="line">my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.0000001</span>)</span><br><span class="line">my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用我们的特性列和优化器配置线性回归模型</span></span><br><span class="line"><span class="comment"># 为梯度下降设置0.0000001的学习率</span></span><br><span class="line">linear_regresor = tf.estimator.LinearRegressor(</span><br><span class="line">    feature_columns=feature_columns,</span><br><span class="line">    optimizer=my_optimizer</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="第四步：定义输入函数"><a href="#第四步：定义输入函数" class="headerlink" title="第四步：定义输入函数"></a>第四步：定义输入函数</h2><p> 要将加利福尼亚州住房数据导入 <code>LinearRegressor</code>，我们需要定义一个输入函数，让它告诉 TensorFlow 如何对数据进行预处理，以及在模型训练期间如何批处理、随机处理和重复数据。</p>
<p>首先，我们将 <em>Pandas</em> 特征数据转换成 NumPy 数组字典。然后，我们可以使用 TensorFlow <a href="https://www.tensorflow.org/programmers_guide/datasets" target="_blank" rel="noopener">Dataset API</a> 根据我们的数据构建 Dataset 对象，并将数据拆分成大小为 <code>batch_size</code> 的多批数据，以按照指定周期数 (num_epochs) 进行重复。</p>
<p><strong>注意</strong>：如果将默认值 <code>num_epochs=None</code> 传递到 <code>repeat()</code>，输入数据会无限期重复。</p>
<p>然后，如果 <code>shuffle</code> 设置为 <code>True</code>，则我们会对数据进行随机处理，以便数据在训练期间以随机方式传递到模型。<code>buffer_size</code> 参数会指定 <code>shuffle</code> 将从中随机抽样的数据集的大小。</p>
<p>最后，输入函数会为该数据集构建一个迭代器，并向 LinearRegressor 返回下一批数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_input_fn</span><span class="params">(features, targets, batch_size=<span class="number">1</span>, shuffle=True, num_epochs=None)</span>:</span></span><br><span class="line">    <span class="string">"""训练一个特征的线性回归模型</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">      features: pandas DataFrame 类型的 features</span></span><br><span class="line"><span class="string">      targets: pandas DataFrame 类型的 targets</span></span><br><span class="line"><span class="string">      batch_size: Size of batches to be passed to the model</span></span><br><span class="line"><span class="string">      shuffle: True or False. 是否随机化 data.</span></span><br><span class="line"><span class="string">      num_epochs: 数据应重复的周期数. None = repeat indefinitely</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      Tuple of (features, labels) for next data batch</span></span><br><span class="line"><span class="string">      元组：下一个数据批处理的(特征、标签/目标)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 将pandas数据转换为numpy数组字典</span></span><br><span class="line">    features = &#123;key: np.array(value) <span class="keyword">for</span> key, value <span class="keyword">in</span> dict(features).items()&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建dataset，并且拆分为batch_size个数据    </span></span><br><span class="line">    ds = Dataset.from_tensor_slices((features, targets))  <span class="comment"># 限制为最大2GB</span></span><br><span class="line">    ds = ds.batch(batch_size).repeat(num_epochs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># shuffle</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        ds = ds.shuffle(buffer_size=<span class="number">10000</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 返回下一批数据</span></span><br><span class="line">    features, labels = ds.make_one_shot_iterator().get_next()</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br></pre></td></tr></table></figure>
<h2 id="第五步：训练数据"><a href="#第五步：训练数据" class="headerlink" title="第五步：训练数据"></a>第五步：训练数据</h2><p> 现在，我们可以在 <code>linear_regressor</code> 上调用 <code>train()</code> 来训练模型。我们会将 <code>my_input_fn</code> 封装在 <code>lambda</code> 中，以便可以将 <code>my_feature</code> 和 <code>target</code> 作为参数传入（有关详情，请参阅此 <a href="https://www.tensorflow.org/get_started/input_fn#passing_input_fn_data_to_your_model" target="_blank" rel="noopener">TensorFlow 输入函数教程</a>），首先，我们会训练 100 步。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_ = linear_regresor.train(</span><br><span class="line">    input_fn = <span class="keyword">lambda</span>: my_input_fn(my_feature, targets),</span><br><span class="line">    steps=<span class="number">100</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="第六步：-评估模型"><a href="#第六步：-评估模型" class="headerlink" title="第六步： 评估模型"></a>第六步： 评估模型</h2><p>我们基于该训练数据做一次预测，看看我们的模型在训练期间与这些数据的拟合情况。</p>
<p><strong>注意</strong>：训练误差可以衡量您的模型与训练数据的拟合情况，但并<strong>_不能_</strong>衡量模型<strong><em>泛化到新数据</em></strong>的效果。在后面的练习中，您将探索如何拆分数据以评估模型的泛化能力。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 为预测创建一个输入函数</span></span><br><span class="line">predication_input_fn = <span class="keyword">lambda</span>: my_input_fn(my_feature, targets, num_epochs=<span class="number">1</span>, shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 linear_regressor 上调用predict()进行预测</span></span><br><span class="line">predictions = linear_regresor.predict(input_fn=predication_input_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将预测格式化为一个NumPy的数组，这样我们就可以计算错误度量</span></span><br><span class="line">predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> predictions])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出方差</span></span><br><span class="line">mean_squared_error = metrics.mean_squared_error(predictions, targets)</span><br><span class="line">root_mean_squared_error = math.sqrt(mean_squared_error)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Mean Squared Error (on training data): %0.3f"</span> % mean_squared_error)</span><br><span class="line">print(<span class="string">"Root Mean Squared Error (on training data): %0.3f"</span> % root_mean_squared_error)</span><br></pre></td></tr></table></figure>
<pre><code>Mean Squared Error (on training data): 56367.025
Root Mean Squared Error (on training data): 237.417
</code></pre><p>如何判断误差有多大？<br>由于均方误差 (MSE) 很难解读，因此我们经常查看的是均方根误差 (RMSE)。RMSE 的一个很好的特性是，它可以在与原目标相同的规模下解读。</p>
<p>我们来比较一下 RMSE 与目标最大值和最小值的差值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">min_house_value = california_housing_dataframe[<span class="string">"median_house_value"</span>].min()</span><br><span class="line">max_house_value = california_housing_dataframe[<span class="string">"median_house_value"</span>].max()</span><br><span class="line">min_max_difference = max_house_value - min_house_value</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Min. Median House Value: %0.3f"</span> % min_house_value)</span><br><span class="line">print(<span class="string">"Max. Median House Value: %0.3f"</span> % max_house_value)</span><br><span class="line">print(<span class="string">"Difference between Min. and Max.: %0.3f"</span> % min_max_difference)</span><br><span class="line">print(<span class="string">"Root Mean Squared Error: %0.3f"</span> % root_mean_squared_error)</span><br></pre></td></tr></table></figure>
<pre><code>Min. Median House Value: 14.999
Max. Median House Value: 500.001
Difference between Min. and Max.: 485.002
Root Mean Squared Error: 237.417
</code></pre><p>我们的误差跨越目标值近一般范围，为了进一步缩小误差，首先了解一下我们的预测predictions和targets的总体统计信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">calibration_data = pd.DataFrame()</span><br><span class="line">calibration_data[<span class="string">"predictions"</span>] = pd.Series(predictions)</span><br><span class="line">calibration_data[<span class="string">"targets"</span>] = pd.Series(targets)</span><br><span class="line">calibration_data.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>predictions</th>
      <th>targets</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>17000.0</td>
      <td>17000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.1</td>
      <td>207.3</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.1</td>
      <td>116.0</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.0</td>
      <td>15.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.1</td>
      <td>119.4</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.1</td>
      <td>180.4</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.2</td>
      <td>265.0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.9</td>
      <td>500.0</td>
    </tr>
  </tbody>
</table>
</div>

<p>此信息也许有些帮助，比较看相差还是蛮大的，通过数据可视化来观察下</p>
<p>我们知道，单个特征的线性回归可以绘制一条将输入 x 映射到输出 y 的线。</p>
<p>首先，获得均匀分布的随机数据样本，以便绘制可辨识的散点图。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sample = california_housing_dataframe.sample(n=<span class="number">300</span>)  <span class="comment"># 随机抽样300个来观察</span></span><br></pre></td></tr></table></figure>
<p>然后，根据模型的偏差项和特征权重绘制学习线，并绘制散点图。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取 total_rooms 最大与最小值</span></span><br><span class="line">x_0 = sample[<span class="string">"total_rooms"</span>].min()</span><br><span class="line">x_1 = sample[<span class="string">"total_rooms"</span>].max()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检索训练过程中产生的最终权重和偏差</span></span><br><span class="line">weight = linear_regresor.get_variable_value(<span class="string">"linear/linear_model/total_rooms/weights"</span>)[<span class="number">0</span>]</span><br><span class="line">bias = linear_regresor.get_variable_value(<span class="string">"linear/linear_model/bias_weights"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取total_rooms预测的最小和最大值</span></span><br><span class="line">y_0 = weight * x_0 + bias</span><br><span class="line">y_1 = weight * x_1 + bias</span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在我们有两个坐标后 画出回归线</span></span><br><span class="line">plt.plot([x_0, x_1], [y_0, y_1], c=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写上每个轴代表的含义</span></span><br><span class="line">plt.ylabel(<span class="string">"median_house_value"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"total_rooms"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出sample 数据的散点图</span></span><br><span class="line">plt.scatter(sample[<span class="string">"total_rooms"</span>], sample[<span class="string">"median_house_value"</span>])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="output_32_0.png" alt="png"></p>
<p>这条线看起来明显和目标相差很大，综上所述，这些初级健全性检查提示我们也许可以找到更好的线。</p>
<h1 id="调整模型超参数"><a href="#调整模型超参数" class="headerlink" title="调整模型超参数"></a>调整模型超参数</h1><p>我们把以上所学的东西整理到一个函数中，以方便我们更容易的调整参数和观察变化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(learning_rate, steps, batch_size, input_feature=<span class="string">"total_rooms"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""一个线性回归的训练模型</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        learning_rate: 学习速率 float</span></span><br><span class="line"><span class="string">        steps: 训练总次数 int</span></span><br><span class="line"><span class="string">        batch_size: 批处理大小 非0 int</span></span><br><span class="line"><span class="string">        input_feature: 一个' string '，指定一个来自' california_housing_dataframe '的列用作输入特性。          </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    periods = <span class="number">10</span>  <span class="comment"># 周期</span></span><br><span class="line">    steps_per_period = steps / periods</span><br><span class="line">    </span><br><span class="line">    my_feature = input_feature</span><br><span class="line">    my_feature_data = california_housing_dataframe[[my_feature]]</span><br><span class="line">    my_label = <span class="string">"median_house_value"</span></span><br><span class="line">    targets = california_housing_dataframe[my_label]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建 feature columns</span></span><br><span class="line">    feature_columns = [tf.feature_column.numeric_column(my_feature)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建 input feature</span></span><br><span class="line">    training_input_fn = <span class="keyword">lambda</span>: my_input_fn(my_feature_data, targets, batch_size=batch_size)</span><br><span class="line">    prediction_input_fn = <span class="keyword">lambda</span>: my_input_fn(my_feature_data, targets, num_epochs=<span class="number">1</span>, shuffle=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建 一个线性回归对象</span></span><br><span class="line">    my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line">    linear_regressor = tf.estimator.LinearRegressor(</span><br><span class="line">        feature_columns=feature_columns,</span><br><span class="line">        optimizer=my_optimizer</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置回归线的状态</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">15</span>, <span class="number">6</span>))</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    plt.title(<span class="string">"Learned Line by Period"</span>)</span><br><span class="line">    plt.ylabel(my_label)</span><br><span class="line">    plt.xlabel(my_feature)</span><br><span class="line">    sample = california_housing_dataframe.sample(n=<span class="number">300</span>)</span><br><span class="line">    plt.scatter(sample[my_feature], sample[my_label])</span><br><span class="line">    colors = [cm.coolwarm(x) <span class="keyword">for</span> x <span class="keyword">in</span> np.linspace(<span class="number">-1</span>, <span class="number">1</span>, periods)]</span><br><span class="line">    <span class="comment"># 训练模型，但是在循环中这样做，这样我们就可以周期性地评估</span></span><br><span class="line">    <span class="comment"># 损失指标</span></span><br><span class="line">    print(<span class="string">"Training model..."</span>)</span><br><span class="line">    print(<span class="string">"RMSE (on training data):"</span>)</span><br><span class="line">    root_mean_squared_errors = [] </span><br><span class="line">    <span class="keyword">for</span> period <span class="keyword">in</span> range(<span class="number">0</span>, periods):</span><br><span class="line">        <span class="comment"># 训练模型，从之前的状态开始</span></span><br><span class="line">        linear_regressor.train(</span><br><span class="line">            input_fn=training_input_fn,</span><br><span class="line">            steps=steps_per_period</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 计算预测值</span></span><br><span class="line">        predictions = linear_regressor.predict(input_fn=prediction_input_fn)</span><br><span class="line">        predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> predictions])</span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        root_mean_squared_error = math.sqrt(</span><br><span class="line">            metrics.mean_squared_error(predictions, targets))</span><br><span class="line">        print(<span class="string">"   period %02d : %0.2f"</span> % (period, root_mean_squared_error))</span><br><span class="line">        <span class="comment"># 添加loss 到list</span></span><br><span class="line">        root_mean_squared_errors.append(root_mean_squared_error)</span><br><span class="line">        <span class="comment"># 纪录权重和偏差</span></span><br><span class="line">        y_extents = np.array([<span class="number">0</span>, sample[my_label].max()])</span><br><span class="line"></span><br><span class="line">        weight = linear_regressor.get_variable_value(<span class="string">'linear/linear_model/%s/weights'</span> % input_feature)[<span class="number">0</span>]</span><br><span class="line">        bias = linear_regressor.get_variable_value(<span class="string">'linear/linear_model/bias_weights'</span>)</span><br><span class="line"></span><br><span class="line">        x_extents = (y_extents - bias) / weight</span><br><span class="line">        x_extents = np.maximum(np.minimum(x_extents,</span><br><span class="line">                                          sample[my_feature].max()),</span><br><span class="line">                               sample[my_feature].min())</span><br><span class="line">        y_extents = weight * x_extents + bias</span><br><span class="line">        plt.plot(x_extents, y_extents, color=colors[period]) </span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"Model training finished."</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出一个周期内损失指标的图表。</span></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'RMSE'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Periods'</span>)</span><br><span class="line">    plt.title(<span class="string">"Root Mean Squared Error vs. Periods"</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.plot(root_mean_squared_errors)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出带有校准数据的表</span></span><br><span class="line">    calibration_data = pd.DataFrame()</span><br><span class="line">    calibration_data[<span class="string">"predictions"</span>] = pd.Series(predictions)</span><br><span class="line">    calibration_data[<span class="string">"targets"</span>] = pd.Series(targets)</span><br><span class="line">    display.display(calibration_data.describe())</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Final RMSE (on training data): %0.2f"</span> % root_mean_squared_error)  </span><br><span class="line">    <span class="keyword">return</span> calibration_data</span><br></pre></td></tr></table></figure>
<p>设置参数初步训练下一试试</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">calibration_data = train_model(</span><br><span class="line">    learning_rate=<span class="number">0.00001</span>,</span><br><span class="line">    steps=<span class="number">100</span>,</span><br><span class="line">    batch_size=<span class="number">1</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
   period 00 : 236.40
   period 01 : 235.26
   period 02 : 234.10
   period 03 : 232.96
   period 04 : 231.87
   period 05 : 230.78
   period 06 : 229.62
   period 07 : 228.54
   period 08 : 227.36
   period 09 : 226.38
Model training finished.
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>predictions</th>
      <th>targets</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>17000.0</td>
      <td>17000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>13.2</td>
      <td>207.3</td>
    </tr>
    <tr>
      <th>std</th>
      <td>10.9</td>
      <td>116.0</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.0</td>
      <td>15.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>7.3</td>
      <td>119.4</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>10.6</td>
      <td>180.4</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>15.8</td>
      <td>265.0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>189.7</td>
      <td>500.0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>Final RMSE (on training data): 226.38
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>predictions</th>
      <th>targets</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>9.7</td>
      <td>66.9</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10.6</td>
      <td>80.1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.0</td>
      <td>85.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>13.6</td>
      <td>73.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>14.0</td>
      <td>65.5</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>16995</th>
      <td>14.1</td>
      <td>111.4</td>
    </tr>
    <tr>
      <th>16996</th>
      <td>7.5</td>
      <td>79.0</td>
    </tr>
    <tr>
      <th>16997</th>
      <td>20.9</td>
      <td>103.6</td>
    </tr>
    <tr>
      <th>16998</th>
      <td>12.6</td>
      <td>85.8</td>
    </tr>
    <tr>
      <th>16999</th>
      <td>10.9</td>
      <td>94.6</td>
    </tr>
  </tbody>
</table>
<p>17000 rows × 2 columns</p>
</div>

<p><img src="output_37_4.png" alt="png"></p>
<p>差距还是蛮大的，改变参数试试</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">calibration_data = train_model(</span><br><span class="line">    learning_rate=<span class="number">0.00002</span>,</span><br><span class="line">    steps=<span class="number">500</span>,</span><br><span class="line">    batch_size=<span class="number">5</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
   period 00 : 226.36
   period 01 : 216.00
   period 02 : 206.55
   period 03 : 198.01
   period 04 : 191.20
   period 05 : 185.74
   period 06 : 182.91
   period 07 : 179.02
   period 08 : 176.59
   period 09 : 175.67
Model training finished.
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>predictions</th>
      <th>targets</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>17000.0</td>
      <td>17000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>115.8</td>
      <td>207.3</td>
    </tr>
    <tr>
      <th>std</th>
      <td>95.5</td>
      <td>116.0</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.1</td>
      <td>15.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>64.0</td>
      <td>119.4</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>93.2</td>
      <td>180.4</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>138.0</td>
      <td>265.0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1661.6</td>
      <td>500.0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>Final RMSE (on training data): 175.67
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>predictions</th>
      <th>targets</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>110.0</td>
      <td>66.9</td>
    </tr>
    <tr>
      <th>1</th>
      <td>51.7</td>
      <td>80.1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>144.4</td>
      <td>85.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>79.7</td>
      <td>73.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>77.4</td>
      <td>65.5</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>16995</th>
      <td>47.5</td>
      <td>111.4</td>
    </tr>
    <tr>
      <th>16996</th>
      <td>138.7</td>
      <td>79.0</td>
    </tr>
    <tr>
      <th>16997</th>
      <td>248.7</td>
      <td>103.6</td>
    </tr>
    <tr>
      <th>16998</th>
      <td>117.8</td>
      <td>85.8</td>
    </tr>
    <tr>
      <th>16999</th>
      <td>46.4</td>
      <td>94.6</td>
    </tr>
  </tbody>
</table>
<p>17000 rows × 2 columns</p>
</div>

<p><img src="output_39_4.png" alt="png"></p>
<h2 id="有适用于模型调整的标准启发法吗？"><a href="#有适用于模型调整的标准启发法吗？" class="headerlink" title="有适用于模型调整的标准启发法吗？"></a>有适用于模型调整的标准启发法吗？</h2><p>降低RMSE，这是一个常见的问题。简短的答案是，不同超参数的效果取决于数据。因此，不存在必须遵循的规则，您需要对自己的数据进行测试。</p>
<p>即便如此，我们仍在下面列出了几条可为您提供指导的经验法则：</p>
<ul>
<li>训练误差应该稳步减小，刚开始是急剧减小，最终应随着训练收敛达到平稳状态。</li>
<li>如果训练尚未收敛，尝试运行更长的时间。</li>
<li>如果训练误差减小速度过慢，则提高学习速率也许有助于加快其减小速度。<ul>
<li>但有时如果学习速率过高，训练误差的减小速度反而会变慢。</li>
</ul>
</li>
<li>如果训练误差变化很大，尝试降低学习速率。<ul>
<li>较低的学习速率和较大的步数/较大的批量大小通常是不错的组合。</li>
</ul>
</li>
<li>批量大小过小也会导致不稳定情况。不妨先尝试 100 或 1000 等较大的值，然后逐渐减小值的大小，直到出现性能降低的情况。</li>
</ul>
<p>重申一下，切勿严格遵循这些经验法则，因为效果取决于数据。请始终进行试验和验证。</p>
<h2 id="尝试其他特征"><a href="#尝试其他特征" class="headerlink" title="尝试其他特征"></a>尝试其他特征</h2><p>使用 <code>population</code> 特征替换 <code>total_rooms</code> 特征，看看能否取得更好的效果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">calibration_data = train_model(</span><br><span class="line">    learning_rate=<span class="number">0.00002</span>,</span><br><span class="line">    steps=<span class="number">1000</span>,</span><br><span class="line">    batch_size=<span class="number">5</span>,</span><br><span class="line">    input_feature=<span class="string">"population"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
   period 00 : 225.40
   period 01 : 214.37
   period 02 : 204.42
   period 03 : 195.76
   period 04 : 188.29
   period 05 : 182.98
   period 06 : 178.93
   period 07 : 175.94
   period 08 : 174.97
   period 09 : 175.14
Model training finished.
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>predictions</th>
      <th>targets</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>17000.0</td>
      <td>17000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>120.1</td>
      <td>207.3</td>
    </tr>
    <tr>
      <th>std</th>
      <td>96.4</td>
      <td>116.0</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.3</td>
      <td>15.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>66.4</td>
      <td>119.4</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>98.0</td>
      <td>180.4</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>144.6</td>
      <td>265.0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>2997.3</td>
      <td>500.0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>Final RMSE (on training data): 175.14
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>predictions</th>
      <th>targets</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>116.6</td>
      <td>66.9</td>
    </tr>
    <tr>
      <th>1</th>
      <td>829.3</td>
      <td>80.1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>97.7</td>
      <td>85.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>136.3</td>
      <td>73.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>41.7</td>
      <td>65.5</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>16995</th>
      <td>56.4</td>
      <td>111.4</td>
    </tr>
    <tr>
      <th>16996</th>
      <td>12.3</td>
      <td>79.0</td>
    </tr>
    <tr>
      <th>16997</th>
      <td>83.9</td>
      <td>103.6</td>
    </tr>
    <tr>
      <th>16998</th>
      <td>84.3</td>
      <td>85.8</td>
    </tr>
    <tr>
      <th>16999</th>
      <td>70.1</td>
      <td>94.6</td>
    </tr>
  </tbody>
</table>
<p>17000 rows × 2 columns</p>
</div>

<p><img src="output_42_4.png" alt="png"></p>
<h1 id="合成特征和离群值"><a href="#合成特征和离群值" class="headerlink" title="合成特征和离群值"></a>合成特征和离群值</h1><h2 id="尝试合成特征"><a href="#尝试合成特征" class="headerlink" title="尝试合成特征"></a>尝试合成特征</h2><p><code>total_rooms</code> 和 <code>population</code> 特征都会统计指定街区的相关总计数据。</p>
<p>但是，如果一个街区比另一个街区的人口更密集，会怎么样？我们可以创建一个合成特征（即 <code>total_rooms</code> 与 <code>population</code> 的比例）来探索街区人口密度与房屋价值中位数之间的关系。</p>
<p>在以下单元格中，创建一个名为 <code>rooms_per_person</code> 的特征，并将其用作 <code>train_model()</code> 的 <code>input_feature</code>。</p>
<p>通过调整学习速率，您使用这一特征可以获得的最佳效果是什么？（效果越好，回归线与数据的拟合度就越高，最终 RMSE 也会越低。）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">california_housing_dataframe[<span class="string">"rooms_per_person"</span>] = (</span><br><span class="line">    california_housing_dataframe[<span class="string">"total_rooms"</span>] / california_housing_dataframe[<span class="string">"population"</span>])</span><br><span class="line"></span><br><span class="line">calibration_data = train_model(</span><br><span class="line">    learning_rate=<span class="number">0.05</span>,</span><br><span class="line">    steps=<span class="number">500</span>,</span><br><span class="line">    batch_size=<span class="number">5</span>,</span><br><span class="line">    input_feature=<span class="string">"rooms_per_person"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
   period 00 : 214.75
   period 01 : 193.24
   period 02 : 176.45
   period 03 : 160.89
   period 04 : 150.13
   period 05 : 146.30
   period 06 : 145.44
   period 07 : 146.13
   period 08 : 147.65
   period 09 : 149.22
Model training finished.
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>predictions</th>
      <th>targets</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>17000.0</td>
      <td>17000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>200.9</td>
      <td>207.3</td>
    </tr>
    <tr>
      <th>std</th>
      <td>93.2</td>
      <td>116.0</td>
    </tr>
    <tr>
      <th>min</th>
      <td>44.5</td>
      <td>15.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>164.4</td>
      <td>119.4</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>197.8</td>
      <td>180.4</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>226.2</td>
      <td>265.0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>4443.7</td>
      <td>500.0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>Final RMSE (on training data): 149.22
</code></pre><p><img src="output_45_3.png" alt="png"></p>
<h2 id="识别离群值"><a href="#识别离群值" class="headerlink" title="识别离群值"></a>识别离群值</h2><p>通过创建预测值与目标值的散点图来可视化模型效果。理想情况下，这些值将位于一条完全相关的对角线上。</p>
<p>使用您在任务 1 中训练过的人均房间数模型，并使用 Pyplot 的 <code>scatter()</code> 创建预测值与目标值的散点图。</p>
<p>您是否看到任何异常情况？通过查看 <code>rooms_per_person</code> 中值的分布情况，将这些异常情况追溯到源数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">6</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.scatter(calibration_data[<span class="string">"predictions"</span>], calibration_data[<span class="string">"targets"</span>])</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">_ = california_housing_dataframe[<span class="string">"rooms_per_person"</span>].hist()</span><br></pre></td></tr></table></figure>
<p><img src="output_47_0.png" alt="png"></p>
<p> 校准数据显示，大多数散点与一条线对齐。这条线几乎是垂直的，我们稍后再讲解。现在，我们重点关注偏离这条线的点。我们注意到这些点的数量相对较少。</p>
<p>观察我们绘制 <code>rooms_per_person</code> 的直方图，则会发现我们的输入数据中有少量离群值</p>
<h2 id="截取离群值"><a href="#截取离群值" class="headerlink" title="截取离群值"></a>截取离群值</h2><p>将 <code>rooms_per_person</code> 的离群值设置为相对合理的最小值或最大值来进一步改进模型拟合情况。</p>
<p>以下是一个如何将函数应用于 Pandas <code>Series</code> 的简单示例，供您参考：</p>
<pre><code>clipped_feature = my_dataframe[&quot;my_feature_name&quot;].apply(lambda x: max(x, 0))
</code></pre><p>上述 <code>clipped_feature</code> 没有小于 <code>0</code> 的值。</p>
<p>观察直方图发现大多数数值都小于5，我们将<code>rooms_per_person</code> 的值截取为5，然后绘制直方图再次检查结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">california_housing_dataframe[<span class="string">"rooms_per_person"</span>] = (</span><br><span class="line">    california_housing_dataframe[<span class="string">"rooms_per_person"</span>]).apply(<span class="keyword">lambda</span> x: min(x, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">_ = california_housing_dataframe[<span class="string">"rooms_per_person"</span>].hist()</span><br></pre></td></tr></table></figure>
<p><img src="output_50_0.png" alt="png"></p>
<p>验证截取是否有效，我们再训练一次模型，并再次输出校准数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">calibration_data = train_model(</span><br><span class="line">    learning_rate=<span class="number">0.05</span>,</span><br><span class="line">    steps=<span class="number">500</span>,</span><br><span class="line">    batch_size=<span class="number">5</span>,</span><br><span class="line">    input_feature=<span class="string">"rooms_per_person"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
   period 00 : 214.33
   period 01 : 192.43
   period 02 : 172.30
   period 03 : 155.07
   period 04 : 142.01
   period 05 : 134.29
   period 06 : 129.31
   period 07 : 128.99
   period 08 : 127.60
   period 09 : 126.94
Model training finished.
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>predictions</th>
      <th>targets</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>17000.0</td>
      <td>17000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>189.5</td>
      <td>207.3</td>
    </tr>
    <tr>
      <th>std</th>
      <td>49.3</td>
      <td>116.0</td>
    </tr>
    <tr>
      <th>min</th>
      <td>45.1</td>
      <td>15.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>158.1</td>
      <td>119.4</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>189.6</td>
      <td>180.4</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>216.4</td>
      <td>265.0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>419.5</td>
      <td>500.0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>Final RMSE (on training data): 126.94
</code></pre><p><img src="output_52_3.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_ = plt.scatter(calibration_data[<span class="string">"predictions"</span>], calibration_data[<span class="string">"targets"</span>])</span><br></pre></td></tr></table></figure>
<p><img src="output_53_0.png" alt="png"></p>
]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow 4</title>
    <url>/2019/01/08/TensorFlow_4/</url>
    <content><![CDATA[<h1 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h1><p>通常我们会把数据分配为三分，训练集，交叉验证集和测试集，这样做的好处是为了避免过拟合，能够更好的泛化。</p>
<p><img src="https://developers.google.com/machine-learning/crash-course/images/PartitionThreeSets.svg?hl=zh-cn" alt="训练集合"></p>
<p>添加验证集后，我们的工作流程大概是这样的：</p>
<p><img src="https://developers.google.com/machine-learning/crash-course/images/WorkflowWithValidationSet.svg?hl=zh-cn" alt="工作流程"></p>
<p>接下来的练习是尝试使用这个流程来训练</p>
<p>与在之前的练习中一样，我们将使用加利福尼亚州住房数据集，尝试根据 1990 年的人口普查数据在城市街区级别预测 median_house_value。</p>
<h1 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h1><p>首先加载并准备数据。这一次使用多个特征，因此把逻辑模块化，以方便对特征进行预处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> gridspec</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line">tf.logging.set_verbosity(tf.logging.ERROR)</span><br><span class="line">pd.options.display.max_rows = <span class="number">10</span></span><br><span class="line">pd.options.display.float_format = <span class="string">'&#123;:.1f&#125;'</span>.format</span><br><span class="line"></span><br><span class="line">california_housing_dataframe = pd.read_csv(<span class="string">"https://download.mlcc.google.cn/mledu-datasets/california_housing_train.csv"</span>, sep=<span class="string">","</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_features</span><span class="params">(california_housing_dataframe)</span>:</span></span><br><span class="line">    <span class="string">"""从加州住房数据集获取输入数据</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        california_housing_dataframe:panda的 DataFrame 类型的数据集</span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">        用于模型feature的DataFrame        </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    selected_features = california_housing_dataframe[</span><br><span class="line">        [<span class="string">"latitude"</span>,</span><br><span class="line">         <span class="string">"longitude"</span>,</span><br><span class="line">         <span class="string">"housing_median_age"</span>,</span><br><span class="line">         <span class="string">"total_rooms"</span>,</span><br><span class="line">         <span class="string">"total_bedrooms"</span>,</span><br><span class="line">         <span class="string">"population"</span>,</span><br><span class="line">         <span class="string">"households"</span>,</span><br><span class="line">         <span class="string">"median_income"</span>]]</span><br><span class="line">    </span><br><span class="line">    processed_features = selected_features.copy()</span><br><span class="line">    <span class="comment"># Create a synthetic feature.</span></span><br><span class="line">    processed_features[<span class="string">"rooms_per_person"</span>] = (</span><br><span class="line">        california_housing_dataframe[<span class="string">"total_rooms"</span>] /</span><br><span class="line">        california_housing_dataframe[<span class="string">"population"</span>])</span><br><span class="line">    <span class="keyword">return</span> processed_features</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_targets</span><span class="params">(california_housing_dataframe)</span>:</span></span><br><span class="line">    <span class="string">"""准备目标特性(即来自加州住房数据集。</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        california_housing_dataframe:panda的 DataFrame 类型的数据集</span></span><br><span class="line"><span class="string">    来自加州住房数据集。</span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">        用于模型feature的DataFrame </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    output_targets = pd.DataFrame()</span><br><span class="line">    <span class="comment"># 将targets的单位扩到到千为单位</span></span><br><span class="line">    output_targets[<span class="string">"median_house_value"</span>] = (</span><br><span class="line">        california_housing_dataframe[<span class="string">"median_house_value"</span>] / <span class="number">1000.0</span>)</span><br><span class="line">    <span class="keyword">return</span> output_targets</span><br></pre></td></tr></table></figure>
<p>我们从 17000 个样本中选前 12000 个样本 作 <strong>训练集</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">training_examples = preprocess_features(california_housing_dataframe.head(<span class="number">12000</span>))</span><br><span class="line">training_examples.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>latitude</th>
      <th>longitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>rooms_per_person</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>34.6</td>
      <td>-118.5</td>
      <td>27.5</td>
      <td>2655.7</td>
      <td>547.1</td>
      <td>1476.0</td>
      <td>505.4</td>
      <td>3.8</td>
      <td>1.9</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.6</td>
      <td>1.2</td>
      <td>12.1</td>
      <td>2258.1</td>
      <td>434.3</td>
      <td>1174.3</td>
      <td>391.7</td>
      <td>1.9</td>
      <td>1.3</td>
    </tr>
    <tr>
      <th>min</th>
      <td>32.5</td>
      <td>-121.4</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>0.5</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>33.8</td>
      <td>-118.9</td>
      <td>17.0</td>
      <td>1451.8</td>
      <td>299.0</td>
      <td>815.0</td>
      <td>283.0</td>
      <td>2.5</td>
      <td>1.4</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>34.0</td>
      <td>-118.2</td>
      <td>28.0</td>
      <td>2113.5</td>
      <td>438.0</td>
      <td>1207.0</td>
      <td>411.0</td>
      <td>3.5</td>
      <td>1.9</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>34.4</td>
      <td>-117.8</td>
      <td>36.0</td>
      <td>3146.0</td>
      <td>653.0</td>
      <td>1777.0</td>
      <td>606.0</td>
      <td>4.6</td>
      <td>2.3</td>
    </tr>
    <tr>
      <th>max</th>
      <td>41.8</td>
      <td>-114.3</td>
      <td>52.0</td>
      <td>37937.0</td>
      <td>5471.0</td>
      <td>35682.0</td>
      <td>5189.0</td>
      <td>15.0</td>
      <td>55.2</td>
    </tr>
  </tbody>
</table>
</div>

<figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line">training_targets = preprocess_targets(california_housing_dataframe.head(12000))</span><br><span class="line">training_targets.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>median_house_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>12000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>198.0</td>
    </tr>
    <tr>
      <th>std</th>
      <td>111.9</td>
    </tr>
    <tr>
      <th>min</th>
      <td>15.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>117.1</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>170.5</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>244.4</td>
    </tr>
    <tr>
      <th>max</th>
      <td>500.0</td>
    </tr>
  </tbody>
</table>
</div>

<p>我们从 17000 个样本中选择后 5000 个为 <strong>验证集</strong></p>
<figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line">validation_examples = preprocess_features(california_housing_dataframe.tail(5000))</span><br><span class="line">validation_examples.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>latitude</th>
      <th>longitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>rooms_per_person</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>38.1</td>
      <td>-122.2</td>
      <td>31.3</td>
      <td>2614.8</td>
      <td>521.1</td>
      <td>1318.1</td>
      <td>491.2</td>
      <td>4.1</td>
      <td>2.1</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.9</td>
      <td>0.5</td>
      <td>13.4</td>
      <td>1979.6</td>
      <td>388.5</td>
      <td>1073.7</td>
      <td>366.5</td>
      <td>2.0</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>min</th>
      <td>36.1</td>
      <td>-124.3</td>
      <td>1.0</td>
      <td>8.0</td>
      <td>1.0</td>
      <td>8.0</td>
      <td>1.0</td>
      <td>0.5</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>37.5</td>
      <td>-122.4</td>
      <td>20.0</td>
      <td>1481.0</td>
      <td>292.0</td>
      <td>731.0</td>
      <td>278.0</td>
      <td>2.7</td>
      <td>1.7</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>37.8</td>
      <td>-122.1</td>
      <td>31.0</td>
      <td>2164.0</td>
      <td>424.0</td>
      <td>1074.0</td>
      <td>403.0</td>
      <td>3.7</td>
      <td>2.1</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>38.4</td>
      <td>-121.9</td>
      <td>42.0</td>
      <td>3161.2</td>
      <td>635.0</td>
      <td>1590.2</td>
      <td>603.0</td>
      <td>5.1</td>
      <td>2.4</td>
    </tr>
    <tr>
      <th>max</th>
      <td>42.0</td>
      <td>-121.4</td>
      <td>52.0</td>
      <td>32627.0</td>
      <td>6445.0</td>
      <td>28566.0</td>
      <td>6082.0</td>
      <td>15.0</td>
      <td>18.3</td>
    </tr>
  </tbody>
</table>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">validation_targets = preprocess_targets(california_housing_dataframe.tail(<span class="number">5000</span>))</span><br><span class="line">validation_targets.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>median_house_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>5000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>229.5</td>
    </tr>
    <tr>
      <th>std</th>
      <td>122.5</td>
    </tr>
    <tr>
      <th>min</th>
      <td>15.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>130.4</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>213.0</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>303.2</td>
    </tr>
    <tr>
      <th>max</th>
      <td>500.0</td>
    </tr>
  </tbody>
</table>
</div>

<h1 id="检查数据"><a href="#检查数据" class="headerlink" title="检查数据"></a>检查数据</h1><p> 我们根据基准预期情况检查一下我们的数据：</p>
<ul>
<li><p>对于一些值（例如 <code>median_house_value</code>），我们可以检查这些值是否位于合理的范围内（请注意，这是 1990 年的数据，不是现在的！）。</p>
</li>
<li><p>对于 <code>latitude</code> 和 <code>longitude</code> 等其他值，我们可以通过 Google 进行快速搜索，并快速检查一下它们与预期值是否一致。</p>
</li>
</ul>
<p>如果您仔细看，可能会发现下列异常情况：</p>
<ul>
<li><p><code>median_income</code> 位于 3 到 15 的范围内。我们完全不清楚此范围究竟指的是什么，看起来可能是某对数尺度？无法找到相关记录；我们所能假设的只是，值越高，相应的收入越高。</p>
</li>
<li><p><code>median_house_value</code> 的最大值是 500001。这看起来像是某种人为设定的上限。</p>
</li>
<li><p><code>rooms_per_person</code> 特征通常在正常范围内，其中第 75 百分位数的值约为 2。但也有一些非常大的值（例如 18 或 55），这可能表明数据有一定程度的损坏。</p>
</li>
</ul>
<h1 id="绘制维度-经度与房屋价值中位数的曲线图"><a href="#绘制维度-经度与房屋价值中位数的曲线图" class="headerlink" title="绘制维度/经度与房屋价值中位数的曲线图"></a>绘制维度/经度与房屋价值中位数的曲线图</h1><p> 我们来详细了解一下 <strong><code>latitude</code></strong> 和 <strong><code>longitude</code></strong> 这两个特征。它们是相关城市街区的地理坐标。</p>
<p>利用这两个特征可以提供出色的可视化结果 - 我们来绘制 <code>latitude</code> 和 <code>longitude</code> 的曲线图，然后用颜色标注 <code>median_house_value</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_scatter</span><span class="params">(training_examples, training_targets, validation_examples, validation_targets)</span>:</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">13</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">    ax = plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    ax.set_title(<span class="string">"Validation Data"</span>)</span><br><span class="line"></span><br><span class="line">    ax.set_autoscaley_on(<span class="keyword">False</span>)</span><br><span class="line">    ax.set_ylim([<span class="number">32</span>, <span class="number">43</span>])</span><br><span class="line">    ax.set_autoscalex_on(<span class="keyword">False</span>)</span><br><span class="line">    ax.set_xlim([<span class="number">-126</span>, <span class="number">-112</span>])</span><br><span class="line">    plt.scatter(validation_examples[<span class="string">"longitude"</span>],</span><br><span class="line">                validation_examples[<span class="string">"latitude"</span>],</span><br><span class="line">                cmap=<span class="string">"coolwarm"</span>,</span><br><span class="line">                c=validation_targets[<span class="string">"median_house_value"</span>] / validation_targets[<span class="string">"median_house_value"</span>].max())</span><br><span class="line"></span><br><span class="line">    ax = plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    ax.set_title(<span class="string">"Training Date"</span>)</span><br><span class="line"></span><br><span class="line">    ax.set_autoscaley_on(<span class="keyword">False</span>)</span><br><span class="line">    ax.set_ylim([<span class="number">32</span>, <span class="number">43</span>])</span><br><span class="line">    ax.set_autoscalex_on(<span class="keyword">False</span>)</span><br><span class="line">    ax.set_xlim([<span class="number">-126</span>, <span class="number">-112</span>])</span><br><span class="line">    plt.scatter(training_examples[<span class="string">"longitude"</span>],</span><br><span class="line">                training_examples[<span class="string">"latitude"</span>],</span><br><span class="line">                cmap=<span class="string">"coolwarm"</span>,</span><br><span class="line">                c=training_targets[<span class="string">"median_house_value"</span>] / training_targets[<span class="string">"median_house_value"</span>].max())</span><br><span class="line">    _ = plt.plot()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_scatter(training_examples, training_targets, validation_examples, validation_targets)</span><br></pre></td></tr></table></figure>
<p><img src="output_15_0.png" alt="png"></p>
<p>现在应该已经呈现出一幅不错的加利福尼亚州地图了，其中旧金山和洛杉矶等住房成本高昂的地区用红色表示。</p>
<p>根据训练集呈现的地图有几分像<a href="https://www.google.com/maps/place/California/@37.1870174,-123.7642688,6z/data=!3m1!4b1!4m2!3m1!1s0x808fb9fe5f285e3d:0x8b5109a227086f55" target="_blank" rel="noopener">真正的地图</a>，但根据验证集呈现的明显不像。</p>
<p> 查看上面的摘要统计信息表格时，很容易产生想知道如何进行有用的数据检查的想法。每个街区 total_rooms 的第 <sup>75</sup> 百分位的正确值是什么？</p>
<p>需要注意的关键一点是，对于任何指定特征或列，训练集和验证集之间的值的分布应该大致相同。</p>
<p>我们真正需要担心的是，真实情况并非这样，这一事实表明我们创建训练集和验证集的拆分方式很可能存在问题。</p>
<h1 id="随机化处理数据"><a href="#随机化处理数据" class="headerlink" title="随机化处理数据"></a>随机化处理数据</h1><p>我们需要在读入数据时，对数据进行随机化处理的。</p>
<p>如果我们在创建训练集和验证集之前，没有对数据进行正确的随机化处理，那么以某种特定顺序接收数据可能会导致出现问题（似乎就是此时的问题）。</p>
<p> 发现并解决问题后，重新运行上面的 <code>latitude</code>/<code>longitude</code> 绘图单元格，并确认我们的健全性检查的结果看上去更好了。</p>
<p>顺便提一下，在这一步中，我们会学到一项重要经验。</p>
<p><strong>机器学习中的调试通常是<em>数据调试</em>而不是代码调试。</strong></p>
<p>如果数据有误，即使最高级的机器学习代码也挽救不了局面。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">california_housing_dataframe = california_housing_dataframe.reindex(</span><br><span class="line">     np.random.permutation(california_housing_dataframe.index))</span><br><span class="line"></span><br><span class="line">training_examples = preprocess_features(california_housing_dataframe.head(<span class="number">12000</span>))</span><br><span class="line">training_targets = preprocess_targets(california_housing_dataframe.head(<span class="number">12000</span>))</span><br><span class="line"></span><br><span class="line">validation_examples = preprocess_features(california_housing_dataframe.tail(<span class="number">5000</span>))</span><br><span class="line">validation_targets = preprocess_targets(california_housing_dataframe.tail(<span class="number">5000</span>))</span><br><span class="line"></span><br><span class="line">plot_scatter(training_examples, training_targets, validation_examples, validation_targets)</span><br></pre></td></tr></table></figure>
<p><img src="output_19_0.png" alt="png"></p>
<p>好的，这次的结果来看训练集和验证集都有相似的分布。</p>
<h1 id="训练和评估模型"><a href="#训练和评估模型" class="headerlink" title="训练和评估模型"></a>训练和评估模型</h1><p>尝试不同的超参数，获得最佳验证效果。</p>
<p>首先定义输入函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_input_fn</span><span class="params">(features, targets, batch_size=<span class="number">1</span>, shuffle=True, num_epochs=None)</span>:</span></span><br><span class="line">    features = &#123;key: np.array(value) <span class="keyword">for</span> key, value <span class="keyword">in</span> dict(features).items()&#125;</span><br><span class="line">    </span><br><span class="line">    ds = Dataset.from_tensor_slices((features, targets))</span><br><span class="line">    ds = ds.batch(batch_size).repeat(num_epochs)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        ds = ds.shuffle(<span class="number">10000</span>)</span><br><span class="line">    </span><br><span class="line">    features, labels = ds.make_one_shot_iterator().get_next()</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br></pre></td></tr></table></figure>
<p>由于我们现在使用的是多个输入特征，因此需要把用于将特征列配置为独立函数的代码模块化。（目前此代码相当简单，因为我们的所有特征都是数值，但当我们在今后的练习中使用其他类型的特征时，会基于此代码进行构建。）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">construct_feature_columns</span><span class="params">(input_features)</span>:</span></span><br><span class="line">    <span class="string">"""构造TensorFlow特征列</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            input_features:要使用的数字输入特性的名称。</span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            一个 feature columns 集合</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> set([tf.feature_column.numeric_column(my_feature) </span><br><span class="line">                <span class="keyword">for</span> my_feature <span class="keyword">in</span> input_features])</span><br></pre></td></tr></table></figure>
<p> 接下来，继续完成 <code>train_model()</code> 代码，以设置输入函数和计算预测。</p>
<p><strong>注意</strong>：可以参考以前的练习中的代码，但要确保针对相应数据集调用 <code>predict()</code>。</p>
<p>比较训练数据和验证数据的损失。使用一个原始特征时，我们得到的最佳均方根误差 (RMSE) 约为 180。</p>
<p>现在我们可以使用多个特征，不妨看一下可以获得多好的结果。</p>
<p>使用我们之前了解的一些方法检查数据。这些方法可能包括：</p>
<ul>
<li><p>比较预测值和实际目标值的分布情况</p>
</li>
<li><p>绘制预测值和目标值的散点图</p>
</li>
<li><p>使用 <code>latitude</code> 和 <code>longitude</code> 绘制两个验证数据散点图：</p>
<ul>
<li>一个散点图将颜色映射到实际目标 <code>median_house_value</code></li>
<li>另一个散点图将颜色映射到预测的 <code>median_house_value</code>，并排进行比较。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">    strps,</span></span></span><br><span class="line"><span class="function"><span class="params">    batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_targets,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_targets)</span>:</span></span><br><span class="line">    <span class="string">"""训练多元特征的线性回归模型</span></span><br><span class="line"><span class="string">        除训练外，此功能还打印训练进度信息，</span></span><br><span class="line"><span class="string">        以及随着时间的推移而失去的训练和验证。</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        learning_rate:一个float，表示学习率</span></span><br><span class="line"><span class="string">        steps:一个非零的int，训练步骤的总数。训练步骤</span></span><br><span class="line"><span class="string">            由使用单个批处理的向前和向后传递组成。</span></span><br><span class="line"><span class="string">        batch_size:一个非零的int</span></span><br><span class="line"><span class="string">        training_example: DataFrame 包含一个或多个列</span></span><br><span class="line"><span class="string">        '  california_housing_dataframe '作为训练的输入feature</span></span><br><span class="line"><span class="string">        training_targets:一个' DataFrame '，它只包含一列</span></span><br><span class="line"><span class="string">        ' california_housing_dataframe '作为训练的目标。</span></span><br><span class="line"><span class="string">        validation_example: ' DataFrame '包含一个或多个列</span></span><br><span class="line"><span class="string">        ' california_housing_dataframe '作为验证的输入feature</span></span><br><span class="line"><span class="string">        validation_targets: ' DataFrame '，仅包含来自其中的一列</span></span><br><span class="line"><span class="string">        ' california_housing_dataframe '作为验证的目标。</span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">        在训练数据上训练的“线性回归器”对象</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    periods = <span class="number">10</span></span><br><span class="line">    steps_per_period = strps / periods</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建一个线性回归对象</span></span><br><span class="line">    my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line">    linear_regressor = tf.estimator.LinearRegressor(</span><br><span class="line">        feature_columns=construct_feature_columns(training_examples),</span><br><span class="line">        optimizer=my_optimizer</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建输入函数</span></span><br><span class="line">    training_input_fn = <span class="keyword">lambda</span>: my_input_fn(</span><br><span class="line">        training_examples,</span><br><span class="line">        training_targets[<span class="string">"median_house_value"</span>],</span><br><span class="line">        batch_size=batch_size)</span><br><span class="line">    </span><br><span class="line">    predict_training_input_fn = <span class="keyword">lambda</span>: my_input_fn(</span><br><span class="line">      training_examples, </span><br><span class="line">      training_targets[<span class="string">"median_house_value"</span>], </span><br><span class="line">      num_epochs=<span class="number">1</span>, </span><br><span class="line">      shuffle=<span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">    predict_validation_input_fn = <span class="keyword">lambda</span>: my_input_fn(</span><br><span class="line">        validation_examples, </span><br><span class="line">        validation_targets[<span class="string">"median_house_value"</span>],</span><br><span class="line">        num_epochs=<span class="number">1</span>,</span><br><span class="line">        shuffle=<span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#训练模型，但要在循环中进行，这样我们才能定期评估</span></span><br><span class="line">    <span class="comment">#损失指标</span></span><br><span class="line">    print(<span class="string">"Training model..."</span>)</span><br><span class="line">    print(<span class="string">"RMSE (on training data):"</span>)</span><br><span class="line">    training_rmse = []</span><br><span class="line">    validation_rmse = []</span><br><span class="line">    <span class="keyword">for</span> period <span class="keyword">in</span> range (<span class="number">0</span>, periods):</span><br><span class="line">      <span class="comment"># Train the model, starting from the prior state.</span></span><br><span class="line">      linear_regressor.train(</span><br><span class="line">          input_fn=training_input_fn,</span><br><span class="line">          steps=steps_per_period,</span><br><span class="line">      )</span><br><span class="line">      <span class="comment"># Take a break and compute predictions.</span></span><br><span class="line">      training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn)</span><br><span class="line">      training_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> training_predictions])</span><br><span class="line">      </span><br><span class="line">      validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)</span><br><span class="line">      validation_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> validation_predictions])</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">      <span class="comment"># Compute training and validation loss.</span></span><br><span class="line">      training_root_mean_squared_error = math.sqrt(</span><br><span class="line">          metrics.mean_squared_error(training_predictions, training_targets))</span><br><span class="line">      validation_root_mean_squared_error = math.sqrt(</span><br><span class="line">          metrics.mean_squared_error(validation_predictions, validation_targets))</span><br><span class="line">      <span class="comment"># Occasionally print the current loss.</span></span><br><span class="line">      print(<span class="string">"  period %02d : %0.2f"</span> % (period, training_root_mean_squared_error))</span><br><span class="line">      <span class="comment"># Add the loss metrics from this period to our list.</span></span><br><span class="line">      training_rmse.append(training_root_mean_squared_error)</span><br><span class="line">      validation_rmse.append(validation_root_mean_squared_error)</span><br><span class="line">    print(<span class="string">"Model training finished."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Output a graph of loss metrics over periods.</span></span><br><span class="line">    plt.ylabel(<span class="string">"RMSE"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Periods"</span>)</span><br><span class="line">    plt.title(<span class="string">"Root Mean Squared Error vs. Periods"</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.plot(training_rmse, label=<span class="string">"training"</span>)</span><br><span class="line">    plt.plot(validation_rmse, label=<span class="string">"validation"</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> linear_regressor</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">linear_regressor = train_model(</span><br><span class="line">    learning_rate=<span class="number">0.00003</span>,</span><br><span class="line">    strps=<span class="number">500</span>,</span><br><span class="line">    batch_size=<span class="number">5</span>,</span><br><span class="line">    training_examples=training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
  period 00 : 217.57
  period 01 : 200.14
  period 02 : 185.74
  period 03 : 175.52
  period 04 : 170.71
  period 05 : 167.06
  period 06 : 165.72
  period 07 : 165.65
  period 08 : 166.77
  period 09 : 168.40
Model training finished.
</code></pre><p><img src="output_27_1.png" alt="png"></p>
<h1 id="基于测试数据进行评估"><a href="#基于测试数据进行评估" class="headerlink" title="基于测试数据进行评估"></a>基于测试数据进行评估</h1><p><strong>载入测试数据集并据此评估模型。</strong></p>
<p>我们已对验证数据进行了大量迭代。接下来确保我们没有过拟合该特定样本集的特性。</p>
<p>测试数据集位于<a href="https://download.mlcc.google.cn/mledu-datasets/california_housing_test.csv" target="_blank" rel="noopener">此处</a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">california_housing_test_data = pd.read_csv(<span class="string">"https://download.mlcc.google.cn/mledu-datasets/california_housing_test.csv"</span>, sep=<span class="string">","</span>)</span><br><span class="line"></span><br><span class="line">test_examples = preprocess_features(california_housing_test_data)</span><br><span class="line">test_targets = preprocess_targets(california_housing_test_data)</span><br><span class="line"></span><br><span class="line">predict_test_input_fn = <span class="keyword">lambda</span>: my_input_fn(</span><br><span class="line">      test_examples, </span><br><span class="line">      test_targets[<span class="string">"median_house_value"</span>], </span><br><span class="line">      num_epochs=<span class="number">1</span>, </span><br><span class="line">      shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">test_predictions = linear_regressor.predict(input_fn=predict_test_input_fn)</span><br><span class="line">test_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> test_predictions])</span><br><span class="line"></span><br><span class="line">root_mean_squared_error = math.sqrt(</span><br><span class="line">    metrics.mean_squared_error(test_predictions, test_targets))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Final RMSE (on test data): %0.2f"</span> % root_mean_squared_error)</span><br></pre></td></tr></table></figure>
<pre><code>Final RMSE (on test data): 162.99
</code></pre>]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow 5</title>
    <url>/2019/01/09/TensorFlow_5/</url>
    <content><![CDATA[<h1 id="表示法（Representation）"><a href="#表示法（Representation）" class="headerlink" title="表示法（Representation）"></a>表示法（Representation）</h1><p>机器学习模型不能直接看到、听到或感知输入样本。必须创建数据表示，为模型提供有用的信号来了解数据的关键特性。也就是说，为了训练模型，必须选择最能代表数据的特征集。</p>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>传统编程的关注点是代码。在机器学习项目中，关注点变成了表示。也就是说，开发者通过添加和改善特征来调整模型。</p>
<h3 id="将原始数据映射到特征"><a href="#将原始数据映射到特征" class="headerlink" title="将原始数据映射到特征"></a>将原始数据映射到特征</h3><p>下图左侧边是来自数据源的原始数据，右侧表示特征矢量，也就是组成数据集中样本的浮点值集。 特征工程指的是将原始数据转换为特征矢量。进行特征工程预计需要大量时间。</p>
<p>机器学习模型通常期望样本表示为实数矢量。这种矢量的构建方法如下：为每个字段衍生特征，然后将它们全部连接到一起</p>
<p><img src="https://developers.google.com/machine-learning/crash-course/images/RawDataToFeatureVector.svg" alt="数据映射到特征"></p>
<h3 id="映射数值"><a href="#映射数值" class="headerlink" title="映射数值"></a>映射数值</h3><p>机器学习模型根据浮点值进行训练，因此整数和浮点原始数据不需要特殊编码。正如下图 所示，将原始整数值 6 转换为特征值 6.0 是没有意义的：</p>
<p><img src="https://developers.google.com/machine-learning/crash-course/images/FloatingPointFeatures.svg" alt="映射数值"></p>
<h3 id="映射字符串值"><a href="#映射字符串值" class="headerlink" title="映射字符串值"></a>映射字符串值</h3><p>模型无法通过字符串值学习规律，因此您需要进行一些特征工程来将这些值转换为数字形式：</p>
<pre><code>1.首先，为您要表示的所有特征的字符串值定义一个词汇表。对于 street_name 特征，该词汇表中将包含您知道的所有街道。
2.然后，使用该词汇表创建一个独热编码，用于将指定字符串值表示为二元矢量。在该矢量（与指定的字符串值对应）中：

    ·只有一个元素设为 1。
    ·其他所有元素均设为 0。
    ·该矢量的长度等于词汇表中的元素数。
</code></pre><p>下图显示了某条特定街道 (Shorebird Way) 的独热编码。在此二元矢量中，代表 Shorebird Way 的元素的值为 1，而代表所有其他街道的元素的值为 θ。</p>
<p><img src="https://developers.google.com/machine-learning/crash-course/images/OneHotEncoding.svg" alt="映射字符串值"></p>
<h3 id="映射分类（枚举）值"><a href="#映射分类（枚举）值" class="headerlink" title="映射分类（枚举）值"></a>映射分类（枚举）值</h3><p>分类特征具有一组离散的可能值。例如，名为 Lowland Countries 的特征只包含 3 个可能值：</p>
<pre><code>{&apos;Netherlands&apos;, &apos;Belgium&apos;, &apos;Luxembourg&apos;}
</code></pre><p>您可能会将分类特征（如 Lowland Countries）编码为枚举类型或表示不同值的整数离散集。例如：</p>
<pre><code>将荷兰表示为 0
将比利时表示为 1
将卢森堡表示为 2
</code></pre><p>不过，机器学习模型通常将每个分类特征表示为单独的布尔值。例如，Lowland Countries 在模型中可以表示为 3 个单独的布尔值特征：</p>
<pre><code>x1：是荷兰吗？
x2：是比利时吗？
x3：是卢森堡吗？
</code></pre><p>采用这种方法编码还可以简化某个值可能属于多个分类这种情况（例如，“与法国接壤”对于比利时和卢森堡来说都是 True）。</p>
<h2 id="良好特征的特点"><a href="#良好特征的特点" class="headerlink" title="良好特征的特点"></a>良好特征的特点</h2><p>我们探索了将原始数据映射到合适特征矢量的方法，但这只是工作的一部分。现在，我们必须探索什么样的值才算这些特征矢量中良好的特征。</p>
<h3 id="避免很少使用的离散特征值"><a href="#避免很少使用的离散特征值" class="headerlink" title="避免很少使用的离散特征值"></a>避免很少使用的离散特征值</h3><p>良好的特征值应该在数据集中出现大约 5 次以上。这样一来，模型就可以学习该特征值与标签是如何关联的。也就是说，大量离散值相同的样本可让模型有机会了解不同设置中的特征，从而判断何时可以对标签很好地做出预测。</p>
<p>例如，house_type 特征可能包含大量样本，其中它的值为 victorian：</p>
<pre><code>house_type: victorian
</code></pre><p>相反，如果某个特征的值仅出现一次或者很少出现，则模型就无法根据该特征进行预测。</p>
<p>例如，unique_house_id 就不适合作为特征，因为每个值只使用一次，模型无法从中学习任何规律：</p>
<pre><code>unique_house_id: 8SK982ZZ1242Z
</code></pre><h3 id="最好具有清晰明确的含义"><a href="#最好具有清晰明确的含义" class="headerlink" title="最好具有清晰明确的含义"></a>最好具有清晰明确的含义</h3><p>每个特征对于项目中的任何人来说都应该具有清晰明确的含义。</p>
<p>例如，下面的房龄适合作为特征，可立即识别为年龄：</p>
<pre><code>house_age: 27
</code></pre><p>相反，对于下方特征值的含义，除了创建它的工程师，其他人恐怕辨识不出：</p>
<pre><code>house_age: 851472000
</code></pre><p>在某些情况下，混乱的数据（而不是糟糕的工程选择）会导致含义不清晰的值。</p>
<p>例如，以下 user_age 的来源没有检查值恰当与否：</p>
<pre><code>user_age: 277
</code></pre><h3 id="不要将“神奇”的值与实际数据混为一谈"><a href="#不要将“神奇”的值与实际数据混为一谈" class="headerlink" title="不要将“神奇”的值与实际数据混为一谈"></a>不要将“神奇”的值与实际数据混为一谈</h3><p>良好的浮点特征不包含超出范围的异常断点或“神奇”的值。</p>
<p>例如，假设一个特征具有 0 到 1 之间的浮点值。那么，如下值是可以接受的：</p>
<pre><code>quality_rating: 0.82
quality_rating: 0.37
</code></pre><p>不过，如果用户没有输入 quality_rating，则数据集可能使用如下神奇值来表示不存在该值：</p>
<pre><code>quality_rating: -1
</code></pre><p>为解决神奇值的问题，需将该特征转换为两个特征：</p>
<pre><code>一个特征只存储质量评分，不含神奇值。
一个特征存储布尔值，表示是否提供了 quality_rating。为该布尔值特征指定一个名称，例如 is_quality_rating_defined。
</code></pre><h3 id="考虑上游不稳定性"><a href="#考虑上游不稳定性" class="headerlink" title="考虑上游不稳定性"></a>考虑上游不稳定性</h3><p>特征的定义不应随时间发生变化。</p>
<p>例如，下列值是有用的，因为城市名称一般不会改变。（注意，我们仍然需要将“br/sao_paulo”这样的字符串转换为独热矢量。）</p>
<pre><code>city_id: &quot;br/sao_paulo&quot;
</code></pre><p>但收集由其他模型推理的值会产生额外成本。可能值“219”目前代表圣保罗，但这种表示在未来运行其他模型时可能轻易发生变化：</p>
<pre><code>inferred_city_cluster: &quot;219&quot;
</code></pre><h2 id="数据清理"><a href="#数据清理" class="headerlink" title="数据清理"></a>数据清理</h2><p>苹果树结出的果子有品相上乘的，也有虫蛀坏果。而高端便利店出售的苹果是 100% 完美的水果。从果园到水果店之间，专门有人花费大量时间将坏苹果剔除或给可以挽救的苹果涂上一层薄薄的蜡。作为一名机器学习工程师，您将花费大量的时间挑出坏样本并加工可以挽救的样本。即使是非常少量的“坏苹果”也会破坏掉一个大规模数据集。</p>
<h3 id="缩放特征值"><a href="#缩放特征值" class="headerlink" title="缩放特征值"></a>缩放特征值</h3><p>缩放是指将浮点特征值从自然范围（例如 100 到 900）转换为标准范围（例如 0 到 1 或 -1 到 +1）。如果某个特征集只包含一个特征，则缩放可以提供的实际好处微乎其微或根本没有。不过，如果特征集包含多个特征，则缩放特征可以带来以下优势：</p>
<pre><code>·帮助梯度下降法更快速地收敛。
·帮助避免“NaN 陷阱”。在这种陷阱中，模型中的一个数值变成 NaN（例如，当某个值在训练期间超出浮点精确率限制时），并且模型中的所有其他数值最终也会因数学运算而变成 NaN。
·帮助模型为每个特征确定合适的权重。如果没有进行特征缩放，则模型会对范围较大的特征投入过多精力。
</code></pre><p>您不需要对每个浮点特征进行完全相同的缩放。即使特征 A 的范围是 -1 到 +1，同时特征 B 的范围是 -3 到 +3，也不会产生什么恶劣的影响。不过，如果特征 B 的范围是 5000 到 100000，您的模型会出现糟糕的响应。</p>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">要缩放数字数据，一种显而易见的方法是将 [最小值，最大值] 以线性方式映射到较小的范围，例如 [<span class="number">-1</span>，+<span class="number">1</span>]。</span><br><span class="line"></span><br><span class="line">另一种热门的缩放策略是计算每个值的 Z 得分。Z 得分与距离均值的标准偏差数相关。换而言之：</span><br><span class="line"></span><br><span class="line">    scaled value = (value - mean) / stddev</span><br><span class="line">    </span><br><span class="line">例如，给定以下条件：</span><br><span class="line"></span><br><span class="line">    ·均值 = <span class="number">100</span></span><br><span class="line">    ·标准偏差 = <span class="number">20</span></span><br><span class="line">    ·原始值 = <span class="number">130</span></span><br><span class="line">    </span><br><span class="line">则：</span><br><span class="line"></span><br><span class="line">  scaled_value = (<span class="number">130</span> - <span class="number">100</span>) / <span class="number">20</span></span><br><span class="line">  scaled_value = <span class="number">1.5</span></span><br><span class="line">使用 Z 得分进行缩放意味着，大多数缩放后的值将介于 <span class="number">-3</span> 和 +<span class="number">3</span> 之间，而少量值将略高于或低于该范围。</span><br></pre></td></tr></table></figure>
<h3 id="处理极端离群值"><a href="#处理极端离群值" class="headerlink" title="处理极端离群值"></a>处理极端离群值</h3><p>分箱</p>
<h3 id="清查"><a href="#清查" class="headerlink" title="清查"></a>清查</h3><p>截至目前，我们假定用于训练和测试的所有数据都是值得信赖的。在现实生活中，数据集中的很多样本是不可靠的，原因有以下一种或多种：</p>
<pre><code>遗漏值。 例如，有人忘记为某个房屋的年龄输入值。
重复样本。 例如，服务器错误地将同一条记录上传了两次。
不良标签。 例如，有人错误地将一颗橡树的图片标记为枫树。
不良特征值。 例如，有人输入了多余的位数，或者温度计被遗落在太阳底下。
</code></pre><p>一旦检测到存在这些问题，您通常需要将相应样本从数据集中移除，从而“修正”不良样本。要检测遗漏值或重复样本，可以编写一个简单的程序。检测不良特征值或标签可能会比较棘手。</p>
<p>除了检测各个不良样本之外，还必须检测集合中的不良数据。直方图是一种用于可视化集合中数据的很好机制。此外，收集如下统计信息也会有所帮助：</p>
<pre><code>最大值和最小值
均值和中间值
标准偏差
</code></pre><p>考虑生成离散特征的最常见值列表。例如，country:uk 的样本数是否符合您的预期？language:jp 是否真的应该作为您数据集中的最常用语言？</p>
<p>了解数据</p>
<p>遵循以下规则：</p>
<pre><code>记住预期的数据状态。
确认数据是否满足这些预期（或者您可以解释为何数据不满足预期）。
仔细检查训练数据是否与其他来源（例如信息中心）的数据一致。
像处理任何任务关键型代码一样谨慎处理您的数据。良好的机器学习依赖于良好的数据。
</code></pre><h1 id="特征集编程练习"><a href="#特征集编程练习" class="headerlink" title="特征集编程练习"></a>特征集编程练习</h1><p>创建一个包含极少特征但效果与更复杂的特征集一样出色的集合</p>
<h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2><p>和之前一样，我们先加载并准备加利福尼亚州住房数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> gridspec</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line">tf.logging.set_verbosity(tf.logging.ERROR)</span><br><span class="line">pd.options.display.max_rows = <span class="number">10</span></span><br><span class="line">pd.options.display.float_format = <span class="string">'&#123;:.1f&#125;'</span>.format</span><br><span class="line"></span><br><span class="line">california_housing_dataframe = pd.read_csv(<span class="string">"https://download.mlcc.google.cn/mledu-datasets/california_housing_train.csv"</span>, sep=<span class="string">","</span>)</span><br><span class="line"></span><br><span class="line">california_housing_dataframe = california_housing_dataframe.reindex(</span><br><span class="line">    np.random.permutation(california_housing_dataframe.index))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 预处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_features</span><span class="params">(california_housing_dataframe)</span>:</span></span><br><span class="line">    selected_features = california_housing_dataframe[</span><br><span class="line">        [<span class="string">"latitude"</span>,</span><br><span class="line">         <span class="string">"longitude"</span>,</span><br><span class="line">         <span class="string">"housing_median_age"</span>,</span><br><span class="line">         <span class="string">"total_rooms"</span>,</span><br><span class="line">         <span class="string">"total_bedrooms"</span>,</span><br><span class="line">         <span class="string">"population"</span>,</span><br><span class="line">         <span class="string">"households"</span>,</span><br><span class="line">         <span class="string">"median_income"</span>]]</span><br><span class="line">    </span><br><span class="line">    processed_features = selected_features.copy()</span><br><span class="line">    </span><br><span class="line">    processed_features[<span class="string">"rooms_per_person"</span>] = (</span><br><span class="line">        california_housing_dataframe[<span class="string">"total_rooms"</span>]/</span><br><span class="line">        california_housing_dataframe[<span class="string">"population"</span>])</span><br><span class="line">    <span class="keyword">return</span> processed_features</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_targets</span><span class="params">(california_housing_dataframe)</span>:</span></span><br><span class="line">    output_targets = pd.DataFrame()</span><br><span class="line">    <span class="comment"># Scale the target to be in units of thousands of dollars.</span></span><br><span class="line">    output_targets[<span class="string">"median_house_value"</span>] = (</span><br><span class="line">      california_housing_dataframe[<span class="string">"median_house_value"</span>] / <span class="number">1000.0</span>)</span><br><span class="line">    <span class="keyword">return</span> output_targets</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 抽取前 12000 个数据作训练集</span></span><br><span class="line">training_examples = preprocess_features(california_housing_dataframe.head(<span class="number">12000</span>))</span><br><span class="line">training_targets = preprocess_targets(california_housing_dataframe.head(<span class="number">12000</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 抽取最后 5000 个作验证集</span></span><br><span class="line">validation_examples = preprocess_features(california_housing_dataframe.tail(<span class="number">5000</span>))</span><br><span class="line">validation_targets = preprocess_targets(california_housing_dataframe.tail(<span class="number">5000</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查一下我们的数据是否正常合理</span></span><br><span class="line">print(<span class="string">"Training examples summary:"</span>)</span><br><span class="line">display.display(training_examples.describe())</span><br><span class="line">print(<span class="string">"Validation examples summary:"</span>)</span><br><span class="line">display.display(validation_examples.describe())</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Training targets summary:"</span>)</span><br><span class="line">display.display(training_targets.describe())</span><br><span class="line">print(<span class="string">"Validation targets summary:"</span>)</span><br><span class="line">display.display(validation_targets.describe())</span><br></pre></td></tr></table></figure>
<pre><code>Training examples summary:
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>latitude</th>
      <th>longitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>rooms_per_person</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>35.6</td>
      <td>-119.6</td>
      <td>28.6</td>
      <td>2655.8</td>
      <td>543.1</td>
      <td>1433.2</td>
      <td>504.6</td>
      <td>3.9</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>std</th>
      <td>2.1</td>
      <td>2.0</td>
      <td>12.6</td>
      <td>2180.4</td>
      <td>425.2</td>
      <td>1122.9</td>
      <td>387.0</td>
      <td>1.9</td>
      <td>1.1</td>
    </tr>
    <tr>
      <th>min</th>
      <td>32.5</td>
      <td>-124.3</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.5</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>33.9</td>
      <td>-121.8</td>
      <td>18.0</td>
      <td>1467.0</td>
      <td>298.0</td>
      <td>793.0</td>
      <td>283.0</td>
      <td>2.6</td>
      <td>1.5</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>34.2</td>
      <td>-118.5</td>
      <td>29.0</td>
      <td>2127.0</td>
      <td>435.0</td>
      <td>1170.0</td>
      <td>410.0</td>
      <td>3.5</td>
      <td>1.9</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>37.7</td>
      <td>-118.0</td>
      <td>37.0</td>
      <td>3157.2</td>
      <td>653.0</td>
      <td>1726.0</td>
      <td>608.0</td>
      <td>4.8</td>
      <td>2.3</td>
    </tr>
    <tr>
      <th>max</th>
      <td>42.0</td>
      <td>-114.6</td>
      <td>52.0</td>
      <td>32627.0</td>
      <td>6445.0</td>
      <td>28566.0</td>
      <td>6082.0</td>
      <td>15.0</td>
      <td>55.2</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>Validation examples summary:
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>latitude</th>
      <th>longitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>rooms_per_person</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>35.7</td>
      <td>-119.6</td>
      <td>28.6</td>
      <td>2614.5</td>
      <td>530.5</td>
      <td>1420.8</td>
      <td>493.0</td>
      <td>3.9</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>std</th>
      <td>2.1</td>
      <td>2.0</td>
      <td>12.6</td>
      <td>2178.9</td>
      <td>412.5</td>
      <td>1205.6</td>
      <td>378.5</td>
      <td>2.0</td>
      <td>1.3</td>
    </tr>
    <tr>
      <th>min</th>
      <td>32.5</td>
      <td>-124.3</td>
      <td>1.0</td>
      <td>18.0</td>
      <td>3.0</td>
      <td>8.0</td>
      <td>3.0</td>
      <td>0.5</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>33.9</td>
      <td>-121.8</td>
      <td>18.0</td>
      <td>1445.0</td>
      <td>294.0</td>
      <td>780.0</td>
      <td>276.8</td>
      <td>2.6</td>
      <td>1.5</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>34.3</td>
      <td>-118.5</td>
      <td>29.0</td>
      <td>2130.5</td>
      <td>430.0</td>
      <td>1159.5</td>
      <td>406.0</td>
      <td>3.5</td>
      <td>1.9</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>37.7</td>
      <td>-118.0</td>
      <td>37.0</td>
      <td>3129.2</td>
      <td>637.0</td>
      <td>1700.8</td>
      <td>595.0</td>
      <td>4.7</td>
      <td>2.3</td>
    </tr>
    <tr>
      <th>max</th>
      <td>41.8</td>
      <td>-114.3</td>
      <td>52.0</td>
      <td>37937.0</td>
      <td>5471.0</td>
      <td>35682.0</td>
      <td>5189.0</td>
      <td>15.0</td>
      <td>52.0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>Training targets summary:
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>median_house_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>12000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>207.5</td>
    </tr>
    <tr>
      <th>std</th>
      <td>115.4</td>
    </tr>
    <tr>
      <th>min</th>
      <td>15.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>119.9</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>181.1</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>265.6</td>
    </tr>
    <tr>
      <th>max</th>
      <td>500.0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>Validation targets summary:
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>median_house_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>5000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>206.9</td>
    </tr>
    <tr>
      <th>std</th>
      <td>117.5</td>
    </tr>
    <tr>
      <th>min</th>
      <td>15.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>118.8</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>178.4</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>263.1</td>
    </tr>
    <tr>
      <th>max</th>
      <td>500.0</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="构建良好的特征集"><a href="#构建良好的特征集" class="headerlink" title="构建良好的特征集"></a>构建良好的特征集</h2><p><strong>如果只使用 2 个或 3 个特征，您可以获得的最佳效果是什么？</strong></p>
<p><strong>相关矩阵</strong>展现了两两比较的相关性，既包括每个特征与目标特征之间的比较，也包括每个特征与其他特征之间的比较。</p>
<p>在这里，相关性被定义为<a href="https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient" target="_blank" rel="noopener">皮尔逊相关系数</a>。您不必理解具体数学原理也可完成本练习。</p>
<p>相关性值具有以下含义：</p>
<ul>
<li><code>-1.0</code>：完全负相关</li>
<li><code>0.0</code>：不相关</li>
<li><code>1.0</code>：完全正相关</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correlation_dataframe = training_examples.copy()</span><br><span class="line">correlation_dataframe[<span class="string">"target"</span>] = training_targets[<span class="string">"median_house_value"</span>]</span><br><span class="line"></span><br><span class="line">correlation_dataframe.corr()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>latitude</th>
      <th>longitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>rooms_per_person</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>latitude</th>
      <td>1.0</td>
      <td>-0.9</td>
      <td>0.0</td>
      <td>-0.0</td>
      <td>-0.1</td>
      <td>-0.1</td>
      <td>-0.1</td>
      <td>-0.1</td>
      <td>0.1</td>
      <td>-0.1</td>
    </tr>
    <tr>
      <th>longitude</th>
      <td>-0.9</td>
      <td>1.0</td>
      <td>-0.1</td>
      <td>0.0</td>
      <td>0.1</td>
      <td>0.1</td>
      <td>0.1</td>
      <td>-0.0</td>
      <td>-0.1</td>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>housing_median_age</th>
      <td>0.0</td>
      <td>-0.1</td>
      <td>1.0</td>
      <td>-0.4</td>
      <td>-0.3</td>
      <td>-0.3</td>
      <td>-0.3</td>
      <td>-0.1</td>
      <td>-0.1</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>total_rooms</th>
      <td>-0.0</td>
      <td>0.0</td>
      <td>-0.4</td>
      <td>1.0</td>
      <td>0.9</td>
      <td>0.9</td>
      <td>0.9</td>
      <td>0.2</td>
      <td>0.1</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>total_bedrooms</th>
      <td>-0.1</td>
      <td>0.1</td>
      <td>-0.3</td>
      <td>0.9</td>
      <td>1.0</td>
      <td>0.9</td>
      <td>1.0</td>
      <td>-0.0</td>
      <td>0.1</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>population</th>
      <td>-0.1</td>
      <td>0.1</td>
      <td>-0.3</td>
      <td>0.9</td>
      <td>0.9</td>
      <td>1.0</td>
      <td>0.9</td>
      <td>-0.0</td>
      <td>-0.1</td>
      <td>-0.0</td>
    </tr>
    <tr>
      <th>households</th>
      <td>-0.1</td>
      <td>0.1</td>
      <td>-0.3</td>
      <td>0.9</td>
      <td>1.0</td>
      <td>0.9</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>-0.0</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>median_income</th>
      <td>-0.1</td>
      <td>-0.0</td>
      <td>-0.1</td>
      <td>0.2</td>
      <td>-0.0</td>
      <td>-0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.3</td>
      <td>0.7</td>
    </tr>
    <tr>
      <th>rooms_per_person</th>
      <td>0.1</td>
      <td>-0.1</td>
      <td>-0.1</td>
      <td>0.1</td>
      <td>0.1</td>
      <td>-0.1</td>
      <td>-0.0</td>
      <td>0.3</td>
      <td>1.0</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>target</th>
      <td>-0.1</td>
      <td>-0.0</td>
      <td>0.1</td>
      <td>0.1</td>
      <td>0.0</td>
      <td>-0.0</td>
      <td>0.1</td>
      <td>0.7</td>
      <td>0.2</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>

<p> 理想情况下，我们希望具有与目标密切相关的特征。</p>
<p>此外，我们还希望有一些相互之间的相关性不太密切的特征，以便它们添加独立信息。</p>
<p>利用这些信息来尝试移除特征。您也可以尝试构建其他合成特征，例如两个原始特征的比例。</p>
<p>为方便起见，我们已经添加了前一个练习的训练代码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">construct_feature_columns</span><span class="params">(input_features)</span>:</span></span><br><span class="line">  <span class="string">"""Construct the TensorFlow Feature Columns.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_features: The names of the numerical input features to use.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A set of feature columns</span></span><br><span class="line"><span class="string">  """</span> </span><br><span class="line">  <span class="keyword">return</span> set([tf.feature_column.numeric_column(my_feature)</span><br><span class="line">              <span class="keyword">for</span> my_feature <span class="keyword">in</span> input_features])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_input_fn</span><span class="params">(features, targets, batch_size=<span class="number">1</span>, shuffle=True, num_epochs=None)</span>:</span></span><br><span class="line">    <span class="string">"""Trains a linear regression model.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      features: pandas DataFrame of features</span></span><br><span class="line"><span class="string">      targets: pandas DataFrame of targets</span></span><br><span class="line"><span class="string">      batch_size: Size of batches to be passed to the model</span></span><br><span class="line"><span class="string">      shuffle: True or False. Whether to shuffle the data.</span></span><br><span class="line"><span class="string">      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      Tuple of (features, labels) for next data batch</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Convert pandas data into a dict of np arrays.</span></span><br><span class="line">    features = &#123;key:np.array(value) <span class="keyword">for</span> key,value <span class="keyword">in</span> dict(features).items()&#125;                                           </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Construct a dataset, and configure batching/repeating.</span></span><br><span class="line">    ds = Dataset.from_tensor_slices((features,targets)) <span class="comment"># warning: 2GB limit</span></span><br><span class="line">    ds = ds.batch(batch_size).repeat(num_epochs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Shuffle the data, if specified.</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">      ds = ds.shuffle(<span class="number">10000</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Return the next batch of data.</span></span><br><span class="line">    features, labels = ds.make_one_shot_iterator().get_next()</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">    steps,</span></span></span><br><span class="line"><span class="function"><span class="params">    batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_targets,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_targets)</span>:</span></span><br><span class="line">  <span class="string">"""Trains a linear regression model.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  In addition to training, this function also prints training progress information,</span></span><br><span class="line"><span class="string">  as well as a plot of the training and validation loss over time.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    learning_rate: A `float`, the learning rate.</span></span><br><span class="line"><span class="string">    steps: A non-zero `int`, the total number of training steps. A training step</span></span><br><span class="line"><span class="string">      consists of a forward and backward pass using a single batch.</span></span><br><span class="line"><span class="string">    batch_size: A non-zero `int`, the batch size.</span></span><br><span class="line"><span class="string">    training_examples: A `DataFrame` containing one or more columns from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as input features for training.</span></span><br><span class="line"><span class="string">    training_targets: A `DataFrame` containing exactly one column from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as target for training.</span></span><br><span class="line"><span class="string">    validation_examples: A `DataFrame` containing one or more columns from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as input features for validation.</span></span><br><span class="line"><span class="string">    validation_targets: A `DataFrame` containing exactly one column from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as target for validation.</span></span><br><span class="line"><span class="string">      </span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A `LinearRegressor` object trained on the training data.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  periods = <span class="number">10</span></span><br><span class="line">  steps_per_period = steps / periods</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Create a linear regressor object.</span></span><br><span class="line">  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line">  linear_regressor = tf.estimator.LinearRegressor(</span><br><span class="line">      feature_columns=construct_feature_columns(training_examples),</span><br><span class="line">      optimizer=my_optimizer</span><br><span class="line">  )</span><br><span class="line">    </span><br><span class="line">  <span class="comment"># Create input functions.</span></span><br><span class="line">  training_input_fn = <span class="keyword">lambda</span>: my_input_fn(training_examples, </span><br><span class="line">                                          training_targets[<span class="string">"median_house_value"</span>], </span><br><span class="line">                                          batch_size=batch_size)</span><br><span class="line">  predict_training_input_fn = <span class="keyword">lambda</span>: my_input_fn(training_examples, </span><br><span class="line">                                                  training_targets[<span class="string">"median_house_value"</span>], </span><br><span class="line">                                                  num_epochs=<span class="number">1</span>, </span><br><span class="line">                                                  shuffle=<span class="keyword">False</span>)</span><br><span class="line">  predict_validation_input_fn = <span class="keyword">lambda</span>: my_input_fn(validation_examples, </span><br><span class="line">                                                    validation_targets[<span class="string">"median_house_value"</span>], </span><br><span class="line">                                                    num_epochs=<span class="number">1</span>, </span><br><span class="line">                                                    shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Train the model, but do so inside a loop so that we can periodically assess</span></span><br><span class="line">  <span class="comment"># loss metrics.</span></span><br><span class="line">  print(<span class="string">"Training model..."</span>)</span><br><span class="line">  print(<span class="string">"RMSE (on training data):"</span>)</span><br><span class="line">  training_rmse = []</span><br><span class="line">  validation_rmse = []</span><br><span class="line">  <span class="keyword">for</span> period <span class="keyword">in</span> range (<span class="number">0</span>, periods):</span><br><span class="line">    <span class="comment"># Train the model, starting from the prior state.</span></span><br><span class="line">    linear_regressor.train(</span><br><span class="line">        input_fn=training_input_fn,</span><br><span class="line">        steps=steps_per_period,</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># Take a break and compute predictions.</span></span><br><span class="line">    training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn)</span><br><span class="line">    training_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> training_predictions])</span><br><span class="line">    </span><br><span class="line">    validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)</span><br><span class="line">    validation_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> validation_predictions])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute training and validation loss.</span></span><br><span class="line">    training_root_mean_squared_error = math.sqrt(</span><br><span class="line">        metrics.mean_squared_error(training_predictions, training_targets))</span><br><span class="line">    validation_root_mean_squared_error = math.sqrt(</span><br><span class="line">        metrics.mean_squared_error(validation_predictions, validation_targets))</span><br><span class="line">    <span class="comment"># Occasionally print the current loss.</span></span><br><span class="line">    print(<span class="string">"  period %02d : %0.2f"</span> % (period, training_root_mean_squared_error))</span><br><span class="line">    <span class="comment"># Add the loss metrics from this period to our list.</span></span><br><span class="line">    training_rmse.append(training_root_mean_squared_error)</span><br><span class="line">    validation_rmse.append(validation_root_mean_squared_error)</span><br><span class="line">  print(<span class="string">"Model training finished."</span>)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Output a graph of loss metrics over periods.</span></span><br><span class="line">  plt.ylabel(<span class="string">"RMSE"</span>)</span><br><span class="line">  plt.xlabel(<span class="string">"Periods"</span>)</span><br><span class="line">  plt.title(<span class="string">"Root Mean Squared Error vs. Periods"</span>)</span><br><span class="line">  plt.tight_layout()</span><br><span class="line">  plt.plot(training_rmse, label=<span class="string">"training"</span>)</span><br><span class="line">  plt.plot(validation_rmse, label=<span class="string">"validation"</span>)</span><br><span class="line">  plt.legend()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> linear_regressor</span><br></pre></td></tr></table></figure>
<p>搜索一组效果良好的特征和训练参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">minimal_features = [</span><br><span class="line">  <span class="string">"latitude"</span>,</span><br><span class="line">  <span class="string">"longitude"</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">minimal_training_examples = training_examples[minimal_features]</span><br><span class="line">minimal_validation_examples = validation_examples[minimal_features]</span><br><span class="line"></span><br><span class="line">_ = train_model(</span><br><span class="line">    learning_rate=<span class="number">0.01</span>,</span><br><span class="line">    steps=<span class="number">500</span>,</span><br><span class="line">    batch_size=<span class="number">5</span>,</span><br><span class="line">    training_examples=minimal_training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=minimal_validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">minimal_features = [</span><br><span class="line">  <span class="string">"median_income"</span>,</span><br><span class="line">  <span class="string">"latitude"</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">minimal_training_examples = training_examples[minimal_features]</span><br><span class="line">minimal_validation_examples = validation_examples[minimal_features]</span><br><span class="line"></span><br><span class="line">_ = train_model(</span><br><span class="line">    learning_rate=<span class="number">0.01</span>,</span><br><span class="line">    steps=<span class="number">500</span>,</span><br><span class="line">    batch_size=<span class="number">5</span>,</span><br><span class="line">    training_examples=minimal_training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=minimal_validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
  period 00 : 115.69
  period 01 : 115.56
  period 02 : 116.82
  period 03 : 115.67
  period 04 : 115.57
  period 05 : 116.72
  period 06 : 116.71
  period 07 : 119.22
  period 08 : 115.56
  period 09 : 115.40
Model training finished.
</code></pre><p><img src="output_22_1.png" alt="png"></p>
<pre><code>Training model...
RMSE (on training data):
  period 00 : 165.32
  period 01 : 125.38
  period 02 : 116.53
  period 03 : 115.99
  period 04 : 115.63
  period 05 : 114.81
  period 06 : 114.06
  period 07 : 113.58
  period 08 : 114.35
  period 09 : 112.88
Model training finished.
</code></pre><p><img src="output_22_3.png" alt="png"></p>
<p>观察发现 经度 和 维度 貌似是没有什么相关度的，个人收入的中位数 和 维度 是一组比较好的特征组。</p>
<h2 id="更好地利用纬度"><a href="#更好地利用纬度" class="headerlink" title="更好地利用纬度"></a>更好地利用纬度</h2><p>绘制 <code>latitude</code> 与 <code>median_house_value</code> 的图形后，表明两者确实不存在线性关系。</p>
<p>不过，有几个峰值与洛杉矶和旧金山大致相对应。</p>
<figure class="highlight prolog"><table><tr><td class="code"><pre><span class="line">plt.scatter(training_examples[<span class="string">"latitude"</span>], training_targets[<span class="string">"median_house_value"</span>])</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.collections.PathCollection at 0x7fb5a238afd0&gt;
</code></pre><p><img src="output_25_1.png" alt="png"></p>
<p> <strong>尝试创建一些能够更好地利用纬度的合成特征。</strong></p>
<p>例如，您可以创建某个特征，将 <code>latitude</code> 映射到值 <code>|latitude - 38|</code>，并将该特征命名为 <code>distance_from_san_francisco</code>。</p>
<p>或者，您可以将该空间分成 10 个不同的分桶（例如 <code>latitude_32_to_33</code>、<code>latitude_33_to_34</code> 等）：如果 <code>latitude</code> 位于相应分桶范围内，则显示值 <code>1.0</code>；如果不在范围内，则显示值 <code>0.0</code>。</p>
<p>使用相关矩阵来指导您构建合成特征；如果您发现效果还不错的合成特征，可以将其添加到您的模型中。</p>
<p> 除了 <code>latitude</code> 之外，我们还会保留 <code>median_income</code>，以便与之前的结果进行比较。</p>
<p>我们决定对纬度进行分桶。在 Pandas 中使用 <code>Series.apply</code> 执行此操作相当简单。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">LATITUDE_RANGES = zip(range(<span class="number">32</span>, <span class="number">44</span>), range(<span class="number">33</span>, <span class="number">45</span>))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_and_transform_features</span><span class="params">(source_df)</span>:</span></span><br><span class="line">    selected_examples = pd.DataFrame()</span><br><span class="line">    selected_examples[<span class="string">"median_income"</span>] = source_df[<span class="string">"median_income"</span>]</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> LATITUDE_RANGES:</span><br><span class="line">      selected_examples[<span class="string">"latitude_%d_to_%d"</span> % r] = source_df[<span class="string">"latitude"</span>].apply(</span><br><span class="line">        <span class="keyword">lambda</span> l: <span class="number">1.0</span> <span class="keyword">if</span> l &gt;= r[<span class="number">0</span>] <span class="keyword">and</span> l &lt; r[<span class="number">1</span>] <span class="keyword">else</span> <span class="number">0.0</span>)</span><br><span class="line">    <span class="keyword">return</span> selected_examples</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">selected_training_examples = select_and_transform_features(training_examples)</span><br><span class="line">selected_training_examples.head(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>median_income</th>
      <th>latitude_32_to_33</th>
      <th>latitude_33_to_34</th>
      <th>latitude_34_to_35</th>
      <th>latitude_35_to_36</th>
      <th>latitude_36_to_37</th>
      <th>latitude_37_to_38</th>
      <th>latitude_38_to_39</th>
      <th>latitude_39_to_40</th>
      <th>latitude_40_to_41</th>
      <th>latitude_41_to_42</th>
      <th>latitude_42_to_43</th>
      <th>latitude_43_to_44</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4828</th>
      <td>3.4</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>6103</th>
      <td>4.7</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">selected_validation_examples = select_and_transform_features(validation_examples)</span><br><span class="line">selected_validation_examples.head(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>median_income</th>
      <th>latitude_32_to_33</th>
      <th>latitude_33_to_34</th>
      <th>latitude_34_to_35</th>
      <th>latitude_35_to_36</th>
      <th>latitude_36_to_37</th>
      <th>latitude_37_to_38</th>
      <th>latitude_38_to_39</th>
      <th>latitude_39_to_40</th>
      <th>latitude_40_to_41</th>
      <th>latitude_41_to_42</th>
      <th>latitude_42_to_43</th>
      <th>latitude_43_to_44</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>8959</th>
      <td>7.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>8330</th>
      <td>2.6</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_ = train_model(</span><br><span class="line">    learning_rate=<span class="number">0.01</span>,</span><br><span class="line">    steps=<span class="number">500</span>,</span><br><span class="line">    batch_size=<span class="number">5</span>,</span><br><span class="line">    training_examples=selected_training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=selected_validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
  period 00 : 227.14
  period 01 : 216.98
  period 02 : 206.92
  period 03 : 196.96
  period 04 : 187.10
  period 05 : 177.38
  period 06 : 167.81
  period 07 : 158.44
  period 08 : 149.28
  period 09 : 140.38
Model training finished.
</code></pre><p><img src="output_30_1.png" alt="png"></p>
]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow 6</title>
    <url>/2019/01/14/TensorFlow_6/</url>
    <content><![CDATA[<h1 id="特征组合-Feature-Crosses"><a href="#特征组合-Feature-Crosses" class="headerlink" title="特征组合 (Feature Crosses)"></a>特征组合 (Feature Crosses)</h1><p>特征组合是指两个或多个特征相乘形成的合成特征。特征的相乘组合可以提供超出这些特征单独能够提供的预测能力。</p>
<h2 id="对非线性规律进行编码"><a href="#对非线性规律进行编码" class="headerlink" title="对非线性规律进行编码"></a>对非线性规律进行编码</h2><h2 id="组合独热矢量"><a href="#组合独热矢量" class="headerlink" title="组合独热矢量"></a>组合独热矢量</h2><p>到目前为止，我们已经重点介绍了如何对两个单独的浮点特征进行特征组合。在实践中，机器学习模型很少会组合连续特征。不过，机器学习模型却经常组合独热特征矢量，将独热特征矢量的特征组合视为逻辑连接。例如，假设我们具有以下两个特征：国家/地区和语言。对每个特征进行独热编码会生成具有二元特征的矢量，这些二元特征可解读为 country=USA, country=France 或 language=English, language=Spanish。然后，如果您对这些独热编码进行特征组合，则会得到可解读为逻辑连接的二元特征，如下所示：</p>
<pre><code>country:usa AND language:spanish
</code></pre><p>再举一个例子，假设您对纬度和经度进行分箱，获得单独的独热 5 元素特征矢量。例如，指定的纬度和经度可以表示如下：</p>
<pre><code>binned_latitude = [0, 0, 0, 1, 0]
binned_longitude = [0, 1, 0, 0, 0]
</code></pre><p>假设您对这两个特征矢量创建了特征组合：</p>
<pre><code>binned_latitude X binned_longitude
</code></pre><p>此特征组合是一个 25 元素独热矢量（24 个 0 和 1 个 1）。该组合中的单个 1 表示纬度与经度的特定连接。然后，您的模型就可以了解到有关这种连接的特定关联性。</p>
<p>假设我们更粗略地对纬度和经度进行分箱，如下所示：</p>
<pre><code>binned_latitude(lat) = [
  0  &lt; lat &lt;= 10
  10 &lt; lat &lt;= 20
  20 &lt; lat &lt;= 30
]

binned_longitude(lon) = [
  0  &lt; lon &lt;= 15
  15 &lt; lon &lt;= 30
]
</code></pre><p>针对这些粗略分箱创建特征组合会生成具有以下含义的合成特征：</p>
<pre><code>binned_latitude_X_longitude(lat, lon) = [
  0  &lt; lat &lt;= 10 AND 0  &lt; lon &lt;= 15
  0  &lt; lat &lt;= 10 AND 15 &lt; lon &lt;= 30
  10 &lt; lat &lt;= 20 AND 0  &lt; lon &lt;= 15
  10 &lt; lat &lt;= 20 AND 15 &lt; lon &lt;= 30
  20 &lt; lat &lt;= 30 AND 0  &lt; lon &lt;= 15
  20 &lt; lat &lt;= 30 AND 15 &lt; lon &lt;= 30
]
</code></pre><p>现在，假设我们的模型需要根据以下两个特征来预测狗主人对狗狗的满意程度：</p>
<p>行为类型（吠叫、叫、偎依等）<br>时段<br>如果我们根据这两个特征构建以下特征组合：</p>
<pre><code>[behavior type X time of day]
</code></pre><p>我们最终获得的预测能力将远远超过任一特征单独的预测能力。例如，如果狗狗在下午 5 点主人下班回来时（快乐地）叫喊，可能表示对主人满意度的正面预测结果。如果狗狗在凌晨 3 点主人熟睡时（也许痛苦地）哀叫，可能表示对主人满意度的强烈负面预测结果。</p>
<p>线性学习器可以很好地扩展到大量数据。对大规模数据集使用特征组合是学习高度复杂模型的一种有效策略。神经网络可提供另一种策略。</p>
<h1 id="编程练习"><a href="#编程练习" class="headerlink" title="编程练习"></a>编程练习</h1><ul>
<li>通过添加其他合成特征来改进线性回归模型（这是前一个练习的延续）</li>
<li>使用输入函数将 Pandas <code>DataFrame</code> 对象转换为 <code>Tensors</code>，并在 <code>fit()</code> 和 <code>predict()</code> 中调用输入函数</li>
<li>使用 FTRL 优化算法进行模型训练</li>
<li>通过独热编码、分箱和特征组合创建新的合成特征</li>
</ul>
<h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2><p> 首先，我们来定义输入并创建数据加载代码，正如我们在之前的练习中所做的那样。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> gridspec</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line">tf.logging.set_verbosity(tf.logging.ERROR)</span><br><span class="line">pd.options.display.max_rows = <span class="number">10</span></span><br><span class="line">pd.options.display.float_format = <span class="string">'&#123;:.1f&#125;'</span>.format</span><br><span class="line"></span><br><span class="line">california_housing_dataframe = pd.read_csv(<span class="string">"https://download.mlcc.google.cn/mledu-datasets/california_housing_train.csv"</span>, sep=<span class="string">","</span>)</span><br><span class="line"></span><br><span class="line">california_housing_dataframe = california_housing_dataframe.reindex(</span><br><span class="line">    np.random.permutation(california_housing_dataframe.index))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_features</span><span class="params">(california_housing_dataframe)</span>:</span></span><br><span class="line">  <span class="string">"""Prepares input features from California housing data set.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    california_housing_dataframe: A Pandas DataFrame expected to contain data</span></span><br><span class="line"><span class="string">      from the California housing data set.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A DataFrame that contains the features to be used for the model, including</span></span><br><span class="line"><span class="string">    synthetic features.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  selected_features = california_housing_dataframe[</span><br><span class="line">    [<span class="string">"latitude"</span>,</span><br><span class="line">     <span class="string">"longitude"</span>,</span><br><span class="line">     <span class="string">"housing_median_age"</span>,</span><br><span class="line">     <span class="string">"total_rooms"</span>,</span><br><span class="line">     <span class="string">"total_bedrooms"</span>,</span><br><span class="line">     <span class="string">"population"</span>,</span><br><span class="line">     <span class="string">"households"</span>,</span><br><span class="line">     <span class="string">"median_income"</span>]]</span><br><span class="line">  processed_features = selected_features.copy()</span><br><span class="line">  <span class="comment"># Create a synthetic feature.</span></span><br><span class="line">  processed_features[<span class="string">"rooms_per_person"</span>] = (</span><br><span class="line">    california_housing_dataframe[<span class="string">"total_rooms"</span>] /</span><br><span class="line">    california_housing_dataframe[<span class="string">"population"</span>])</span><br><span class="line">  <span class="keyword">return</span> processed_features</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_targets</span><span class="params">(california_housing_dataframe)</span>:</span></span><br><span class="line">  <span class="string">"""Prepares target features (i.e., labels) from California housing data set.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    california_housing_dataframe: A Pandas DataFrame expected to contain data</span></span><br><span class="line"><span class="string">      from the California housing data set.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A DataFrame that contains the target feature.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  output_targets = pd.DataFrame()</span><br><span class="line">  <span class="comment"># Scale the target to be in units of thousands of dollars.</span></span><br><span class="line">  output_targets[<span class="string">"median_house_value"</span>] = (</span><br><span class="line">    california_housing_dataframe[<span class="string">"median_house_value"</span>] / <span class="number">1000.0</span>)</span><br><span class="line">  <span class="keyword">return</span> output_targets</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Choose the first 12000 (out of 17000) examples for training.</span></span><br><span class="line">training_examples = preprocess_features(california_housing_dataframe.head(<span class="number">12000</span>))</span><br><span class="line">training_targets = preprocess_targets(california_housing_dataframe.head(<span class="number">12000</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Choose the last 5000 (out of 17000) examples for validation.</span></span><br><span class="line">validation_examples = preprocess_features(california_housing_dataframe.tail(<span class="number">5000</span>))</span><br><span class="line">validation_targets = preprocess_targets(california_housing_dataframe.tail(<span class="number">5000</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Double-check that we've done the right thing.</span></span><br><span class="line">print(<span class="string">"Training examples summary:"</span>)</span><br><span class="line">display.display(training_examples.describe())</span><br><span class="line">print(<span class="string">"Validation examples summary:"</span>)</span><br><span class="line">display.display(validation_examples.describe())</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Training targets summary:"</span>)</span><br><span class="line">display.display(training_targets.describe())</span><br><span class="line">print(<span class="string">"Validation targets summary:"</span>)</span><br><span class="line">display.display(validation_targets.describe())</span><br></pre></td></tr></table></figure>
<pre><code>Training examples summary:
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>latitude</th>
      <th>longitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>rooms_per_person</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>35.6</td>
      <td>-119.5</td>
      <td>28.6</td>
      <td>2643.5</td>
      <td>539.4</td>
      <td>1430.0</td>
      <td>501.4</td>
      <td>3.9</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>std</th>
      <td>2.1</td>
      <td>2.0</td>
      <td>12.6</td>
      <td>2203.2</td>
      <td>423.9</td>
      <td>1153.9</td>
      <td>388.0</td>
      <td>1.9</td>
      <td>1.2</td>
    </tr>
    <tr>
      <th>min</th>
      <td>32.5</td>
      <td>-124.3</td>
      <td>1.0</td>
      <td>8.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.5</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>33.9</td>
      <td>-121.8</td>
      <td>18.0</td>
      <td>1457.8</td>
      <td>295.0</td>
      <td>788.0</td>
      <td>280.0</td>
      <td>2.6</td>
      <td>1.5</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>34.2</td>
      <td>-118.5</td>
      <td>29.0</td>
      <td>2121.0</td>
      <td>432.0</td>
      <td>1165.0</td>
      <td>407.0</td>
      <td>3.5</td>
      <td>1.9</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>37.7</td>
      <td>-118.0</td>
      <td>37.0</td>
      <td>3149.2</td>
      <td>647.0</td>
      <td>1717.0</td>
      <td>603.2</td>
      <td>4.8</td>
      <td>2.3</td>
    </tr>
    <tr>
      <th>max</th>
      <td>42.0</td>
      <td>-114.3</td>
      <td>52.0</td>
      <td>37937.0</td>
      <td>5471.0</td>
      <td>35682.0</td>
      <td>5189.0</td>
      <td>15.0</td>
      <td>55.2</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>Validation examples summary:
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>latitude</th>
      <th>longitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>rooms_per_person</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>35.7</td>
      <td>-119.6</td>
      <td>28.7</td>
      <td>2644.1</td>
      <td>539.3</td>
      <td>1428.5</td>
      <td>500.7</td>
      <td>3.9</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>std</th>
      <td>2.1</td>
      <td>2.0</td>
      <td>12.6</td>
      <td>2123.4</td>
      <td>415.6</td>
      <td>1133.3</td>
      <td>376.0</td>
      <td>1.9</td>
      <td>1.1</td>
    </tr>
    <tr>
      <th>min</th>
      <td>32.5</td>
      <td>-124.3</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>6.0</td>
      <td>2.0</td>
      <td>0.5</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>33.9</td>
      <td>-121.8</td>
      <td>18.0</td>
      <td>1473.0</td>
      <td>300.0</td>
      <td>792.0</td>
      <td>285.0</td>
      <td>2.6</td>
      <td>1.5</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>34.3</td>
      <td>-118.5</td>
      <td>29.0</td>
      <td>2148.5</td>
      <td>437.0</td>
      <td>1172.0</td>
      <td>413.5</td>
      <td>3.5</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>37.7</td>
      <td>-118.0</td>
      <td>37.0</td>
      <td>3153.2</td>
      <td>652.2</td>
      <td>1737.0</td>
      <td>608.0</td>
      <td>4.8</td>
      <td>2.3</td>
    </tr>
    <tr>
      <th>max</th>
      <td>42.0</td>
      <td>-114.6</td>
      <td>52.0</td>
      <td>32627.0</td>
      <td>6445.0</td>
      <td>28566.0</td>
      <td>6082.0</td>
      <td>15.0</td>
      <td>41.3</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>Training targets summary:
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>median_house_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>12000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>207.6</td>
    </tr>
    <tr>
      <th>std</th>
      <td>116.2</td>
    </tr>
    <tr>
      <th>min</th>
      <td>15.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>119.8</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>180.4</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>265.7</td>
    </tr>
    <tr>
      <th>max</th>
      <td>500.0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>Validation targets summary:
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>median_house_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>5000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>206.5</td>
    </tr>
    <tr>
      <th>std</th>
      <td>115.6</td>
    </tr>
    <tr>
      <th>min</th>
      <td>15.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>118.8</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>180.1</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>263.4</td>
    </tr>
    <tr>
      <th>max</th>
      <td>500.0</td>
    </tr>
  </tbody>
</table>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">construct_feature_columns</span><span class="params">(input_features)</span>:</span></span><br><span class="line">  <span class="string">"""Construct the TensorFlow Feature Columns.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_features: The names of the numerical input features to use.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A set of feature columns</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="keyword">return</span> set([tf.feature_column.numeric_column(my_feature)</span><br><span class="line">              <span class="keyword">for</span> my_feature <span class="keyword">in</span> input_features])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_input_fn</span><span class="params">(features, targets, batch_size=<span class="number">1</span>, shuffle=True, num_epochs=None)</span>:</span></span><br><span class="line">    <span class="string">"""Trains a linear regression model.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      features: pandas DataFrame of features</span></span><br><span class="line"><span class="string">      targets: pandas DataFrame of targets</span></span><br><span class="line"><span class="string">      batch_size: Size of batches to be passed to the model</span></span><br><span class="line"><span class="string">      shuffle: True or False. Whether to shuffle the data.</span></span><br><span class="line"><span class="string">      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      Tuple of (features, labels) for next data batch</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Convert pandas data into a dict of np arrays.</span></span><br><span class="line">    features = &#123;key:np.array(value) <span class="keyword">for</span> key,value <span class="keyword">in</span> dict(features).items()&#125;                                           </span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Construct a dataset, and configure batching/repeating.</span></span><br><span class="line">    ds = Dataset.from_tensor_slices((features,targets)) <span class="comment"># warning: 2GB limit</span></span><br><span class="line">    ds = ds.batch(batch_size).repeat(num_epochs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Shuffle the data, if specified.</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">      ds = ds.shuffle(<span class="number">10000</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Return the next batch of data.</span></span><br><span class="line">    features, labels = ds.make_one_shot_iterator().get_next()</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br></pre></td></tr></table></figure>
<h2 id="FTRL-优化算法"><a href="#FTRL-优化算法" class="headerlink" title="FTRL 优化算法"></a>FTRL 优化算法</h2><p>高维度线性模型可受益于使用一种基于梯度的优化方法，叫做 FTRL。该算法的优势是针对不同系数以不同方式调整学习速率，如果某些特征很少采用非零值，该算法可能比较实用（也非常适合支持 L1 正则化）。我们可以使用 <a href="https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer" target="_blank" rel="noopener">FtrlOptimizer</a> 来应用 FTRL。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">    steps,</span></span></span><br><span class="line"><span class="function"><span class="params">    batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    feature_columns,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_targets,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_targets)</span>:</span></span><br><span class="line">  <span class="string">"""Trains a linear regression model.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  In addition to training, this function also prints training progress information,</span></span><br><span class="line"><span class="string">  as well as a plot of the training and validation loss over time.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    learning_rate: A `float`, the learning rate.</span></span><br><span class="line"><span class="string">    steps: A non-zero `int`, the total number of training steps. A training step</span></span><br><span class="line"><span class="string">      consists of a forward and backward pass using a single batch.</span></span><br><span class="line"><span class="string">    feature_columns: A `set` specifying the input feature columns to use.</span></span><br><span class="line"><span class="string">    training_examples: A `DataFrame` containing one or more columns from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as input features for training.</span></span><br><span class="line"><span class="string">    training_targets: A `DataFrame` containing exactly one column from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as target for training.</span></span><br><span class="line"><span class="string">    validation_examples: A `DataFrame` containing one or more columns from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as input features for validation.</span></span><br><span class="line"><span class="string">    validation_targets: A `DataFrame` containing exactly one column from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as target for validation.</span></span><br><span class="line"><span class="string">      </span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A `LinearRegressor` object trained on the training data.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  periods = <span class="number">10</span></span><br><span class="line">  steps_per_period = steps / periods</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Create a linear regressor object.</span></span><br><span class="line">  my_optimizer = tf.train.FtrlOptimizer(learning_rate=learning_rate)</span><br><span class="line">  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line">  linear_regressor = tf.estimator.LinearRegressor(</span><br><span class="line">      feature_columns=feature_columns,</span><br><span class="line">      optimizer=my_optimizer</span><br><span class="line">  )</span><br><span class="line">  </span><br><span class="line">  training_input_fn = <span class="keyword">lambda</span>: my_input_fn(training_examples, </span><br><span class="line">                                          training_targets[<span class="string">"median_house_value"</span>], </span><br><span class="line">                                          batch_size=batch_size)</span><br><span class="line">  predict_training_input_fn = <span class="keyword">lambda</span>: my_input_fn(training_examples, </span><br><span class="line">                                                  training_targets[<span class="string">"median_house_value"</span>], </span><br><span class="line">                                                  num_epochs=<span class="number">1</span>, </span><br><span class="line">                                                  shuffle=<span class="keyword">False</span>)</span><br><span class="line">  predict_validation_input_fn = <span class="keyword">lambda</span>: my_input_fn(validation_examples, </span><br><span class="line">                                                    validation_targets[<span class="string">"median_house_value"</span>], </span><br><span class="line">                                                    num_epochs=<span class="number">1</span>, </span><br><span class="line">                                                    shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Train the model, but do so inside a loop so that we can periodically assess</span></span><br><span class="line">  <span class="comment"># loss metrics.</span></span><br><span class="line">  print(<span class="string">"Training model..."</span>)</span><br><span class="line">  print(<span class="string">"RMSE (on training data):"</span>)</span><br><span class="line">  training_rmse = []</span><br><span class="line">  validation_rmse = []</span><br><span class="line">  <span class="keyword">for</span> period <span class="keyword">in</span> range (<span class="number">0</span>, periods):</span><br><span class="line">    <span class="comment"># Train the model, starting from the prior state.</span></span><br><span class="line">    linear_regressor.train(</span><br><span class="line">        input_fn=training_input_fn,</span><br><span class="line">        steps=steps_per_period</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># Take a break and compute predictions.</span></span><br><span class="line">    training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn)</span><br><span class="line">    training_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> training_predictions])</span><br><span class="line">    validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)</span><br><span class="line">    validation_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> validation_predictions])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute training and validation loss.</span></span><br><span class="line">    training_root_mean_squared_error = math.sqrt(</span><br><span class="line">        metrics.mean_squared_error(training_predictions, training_targets))</span><br><span class="line">    validation_root_mean_squared_error = math.sqrt(</span><br><span class="line">        metrics.mean_squared_error(validation_predictions, validation_targets))</span><br><span class="line">    <span class="comment"># Occasionally print the current loss.</span></span><br><span class="line">    print(<span class="string">"  period %02d : %0.2f"</span> % (period, training_root_mean_squared_error))</span><br><span class="line">    <span class="comment"># Add the loss metrics from this period to our list.</span></span><br><span class="line">    training_rmse.append(training_root_mean_squared_error)</span><br><span class="line">    validation_rmse.append(validation_root_mean_squared_error)</span><br><span class="line">  print(<span class="string">"Model training finished."</span>)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Output a graph of loss metrics over periods.</span></span><br><span class="line">  plt.ylabel(<span class="string">"RMSE"</span>)</span><br><span class="line">  plt.xlabel(<span class="string">"Periods"</span>)</span><br><span class="line">  plt.title(<span class="string">"Root Mean Squared Error vs. Periods"</span>)</span><br><span class="line">  plt.tight_layout()</span><br><span class="line">  plt.plot(training_rmse, label=<span class="string">"training"</span>)</span><br><span class="line">  plt.plot(validation_rmse, label=<span class="string">"validation"</span>)</span><br><span class="line">  plt.legend()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> linear_regressor</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_ = train_model(</span><br><span class="line">    learning_rate=<span class="number">1.0</span>,</span><br><span class="line">    steps=<span class="number">500</span>,</span><br><span class="line">    batch_size=<span class="number">100</span>,</span><br><span class="line">    feature_columns=construct_feature_columns(training_examples),</span><br><span class="line">    training_examples=training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
  period 00 : 180.73
  period 01 : 170.12
  period 02 : 111.43
  period 03 : 157.79
  period 04 : 147.28
  period 05 : 139.07
  period 06 : 120.82
  period 07 : 148.39
  period 08 : 118.07
  period 09 : 117.32
Model training finished.
</code></pre><p><img src="TensorFlow_6_13_1.png" alt="png"></p>
<h2 id="离散特征的独热编码"><a href="#离散特征的独热编码" class="headerlink" title="离散特征的独热编码"></a>离散特征的独热编码</h2><p>通常，在训练逻辑回归模型之前，离散（即字符串、枚举、整数）特征会转换为二元特征系列。</p>
<p>例如，假设我们创建了一个合成特征，可以采用 <code>0</code>、<code>1</code> 或 <code>2</code> 中的任何值，并且我们还具有以下几个训练点：</p>
<table>
<thead>
<tr>
<th></th>
<th>feature_value</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>对于每个可能的分类值，我们都会创建一个新的<strong>二元实值</strong>特征，该特征只能采用两个可能值中的一个：如果示例中包含该值，则值为 1.0；如果不包含，则值为 0.0。在上述示例中，分类特征会被转换成三个特征，现在训练点如下所示：</p>
<table>
<thead>
<tr>
<th></th>
<th>feature_value_0</th>
<th>feature_value_1</th>
<th>feature_value_2</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.0</td>
<td>0.0</td>
<td>1.0</td>
</tr>
<tr>
<td>1</td>
<td>1.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
<tr>
<td>2</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
</tr>
</tbody>
</table>
<h2 id="分桶（分箱）特征"><a href="#分桶（分箱）特征" class="headerlink" title="分桶（分箱）特征"></a>分桶（分箱）特征</h2><p>分桶也称为分箱。</p>
<p>例如，我们可以将 <code>population</code> 分为以下 3 个分桶：</p>
<ul>
<li><code>bucket_0</code> (<code>&lt; 5000</code>)：对应于人口分布较少的街区</li>
<li><code>bucket_1</code> (<code>5000 - 25000</code>)：对应于人口分布适中的街区</li>
<li><code>bucket_2</code> (<code>&gt; 25000</code>)：对应于人口分布较多的街区</li>
</ul>
<p>根据前面的分桶定义，以下 <code>population</code> 矢量：</p>
<pre><code>[[10001], [42004], [2500], [18000]]
</code></pre><p>将变成以下经过分桶的特征矢量：</p>
<pre><code>[[1], [2], [0], [1]]
</code></pre><p>这些特征值现在是分桶索引。请注意，这些索引被视为离散特征。通常情况下，这些特征将被进一步转换为上述独热表示法，但这是以透明方式实现的。</p>
<p>要为分桶特征定义特征列，我们可以使用 <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column" target="_blank" rel="noopener"><code>bucketized_column</code></a>（而不是使用 <code>numeric_column</code>），该列将数字列作为输入，并使用 <code>boundaries</code> 参数中指定的分桶边界将其转换为分桶特征。以下代码为 <code>households</code> 和 <code>longitude</code> 定义了分桶特征列；<code>get_quantile_based_boundaries</code> 函数会根据分位数计算边界，以便每个分桶包含相同数量的元素。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_quantile_based_boundaries</span><span class="params">(feature_values, num_buckets)</span>:</span></span><br><span class="line">    boundaries = np.arange(<span class="number">1.0</span>, num_buckets) / num_buckets</span><br><span class="line">    quantiles = feature_values.quantile(boundaries)</span><br><span class="line">    <span class="keyword">return</span> [quantiles[q] <span class="keyword">for</span> q <span class="keyword">in</span> quantiles.keys()]</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 把 households 分成7桶</span></span><br><span class="line">households = tf.feature_column.numeric_column(<span class="string">"housholds"</span>)</span><br><span class="line">bucketized_households = tf.feature_column.bucketized_column(</span><br><span class="line">    households, boundaries=get_quantile_based_boundaries(</span><br><span class="line">    california_housing_dataframe[<span class="string">"households"</span>], <span class="number">7</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把 longitude 分成10桶</span></span><br><span class="line">longitude = tf.feature_column.numeric_column(<span class="string">"longitude"</span>)</span><br><span class="line">bucketized_longitude = tf.feature_column.bucketized_column(</span><br><span class="line">    longitude, boundaries=get_quantile_based_boundaries(</span><br><span class="line">    california_housing_dataframe[<span class="string">"longitude"</span>], <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<h2 id="任务-1：使用分桶特征列训练模型"><a href="#任务-1：使用分桶特征列训练模型" class="headerlink" title="任务 1：使用分桶特征列训练模型"></a>任务 1：使用分桶特征列训练模型</h2><p><strong>将我们示例中的所有实值特征进行分桶，训练模型，然后查看结果是否有所改善。</strong></p>
<p>在前面的代码块中，两个实值列（即 <code>households</code> 和 <code>longitude</code>）已被转换为分桶特征列。您的任务是对其余的列进行分桶，然后运行代码来训练模型。您可以采用各种启发法来确定分桶的范围。本练习使用了分位数技巧，通过这种方式选择分桶边界后，每个分桶将包含相同数量的样本。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">construct_feature_columns</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="string">"""Construct the TensorFlow Feature Columns.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A set of feature columns</span></span><br><span class="line"><span class="string">  """</span> </span><br><span class="line">  households = tf.feature_column.numeric_column(<span class="string">"households"</span>)</span><br><span class="line">  longitude = tf.feature_column.numeric_column(<span class="string">"longitude"</span>)</span><br><span class="line">  latitude = tf.feature_column.numeric_column(<span class="string">"latitude"</span>)</span><br><span class="line">  housing_median_age = tf.feature_column.numeric_column(<span class="string">"housing_median_age"</span>)</span><br><span class="line">  median_income = tf.feature_column.numeric_column(<span class="string">"median_income"</span>)</span><br><span class="line">  rooms_per_person = tf.feature_column.numeric_column(<span class="string">"rooms_per_person"</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Divide households into 7 buckets.</span></span><br><span class="line">  bucketized_households = tf.feature_column.bucketized_column(</span><br><span class="line">    households, boundaries=get_quantile_based_boundaries(</span><br><span class="line">      training_examples[<span class="string">"households"</span>], <span class="number">7</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Divide longitude into 10 buckets.</span></span><br><span class="line">  bucketized_longitude = tf.feature_column.bucketized_column(</span><br><span class="line">    longitude, boundaries=get_quantile_based_boundaries(</span><br><span class="line">      training_examples[<span class="string">"longitude"</span>], <span class="number">10</span>))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Divide latitude into 10 buckets.</span></span><br><span class="line">  bucketized_latitude = tf.feature_column.bucketized_column(</span><br><span class="line">    latitude, boundaries=get_quantile_based_boundaries(</span><br><span class="line">      training_examples[<span class="string">"latitude"</span>], <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Divide housing_median_age into 7 buckets.</span></span><br><span class="line">  bucketized_housing_median_age = tf.feature_column.bucketized_column(</span><br><span class="line">    housing_median_age, boundaries=get_quantile_based_boundaries(</span><br><span class="line">      training_examples[<span class="string">"housing_median_age"</span>], <span class="number">7</span>))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Divide median_income into 7 buckets.</span></span><br><span class="line">  bucketized_median_income = tf.feature_column.bucketized_column(</span><br><span class="line">    median_income, boundaries=get_quantile_based_boundaries(</span><br><span class="line">      training_examples[<span class="string">"median_income"</span>], <span class="number">7</span>))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Divide rooms_per_person into 7 buckets.</span></span><br><span class="line">  bucketized_rooms_per_person = tf.feature_column.bucketized_column(</span><br><span class="line">    rooms_per_person, boundaries=get_quantile_based_boundaries(</span><br><span class="line">      training_examples[<span class="string">"rooms_per_person"</span>], <span class="number">7</span>))</span><br><span class="line">  </span><br><span class="line">  feature_columns = set([</span><br><span class="line">    bucketized_longitude,</span><br><span class="line">    bucketized_latitude,</span><br><span class="line">    bucketized_housing_median_age,</span><br><span class="line">    bucketized_households,</span><br><span class="line">    bucketized_median_income,</span><br><span class="line">    bucketized_rooms_per_person])</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> feature_columns</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_ = train_model(</span><br><span class="line">    learning_rate=<span class="number">1.0</span>,</span><br><span class="line">    steps=<span class="number">500</span>,</span><br><span class="line">    batch_size=<span class="number">100</span>,</span><br><span class="line">    feature_columns=construct_feature_columns(),</span><br><span class="line">    training_examples=training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
  period 00 : 169.83
  period 01 : 143.51
  period 02 : 126.97
  period 03 : 115.77
  period 04 : 107.83
  period 05 : 101.97
  period 06 : 97.44
  period 07 : 93.81
  period 08 : 90.89
  period 09 : 88.49
Model training finished.
</code></pre><p><img src="TensorFlow_6_19_1.png" alt="png"></p>
<h2 id="特征组合"><a href="#特征组合" class="headerlink" title="特征组合"></a>特征组合</h2><p>组合两个（或更多个）特征是使用线性模型来学习非线性关系的一种聪明做法。在我们的问题中，如果我们只使用 <code>latitude</code> 特征进行学习，那么该模型可能会发现特定纬度（或特定纬度范围内，因为我们已经将其分桶）的城市街区更可能比其他街区住房成本高昂。<code>longitude</code> 特征的情况与此类似。但是，如果我们将 <code>longitude</code> 与 <code>latitude</code> 组合，产生的组合特征则代表一个明确的城市街区。如果模型发现某些城市街区（位于特定纬度和经度范围内）更可能比其他街区住房成本高昂，那么这将是比单独考虑两个特征更强烈的信号。</p>
<p>目前，特征列 API 仅支持组合离散特征。要组合两个连续的值（比如 <code>latitude</code> 或 <code>longitude</code>），我们可以对其进行分桶。</p>
<p>如果我们组合 <code>latitude</code> 和 <code>longitude</code> 特征（例如，假设 <code>longitude</code> 被分到 <code>2</code> 个分桶中，而 <code>latitude</code> 有 <code>3</code> 个分桶），我们实际上会得到 6 个组合的二元特征。当我们训练模型时，每个特征都会分别获得自己的权重。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">construct_feature_columns</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="string">"""Construct the TensorFlow Feature Columns.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A set of feature columns</span></span><br><span class="line"><span class="string">  """</span> </span><br><span class="line">  households = tf.feature_column.numeric_column(<span class="string">"households"</span>)</span><br><span class="line">  longitude = tf.feature_column.numeric_column(<span class="string">"longitude"</span>)</span><br><span class="line">  latitude = tf.feature_column.numeric_column(<span class="string">"latitude"</span>)</span><br><span class="line">  housing_median_age = tf.feature_column.numeric_column(<span class="string">"housing_median_age"</span>)</span><br><span class="line">  median_income = tf.feature_column.numeric_column(<span class="string">"median_income"</span>)</span><br><span class="line">  rooms_per_person = tf.feature_column.numeric_column(<span class="string">"rooms_per_person"</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Divide households into 7 buckets.</span></span><br><span class="line">  bucketized_households = tf.feature_column.bucketized_column(</span><br><span class="line">    households, boundaries=get_quantile_based_boundaries(</span><br><span class="line">      training_examples[<span class="string">"households"</span>], <span class="number">7</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Divide longitude into 10 buckets.</span></span><br><span class="line">  bucketized_longitude = tf.feature_column.bucketized_column(</span><br><span class="line">    longitude, boundaries=get_quantile_based_boundaries(</span><br><span class="line">      training_examples[<span class="string">"longitude"</span>], <span class="number">10</span>))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Divide latitude into 10 buckets.</span></span><br><span class="line">  bucketized_latitude = tf.feature_column.bucketized_column(</span><br><span class="line">    latitude, boundaries=get_quantile_based_boundaries(</span><br><span class="line">      training_examples[<span class="string">"latitude"</span>], <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Divide housing_median_age into 7 buckets.</span></span><br><span class="line">  bucketized_housing_median_age = tf.feature_column.bucketized_column(</span><br><span class="line">    housing_median_age, boundaries=get_quantile_based_boundaries(</span><br><span class="line">      training_examples[<span class="string">"housing_median_age"</span>], <span class="number">7</span>))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Divide median_income into 7 buckets.</span></span><br><span class="line">  bucketized_median_income = tf.feature_column.bucketized_column(</span><br><span class="line">    median_income, boundaries=get_quantile_based_boundaries(</span><br><span class="line">      training_examples[<span class="string">"median_income"</span>], <span class="number">7</span>))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Divide rooms_per_person into 7 buckets.</span></span><br><span class="line">  bucketized_rooms_per_person = tf.feature_column.bucketized_column(</span><br><span class="line">    rooms_per_person, boundaries=get_quantile_based_boundaries(</span><br><span class="line">      training_examples[<span class="string">"rooms_per_person"</span>], <span class="number">7</span>))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 为long_x_lat feature cross做一个特性列</span></span><br><span class="line">  long_x_lat = tf.feature_column.crossed_column(</span><br><span class="line">  set([bucketized_longitude, bucketized_latitude]), hash_bucket_size=<span class="number">1000</span>) </span><br><span class="line">  </span><br><span class="line">  feature_columns = set([</span><br><span class="line">    bucketized_longitude,</span><br><span class="line">    bucketized_latitude,</span><br><span class="line">    bucketized_housing_median_age,</span><br><span class="line">    bucketized_households,</span><br><span class="line">    bucketized_median_income,</span><br><span class="line">    bucketized_rooms_per_person,</span><br><span class="line">    long_x_lat])</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> feature_columns</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_ = train_model(</span><br><span class="line">    learning_rate=<span class="number">1.0</span>,</span><br><span class="line">    steps=<span class="number">500</span>,</span><br><span class="line">    batch_size=<span class="number">100</span>,</span><br><span class="line">    feature_columns=construct_feature_columns(),</span><br><span class="line">    training_examples=training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
  period 00 : 163.99
  period 01 : 135.79
  period 02 : 118.69
  period 03 : 107.28
  period 04 : 99.33
  period 05 : 93.53
  period 06 : 89.03
  period 07 : 85.54
  period 08 : 82.65
  period 09 : 80.32
Model training finished.
</code></pre><p><img src="TensorFlow_6_22_1.png" alt="png"></p>
]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow 8</title>
    <url>/2019/02/20/TensorFlow_8/</url>
    <content><![CDATA[<h1 id="神经网络简介"><a href="#神经网络简介" class="headerlink" title="神经网络简介"></a>神经网络简介</h1><ul>
<li>使用 TensorFlow <code>DNNRegressor</code> 类定义神经网络 (NN) 及其隐藏层</li>
<li>训练神经网络学习数据集中的非线性规律，并实现比线性回归模型更好的效果</li>
</ul>
<p>在之前的练习中，我们使用合成特征来帮助模型学习非线性规律。</p>
<p>一组重要的非线性关系是纬度和经度的关系，但也可能存在其他非线性关系。</p>
<p>现在我们从之前练习中的逻辑回归任务回到标准的（线性）回归任务。也就是说，我们将直接预测 <code>median_house_value</code>。</p>
<h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2><p>加载数据并创建特征定义。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> gridspec</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line">tf.logging.set_verbosity(tf.logging.ERROR)</span><br><span class="line">pd.options.display.max_rows = <span class="number">10</span></span><br><span class="line">pd.options.display.float_format = <span class="string">'&#123;:.1f&#125;'</span>.format</span><br><span class="line"></span><br><span class="line">california_housing_dataframe = pd.read_csv(<span class="string">"https://download.mlcc.google.cn/mledu-datasets/california_housing_train.csv"</span>, sep=<span class="string">","</span>)</span><br><span class="line"></span><br><span class="line">california_housing_dataframe = california_housing_dataframe.reindex(</span><br><span class="line">    np.random.permutation(california_housing_dataframe.index))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_features</span><span class="params">(california_housing_dataframe)</span>:</span></span><br><span class="line">  <span class="string">"""Prepares input features from California housing data set.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    california_housing_dataframe: A Pandas DataFrame expected to contain data</span></span><br><span class="line"><span class="string">      from the California housing data set.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A DataFrame that contains the features to be used for the model, including</span></span><br><span class="line"><span class="string">    synthetic features.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  selected_features = california_housing_dataframe[</span><br><span class="line">    [<span class="string">"latitude"</span>,</span><br><span class="line">     <span class="string">"longitude"</span>,</span><br><span class="line">     <span class="string">"housing_median_age"</span>,</span><br><span class="line">     <span class="string">"total_rooms"</span>,</span><br><span class="line">     <span class="string">"total_bedrooms"</span>,</span><br><span class="line">     <span class="string">"population"</span>,</span><br><span class="line">     <span class="string">"households"</span>,</span><br><span class="line">     <span class="string">"median_income"</span>]]</span><br><span class="line">  processed_features = selected_features.copy()</span><br><span class="line">  <span class="comment"># Create a synthetic feature.</span></span><br><span class="line">  processed_features[<span class="string">"rooms_per_person"</span>] = (</span><br><span class="line">    california_housing_dataframe[<span class="string">"total_rooms"</span>] /</span><br><span class="line">    california_housing_dataframe[<span class="string">"population"</span>])</span><br><span class="line">  <span class="keyword">return</span> processed_features</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_targets</span><span class="params">(california_housing_dataframe)</span>:</span></span><br><span class="line">  <span class="string">"""Prepares target features (i.e., labels) from California housing data set.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    california_housing_dataframe: A Pandas DataFrame expected to contain data</span></span><br><span class="line"><span class="string">      from the California housing data set.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A DataFrame that contains the target feature.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  output_targets = pd.DataFrame()</span><br><span class="line">  <span class="comment"># Scale the target to be in units of thousands of dollars.</span></span><br><span class="line">  output_targets[<span class="string">"median_house_value"</span>] = (</span><br><span class="line">    california_housing_dataframe[<span class="string">"median_house_value"</span>] / <span class="number">1000.0</span>)</span><br><span class="line">  <span class="keyword">return</span> output_targets</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Choose the first 12000 (out of 17000) examples for training.</span></span><br><span class="line">training_examples = preprocess_features(california_housing_dataframe.head(<span class="number">12000</span>))</span><br><span class="line">training_targets = preprocess_targets(california_housing_dataframe.head(<span class="number">12000</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Choose the last 5000 (out of 17000) examples for validation.</span></span><br><span class="line">validation_examples = preprocess_features(california_housing_dataframe.tail(<span class="number">5000</span>))</span><br><span class="line">validation_targets = preprocess_targets(california_housing_dataframe.tail(<span class="number">5000</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Double-check that we've done the right thing.</span></span><br><span class="line">print(<span class="string">"Training examples summary:"</span>)</span><br><span class="line">display.display(training_examples.describe())</span><br><span class="line">print(<span class="string">"Validation examples summary:"</span>)</span><br><span class="line">display.display(validation_examples.describe())</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Training targets summary:"</span>)</span><br><span class="line">display.display(training_targets.describe())</span><br><span class="line">print(<span class="string">"Validation targets summary:"</span>)</span><br><span class="line">display.display(validation_targets.describe())</span><br></pre></td></tr></table></figure>
<pre><code>Training examples summary:
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>latitude</th>
      <th>longitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>rooms_per_person</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>35.6</td>
      <td>-119.6</td>
      <td>28.7</td>
      <td>2636.9</td>
      <td>537.9</td>
      <td>1429.2</td>
      <td>500.3</td>
      <td>3.9</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>std</th>
      <td>2.1</td>
      <td>2.0</td>
      <td>12.5</td>
      <td>2187.4</td>
      <td>422.5</td>
      <td>1168.5</td>
      <td>387.3</td>
      <td>1.9</td>
      <td>1.1</td>
    </tr>
    <tr>
      <th>min</th>
      <td>32.5</td>
      <td>-124.3</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.5</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>33.9</td>
      <td>-121.8</td>
      <td>18.0</td>
      <td>1461.0</td>
      <td>296.8</td>
      <td>788.0</td>
      <td>281.0</td>
      <td>2.6</td>
      <td>1.5</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>34.2</td>
      <td>-118.5</td>
      <td>29.0</td>
      <td>2116.0</td>
      <td>431.0</td>
      <td>1165.0</td>
      <td>407.5</td>
      <td>3.5</td>
      <td>1.9</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>37.7</td>
      <td>-118.0</td>
      <td>37.0</td>
      <td>3127.0</td>
      <td>645.0</td>
      <td>1713.0</td>
      <td>601.0</td>
      <td>4.8</td>
      <td>2.3</td>
    </tr>
    <tr>
      <th>max</th>
      <td>42.0</td>
      <td>-114.3</td>
      <td>52.0</td>
      <td>37937.0</td>
      <td>6445.0</td>
      <td>35682.0</td>
      <td>6082.0</td>
      <td>15.0</td>
      <td>55.2</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>Validation examples summary:
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>latitude</th>
      <th>longitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>rooms_per_person</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>35.6</td>
      <td>-119.6</td>
      <td>28.4</td>
      <td>2659.9</td>
      <td>542.9</td>
      <td>1430.4</td>
      <td>503.5</td>
      <td>3.9</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>std</th>
      <td>2.1</td>
      <td>2.0</td>
      <td>12.9</td>
      <td>2162.0</td>
      <td>419.1</td>
      <td>1096.8</td>
      <td>377.9</td>
      <td>1.9</td>
      <td>1.3</td>
    </tr>
    <tr>
      <th>min</th>
      <td>32.6</td>
      <td>-124.3</td>
      <td>2.0</td>
      <td>15.0</td>
      <td>4.0</td>
      <td>8.0</td>
      <td>2.0</td>
      <td>0.5</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>33.9</td>
      <td>-121.8</td>
      <td>18.0</td>
      <td>1465.8</td>
      <td>297.0</td>
      <td>793.0</td>
      <td>283.0</td>
      <td>2.6</td>
      <td>1.5</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>34.2</td>
      <td>-118.5</td>
      <td>28.0</td>
      <td>2154.5</td>
      <td>439.0</td>
      <td>1173.0</td>
      <td>413.0</td>
      <td>3.5</td>
      <td>1.9</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>37.7</td>
      <td>-118.0</td>
      <td>37.0</td>
      <td>3216.0</td>
      <td>658.2</td>
      <td>1738.0</td>
      <td>614.0</td>
      <td>4.7</td>
      <td>2.3</td>
    </tr>
    <tr>
      <th>max</th>
      <td>41.9</td>
      <td>-114.6</td>
      <td>52.0</td>
      <td>30401.0</td>
      <td>4957.0</td>
      <td>13251.0</td>
      <td>4339.0</td>
      <td>15.0</td>
      <td>52.0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>Training targets summary:
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>median_house_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>12000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>206.6</td>
    </tr>
    <tr>
      <th>std</th>
      <td>115.5</td>
    </tr>
    <tr>
      <th>min</th>
      <td>15.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>119.2</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>180.8</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>263.3</td>
    </tr>
    <tr>
      <th>max</th>
      <td>500.0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>Validation targets summary:
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>median_house_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>5000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>208.9</td>
    </tr>
    <tr>
      <th>std</th>
      <td>117.0</td>
    </tr>
    <tr>
      <th>min</th>
      <td>15.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>120.2</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>179.2</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>268.3</td>
    </tr>
    <tr>
      <th>max</th>
      <td>500.0</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="构建神经网络"><a href="#构建神经网络" class="headerlink" title="构建神经网络"></a>构建神经网络</h2><p>神经网络由 <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor" target="_blank" rel="noopener">DNNRegressor</a> 类定义。</p>
<p>使用 <strong><code>hidden_units</code></strong> 定义神经网络的结构。<code>hidden_units</code> 参数会创建一个整数列表，其中每个整数对应一个隐藏层，表示其中的节点数。以下面的赋值为例：</p>
<p><code>hidden_units=[3,10]</code></p>
<p>上述赋值为神经网络指定了两个隐藏层：</p>
<ul>
<li>第一个隐藏层包含 3 个节点。</li>
<li>第二个隐藏层包含 10 个节点。</li>
</ul>
<p>如果我们想要添加更多层，可以向该列表添加更多整数。例如，<code>hidden_units=[10,20,30,40]</code> 会创建 4 个分别包含 10、20、30 和 40 个单元的隐藏层。</p>
<p>默认情况下，所有隐藏层都会使用 ReLu 激活函数，且是全连接层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">construct_feature_columns</span><span class="params">(input_features)</span>:</span></span><br><span class="line">  <span class="string">"""Construct the TensorFlow Feature Columns.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_features: The names of the numerical input features to use.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A set of feature columns</span></span><br><span class="line"><span class="string">  """</span> </span><br><span class="line">  <span class="keyword">return</span> set([tf.feature_column.numeric_column(my_feature)</span><br><span class="line">              <span class="keyword">for</span> my_feature <span class="keyword">in</span> input_features])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_input_fn</span><span class="params">(features, targets, batch_size=<span class="number">1</span>, shuffle=True, num_epochs=None)</span>:</span></span><br><span class="line">    <span class="string">"""Trains a neural net regression model.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      features: pandas DataFrame of features</span></span><br><span class="line"><span class="string">      targets: pandas DataFrame of targets</span></span><br><span class="line"><span class="string">      batch_size: Size of batches to be passed to the model</span></span><br><span class="line"><span class="string">      shuffle: True or False. Whether to shuffle the data.</span></span><br><span class="line"><span class="string">      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      Tuple of (features, labels) for next data batch</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Convert pandas data into a dict of np arrays.</span></span><br><span class="line">    features = &#123;key:np.array(value) <span class="keyword">for</span> key,value <span class="keyword">in</span> dict(features).items()&#125;                                             </span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Construct a dataset, and configure batching/repeating.</span></span><br><span class="line">    ds = Dataset.from_tensor_slices((features,targets)) <span class="comment"># warning: 2GB limit</span></span><br><span class="line">    ds = ds.batch(batch_size).repeat(num_epochs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Shuffle the data, if specified.</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">      ds = ds.shuffle(<span class="number">10000</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Return the next batch of data.</span></span><br><span class="line">    features, labels = ds.make_one_shot_iterator().get_next()</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_nn_regression_model</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">    steps,</span></span></span><br><span class="line"><span class="function"><span class="params">    batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    hidden_units,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_targets,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_targets)</span>:</span></span><br><span class="line">    <span class="string">"""Trains a neural network regression model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    In addition to training, this function also prints training progress information,</span></span><br><span class="line"><span class="string">    as well as a plot of the training and validation loss over time.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      learning_rate: A `float`, the learning rate.</span></span><br><span class="line"><span class="string">      steps: A non-zero `int`, the total number of training steps. A training step</span></span><br><span class="line"><span class="string">        consists of a forward and backward pass using a single batch.</span></span><br><span class="line"><span class="string">      batch_size: A non-zero `int`, the batch size.</span></span><br><span class="line"><span class="string">      hidden_units: A `list` of int values, specifying the number of neurons in each layer.</span></span><br><span class="line"><span class="string">      training_examples: A `DataFrame` containing one or more columns from</span></span><br><span class="line"><span class="string">        `california_housing_dataframe` to use as input features for training.</span></span><br><span class="line"><span class="string">      training_targets: A `DataFrame` containing exactly one column from</span></span><br><span class="line"><span class="string">        `california_housing_dataframe` to use as target for training.</span></span><br><span class="line"><span class="string">      validation_examples: A `DataFrame` containing one or more columns from</span></span><br><span class="line"><span class="string">        `california_housing_dataframe` to use as input features for validation.</span></span><br><span class="line"><span class="string">      validation_targets: A `DataFrame` containing exactly one column from</span></span><br><span class="line"><span class="string">        `california_housing_dataframe` to use as target for validation.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      A `DNNRegressor` object trained on the training data.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    periods = <span class="number">10</span></span><br><span class="line">    steps_per_period = steps / periods</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a DNNRrgressor object.</span></span><br><span class="line">    my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line">    dnn_regressor = tf.estimator.DNNRegressor(</span><br><span class="line">        feature_columns=construct_feature_columns(training_examples),</span><br><span class="line">        hidden_units=hidden_units,</span><br><span class="line">        optimizer=my_optimizer</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create input functions.</span></span><br><span class="line">    training_input_fn = <span class="keyword">lambda</span>: my_input_fn(training_examples, </span><br><span class="line">                                            training_targets[<span class="string">"median_house_value"</span>], </span><br><span class="line">                                            batch_size=batch_size)</span><br><span class="line">    predict_training_input_fn = <span class="keyword">lambda</span>: my_input_fn(training_examples, </span><br><span class="line">                                                    training_targets[<span class="string">"median_house_value"</span>], </span><br><span class="line">                                                    num_epochs=<span class="number">1</span>, </span><br><span class="line">                                                    shuffle=<span class="keyword">False</span>)</span><br><span class="line">    predict_validation_input_fn = <span class="keyword">lambda</span>: my_input_fn(validation_examples, </span><br><span class="line">                                                    validation_targets[<span class="string">"median_house_value"</span>], </span><br><span class="line">                                                    num_epochs=<span class="number">1</span>, </span><br><span class="line">                                                    shuffle=<span class="keyword">False</span>)    </span><br><span class="line">    <span class="comment"># Train the model, but do so inside a loop so that we can periodically assess</span></span><br><span class="line">    <span class="comment"># loss metrics. </span></span><br><span class="line">    print(<span class="string">"Training model..."</span>)</span><br><span class="line">    print(<span class="string">"RMSE (on training data):"</span>)</span><br><span class="line">    </span><br><span class="line">    training_rmse = []</span><br><span class="line">    validation_rmse = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> period <span class="keyword">in</span> range(<span class="number">0</span>, periods):</span><br><span class="line">        <span class="comment"># train the model, starting from the prior state.</span></span><br><span class="line">        dnn_regressor.train(</span><br><span class="line">            input_fn=training_input_fn,</span><br><span class="line">            steps=steps_per_period</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># take abreak and compute perdictions.</span></span><br><span class="line">        training_predictions = dnn_regressor.predict(input_fn=predict_training_input_fn)</span><br><span class="line">        training_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> training_predictions])</span><br><span class="line">        </span><br><span class="line">        validation_predictions = dnn_regressor.predict(input_fn=predict_validation_input_fn)</span><br><span class="line">        validation_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> validation_predictions])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># compute training an validation loss</span></span><br><span class="line">        training_root_mean_squared_error = math.sqrt(</span><br><span class="line">            metrics.mean_squared_error(training_predictions, training_targets))</span><br><span class="line">        validation_root_mean_squared_error = math.sqrt(</span><br><span class="line">            metrics.mean_squared_error(validation_predictions, validation_targets))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Occasionally print the current loss.</span></span><br><span class="line">        print(<span class="string">"  period %02d : %0.2f"</span> % (period, training_root_mean_squared_error))</span><br><span class="line">        <span class="comment"># Add the loss metrics from this period to our list.</span></span><br><span class="line">        training_rmse.append(training_root_mean_squared_error)</span><br><span class="line">        validation_rmse.append(validation_root_mean_squared_error)   </span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"Model training finished."</span>)</span><br><span class="line">    <span class="comment"># Output a graph of loss metrics over periods.</span></span><br><span class="line">    plt.ylabel(<span class="string">"RMSE"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Periods"</span>)</span><br><span class="line">    plt.title(<span class="string">"Root Mean Squared Error vs. Periods"</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.plot(training_rmse, label=<span class="string">"training"</span>)</span><br><span class="line">    plt.plot(validation_rmse, label=<span class="string">"validation"</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Final RMSE (on training data):   %0.2f"</span> % training_root_mean_squared_error)</span><br><span class="line">    print(<span class="string">"Final RMSE (on validation data): %0.2f"</span> % validation_root_mean_squared_error)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dnn_regressor</span><br></pre></td></tr></table></figure>
<h2 id="训练神经网络模型"><a href="#训练神经网络模型" class="headerlink" title="训练神经网络模型"></a>训练神经网络模型</h2><p><strong>调整超参数，目标是将 RMSE 降到 110 以下。</strong></p>
<p>我们已经知道，在使用了很多特征的线性回归练习中，110 左右的 RMSE 已经是相当不错的结果。现在我们将得到比它更好的结果。</p>
<p>对于神经网络而言，过拟合是一种真正的潜在危险。您可以查看训练数据损失与验证数据损失之间的差值，以帮助判断模型是否有过拟合的趋势。如果差值开始变大，则通常可以肯定存在过拟合。</p>
<p>下面参数是我写的，也许有更好的参数会获得更低的RMSE。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dnn_regressor = train_nn_regression_model(</span><br><span class="line">    learning_rate=<span class="number">0.002</span>,</span><br><span class="line">    steps=<span class="number">2000</span>,</span><br><span class="line">    batch_size=<span class="number">100</span>,</span><br><span class="line">    hidden_units=[<span class="number">8</span>, <span class="number">10</span>],</span><br><span class="line">    training_examples=training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
  period 00 : 153.67
  period 01 : 136.38
  period 02 : 119.26
  period 03 : 106.87
  period 04 : 107.80
  period 05 : 106.81
  period 06 : 107.47
  period 07 : 109.00
  period 08 : 104.58
  period 09 : 106.55
Model training finished.
Final RMSE (on training data):   106.55
Final RMSE (on validation data): 106.82
</code></pre><p><img src="TensorFlow_8_10_1.png" alt="png"></p>
<h2 id="用测试数据进行评估"><a href="#用测试数据进行评估" class="headerlink" title="用测试数据进行评估"></a>用测试数据进行评估</h2><p><strong>确认验证效果结果经受得住测试数据的检验。</strong></p>
<p>获得满意的模型后，用测试数据评估该模型，以与验证效果进行比较。</p>
<p>测试数据集位于<a href="https://download.mlcc.google.cn/mledu-datasets/california_housing_test.csv" target="_blank" rel="noopener">此处</a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">california_housing_test_data = pd.read_csv(<span class="string">"https://download.mlcc.google.cn/mledu-datasets/california_housing_test.csv"</span>, sep=<span class="string">","</span>)</span><br><span class="line"></span><br><span class="line">test_examples = preprocess_features(california_housing_test_data)</span><br><span class="line">test_targets = preprocess_targets(california_housing_test_data)</span><br><span class="line"></span><br><span class="line">predict_testing_input_fn = <span class="keyword">lambda</span>: my_input_fn(test_examples, </span><br><span class="line">                                               test_targets[<span class="string">"median_house_value"</span>], </span><br><span class="line">                                               num_epochs=<span class="number">1</span>, </span><br><span class="line">                                               shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">test_predictions = dnn_regressor.predict(input_fn=predict_testing_input_fn)</span><br><span class="line">test_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> test_predictions])</span><br><span class="line"></span><br><span class="line">root_mean_squared_error = math.sqrt(</span><br><span class="line">    metrics.mean_squared_error(test_predictions, test_targets))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Final RMSE (on test data): %0.2f"</span> % root_mean_squared_error)</span><br></pre></td></tr></table></figure>
<pre><code>Final RMSE (on test data): 105.23
</code></pre><h1 id="提高神经网络性能"><a href="#提高神经网络性能" class="headerlink" title="提高神经网络性能"></a>提高神经网络性能</h1><p> 通过将特征标准化并应用各种优化算法来提高神经网络的性能</p>
<p><strong>注意</strong>：本练习中介绍的优化方法并非专门针对神经网络；这些方法可有效改进大多数类型的模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_nn_regression_model_optimize</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    my_optimizer,</span></span></span><br><span class="line"><span class="function"><span class="params">    steps,</span></span></span><br><span class="line"><span class="function"><span class="params">    batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    hidden_units,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_targets,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_targets)</span>:</span></span><br><span class="line">  <span class="string">"""Trains a neural network regression model.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  In addition to training, this function also prints training progress information,</span></span><br><span class="line"><span class="string">  as well as a plot of the training and validation loss over time.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    my_optimizer: An instance of `tf.train.Optimizer`, the optimizer to use.</span></span><br><span class="line"><span class="string">    steps: A non-zero `int`, the total number of training steps. A training step</span></span><br><span class="line"><span class="string">      consists of a forward and backward pass using a single batch.</span></span><br><span class="line"><span class="string">    batch_size: A non-zero `int`, the batch size.</span></span><br><span class="line"><span class="string">    hidden_units: A `list` of int values, specifying the number of neurons in each layer.</span></span><br><span class="line"><span class="string">    training_examples: A `DataFrame` containing one or more columns from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as input features for training.</span></span><br><span class="line"><span class="string">    training_targets: A `DataFrame` containing exactly one column from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as target for training.</span></span><br><span class="line"><span class="string">    validation_examples: A `DataFrame` containing one or more columns from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as input features for validation.</span></span><br><span class="line"><span class="string">    validation_targets: A `DataFrame` containing exactly one column from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as target for validation.</span></span><br><span class="line"><span class="string">      </span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A tuple `(estimator, training_losses, validation_losses)`:</span></span><br><span class="line"><span class="string">      estimator: the trained `DNNRegressor` object.</span></span><br><span class="line"><span class="string">      training_losses: a `list` containing the training loss values taken during training.</span></span><br><span class="line"><span class="string">      validation_losses: a `list` containing the validation loss values taken during training.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  periods = <span class="number">10</span></span><br><span class="line">  steps_per_period = steps / periods</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Create a DNNRegressor object.</span></span><br><span class="line">  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line">  dnn_regressor = tf.estimator.DNNRegressor(</span><br><span class="line">      feature_columns=construct_feature_columns(training_examples),</span><br><span class="line">      hidden_units=hidden_units,</span><br><span class="line">      optimizer=my_optimizer</span><br><span class="line">  )</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Create input functions.</span></span><br><span class="line">  training_input_fn = <span class="keyword">lambda</span>: my_input_fn(training_examples, </span><br><span class="line">                                          training_targets[<span class="string">"median_house_value"</span>], </span><br><span class="line">                                          batch_size=batch_size)</span><br><span class="line">  predict_training_input_fn = <span class="keyword">lambda</span>: my_input_fn(training_examples, </span><br><span class="line">                                                  training_targets[<span class="string">"median_house_value"</span>], </span><br><span class="line">                                                  num_epochs=<span class="number">1</span>, </span><br><span class="line">                                                  shuffle=<span class="keyword">False</span>)</span><br><span class="line">  predict_validation_input_fn = <span class="keyword">lambda</span>: my_input_fn(validation_examples, </span><br><span class="line">                                                    validation_targets[<span class="string">"median_house_value"</span>], </span><br><span class="line">                                                    num_epochs=<span class="number">1</span>, </span><br><span class="line">                                                    shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Train the model, but do so inside a loop so that we can periodically assess</span></span><br><span class="line">  <span class="comment"># loss metrics.</span></span><br><span class="line">  print(<span class="string">"Training model..."</span>)</span><br><span class="line">  print(<span class="string">"RMSE (on training data):"</span>)</span><br><span class="line">  training_rmse = []</span><br><span class="line">  validation_rmse = []</span><br><span class="line">  <span class="keyword">for</span> period <span class="keyword">in</span> range (<span class="number">0</span>, periods):</span><br><span class="line">    <span class="comment"># Train the model, starting from the prior state.</span></span><br><span class="line">    dnn_regressor.train(</span><br><span class="line">        input_fn=training_input_fn,</span><br><span class="line">        steps=steps_per_period</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># Take a break and compute predictions.</span></span><br><span class="line">    training_predictions = dnn_regressor.predict(input_fn=predict_training_input_fn)</span><br><span class="line">    training_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> training_predictions])</span><br><span class="line">    </span><br><span class="line">    validation_predictions = dnn_regressor.predict(input_fn=predict_validation_input_fn)</span><br><span class="line">    validation_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> validation_predictions])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute training and validation loss.</span></span><br><span class="line">    training_root_mean_squared_error = math.sqrt(</span><br><span class="line">        metrics.mean_squared_error(training_predictions, training_targets))</span><br><span class="line">    validation_root_mean_squared_error = math.sqrt(</span><br><span class="line">        metrics.mean_squared_error(validation_predictions, validation_targets))</span><br><span class="line">    <span class="comment"># Occasionally print the current loss.</span></span><br><span class="line">    print(<span class="string">"  period %02d : %0.2f"</span> % (period, training_root_mean_squared_error))</span><br><span class="line">    <span class="comment"># Add the loss metrics from this period to our list.</span></span><br><span class="line">    training_rmse.append(training_root_mean_squared_error)</span><br><span class="line">    validation_rmse.append(validation_root_mean_squared_error)</span><br><span class="line">  print(<span class="string">"Model training finished."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Output a graph of loss metrics over periods.</span></span><br><span class="line">  plt.ylabel(<span class="string">"RMSE"</span>)</span><br><span class="line">  plt.xlabel(<span class="string">"Periods"</span>)</span><br><span class="line">  plt.title(<span class="string">"Root Mean Squared Error vs. Periods"</span>)</span><br><span class="line">  plt.tight_layout()</span><br><span class="line">  plt.plot(training_rmse, label=<span class="string">"training"</span>)</span><br><span class="line">  plt.plot(validation_rmse, label=<span class="string">"validation"</span>)</span><br><span class="line">  plt.legend()</span><br><span class="line"></span><br><span class="line">  print(<span class="string">"Final RMSE (on training data):   %0.2f"</span> % training_root_mean_squared_error)</span><br><span class="line">  print(<span class="string">"Final RMSE (on validation data): %0.2f"</span> % validation_root_mean_squared_error)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> dnn_regressor, training_rmse, validation_rmse</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_ = train_nn_regression_model_optimize(</span><br><span class="line">    my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.0007</span>),</span><br><span class="line">    steps=<span class="number">5000</span>,</span><br><span class="line">    batch_size=<span class="number">70</span>,</span><br><span class="line">    hidden_units=[<span class="number">10</span>, <span class="number">10</span>],</span><br><span class="line">    training_examples=training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
  period 00 : 162.48
  period 01 : 157.66
  period 02 : 150.85
  period 03 : 142.03
  period 04 : 131.60
  period 05 : 120.72
  period 06 : 113.46
  period 07 : 110.01
  period 08 : 108.28
  period 09 : 107.34
Model training finished.
Final RMSE (on training data):   107.34
Final RMSE (on validation data): 108.43
</code></pre><p><img src="TensorFlow_8_15_1.png" alt="png"></p>
<h2 id="线性缩放"><a href="#线性缩放" class="headerlink" title="线性缩放"></a>线性缩放</h2><p>将输入标准化以使其位于 (-1, 1) 范围内可能是一种良好的标准做法。这样一来，SGD 在一个维度中采用很大步长（或者在另一维度中采用很小步长）时不会受阻。数值优化的爱好者可能会注意到，这种做法与使用预调节器 (Preconditioner) 的想法是有联系的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_scale</span><span class="params">(series)</span>:</span></span><br><span class="line">  min_val = series.min()</span><br><span class="line">  max_val = series.max()</span><br><span class="line">  scale = (max_val - min_val) / <span class="number">2.0</span></span><br><span class="line">  <span class="keyword">return</span> series.apply(<span class="keyword">lambda</span> x:((x - min_val) / scale) - <span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="使用线性缩放将特征标准化"><a href="#使用线性缩放将特征标准化" class="headerlink" title="使用线性缩放将特征标准化"></a>使用线性缩放将特征标准化</h2><p><strong>将输入标准化到 (-1, 1) 这一范围内。能达到什么程度的效果？</strong></p>
<p>一般来说，当输入特征大致位于相同范围时，神经网络的训练效果最好。</p>
<p>对您的标准化数据进行健全性检查。（如果您忘了将某个特征标准化，会发生什么情况？）</p>
<p> 由于标准化会使用最小值和最大值，我们必须确保在整个数据集中一次性完成该操作。</p>
<p>我们之所以可以这样做，是因为我们所有的数据都在一个 DataFrame 中。如果我们有多个数据集，则最好从训练集中导出标准化参数，然后以相同方式将其应用于测试集。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_linear_scale</span><span class="params">(examples_dataframe)</span>:</span></span><br><span class="line">  <span class="string">"""Returns a version of the input `DataFrame` that has all its features normalized linearly."""</span></span><br><span class="line">  processed_features = pd.DataFrame()</span><br><span class="line">  processed_features[<span class="string">"latitude"</span>] = linear_scale(examples_dataframe[<span class="string">"latitude"</span>])</span><br><span class="line">  processed_features[<span class="string">"longitude"</span>] = linear_scale(examples_dataframe[<span class="string">"longitude"</span>])</span><br><span class="line">  processed_features[<span class="string">"housing_median_age"</span>] = linear_scale(examples_dataframe[<span class="string">"housing_median_age"</span>])</span><br><span class="line">  processed_features[<span class="string">"total_rooms"</span>] = linear_scale(examples_dataframe[<span class="string">"total_rooms"</span>])</span><br><span class="line">  processed_features[<span class="string">"total_bedrooms"</span>] = linear_scale(examples_dataframe[<span class="string">"total_bedrooms"</span>])</span><br><span class="line">  processed_features[<span class="string">"population"</span>] = linear_scale(examples_dataframe[<span class="string">"population"</span>])</span><br><span class="line">  processed_features[<span class="string">"households"</span>] = linear_scale(examples_dataframe[<span class="string">"households"</span>])</span><br><span class="line">  processed_features[<span class="string">"median_income"</span>] = linear_scale(examples_dataframe[<span class="string">"median_income"</span>])</span><br><span class="line">  processed_features[<span class="string">"rooms_per_person"</span>] = linear_scale(examples_dataframe[<span class="string">"rooms_per_person"</span>])</span><br><span class="line">  <span class="keyword">return</span> processed_features</span><br><span class="line"></span><br><span class="line">normalized_dataframe = normalize_linear_scale(preprocess_features(california_housing_dataframe))</span><br><span class="line">normalized_training_examples = normalized_dataframe.head(<span class="number">12000</span>)</span><br><span class="line">normalized_validation_examples = normalized_dataframe.tail(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">_ = train_nn_regression_model_optimize(</span><br><span class="line">    my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.005</span>),</span><br><span class="line">    steps=<span class="number">2000</span>,</span><br><span class="line">    batch_size=<span class="number">50</span>,</span><br><span class="line">    hidden_units=[<span class="number">10</span>, <span class="number">10</span>],</span><br><span class="line">    training_examples=normalized_training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=normalized_validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
  period 00 : 163.23
  period 01 : 115.65
  period 02 : 106.20
  period 03 : 91.27
  period 04 : 79.65
  period 05 : 76.10
  period 06 : 74.00
  period 07 : 72.52
  period 08 : 71.52
  period 09 : 70.72
Model training finished.
Final RMSE (on training data):   70.72
Final RMSE (on validation data): 72.52
</code></pre><p><img src="TensorFlow_8_20_1.png" alt="png"></p>
<p>尝试其他优化器</p>
<p><strong> 使用 AdaGrad 和 Adam 优化器并对比其效果。</strong></p>
<p>AdaGrad 优化器是一种备选方案。AdaGrad 的核心是灵活地修改模型中每个系数的学习率，从而单调降低有效的学习率。该优化器对于凸优化问题非常有效，但不一定适合非凸优化问题的神经网络训练。您可以通过指定 <code>AdagradOptimizer</code>（而不是 <code>GradientDescentOptimizer</code>）来使用 AdaGrad。请注意，对于 AdaGrad，您可能需要使用较大的学习率。</p>
<p>对于非凸优化问题，Adam 有时比 AdaGrad 更有效。要使用 Adam，请调用 <code>tf.train.AdamOptimizer</code> 方法。此方法将几个可选超参数作为参数，但我们的解决方案仅指定其中一个 (<code>learning_rate</code>)。在应用设置中，您应该谨慎指定和调整可选超参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 首先，我们来尝试 AdaGrad。</span></span><br><span class="line">_, adagrad_training_losses, adagrad_validation_losses = train_nn_regression_model_optimize(</span><br><span class="line">    my_optimizer=tf.train.AdagradOptimizer(learning_rate=<span class="number">0.5</span>),</span><br><span class="line">    steps=<span class="number">500</span>,</span><br><span class="line">    batch_size=<span class="number">100</span>,</span><br><span class="line">    hidden_units=[<span class="number">10</span>, <span class="number">10</span>],</span><br><span class="line">    training_examples=normalized_training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=normalized_validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
  period 00 : 84.89
  period 01 : 72.08
  period 02 : 71.06
  period 03 : 71.18
  period 04 : 69.57
  period 05 : 72.57
  period 06 : 69.04
  period 07 : 67.49
  period 08 : 68.83
  period 09 : 69.24
Model training finished.
Final RMSE (on training data):   69.24
Final RMSE (on validation data): 71.91
</code></pre><p><img src="TensorFlow_8_22_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> <span class="comment"># 现在，我们来尝试 Adam。</span></span><br><span class="line">_, adam_training_losses, adam_validation_losses = train_nn_regression_model_optimize(</span><br><span class="line">    my_optimizer=tf.train.AdamOptimizer(learning_rate=<span class="number">0.009</span>),</span><br><span class="line">    steps=<span class="number">500</span>,</span><br><span class="line">    batch_size=<span class="number">100</span>,</span><br><span class="line">    hidden_units=[<span class="number">10</span>, <span class="number">10</span>],</span><br><span class="line">    training_examples=normalized_training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=normalized_validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
  period 00 : 171.53
  period 01 : 108.88
  period 02 : 100.89
  period 03 : 88.85
  period 04 : 76.59
  period 05 : 72.77
  period 06 : 70.84
  period 07 : 70.29
  period 08 : 69.79
  period 09 : 68.84
Model training finished.
Final RMSE (on training data):   68.84
Final RMSE (on validation data): 70.98
</code></pre><p><img src="TensorFlow_8_23_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 我们并排输出损失指标的图表。</span></span><br><span class="line">plt.ylabel(<span class="string">"RMSE"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Periods"</span>)</span><br><span class="line">plt.title(<span class="string">"Root Mean Squared Error vs. Periods"</span>)</span><br><span class="line">plt.plot(adagrad_training_losses, label=<span class="string">'Adagrad training'</span>)</span><br><span class="line">plt.plot(adagrad_validation_losses, label=<span class="string">'Adagrad validation'</span>)</span><br><span class="line">plt.plot(adam_training_losses, label=<span class="string">'Adam training'</span>)</span><br><span class="line">plt.plot(adam_validation_losses, label=<span class="string">'Adam validation'</span>)</span><br><span class="line">_ = plt.legend()</span><br></pre></td></tr></table></figure>
<p><img src="TensorFlow_8_24_0.png" alt="png"></p>
<h2 id="尝试其他标准化方法"><a href="#尝试其他标准化方法" class="headerlink" title="尝试其他标准化方法"></a>尝试其他标准化方法</h2><p><strong>尝试对各种特征使用其他标准化方法，以进一步提高性能。</strong></p>
<p>如果仔细查看转换后数据的汇总统计信息，您可能会注意到，对某些特征进行线性缩放会使其聚集到接近 <code>-1</code> 的位置。</p>
<p>例如，很多特征的中位数约为 <code>-0.8</code>，而不是 <code>0.0</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_ = training_examples.hist(bins=<span class="number">20</span>, figsize=(<span class="number">18</span>, <span class="number">12</span>), xlabelsize=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="TensorFlow_8_26_0.png" alt="png"></p>
<p> 通过选择其他方式来转换这些特征，我们可能会获得更好的效果。</p>
<p>例如，对数缩放可能对某些特征有帮助。或者，截取极端值可能会使剩余部分的信息更加丰富。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_normalize</span><span class="params">(series)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> series.apply(<span class="keyword">lambda</span> x:math.log(x+<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip</span><span class="params">(series, clip_to_min, clip_to_max)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> series.apply(<span class="keyword">lambda</span> x:(</span><br><span class="line">    min(max(x, clip_to_min), clip_to_max)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">z_score_normalize</span><span class="params">(series)</span>:</span></span><br><span class="line">  mean = series.mean()</span><br><span class="line">  std_dv = series.std()</span><br><span class="line">  <span class="keyword">return</span> series.apply(<span class="keyword">lambda</span> x:(x - mean) / std_dv)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_threshold</span><span class="params">(series, threshold)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> series.apply(<span class="keyword">lambda</span> x:(<span class="number">1</span> <span class="keyword">if</span> x &gt; threshold <span class="keyword">else</span> <span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<p> 上述部分包含一些额外的标准化函数。</p>
<p>请注意，如果您将目标标准化，则需要将网络的预测结果非标准化，以便比较损失函数的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span><span class="params">(examples_dataframe)</span>:</span></span><br><span class="line">  <span class="string">"""Returns a version of the input `DataFrame` that has all its features normalized."""</span></span><br><span class="line">  processed_features = pd.DataFrame()</span><br><span class="line"></span><br><span class="line">  processed_features[<span class="string">"households"</span>] = log_normalize(examples_dataframe[<span class="string">"households"</span>])</span><br><span class="line">  processed_features[<span class="string">"median_income"</span>] = log_normalize(examples_dataframe[<span class="string">"median_income"</span>])</span><br><span class="line">  processed_features[<span class="string">"total_bedrooms"</span>] = log_normalize(examples_dataframe[<span class="string">"total_bedrooms"</span>])</span><br><span class="line">  </span><br><span class="line">  processed_features[<span class="string">"latitude"</span>] = linear_scale(examples_dataframe[<span class="string">"latitude"</span>])</span><br><span class="line">  processed_features[<span class="string">"longitude"</span>] = linear_scale(examples_dataframe[<span class="string">"longitude"</span>])</span><br><span class="line">  processed_features[<span class="string">"housing_median_age"</span>] = linear_scale(examples_dataframe[<span class="string">"housing_median_age"</span>])</span><br><span class="line"></span><br><span class="line">  processed_features[<span class="string">"population"</span>] = linear_scale(clip(examples_dataframe[<span class="string">"population"</span>], <span class="number">0</span>, <span class="number">5000</span>))</span><br><span class="line">  processed_features[<span class="string">"rooms_per_person"</span>] = linear_scale(clip(examples_dataframe[<span class="string">"rooms_per_person"</span>], <span class="number">0</span>, <span class="number">5</span>))</span><br><span class="line">  processed_features[<span class="string">"total_rooms"</span>] = linear_scale(clip(examples_dataframe[<span class="string">"total_rooms"</span>], <span class="number">0</span>, <span class="number">10000</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> processed_features</span><br><span class="line"></span><br><span class="line">normalized_dataframe = normalize(preprocess_features(california_housing_dataframe))</span><br><span class="line">normalized_training_examples = normalized_dataframe.head(<span class="number">12000</span>)</span><br><span class="line">normalized_validation_examples = normalized_dataframe.tail(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">_ = train_nn_regression_model_optimize(</span><br><span class="line">    my_optimizer=tf.train.AdagradOptimizer(learning_rate=<span class="number">0.15</span>),</span><br><span class="line">    steps=<span class="number">1000</span>,</span><br><span class="line">    batch_size=<span class="number">50</span>,</span><br><span class="line">    hidden_units=[<span class="number">10</span>, <span class="number">10</span>],</span><br><span class="line">    training_examples=normalized_training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=normalized_validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
  period 00 : 89.38
  period 01 : 75.15
  period 02 : 72.35
  period 03 : 70.70
  period 04 : 70.95
  period 05 : 69.13
  period 06 : 68.46
  period 07 : 68.42
  period 08 : 68.68
  period 09 : 67.93
Model training finished.
Final RMSE (on training data):   67.93
Final RMSE (on validation data): 69.55
</code></pre><p><img src="TensorFlow_8_30_1.png" alt="png"></p>
<h2 id="仅使用纬度和经度特征"><a href="#仅使用纬度和经度特征" class="headerlink" title="仅使用纬度和经度特征"></a>仅使用纬度和经度特征</h2><p><strong>训练仅使用纬度和经度作为特征的神经网络模型。</strong></p>
<p>房地产商喜欢说，地段是房价的唯一重要特征。<br>我们来看看能否通过训练仅使用纬度和经度作为特征的模型来证实这一点。</p>
<p>只有我们的神经网络模型可以从纬度和经度中学会复杂的非线性规律，才能达到我们想要的效果。</p>
<p><strong>注意</strong>：我们可能需要一个网络结构，其层数比我们之前在练习中使用的要多。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">location_location_location</span><span class="params">(examples_dataframe)</span>:</span></span><br><span class="line">  <span class="string">"""Returns a version of the input `DataFrame` that keeps only the latitude and longitude."""</span></span><br><span class="line">  processed_features = pd.DataFrame()</span><br><span class="line">  processed_features[<span class="string">"latitude"</span>] = linear_scale(examples_dataframe[<span class="string">"latitude"</span>])</span><br><span class="line">  processed_features[<span class="string">"longitude"</span>] = linear_scale(examples_dataframe[<span class="string">"longitude"</span>])</span><br><span class="line">  <span class="keyword">return</span> processed_features</span><br><span class="line"></span><br><span class="line">lll_dataframe = location_location_location(preprocess_features(california_housing_dataframe))</span><br><span class="line">lll_training_examples = lll_dataframe.head(<span class="number">12000</span>)</span><br><span class="line">lll_validation_examples = lll_dataframe.tail(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">_ = train_nn_regression_model_optimize(</span><br><span class="line">    my_optimizer=tf.train.AdagradOptimizer(learning_rate=<span class="number">0.05</span>),</span><br><span class="line">    steps=<span class="number">500</span>,</span><br><span class="line">    batch_size=<span class="number">50</span>,</span><br><span class="line">    hidden_units=[<span class="number">10</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">    training_examples=lll_training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=lll_validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
  period 00 : 157.77
  period 01 : 107.67
  period 02 : 105.43
  period 03 : 104.46
  period 04 : 103.15
  period 05 : 101.82
  period 06 : 101.01
  period 07 : 100.61
  period 08 : 100.10
  period 09 : 99.77
Model training finished.
Final RMSE (on training data):   99.77
Final RMSE (on validation data): 100.48
</code></pre><p><img src="TensorFlow_8_32_1.png" alt="png"></p>
<p>最好使纬度和经度保持标准化状态，对于只有两个特征的模型，结果并不算太糟。当然，地产价值在短距离内仍然可能有较大差异。</p>
]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow 7</title>
    <url>/2019/01/14/TensorFlow_7/</url>
    <content><![CDATA[<h1 id="正则化-Regularization-for-Simplicity"><a href="#正则化-Regularization-for-Simplicity" class="headerlink" title="正则化 (Regularization for Simplicity)"></a>正则化 (Regularization for Simplicity)</h1><p>略</p>
<h1 id="查准率和召回率"><a href="#查准率和召回率" class="headerlink" title="查准率和召回率"></a>查准率和召回率</h1><p>略</p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p> 与在之前的练习中一样，我们将使用加利福尼亚州住房数据集，但这次我们会预测某个城市街区的住房成本是否高昂，从而将其转换成一个二元分类问题。此外，我们还会暂时恢复使用默认特征。</p>
<h2 id="构建二元分类问题"><a href="#构建二元分类问题" class="headerlink" title="构建二元分类问题"></a>构建二元分类问题</h2><p>数据集的目标是 <code>median_house_value</code>，它是一个数值（连续值）特征。我们可以通过向此连续值使用阈值来创建一个布尔值标签。</p>
<p>我们希望通过某个城市街区的特征预测该街区的住房成本是否高昂。为了给训练数据和评估数据准备目标，我们针对房屋价值中位数定义了分类阈值 - 第 75 百分位数（约为 265000）。所有高于此阈值的房屋价值标记为 <code>1</code>，其他值标记为 <code>0</code>。</p>
<h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> gridspec</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line">tf.logging.set_verbosity(tf.logging.ERROR)</span><br><span class="line">pd.options.display.max_rows = <span class="number">10</span></span><br><span class="line">pd.options.display.float_format = <span class="string">'&#123;:.1f&#125;'</span>.format</span><br><span class="line"></span><br><span class="line">california_housing_dataframe = pd.read_csv(<span class="string">"https://download.mlcc.google.cn/mledu-datasets/california_housing_train.csv"</span>, sep=<span class="string">","</span>)</span><br><span class="line"></span><br><span class="line">california_housing_dataframe = california_housing_dataframe.reindex(</span><br><span class="line">    np.random.permutation(california_housing_dataframe.index))</span><br></pre></td></tr></table></figure>
<p> 注意以下代码与之前练习中的代码之间稍有不同。我们并没有将 <code>median_house_value</code> 用作目标，而是创建了一个新的二元目标 <code>median_house_value_is_high</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_features</span><span class="params">(california_housing_dataframe)</span>:</span></span><br><span class="line">    selected_features = california_housing_dataframe[</span><br><span class="line">    [<span class="string">"latitude"</span>,</span><br><span class="line">     <span class="string">"longitude"</span>,</span><br><span class="line">     <span class="string">"housing_median_age"</span>,</span><br><span class="line">     <span class="string">"total_rooms"</span>,</span><br><span class="line">     <span class="string">"total_bedrooms"</span>,</span><br><span class="line">     <span class="string">"population"</span>,</span><br><span class="line">     <span class="string">"households"</span>,</span><br><span class="line">     <span class="string">"median_income"</span>]]    </span><br><span class="line">    processed_features = selected_features.copy()</span><br><span class="line">    processed_features[<span class="string">"rooms_per_person"</span>] = (</span><br><span class="line">        california_housing_dataframe[<span class="string">"total_rooms"</span>] /</span><br><span class="line">        california_housing_dataframe[<span class="string">"population"</span>])</span><br><span class="line">    <span class="keyword">return</span> processed_features</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_targets</span><span class="params">(california_housing_dataframe)</span>:</span></span><br><span class="line">    output_targets = pd.DataFrame()</span><br><span class="line">    output_targets[<span class="string">"median_house_value_is_high"</span>] = (</span><br><span class="line">    california_housing_dataframe[<span class="string">"median_house_value"</span>] &gt; <span class="number">265000</span>).astype(float)</span><br><span class="line">    <span class="keyword">return</span> output_targets</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">training_examples = preprocess_features(california_housing_dataframe.head(<span class="number">12000</span>))</span><br><span class="line">training_targets = preprocess_targets(california_housing_dataframe.head(<span class="number">12000</span>))</span><br><span class="line"></span><br><span class="line">validation_examples = preprocess_features(california_housing_dataframe.tail(<span class="number">5000</span>))</span><br><span class="line">validation_targets = preprocess_targets(california_housing_dataframe.tail(<span class="number">5000</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Training examples summary:"</span>)</span><br><span class="line">display.display(training_examples.describe())</span><br><span class="line">print(<span class="string">"Validation examples summary:"</span>)</span><br><span class="line">display.display(validation_examples.describe())</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Training targets summary:"</span>)</span><br><span class="line">display.display(training_targets.describe())</span><br><span class="line">print(<span class="string">"Validation targets summary:"</span>)</span><br><span class="line">display.display(validation_targets.describe())</span><br></pre></td></tr></table></figure>
<pre><code>Training examples summary:
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>latitude</th>
      <th>longitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>rooms_per_person</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
      <td>12000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>35.6</td>
      <td>-119.6</td>
      <td>28.6</td>
      <td>2630.6</td>
      <td>537.6</td>
      <td>1426.3</td>
      <td>499.6</td>
      <td>3.9</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>std</th>
      <td>2.1</td>
      <td>2.0</td>
      <td>12.6</td>
      <td>2156.5</td>
      <td>415.3</td>
      <td>1158.6</td>
      <td>379.3</td>
      <td>1.9</td>
      <td>1.2</td>
    </tr>
    <tr>
      <th>min</th>
      <td>32.5</td>
      <td>-124.3</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>6.0</td>
      <td>1.0</td>
      <td>0.5</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>33.9</td>
      <td>-121.8</td>
      <td>18.0</td>
      <td>1460.8</td>
      <td>297.0</td>
      <td>790.0</td>
      <td>282.0</td>
      <td>2.6</td>
      <td>1.5</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>34.2</td>
      <td>-118.5</td>
      <td>29.0</td>
      <td>2113.5</td>
      <td>432.0</td>
      <td>1168.0</td>
      <td>408.0</td>
      <td>3.5</td>
      <td>1.9</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>37.7</td>
      <td>-118.0</td>
      <td>37.0</td>
      <td>3138.2</td>
      <td>647.0</td>
      <td>1717.2</td>
      <td>603.0</td>
      <td>4.8</td>
      <td>2.3</td>
    </tr>
    <tr>
      <th>max</th>
      <td>42.0</td>
      <td>-114.3</td>
      <td>52.0</td>
      <td>37937.0</td>
      <td>6445.0</td>
      <td>35682.0</td>
      <td>6082.0</td>
      <td>15.0</td>
      <td>55.2</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>Validation examples summary:
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>latitude</th>
      <th>longitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>rooms_per_person</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
      <td>5000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>35.7</td>
      <td>-119.6</td>
      <td>28.6</td>
      <td>2675.1</td>
      <td>543.8</td>
      <td>1437.5</td>
      <td>505.0</td>
      <td>3.9</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>std</th>
      <td>2.1</td>
      <td>2.0</td>
      <td>12.5</td>
      <td>2235.0</td>
      <td>436.1</td>
      <td>1121.7</td>
      <td>396.8</td>
      <td>1.9</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>min</th>
      <td>32.5</td>
      <td>-124.3</td>
      <td>2.0</td>
      <td>12.0</td>
      <td>4.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>0.5</td>
      <td>0.3</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>33.9</td>
      <td>-121.8</td>
      <td>18.0</td>
      <td>1465.8</td>
      <td>295.8</td>
      <td>788.0</td>
      <td>278.0</td>
      <td>2.6</td>
      <td>1.5</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>34.3</td>
      <td>-118.5</td>
      <td>28.0</td>
      <td>2172.0</td>
      <td>438.0</td>
      <td>1165.0</td>
      <td>411.0</td>
      <td>3.5</td>
      <td>1.9</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>37.7</td>
      <td>-118.0</td>
      <td>37.0</td>
      <td>3176.0</td>
      <td>652.0</td>
      <td>1732.2</td>
      <td>609.0</td>
      <td>4.8</td>
      <td>2.3</td>
    </tr>
    <tr>
      <th>max</th>
      <td>41.9</td>
      <td>-114.6</td>
      <td>52.0</td>
      <td>32054.0</td>
      <td>5290.0</td>
      <td>15507.0</td>
      <td>5050.0</td>
      <td>15.0</td>
      <td>27.1</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>Training targets summary:
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>median_house_value_is_high</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>12000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.2</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.4</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code>Validation targets summary:
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>median_house_value_is_high</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>5000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.3</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.4</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.0</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="线性回归的表现"><a href="#线性回归的表现" class="headerlink" title="线性回归的表现"></a>线性回归的表现</h2><p>为了了解逻辑回归为什么有效，我们首先训练一个使用线性回归的简单模型。该模型将使用 <code>{0, 1}</code> 中的值为标签，并尝试预测一个尽可能接近 <code>0</code> 或 <code>1</code> 的连续值。此外，我们希望将输出解读为概率，所以最好模型的输出值可以位于 <code>(0, 1)</code> 范围内。然后我们会应用阈值 <code>0.5</code>，以确定标签。</p>
<p>运行以下单元格，以使用 <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor" target="_blank" rel="noopener">LinearRegressor</a> 训练线性回归模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">construct_feature_columns</span><span class="params">(input_features)</span>:</span></span><br><span class="line">  <span class="string">"""Construct the TensorFlow Feature Columns.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_features: The names of the numerical input features to use.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A set of feature columns</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="keyword">return</span> set([tf.feature_column.numeric_column(my_feature)</span><br><span class="line">              <span class="keyword">for</span> my_feature <span class="keyword">in</span> input_features])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_input_fn</span><span class="params">(features, targets, batch_size=<span class="number">1</span>, shuffle=True, num_epochs=None)</span>:</span></span><br><span class="line">    <span class="string">"""Trains a linear regression model.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      features: pandas DataFrame of features</span></span><br><span class="line"><span class="string">      targets: pandas DataFrame of targets</span></span><br><span class="line"><span class="string">      batch_size: Size of batches to be passed to the model</span></span><br><span class="line"><span class="string">      shuffle: True or False. Whether to shuffle the data.</span></span><br><span class="line"><span class="string">      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      Tuple of (features, labels) for next data batch</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Convert pandas data into a dict of np arrays.</span></span><br><span class="line">    features = &#123;key:np.array(value) <span class="keyword">for</span> key,value <span class="keyword">in</span> dict(features).items()&#125;                                            </span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Construct a dataset, and configure batching/repeating.</span></span><br><span class="line">    ds = Dataset.from_tensor_slices((features,targets)) <span class="comment"># warning: 2GB limit</span></span><br><span class="line">    ds = ds.batch(batch_size).repeat(num_epochs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Shuffle the data, if specified.</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">      ds = ds.shuffle(<span class="number">10000</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Return the next batch of data.</span></span><br><span class="line">    features, labels = ds.make_one_shot_iterator().get_next()</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_linear_regressor_model</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">    steps,</span></span></span><br><span class="line"><span class="function"><span class="params">    batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_targets,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_targets)</span>:</span></span><br><span class="line">  <span class="string">"""Trains a linear regression model.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  In addition to training, this function also prints training progress information,</span></span><br><span class="line"><span class="string">  as well as a plot of the training and validation loss over time.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    learning_rate: A `float`, the learning rate.</span></span><br><span class="line"><span class="string">    steps: A non-zero `int`, the total number of training steps. A training step</span></span><br><span class="line"><span class="string">      consists of a forward and backward pass using a single batch.</span></span><br><span class="line"><span class="string">    batch_size: A non-zero `int`, the batch size.</span></span><br><span class="line"><span class="string">    training_examples: A `DataFrame` containing one or more columns from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as input features for training.</span></span><br><span class="line"><span class="string">    training_targets: A `DataFrame` containing exactly one column from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as target for training.</span></span><br><span class="line"><span class="string">    validation_examples: A `DataFrame` containing one or more columns from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as input features for validation.</span></span><br><span class="line"><span class="string">    validation_targets: A `DataFrame` containing exactly one column from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as target for validation.</span></span><br><span class="line"><span class="string">      </span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A `LinearRegressor` object trained on the training data.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  periods = <span class="number">10</span></span><br><span class="line">  steps_per_period = steps / periods</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Create a linear regressor object.</span></span><br><span class="line">  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line">  linear_regressor = tf.estimator.LinearRegressor(</span><br><span class="line">      feature_columns=construct_feature_columns(training_examples),</span><br><span class="line">      optimizer=my_optimizer</span><br><span class="line">  )</span><br><span class="line">    </span><br><span class="line">  <span class="comment"># Create input functions.  </span></span><br><span class="line">  training_input_fn = <span class="keyword">lambda</span>: my_input_fn(training_examples, </span><br><span class="line">                                          training_targets[<span class="string">"median_house_value_is_high"</span>], </span><br><span class="line">                                          batch_size=batch_size)</span><br><span class="line">  predict_training_input_fn = <span class="keyword">lambda</span>: my_input_fn(training_examples, </span><br><span class="line">                                                  training_targets[<span class="string">"median_house_value_is_high"</span>], </span><br><span class="line">                                                  num_epochs=<span class="number">1</span>, </span><br><span class="line">                                                  shuffle=<span class="keyword">False</span>)</span><br><span class="line">  predict_validation_input_fn = <span class="keyword">lambda</span>: my_input_fn(validation_examples, </span><br><span class="line">                                                    validation_targets[<span class="string">"median_house_value_is_high"</span>], </span><br><span class="line">                                                    num_epochs=<span class="number">1</span>, </span><br><span class="line">                                                    shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Train the model, but do so inside a loop so that we can periodically assess</span></span><br><span class="line">  <span class="comment"># loss metrics.</span></span><br><span class="line">  print(<span class="string">"Training model..."</span>)</span><br><span class="line">  print(<span class="string">"RMSE (on training data):"</span>)</span><br><span class="line">  training_rmse = []</span><br><span class="line">  validation_rmse = []</span><br><span class="line">  <span class="keyword">for</span> period <span class="keyword">in</span> range (<span class="number">0</span>, periods):</span><br><span class="line">    <span class="comment"># Train the model, starting from the prior state.</span></span><br><span class="line">    linear_regressor.train(</span><br><span class="line">        input_fn=training_input_fn,</span><br><span class="line">        steps=steps_per_period</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Take a break and compute predictions.</span></span><br><span class="line">    training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn)</span><br><span class="line">    training_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> training_predictions])</span><br><span class="line">    </span><br><span class="line">    validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)</span><br><span class="line">    validation_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> validation_predictions])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute training and validation loss.</span></span><br><span class="line">    training_root_mean_squared_error = math.sqrt(</span><br><span class="line">        metrics.mean_squared_error(training_predictions, training_targets))</span><br><span class="line">    validation_root_mean_squared_error = math.sqrt(</span><br><span class="line">        metrics.mean_squared_error(validation_predictions, validation_targets))</span><br><span class="line">    <span class="comment"># Occasionally print the current loss.</span></span><br><span class="line">    print(<span class="string">"  period %02d : %0.2f"</span> % (period, training_root_mean_squared_error))</span><br><span class="line">    <span class="comment"># Add the loss metrics from this period to our list.</span></span><br><span class="line">    training_rmse.append(training_root_mean_squared_error)</span><br><span class="line">    validation_rmse.append(validation_root_mean_squared_error)</span><br><span class="line">  print(<span class="string">"Model training finished."</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Output a graph of loss metrics over periods.</span></span><br><span class="line">  plt.ylabel(<span class="string">"RMSE"</span>)</span><br><span class="line">  plt.xlabel(<span class="string">"Periods"</span>)</span><br><span class="line">  plt.title(<span class="string">"Root Mean Squared Error vs. Periods"</span>)</span><br><span class="line">  plt.tight_layout()</span><br><span class="line">  plt.plot(training_rmse, label=<span class="string">"training"</span>)</span><br><span class="line">  plt.plot(validation_rmse, label=<span class="string">"validation"</span>)</span><br><span class="line">  plt.legend()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> linear_regressor</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">linear_regressor = train_linear_regressor_model(</span><br><span class="line">    learning_rate=<span class="number">0.000001</span>,</span><br><span class="line">    steps=<span class="number">200</span>,</span><br><span class="line">    batch_size=<span class="number">20</span>,</span><br><span class="line">    training_examples=training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
  period 00 : 0.45
  period 01 : 0.45
  period 02 : 0.45
  period 03 : 0.45
  period 04 : 0.46
  period 05 : 0.44
  period 06 : 0.44
  period 07 : 0.44
  period 08 : 0.45
  period 09 : 0.44
Model training finished.
</code></pre><p><img src="TensorFlow_7_16_1.png" alt="png"></p>
<h2 id="计算预测的对数损失函数"><a href="#计算预测的对数损失函数" class="headerlink" title="计算预测的对数损失函数"></a>计算预测的对数损失函数</h2><p><strong>检查预测，并确定是否可以使用它们来计算对数损失函数。</strong></p>
<p><code>LinearRegressor</code> 使用的是 L2 损失，在将输出解读为概率时，它并不能有效地惩罚误分类。例如，对于概率分别为 0.9 和 0.9999 的负分类样本是否被分类为正分类，二者之间的差异应该很大，但 L2 损失并不会明显区分这些情况。</p>
<p>相比之下，<code>LogLoss</code>（对数损失函数）对这些”置信错误”的惩罚力度更大。请注意，<code>LogLoss</code> 的定义如下：</p>
<p>$$Log Loss = \sum_{(x,y)\in D} -y \cdot log(y_{pred}) - (1 - y) \cdot log(1 - y_{pred})$$</p>
<p>但我们首先需要获得预测值。我们可以使用 <code>LinearRegressor.predict</code> 获得预测值。</p>
<p>我们可以使用预测和相应目标计算 <code>LogLoss</code> 吗？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predict_validation_input_fn = <span class="keyword">lambda</span>: my_input_fn(validation_examples,</span><br><span class="line">                                                  validation_targets[<span class="string">"median_house_value_is_high"</span>],</span><br><span class="line">                                                  num_epochs=<span class="number">1</span>,</span><br><span class="line">                                                  shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)</span><br><span class="line">validation_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> validation_predictions])</span><br><span class="line">_ = plt.hist(validation_predictions)</span><br></pre></td></tr></table></figure>
<p><img src="TensorFlow_7_19_0.png" alt="png"></p>
<h2 id="训练逻辑回归模型并计算验证集的对数损失函数"><a href="#训练逻辑回归模型并计算验证集的对数损失函数" class="headerlink" title="训练逻辑回归模型并计算验证集的对数损失函数"></a>训练逻辑回归模型并计算验证集的对数损失函数</h2><p>要使用逻辑回归非常简单，用 <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/LinearClassifier" target="_blank" rel="noopener">LinearClassifier</a> 替代 <code>LinearRegressor</code> 即可。完成以下代码。</p>
<p><strong>注意</strong>：在 <code>LinearClassifier</code> 模型上运行 <code>train()</code> 和 <code>predict()</code> 时，您可以通过返回的字典（例如 <code>predictions[&quot;probabilities&quot;]</code>）中的 <code>&quot;probabilities&quot;</code> 键获取实值预测概率。Sklearn 的 <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html" target="_blank" rel="noopener">log_loss</a> 函数可基于这些概率计算对数损失函数，非常方便。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_linear_classifier_model</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">    steps,</span></span></span><br><span class="line"><span class="function"><span class="params">    batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_targets,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_targets)</span>:</span></span><br><span class="line">  <span class="string">"""Trains a linear classification model.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  In addition to training, this function also prints training progress information,</span></span><br><span class="line"><span class="string">  as well as a plot of the training and validation loss over time.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    learning_rate: A `float`, the learning rate.</span></span><br><span class="line"><span class="string">    steps: A non-zero `int`, the total number of training steps. A training step</span></span><br><span class="line"><span class="string">      consists of a forward and backward pass using a single batch.</span></span><br><span class="line"><span class="string">    batch_size: A non-zero `int`, the batch size.</span></span><br><span class="line"><span class="string">    training_examples: A `DataFrame` containing one or more columns from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as input features for training.</span></span><br><span class="line"><span class="string">    training_targets: A `DataFrame` containing exactly one column from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as target for training.</span></span><br><span class="line"><span class="string">    validation_examples: A `DataFrame` containing one or more columns from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as input features for validation.</span></span><br><span class="line"><span class="string">    validation_targets: A `DataFrame` containing exactly one column from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as target for validation.</span></span><br><span class="line"><span class="string">      </span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A `LinearClassifier` object trained on the training data.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  periods = <span class="number">10</span></span><br><span class="line">  steps_per_period = steps / periods</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Create a linear classifier object.</span></span><br><span class="line">  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)  </span><br><span class="line">  linear_classifier = tf.estimator.LinearClassifier(</span><br><span class="line">      feature_columns=construct_feature_columns(training_examples),</span><br><span class="line">      optimizer=my_optimizer</span><br><span class="line">  )</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Create input functions.</span></span><br><span class="line">  training_input_fn = <span class="keyword">lambda</span>: my_input_fn(training_examples, </span><br><span class="line">                                          training_targets[<span class="string">"median_house_value_is_high"</span>], </span><br><span class="line">                                          batch_size=batch_size)</span><br><span class="line">  predict_training_input_fn = <span class="keyword">lambda</span>: my_input_fn(training_examples, </span><br><span class="line">                                                  training_targets[<span class="string">"median_house_value_is_high"</span>], </span><br><span class="line">                                                  num_epochs=<span class="number">1</span>, </span><br><span class="line">                                                  shuffle=<span class="keyword">False</span>)</span><br><span class="line">  predict_validation_input_fn = <span class="keyword">lambda</span>: my_input_fn(validation_examples, </span><br><span class="line">                                                    validation_targets[<span class="string">"median_house_value_is_high"</span>], </span><br><span class="line">                                                    num_epochs=<span class="number">1</span>, </span><br><span class="line">                                                    shuffle=<span class="keyword">False</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Train the model, but do so inside a loop so that we can periodically assess</span></span><br><span class="line">  <span class="comment"># loss metrics.</span></span><br><span class="line">  print(<span class="string">"Training model..."</span>)</span><br><span class="line">  print(<span class="string">"LogLoss (on training data):"</span>)</span><br><span class="line">  training_log_losses = []</span><br><span class="line">  validation_log_losses = []</span><br><span class="line">  <span class="keyword">for</span> period <span class="keyword">in</span> range (<span class="number">0</span>, periods):</span><br><span class="line">    <span class="comment"># Train the model, starting from the prior state.</span></span><br><span class="line">    linear_classifier.train(</span><br><span class="line">        input_fn=training_input_fn,</span><br><span class="line">        steps=steps_per_period</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># Take a break and compute predictions.    </span></span><br><span class="line">    training_probabilities = linear_classifier.predict(input_fn=predict_training_input_fn)</span><br><span class="line">    training_probabilities = np.array([item[<span class="string">'probabilities'</span>] <span class="keyword">for</span> item <span class="keyword">in</span> training_probabilities])</span><br><span class="line">    </span><br><span class="line">    validation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)</span><br><span class="line">    validation_probabilities = np.array([item[<span class="string">'probabilities'</span>] <span class="keyword">for</span> item <span class="keyword">in</span> validation_probabilities])</span><br><span class="line">    </span><br><span class="line">    training_log_loss = metrics.log_loss(training_targets, training_probabilities)</span><br><span class="line">    validation_log_loss = metrics.log_loss(validation_targets, validation_probabilities)</span><br><span class="line">    <span class="comment"># Occasionally print the current loss.</span></span><br><span class="line">    print(<span class="string">"  period %02d : %0.2f"</span> % (period, training_log_loss))</span><br><span class="line">    <span class="comment"># Add the loss metrics from this period to our list.</span></span><br><span class="line">    training_log_losses.append(training_log_loss)</span><br><span class="line">    validation_log_losses.append(validation_log_loss)</span><br><span class="line">  print(<span class="string">"Model training finished."</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Output a graph of loss metrics over periods.</span></span><br><span class="line">  plt.ylabel(<span class="string">"LogLoss"</span>)</span><br><span class="line">  plt.xlabel(<span class="string">"Periods"</span>)</span><br><span class="line">  plt.title(<span class="string">"LogLoss vs. Periods"</span>)</span><br><span class="line">  plt.tight_layout()</span><br><span class="line">  plt.plot(training_log_losses, label=<span class="string">"training"</span>)</span><br><span class="line">  plt.plot(validation_log_losses, label=<span class="string">"validation"</span>)</span><br><span class="line">  plt.legend()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> linear_classifier</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">linear_classifier = train_linear_classifier_model(</span><br><span class="line">    learning_rate=<span class="number">0.000005</span>,</span><br><span class="line">    steps=<span class="number">500</span>,</span><br><span class="line">    batch_size=<span class="number">20</span>,</span><br><span class="line">    training_examples=training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
LogLoss (on training data):
  period 00 : 0.60
  period 01 : 0.58
  period 02 : 0.57
  period 03 : 0.56
  period 04 : 0.55
  period 05 : 0.55
  period 06 : 0.54
  period 07 : 0.55
  period 08 : 0.54
  period 09 : 0.53
Model training finished.
</code></pre><p><img src="TensorFlow_7_22_1.png" alt="png"></p>
<h2 id="计算查准率并为验证集绘制-ROC-曲线"><a href="#计算查准率并为验证集绘制-ROC-曲线" class="headerlink" title="计算查准率并为验证集绘制 ROC 曲线"></a>计算查准率并为验证集绘制 ROC 曲线</h2><p>分类时非常有用的一些指标包括：模型<a href="https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification" target="_blank" rel="noopener">准确率</a>、<a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank" rel="noopener">ROC 曲线</a>和 ROC 曲线下面积 (AUC)。我们会检查这些指标。</p>
<p><code>LinearClassifier.evaluate</code> 可计算准确率和 AUC 等实用指标。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">evaluation_metrics = linear_classifier.evaluate(input_fn=predict_validation_input_fn)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"AUC on the validation set: %0.2f"</span> % evaluation_metrics[<span class="string">'auc'</span>])</span><br><span class="line">print(<span class="string">"Accuracy on the validation set: %0.2f"</span> % evaluation_metrics[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<pre><code>AUC on the validation set: 0.73
Accuracy on the validation set: 0.76
</code></pre><p> 您可以使用类别概率（例如由 <code>LinearClassifier.predict</code><br>和 Sklearn 的 <a href="http://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics" target="_blank" rel="noopener">roc_curve</a> 计算的概率）来获得绘制 ROC 曲线所需的真正例率和假正例率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">validation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)</span><br><span class="line"><span class="comment"># Get just the probabilities for the positive class.</span></span><br><span class="line">validation_probabilities = np.array([item[<span class="string">'probabilities'</span>][<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> validation_probabilities])</span><br><span class="line"></span><br><span class="line">false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(</span><br><span class="line">    validation_targets, validation_probabilities)</span><br><span class="line">plt.plot(false_positive_rate, true_positive_rate, label=<span class="string">"our model"</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], label=<span class="string">"random classifier"</span>)</span><br><span class="line">_ = plt.legend(loc=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="TensorFlow_7_26_0.png" alt="png"></p>
<p> <strong>看看您是否可以调整训练的模型的学习设置，以改善 AUC。</strong></p>
<p>通常情况下，某些指标在提升的同时会损害其他指标，因此您需要找到可以实现理想折中情况的设置。</p>
<p><strong>验证所有指标是否同时有所提升。</strong></p>
<p> 一个可能有用的解决方案是，只要不过拟合，就训练更长时间。</p>
<p>要做到这一点，我们可以增加步数和/或批量大小。</p>
<p>所有指标同时提升，这样，我们的损失指标就可以很好地代理 AUC 和准确率了。</p>
<p>注意它是如何进行很多很多次迭代，只是为了再尽量增加一点 AUC。这种情况很常见，但通常情况下，即使只有一点小小的收获，投入的成本也是值得的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># TUNE THE SETTINGS BELOW TO IMPROVE AUC</span></span><br><span class="line">linear_classifier = train_linear_classifier_model(</span><br><span class="line">    learning_rate=<span class="number">0.0000035</span>,</span><br><span class="line">    steps=<span class="number">20000</span>,</span><br><span class="line">    batch_size=<span class="number">500</span>,</span><br><span class="line">    training_examples=training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br><span class="line"></span><br><span class="line">evaluation_metrics = linear_classifier.evaluate(input_fn=predict_validation_input_fn)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"AUC on the validation set: %0.2f"</span> % evaluation_metrics[<span class="string">'auc'</span>])</span><br><span class="line">print(<span class="string">"Accuracy on the validation set: %0.2f"</span> % evaluation_metrics[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
LogLoss (on training data):
  period 00 : 0.50
  period 01 : 0.49
  period 02 : 0.48
  period 03 : 0.48
  period 04 : 0.48
  period 05 : 0.47
  period 06 : 0.47
  period 07 : 0.47
  period 08 : 0.47
  period 09 : 0.47
Model training finished.
AUC on the validation set: 0.81
Accuracy on the validation set: 0.79
</code></pre><p><img src="TensorFlow_7_28_1.png" alt="png"></p>
<h1 id="稀疏性和-L1-正则化"><a href="#稀疏性和-L1-正则化" class="headerlink" title="稀疏性和 L1 正则化"></a>稀疏性和 L1 正则化</h1><p> 降低复杂性的一种方法是使用正则化函数，它会使权重正好为零。对于线性模型（例如线性回归），权重为零就相当于完全没有使用相应特征。除了可避免过拟合之外，生成的模型还会更加有效。</p>
<p>L1 正则化是一种增加稀疏性的好方法。</p>
<h2 id="设置-1"><a href="#设置-1" class="headerlink" title="设置"></a>设置</h2><p>加载数据并创建特征定义。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_input_fn</span><span class="params">(features, targets, batch_size=<span class="number">1</span>, shuffle=True, num_epochs=None)</span>:</span></span><br><span class="line">    <span class="string">"""Trains a linear regression model.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      features: pandas DataFrame of features</span></span><br><span class="line"><span class="string">      targets: pandas DataFrame of targets</span></span><br><span class="line"><span class="string">      batch_size: Size of batches to be passed to the model</span></span><br><span class="line"><span class="string">      shuffle: True or False. Whether to shuffle the data.</span></span><br><span class="line"><span class="string">      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      Tuple of (features, labels) for next data batch</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Convert pandas data into a dict of np arrays.</span></span><br><span class="line">    features = &#123;key:np.array(value) <span class="keyword">for</span> key,value <span class="keyword">in</span> dict(features).items()&#125;                                            </span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Construct a dataset, and configure batching/repeating.</span></span><br><span class="line">    ds = Dataset.from_tensor_slices((features,targets)) <span class="comment"># warning: 2GB limit</span></span><br><span class="line">    ds = ds.batch(batch_size).repeat(num_epochs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Shuffle the data, if specified.</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">      ds = ds.shuffle(<span class="number">10000</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Return the next batch of data.</span></span><br><span class="line">    features, labels = ds.make_one_shot_iterator().get_next()</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分桶</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_quantile_based_buckets</span><span class="params">(feature_values, num_buckets)</span>:</span></span><br><span class="line">  quantiles = feature_values.quantile(</span><br><span class="line">    [(i+<span class="number">1.</span>)/(num_buckets + <span class="number">1.</span>) <span class="keyword">for</span> i <span class="keyword">in</span> range(num_buckets)])</span><br><span class="line">  <span class="keyword">return</span> [quantiles[q] <span class="keyword">for</span> q <span class="keyword">in</span> quantiles.keys()]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造特征列</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">construct_feature_columns</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="string">"""Construct the TensorFlow Feature Columns.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A set of feature columns</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  bucketized_households = tf.feature_column.bucketized_column(</span><br><span class="line">    tf.feature_column.numeric_column(<span class="string">"households"</span>),</span><br><span class="line">    boundaries=get_quantile_based_buckets(training_examples[<span class="string">"households"</span>], <span class="number">10</span>))</span><br><span class="line">  bucketized_longitude = tf.feature_column.bucketized_column(</span><br><span class="line">    tf.feature_column.numeric_column(<span class="string">"longitude"</span>),</span><br><span class="line">    boundaries=get_quantile_based_buckets(training_examples[<span class="string">"longitude"</span>], <span class="number">50</span>))</span><br><span class="line">  bucketized_latitude = tf.feature_column.bucketized_column(</span><br><span class="line">    tf.feature_column.numeric_column(<span class="string">"latitude"</span>),</span><br><span class="line">    boundaries=get_quantile_based_buckets(training_examples[<span class="string">"latitude"</span>], <span class="number">50</span>))</span><br><span class="line">  bucketized_housing_median_age = tf.feature_column.bucketized_column(</span><br><span class="line">    tf.feature_column.numeric_column(<span class="string">"housing_median_age"</span>),</span><br><span class="line">    boundaries=get_quantile_based_buckets(</span><br><span class="line">      training_examples[<span class="string">"housing_median_age"</span>], <span class="number">10</span>))</span><br><span class="line">  bucketized_total_rooms = tf.feature_column.bucketized_column(</span><br><span class="line">    tf.feature_column.numeric_column(<span class="string">"total_rooms"</span>),</span><br><span class="line">    boundaries=get_quantile_based_buckets(training_examples[<span class="string">"total_rooms"</span>], <span class="number">10</span>))</span><br><span class="line">  bucketized_total_bedrooms = tf.feature_column.bucketized_column(</span><br><span class="line">    tf.feature_column.numeric_column(<span class="string">"total_bedrooms"</span>),</span><br><span class="line">    boundaries=get_quantile_based_buckets(training_examples[<span class="string">"total_bedrooms"</span>], <span class="number">10</span>))</span><br><span class="line">  bucketized_population = tf.feature_column.bucketized_column(</span><br><span class="line">    tf.feature_column.numeric_column(<span class="string">"population"</span>),</span><br><span class="line">    boundaries=get_quantile_based_buckets(training_examples[<span class="string">"population"</span>], <span class="number">10</span>))</span><br><span class="line">  bucketized_median_income = tf.feature_column.bucketized_column(</span><br><span class="line">    tf.feature_column.numeric_column(<span class="string">"median_income"</span>),</span><br><span class="line">    boundaries=get_quantile_based_buckets(training_examples[<span class="string">"median_income"</span>], <span class="number">10</span>))</span><br><span class="line">  bucketized_rooms_per_person = tf.feature_column.bucketized_column(</span><br><span class="line">    tf.feature_column.numeric_column(<span class="string">"rooms_per_person"</span>),</span><br><span class="line">    boundaries=get_quantile_based_buckets(</span><br><span class="line">      training_examples[<span class="string">"rooms_per_person"</span>], <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">  long_x_lat = tf.feature_column.crossed_column(</span><br><span class="line">    set([bucketized_longitude, bucketized_latitude]), hash_bucket_size=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">  feature_columns = set([</span><br><span class="line">    long_x_lat,</span><br><span class="line">    bucketized_longitude,</span><br><span class="line">    bucketized_latitude,</span><br><span class="line">    bucketized_housing_median_age,</span><br><span class="line">    bucketized_total_rooms,</span><br><span class="line">    bucketized_total_bedrooms,</span><br><span class="line">    bucketized_population,</span><br><span class="line">    bucketized_households,</span><br><span class="line">    bucketized_median_income,</span><br><span class="line">    bucketized_rooms_per_person])</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> feature_columns</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_linear_classifier_model</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">    regularization_strength,</span></span></span><br><span class="line"><span class="function"><span class="params">    steps,</span></span></span><br><span class="line"><span class="function"><span class="params">    batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    feature_columns,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_targets,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_targets)</span>:</span></span><br><span class="line">  <span class="string">"""Trains a linear regression model.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  In addition to training, this function also prints training progress information,</span></span><br><span class="line"><span class="string">  as well as a plot of the training and validation loss over time.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    learning_rate: A `float`, the learning rate.</span></span><br><span class="line"><span class="string">    regularization_strength: A `float` that indicates the strength of the L1</span></span><br><span class="line"><span class="string">       regularization. A value of `0.0` means no regularization.</span></span><br><span class="line"><span class="string">    steps: A non-zero `int`, the total number of training steps. A training step</span></span><br><span class="line"><span class="string">      consists of a forward and backward pass using a single batch.</span></span><br><span class="line"><span class="string">    feature_columns: A `set` specifying the input feature columns to use.</span></span><br><span class="line"><span class="string">    training_examples: A `DataFrame` containing one or more columns from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as input features for training.</span></span><br><span class="line"><span class="string">    training_targets: A `DataFrame` containing exactly one column from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as target for training.</span></span><br><span class="line"><span class="string">    validation_examples: A `DataFrame` containing one or more columns from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as input features for validation.</span></span><br><span class="line"><span class="string">    validation_targets: A `DataFrame` containing exactly one column from</span></span><br><span class="line"><span class="string">      `california_housing_dataframe` to use as target for validation.</span></span><br><span class="line"><span class="string">      </span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A `LinearClassifier` object trained on the training data.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  periods = <span class="number">7</span></span><br><span class="line">  steps_per_period = steps / periods</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Create a linear classifier object.</span></span><br><span class="line">  my_optimizer = tf.train.FtrlOptimizer(learning_rate=learning_rate, l1_regularization_strength=regularization_strength)</span><br><span class="line">  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line">  linear_classifier = tf.estimator.LinearClassifier(</span><br><span class="line">      feature_columns=feature_columns,</span><br><span class="line">      optimizer=my_optimizer</span><br><span class="line">  )</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Create input functions.</span></span><br><span class="line">  training_input_fn = <span class="keyword">lambda</span>: my_input_fn(training_examples, </span><br><span class="line">                                          training_targets[<span class="string">"median_house_value_is_high"</span>], </span><br><span class="line">                                          batch_size=batch_size)</span><br><span class="line">  predict_training_input_fn = <span class="keyword">lambda</span>: my_input_fn(training_examples, </span><br><span class="line">                                                  training_targets[<span class="string">"median_house_value_is_high"</span>], </span><br><span class="line">                                                  num_epochs=<span class="number">1</span>, </span><br><span class="line">                                                  shuffle=<span class="keyword">False</span>)</span><br><span class="line">  predict_validation_input_fn = <span class="keyword">lambda</span>: my_input_fn(validation_examples, </span><br><span class="line">                                                    validation_targets[<span class="string">"median_house_value_is_high"</span>], </span><br><span class="line">                                                    num_epochs=<span class="number">1</span>, </span><br><span class="line">                                                    shuffle=<span class="keyword">False</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Train the model, but do so inside a loop so that we can periodically assess</span></span><br><span class="line">  <span class="comment"># loss metrics.</span></span><br><span class="line">  print(<span class="string">"Training model..."</span>)</span><br><span class="line">  print(<span class="string">"LogLoss (on validation data):"</span>)</span><br><span class="line">  training_log_losses = []</span><br><span class="line">  validation_log_losses = []</span><br><span class="line">  <span class="keyword">for</span> period <span class="keyword">in</span> range (<span class="number">0</span>, periods):</span><br><span class="line">    <span class="comment"># Train the model, starting from the prior state.</span></span><br><span class="line">    linear_classifier.train(</span><br><span class="line">        input_fn=training_input_fn,</span><br><span class="line">        steps=steps_per_period</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># Take a break and compute predictions.</span></span><br><span class="line">    training_probabilities = linear_classifier.predict(input_fn=predict_training_input_fn)</span><br><span class="line">    training_probabilities = np.array([item[<span class="string">'probabilities'</span>] <span class="keyword">for</span> item <span class="keyword">in</span> training_probabilities])</span><br><span class="line">    </span><br><span class="line">    validation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)</span><br><span class="line">    validation_probabilities = np.array([item[<span class="string">'probabilities'</span>] <span class="keyword">for</span> item <span class="keyword">in</span> validation_probabilities])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute training and validation loss.</span></span><br><span class="line">    training_log_loss = metrics.log_loss(training_targets, training_probabilities)</span><br><span class="line">    validation_log_loss = metrics.log_loss(validation_targets, validation_probabilities)</span><br><span class="line">    <span class="comment"># Occasionally print the current loss.</span></span><br><span class="line">    print(<span class="string">"  period %02d : %0.2f"</span> % (period, validation_log_loss))</span><br><span class="line">    <span class="comment"># Add the loss metrics from this period to our list.</span></span><br><span class="line">    training_log_losses.append(training_log_loss)</span><br><span class="line">    validation_log_losses.append(validation_log_loss)</span><br><span class="line">  print(<span class="string">"Model training finished."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Output a graph of loss metrics over periods.</span></span><br><span class="line">  plt.ylabel(<span class="string">"LogLoss"</span>)</span><br><span class="line">  plt.xlabel(<span class="string">"Periods"</span>)</span><br><span class="line">  plt.title(<span class="string">"LogLoss vs. Periods"</span>)</span><br><span class="line">  plt.tight_layout()</span><br><span class="line">  plt.plot(training_log_losses, label=<span class="string">"training"</span>)</span><br><span class="line">  plt.plot(validation_log_losses, label=<span class="string">"validation"</span>)</span><br><span class="line">  plt.legend()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> linear_classifier</span><br></pre></td></tr></table></figure>
<h2 id="计算模型大小"><a href="#计算模型大小" class="headerlink" title="计算模型大小"></a>计算模型大小</h2><p>要计算模型大小，只需计算非零参数的数量即可。为此，我们在下面提供了一个辅助函数。该函数深入使用了 Estimator API，如果不了解它的工作原理，也不用担心。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_size</span><span class="params">(estimator)</span>:</span></span><br><span class="line">  variables = estimator.get_variable_names()</span><br><span class="line">  size = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> variable <span class="keyword">in</span> variables:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> any(x <span class="keyword">in</span> variable </span><br><span class="line">               <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">'global_step'</span>,</span><br><span class="line">                         <span class="string">'centered_bias_weight'</span>,</span><br><span class="line">                         <span class="string">'bias_weight'</span>,</span><br><span class="line">                         <span class="string">'Ftrl'</span>]</span><br><span class="line">              ):</span><br><span class="line">      size += np.count_nonzero(estimator.get_variable_value(variable))</span><br><span class="line">  <span class="keyword">return</span> size</span><br></pre></td></tr></table></figure>
<h2 id="减小模型大小"><a href="#减小模型大小" class="headerlink" title="减小模型大小"></a>减小模型大小</h2><p>您的团队需要针对 <em>SmartRing</em> 构建一个准确度高的逻辑回归模型，这种指环非常智能，可以感应城市街区的人口统计特征（<code>median_income</code>、<code>avg_rooms</code>、<code>households</code> 等等），并告诉您指定城市街区的住房成本是否高昂。</p>
<p>由于 SmartRing 很小，因此工程团队已确定它只能处理<strong>参数数量不超过 600 个</strong>的模型。另一方面，产品管理团队也已确定，除非所保留测试集的<strong>对数损失函数低于 0.35</strong>，否则该模型不能发布。</p>
<p>您可以使用秘密武器“L1 正则化”调整模型，使其同时满足大小和准确率限制条件吗？</p>
<h3 id="查找合适的正则化系数。"><a href="#查找合适的正则化系数。" class="headerlink" title="查找合适的正则化系数。"></a>查找合适的正则化系数。</h3><p><strong>查找可同时满足以下两种限制条件的 L1 正则化强度参数：模型的参数数量不超过 600 个且验证集的对数损失函数低于 0.35。</strong></p>
<p>以下代码可帮助您快速开始。您可以通过多种方法向您的模型应用正则化。在此练习中，我们选择使用 <code>FtrlOptimizer</code> 来应用正则化。<code>FtrlOptimizer</code> 是一种设计成使用 L1 正则化比标准梯度下降法得到更好结果的方法。</p>
<p>重申一次，我们会使用整个数据集来训练该模型，因此预计其运行速度会比通常要慢。</p>
<p> 正则化强度为 0.1 应该就足够了。请注意，有一个需要做出折中选择的地方：正则化越强，我们获得的模型就越小，但会影响分类损失。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">linear_classifier = train_linear_classifier_model(</span><br><span class="line">    learning_rate=<span class="number">0.1</span>,</span><br><span class="line">    <span class="comment"># TWEAK THE REGULARIZATION VALUE BELOW</span></span><br><span class="line">    regularization_strength=<span class="number">0.0</span>,</span><br><span class="line">    steps=<span class="number">300</span>,</span><br><span class="line">    batch_size=<span class="number">100</span>,</span><br><span class="line">    feature_columns=construct_feature_columns(),</span><br><span class="line">    training_examples=training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br><span class="line">print(<span class="string">"Model size:"</span>, model_size(linear_classifier))</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
LogLoss (on validation data):
  period 00 : 0.30
  period 01 : 0.27
  period 02 : 0.26
  period 03 : 0.25
  period 04 : 0.24
  period 05 : 0.24
  period 06 : 0.23
Model training finished.
Model size: 790
</code></pre><p><img src="TensorFlow_7_36_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">linear_classifier = train_linear_classifier_model(</span><br><span class="line">    learning_rate=<span class="number">0.1</span>,</span><br><span class="line">    regularization_strength=<span class="number">0.1</span>,</span><br><span class="line">    steps=<span class="number">300</span>,</span><br><span class="line">    batch_size=<span class="number">100</span>,</span><br><span class="line">    feature_columns=construct_feature_columns(),</span><br><span class="line">    training_examples=training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br><span class="line">print(<span class="string">"Model size:"</span>, model_size(linear_classifier))</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
LogLoss (on validation data):
  period 00 : 0.31
  period 01 : 0.27
  period 02 : 0.26
  period 03 : 0.25
  period 04 : 0.24
  period 05 : 0.24
  period 06 : 0.23
Model training finished.
Model size: 764
</code></pre><p><img src="TensorFlow_7_37_1.png" alt="png"></p>
]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow 9</title>
    <url>/2019/02/21/TensorFlow_9/</url>
    <content><![CDATA[<h1 id="使用神经网络对手写数字进行分类"><a href="#使用神经网络对手写数字进行分类" class="headerlink" title="使用神经网络对手写数字进行分类"></a>使用神经网络对手写数字进行分类</h1><p> <img src="https://www.tensorflow.org/versions/r0.11/images/MNIST.png" alt="img"></p>
<ul>
<li>训练线性模型和神经网络，以对传统 <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST</a> 数据集中的手写数字进行分类</li>
<li>比较线性分类模型和神经网络分类模型的效果</li>
<li><p>可视化神经网络隐藏层的权重</p>
<p>我们的目标是将每个输入图片与正确的数字相对应。我们会创建一个包含几个隐藏层的神经网络，并在顶部放置一个归一化指数层，以选出最合适的类别。</p>
<h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2></li>
</ul>
<p>首先，我们下载数据集、导入 TensorFlow 和其他实用工具，并将数据加载到 <em>Pandas</em> <code>DataFrame</code>。请注意，此数据是原始 MNIST 训练数据的样本；我们随机选择了 20000 行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> gridspec</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line">tf.logging.set_verbosity(tf.logging.ERROR)</span><br><span class="line">pd.options.display.max_rows = <span class="number">10</span></span><br><span class="line">pd.options.display.float_format = <span class="string">'&#123;:.1f&#125;'</span>.format</span><br><span class="line"></span><br><span class="line">mnist_dataframe = pd.read_csv(</span><br><span class="line">  <span class="string">"https://download.mlcc.google.cn/mledu-datasets/mnist_train_small.csv"</span>,</span><br><span class="line">  sep=<span class="string">","</span>,</span><br><span class="line">  header=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use just the first 10,000 records for training/validation.</span></span><br><span class="line">mnist_dataframe = mnist_dataframe.head(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">mnist_dataframe = mnist_dataframe.reindex(np.random.permutation(mnist_dataframe.index))</span><br><span class="line">mnist_dataframe.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>775</th>
      <th>776</th>
      <th>777</th>
      <th>778</th>
      <th>779</th>
      <th>780</th>
      <th>781</th>
      <th>782</th>
      <th>783</th>
      <th>784</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>5456</th>
      <td>8</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>934</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2662</th>
      <td>8</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9385</th>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>157</th>
      <td>8</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 785 columns</p>
</div>

<p> 第一列中包含类别标签。其余列中包含特征值，每个像素对应一个特征值，有 <code>28×28=784</code> 个像素值，其中大部分像素值都为零；您也许需要花一分钟时间来确认它们不<em>全部</em>为零。</p>
<p>  <img src="https://www.tensorflow.org/versions/r0.11/images/MNIST-Matrix.png" alt="img"></p>
<p>   这些样本都是分辨率相对较低、对比度相对较高的手写数字图片。<code>0-9</code> 这十个数字中的每个可能出现的数字均由唯一的类别标签表示。因此，这是一个具有 10 个类别的多类别分类问题。</p>
<p>现在，我们解析一下标签和特征，并查看几个样本。注意 <code>loc</code> 的使用，借助 <code>loc</code>，我们能够基于原来的位置抽出各列，因为此数据集中没有标题行。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_labels_and_features</span><span class="params">(dataset)</span>:</span></span><br><span class="line">  <span class="string">"""Extracts labels and features.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  This is a good place to scale or transform the features if needed.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    dataset: A Pandas `Dataframe`, containing the label on the first column and</span></span><br><span class="line"><span class="string">      monochrome pixel values on the remaining columns, in row major order.</span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A `tuple` `(labels, features)`:</span></span><br><span class="line"><span class="string">      labels: A Pandas `Series`.</span></span><br><span class="line"><span class="string">      features: A Pandas `DataFrame`.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  labels = dataset[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># DataFrame.loc index ranges are inclusive at both ends.</span></span><br><span class="line">  features = dataset.loc[:,<span class="number">1</span>:<span class="number">784</span>]</span><br><span class="line">  <span class="comment"># Scale the data to [0, 1] by dividing out the max value, 255.</span></span><br><span class="line">  features = features / <span class="number">255</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> labels, features</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">training_targets, training_examples = parse_labels_and_features(mnist_dataframe[:<span class="number">7500</span>])</span><br><span class="line">training_examples.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>...</th>
      <th>775</th>
      <th>776</th>
      <th>777</th>
      <th>778</th>
      <th>779</th>
      <th>780</th>
      <th>781</th>
      <th>782</th>
      <th>783</th>
      <th>784</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>7500.0</td>
      <td>7500.0</td>
      <td>7500.0</td>
      <td>7500.0</td>
      <td>7500.0</td>
      <td>7500.0</td>
      <td>7500.0</td>
      <td>7500.0</td>
      <td>7500.0</td>
      <td>7500.0</td>
      <td>...</td>
      <td>7500.0</td>
      <td>7500.0</td>
      <td>7500.0</td>
      <td>7500.0</td>
      <td>7500.0</td>
      <td>7500.0</td>
      <td>7500.0</td>
      <td>7500.0</td>
      <td>7500.0</td>
      <td>7500.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.8</td>
      <td>0.2</td>
      <td>1.0</td>
      <td>0.2</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 784 columns</p>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">validation_targets, validation_examples = parse_labels_and_features(mnist_dataframe[<span class="number">7500</span>:<span class="number">10000</span>])</span><br><span class="line">validation_examples.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>...</th>
      <th>775</th>
      <th>776</th>
      <th>777</th>
      <th>778</th>
      <th>779</th>
      <th>780</th>
      <th>781</th>
      <th>782</th>
      <th>783</th>
      <th>784</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>2500.0</td>
      <td>2500.0</td>
      <td>2500.0</td>
      <td>2500.0</td>
      <td>2500.0</td>
      <td>2500.0</td>
      <td>2500.0</td>
      <td>2500.0</td>
      <td>2500.0</td>
      <td>2500.0</td>
      <td>...</td>
      <td>2500.0</td>
      <td>2500.0</td>
      <td>2500.0</td>
      <td>2500.0</td>
      <td>2500.0</td>
      <td>2500.0</td>
      <td>2500.0</td>
      <td>2500.0</td>
      <td>2500.0</td>
      <td>2500.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>1.0</td>
      <td>0.8</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 784 columns</p>
</div>

<p> 显示一个随机样本及其对应的标签。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rand_example = np.random.choice(training_examples.index)</span><br><span class="line">_, ax = plt.subplots()</span><br><span class="line">ax.matshow(training_examples.loc[rand_example].values.reshape(<span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">ax.set_title(<span class="string">"Label: %i"</span> % training_targets.loc[rand_example])</span><br><span class="line">ax.grid(<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p><img src="TensorFlow_9_11_0.png" alt="png"></p>
<h2 id="为-MNIST-构建线性模型"><a href="#为-MNIST-构建线性模型" class="headerlink" title="为 MNIST 构建线性模型"></a>为 MNIST 构建线性模型</h2><p>首先，我们创建一个基准模型，作为比较对象。<code>LinearClassifier</code> 可提供一组 <em>k</em> 类一对多分类器，每个类别（共 <em>k</em> 个）对应一个分类器。</p>
<p>您会发现，除了报告准确率和绘制对数损失函数随时间变化情况的曲线图之外，我们还展示了一个<a href="https://en.wikipedia.org/wiki/Confusion_matrix" target="_blank" rel="noopener"><strong>混淆矩阵</strong></a>。混淆矩阵会显示错误分类为其他类别的类别。哪些数字相互之间容易混淆？</p>
<p>另请注意，我们会使用 <code>log_loss</code> 函数跟踪模型的错误。不应将此函数与用于训练的 <code>LinearClassifier</code> 内部损失函数相混淆。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">construct_feature_columns</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="string">"""Construct the TensorFlow Feature Columns.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A set of feature columns</span></span><br><span class="line"><span class="string">  """</span> </span><br><span class="line">  </span><br><span class="line">  <span class="comment"># There are 784 pixels in each image. </span></span><br><span class="line">  <span class="keyword">return</span> set([tf.feature_column.numeric_column(<span class="string">'pixels'</span>, shape=<span class="number">784</span>)])</span><br></pre></td></tr></table></figure>
<p> 在本次练习中，我们会对训练和预测使用单独的输入函数，并将这些函数分别嵌套在 <code>create_training_input_fn()</code> 和 <code>create_predict_input_fn()</code> 中，这样一来，我们就可以调用这些函数，以返回相应的 <code>_input_fn</code>，并将其传递到 <code>.train()</code> 和 <code>.predict()</code> 调用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_training_input_fn</span><span class="params">(features, labels, batch_size, num_epochs=None, shuffle=True)</span>:</span></span><br><span class="line">  <span class="string">"""A custom input_fn for sending MNIST data to the estimator for training.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    features: The training features.</span></span><br><span class="line"><span class="string">    labels: The training labels.</span></span><br><span class="line"><span class="string">    batch_size: Batch size to use during training.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A function that returns batches of training features and labels during</span></span><br><span class="line"><span class="string">    training.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_input_fn</span><span class="params">(num_epochs=None, shuffle=True)</span>:</span></span><br><span class="line">    <span class="comment"># Input pipelines are reset with each call to .train(). To ensure model</span></span><br><span class="line">    <span class="comment"># gets a good sampling of data, even when number of steps is small, we </span></span><br><span class="line">    <span class="comment"># shuffle all the data before creating the Dataset object</span></span><br><span class="line">    idx = np.random.permutation(features.index)</span><br><span class="line">    raw_features = &#123;<span class="string">"pixels"</span>:features.reindex(idx)&#125;</span><br><span class="line">    raw_targets = np.array(labels[idx])</span><br><span class="line">   </span><br><span class="line">    ds = Dataset.from_tensor_slices((raw_features,raw_targets)) <span class="comment"># warning: 2GB limit</span></span><br><span class="line">    ds = ds.batch(batch_size).repeat(num_epochs)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">      ds = ds.shuffle(<span class="number">10000</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Return the next batch of data.</span></span><br><span class="line">    feature_batch, label_batch = ds.make_one_shot_iterator().get_next()</span><br><span class="line">    <span class="keyword">return</span> feature_batch, label_batch</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> _input_fn</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_predict_input_fn</span><span class="params">(features, labels, batch_size)</span>:</span></span><br><span class="line">  <span class="string">"""A custom input_fn for sending mnist data to the estimator for predictions.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    features: The features to base predictions on.</span></span><br><span class="line"><span class="string">    labels: The labels of the prediction examples.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A function that returns features and labels for predictions.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_input_fn</span><span class="params">()</span>:</span></span><br><span class="line">    raw_features = &#123;<span class="string">"pixels"</span>: features.values&#125;</span><br><span class="line">    raw_targets = np.array(labels)</span><br><span class="line">    </span><br><span class="line">    ds = Dataset.from_tensor_slices((raw_features, raw_targets)) <span class="comment"># warning: 2GB limit</span></span><br><span class="line">    ds = ds.batch(batch_size)</span><br><span class="line">    </span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Return the next batch of data.</span></span><br><span class="line">    feature_batch, label_batch = ds.make_one_shot_iterator().get_next()</span><br><span class="line">    <span class="keyword">return</span> feature_batch, label_batch</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> _input_fn</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_linear_classification_model</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">    steps,</span></span></span><br><span class="line"><span class="function"><span class="params">    batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_targets,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_targets)</span>:</span></span><br><span class="line">  <span class="string">"""Trains a linear classification model for the MNIST digits dataset.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  In addition to training, this function also prints training progress information,</span></span><br><span class="line"><span class="string">  a plot of the training and validation loss over time, and a confusion</span></span><br><span class="line"><span class="string">  matrix.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    learning_rate: A `float`, the learning rate to use.</span></span><br><span class="line"><span class="string">    steps: A non-zero `int`, the total number of training steps. A training step</span></span><br><span class="line"><span class="string">      consists of a forward and backward pass using a single batch.</span></span><br><span class="line"><span class="string">    batch_size: A non-zero `int`, the batch size.</span></span><br><span class="line"><span class="string">    training_examples: A `DataFrame` containing the training features.</span></span><br><span class="line"><span class="string">    training_targets: A `DataFrame` containing the training labels.</span></span><br><span class="line"><span class="string">    validation_examples: A `DataFrame` containing the validation features.</span></span><br><span class="line"><span class="string">    validation_targets: A `DataFrame` containing the validation labels.</span></span><br><span class="line"><span class="string">      </span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    The trained `LinearClassifier` object.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  periods = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">  steps_per_period = steps / periods  </span><br><span class="line">  <span class="comment"># Create the input functions.</span></span><br><span class="line">  predict_training_input_fn = create_predict_input_fn(</span><br><span class="line">    training_examples, training_targets, batch_size)</span><br><span class="line">  predict_validation_input_fn = create_predict_input_fn(</span><br><span class="line">    validation_examples, validation_targets, batch_size)</span><br><span class="line">  training_input_fn = create_training_input_fn(</span><br><span class="line">    training_examples, training_targets, batch_size)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Create a LinearClassifier object.</span></span><br><span class="line">  my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)</span><br><span class="line">  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line">  classifier = tf.estimator.LinearClassifier(</span><br><span class="line">      feature_columns=construct_feature_columns(),</span><br><span class="line">      n_classes=<span class="number">10</span>,</span><br><span class="line">      optimizer=my_optimizer,</span><br><span class="line">      config=tf.estimator.RunConfig(keep_checkpoint_max=<span class="number">1</span>)</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Train the model, but do so inside a loop so that we can periodically assess</span></span><br><span class="line">  <span class="comment"># loss metrics.</span></span><br><span class="line">  print(<span class="string">"Training model..."</span>)</span><br><span class="line">  print(<span class="string">"LogLoss error (on validation data):"</span>)</span><br><span class="line">  training_errors = []</span><br><span class="line">  validation_errors = []</span><br><span class="line">  <span class="keyword">for</span> period <span class="keyword">in</span> range (<span class="number">0</span>, periods):</span><br><span class="line">    <span class="comment"># Train the model, starting from the prior state.</span></span><br><span class="line">    classifier.train(</span><br><span class="line">        input_fn=training_input_fn,</span><br><span class="line">        steps=steps_per_period</span><br><span class="line">    )</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Take a break and compute probabilities.</span></span><br><span class="line">    training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))</span><br><span class="line">    training_probabilities = np.array([item[<span class="string">'probabilities'</span>] <span class="keyword">for</span> item <span class="keyword">in</span> training_predictions])</span><br><span class="line">    training_pred_class_id = np.array([item[<span class="string">'class_ids'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> training_predictions])</span><br><span class="line">    training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))</span><br><span class="line">    validation_probabilities = np.array([item[<span class="string">'probabilities'</span>] <span class="keyword">for</span> item <span class="keyword">in</span> validation_predictions])    </span><br><span class="line">    validation_pred_class_id = np.array([item[<span class="string">'class_ids'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> validation_predictions])</span><br><span class="line">    validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,<span class="number">10</span>)    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute training and validation errors.</span></span><br><span class="line">    training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)</span><br><span class="line">    validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)</span><br><span class="line">    <span class="comment"># Occasionally print the current loss.</span></span><br><span class="line">    print(<span class="string">"  period %02d : %0.2f"</span> % (period, validation_log_loss))</span><br><span class="line">    <span class="comment"># Add the loss metrics from this period to our list.</span></span><br><span class="line">    training_errors.append(training_log_loss)</span><br><span class="line">    validation_errors.append(validation_log_loss)</span><br><span class="line">  print(<span class="string">"Model training finished."</span>)</span><br><span class="line">  <span class="comment"># Remove event files to save disk space.</span></span><br><span class="line">  _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, <span class="string">'events.out.tfevents*'</span>)))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Calculate final predictions (not probabilities, as above).</span></span><br><span class="line">  final_predictions = classifier.predict(input_fn=predict_validation_input_fn)</span><br><span class="line">  final_predictions = np.array([item[<span class="string">'class_ids'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> final_predictions])</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  accuracy = metrics.accuracy_score(validation_targets, final_predictions)</span><br><span class="line">  print(<span class="string">"Final accuracy (on validation data): %0.2f"</span> % accuracy)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Output a graph of loss metrics over periods.</span></span><br><span class="line">  plt.ylabel(<span class="string">"LogLoss"</span>)</span><br><span class="line">  plt.xlabel(<span class="string">"Periods"</span>)</span><br><span class="line">  plt.title(<span class="string">"LogLoss vs. Periods"</span>)</span><br><span class="line">  plt.plot(training_errors, label=<span class="string">"training"</span>)</span><br><span class="line">  plt.plot(validation_errors, label=<span class="string">"validation"</span>)</span><br><span class="line">  plt.legend()</span><br><span class="line">  plt.show()</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Output a plot of the confusion matrix.</span></span><br><span class="line">  cm = metrics.confusion_matrix(validation_targets, final_predictions)</span><br><span class="line">  <span class="comment"># Normalize the confusion matrix by row (i.e by the number of samples</span></span><br><span class="line">  <span class="comment"># in each class).</span></span><br><span class="line">  cm_normalized = cm.astype(<span class="string">"float"</span>) / cm.sum(axis=<span class="number">1</span>)[:, np.newaxis]</span><br><span class="line">  ax = sns.heatmap(cm_normalized, cmap=<span class="string">"bone_r"</span>)</span><br><span class="line">  ax.set_aspect(<span class="number">1</span>)</span><br><span class="line">  plt.title(<span class="string">"Confusion matrix"</span>)</span><br><span class="line">  plt.ylabel(<span class="string">"True label"</span>)</span><br><span class="line">  plt.xlabel(<span class="string">"Predicted label"</span>)</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> classifier</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_ = train_linear_classification_model(</span><br><span class="line">    learning_rate=<span class="number">0.03</span>,</span><br><span class="line">    steps=<span class="number">1000</span>,</span><br><span class="line">    batch_size=<span class="number">30</span>,</span><br><span class="line">    training_examples=training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
LogLoss error (on validation data):
  period 00 : 4.39
  period 01 : 4.01
  period 02 : 3.77
  period 03 : 3.84
  period 04 : 3.70
  period 05 : 3.59
  period 06 : 3.65
  period 07 : 3.54
  period 08 : 3.50
  period 09 : 3.55
Model training finished.
Final accuracy (on validation data): 0.90
</code></pre><p><img src="TensorFlow_9_18_1.png" alt="png"></p>
<p><img src="TensorFlow_9_18_2.png" alt="png"></p>
<h2 id="使用神经网络替换线性分类器"><a href="#使用神经网络替换线性分类器" class="headerlink" title="使用神经网络替换线性分类器"></a>使用神经网络替换线性分类器</h2><p><strong>使用 <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier" target="_blank" rel="noopener"><code>DNNClassifier</code></a> 替换上面的 LinearClassifier，并查找可实现 0.95 或更高准确率的参数组合。</strong></p>
<p>您可能希望尝试 Dropout 等其他正则化方法。这些额外的正则化方法已记录在 <code>DNNClassifier</code> 类的注释中。</p>
<p> 除了神经网络专用配置（例如隐藏单元的超参数）之外，以下代码与原始的 <code>LinearClassifer</code> 训练代码几乎完全相同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_nn_classification_model</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">    steps,</span></span></span><br><span class="line"><span class="function"><span class="params">    batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    hidden_units,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_targets,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_targets)</span>:</span></span><br><span class="line">  <span class="string">"""Trains a neural network classification model for the MNIST digits dataset.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  In addition to training, this function also prints training progress information,</span></span><br><span class="line"><span class="string">  a plot of the training and validation loss over time, as well as a confusion</span></span><br><span class="line"><span class="string">  matrix.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    learning_rate: A `float`, the learning rate to use.</span></span><br><span class="line"><span class="string">    steps: A non-zero `int`, the total number of training steps. A training step</span></span><br><span class="line"><span class="string">      consists of a forward and backward pass using a single batch.</span></span><br><span class="line"><span class="string">    batch_size: A non-zero `int`, the batch size.</span></span><br><span class="line"><span class="string">    hidden_units: A `list` of int values, specifying the number of neurons in each layer.</span></span><br><span class="line"><span class="string">    training_examples: A `DataFrame` containing the training features.</span></span><br><span class="line"><span class="string">    training_targets: A `DataFrame` containing the training labels.</span></span><br><span class="line"><span class="string">    validation_examples: A `DataFrame` containing the validation features.</span></span><br><span class="line"><span class="string">    validation_targets: A `DataFrame` containing the validation labels.</span></span><br><span class="line"><span class="string">      </span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    The trained `DNNClassifier` object.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  periods = <span class="number">10</span></span><br><span class="line">  <span class="comment"># Caution: input pipelines are reset with each call to train. </span></span><br><span class="line">  <span class="comment"># If the number of steps is small, your model may never see most of the data.  </span></span><br><span class="line">  <span class="comment"># So with multiple `.train` calls like this you may want to control the length </span></span><br><span class="line">  <span class="comment"># of training with num_epochs passed to the input_fn. Or, you can do a really-big shuffle, </span></span><br><span class="line">  <span class="comment"># or since it's in-memory data, shuffle all the data in the `input_fn`.</span></span><br><span class="line">  steps_per_period = steps / periods  </span><br><span class="line">    </span><br><span class="line">  <span class="comment"># Create the input functions.</span></span><br><span class="line">  predict_training_input_fn = create_predict_input_fn(</span><br><span class="line">    training_examples, training_targets, batch_size)</span><br><span class="line">  predict_validation_input_fn = create_predict_input_fn(</span><br><span class="line">    validation_examples, validation_targets, batch_size)</span><br><span class="line">  training_input_fn = create_training_input_fn(</span><br><span class="line">    training_examples, training_targets, batch_size)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Create feature columns.</span></span><br><span class="line">  feature_columns = [tf.feature_column.numeric_column(<span class="string">'pixels'</span>, shape=<span class="number">784</span>)]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Create a DNNClassifier object.</span></span><br><span class="line">  my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)</span><br><span class="line">  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line">  classifier = tf.estimator.DNNClassifier(</span><br><span class="line">      feature_columns=feature_columns,</span><br><span class="line">      n_classes=<span class="number">10</span>,</span><br><span class="line">      hidden_units=hidden_units,</span><br><span class="line">      optimizer=my_optimizer,</span><br><span class="line">      config=tf.contrib.learn.RunConfig(keep_checkpoint_max=<span class="number">1</span>)</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Train the model, but do so inside a loop so that we can periodically assess</span></span><br><span class="line">  <span class="comment"># loss metrics.</span></span><br><span class="line">  print(<span class="string">"Training model..."</span>)</span><br><span class="line">  print(<span class="string">"LogLoss error (on validation data):"</span>)</span><br><span class="line">  training_errors = []</span><br><span class="line">  validation_errors = []</span><br><span class="line">  <span class="keyword">for</span> period <span class="keyword">in</span> range (<span class="number">0</span>, periods):</span><br><span class="line">    <span class="comment"># Train the model, starting from the prior state.</span></span><br><span class="line">    classifier.train(</span><br><span class="line">        input_fn=training_input_fn,</span><br><span class="line">        steps=steps_per_period</span><br><span class="line">    )</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Take a break and compute probabilities.</span></span><br><span class="line">    training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))</span><br><span class="line">    training_probabilities = np.array([item[<span class="string">'probabilities'</span>] <span class="keyword">for</span> item <span class="keyword">in</span> training_predictions])</span><br><span class="line">    training_pred_class_id = np.array([item[<span class="string">'class_ids'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> training_predictions])</span><br><span class="line">    training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))</span><br><span class="line">    validation_probabilities = np.array([item[<span class="string">'probabilities'</span>] <span class="keyword">for</span> item <span class="keyword">in</span> validation_predictions])    </span><br><span class="line">    validation_pred_class_id = np.array([item[<span class="string">'class_ids'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> validation_predictions])</span><br><span class="line">    validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,<span class="number">10</span>)    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute training and validation errors.</span></span><br><span class="line">    training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)</span><br><span class="line">    validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)</span><br><span class="line">    <span class="comment"># Occasionally print the current loss.</span></span><br><span class="line">    print(<span class="string">"  period %02d : %0.2f"</span> % (period, validation_log_loss))</span><br><span class="line">    <span class="comment"># Add the loss metrics from this period to our list.</span></span><br><span class="line">    training_errors.append(training_log_loss)</span><br><span class="line">    validation_errors.append(validation_log_loss)</span><br><span class="line">  print(<span class="string">"Model training finished."</span>)</span><br><span class="line">  <span class="comment"># Remove event files to save disk space.</span></span><br><span class="line">  _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, <span class="string">'events.out.tfevents*'</span>)))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Calculate final predictions (not probabilities, as above).</span></span><br><span class="line">  final_predictions = classifier.predict(input_fn=predict_validation_input_fn)</span><br><span class="line">  final_predictions = np.array([item[<span class="string">'class_ids'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> final_predictions])</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  accuracy = metrics.accuracy_score(validation_targets, final_predictions)</span><br><span class="line">  print(<span class="string">"Final accuracy (on validation data): %0.2f"</span> % accuracy)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Output a graph of loss metrics over periods.</span></span><br><span class="line">  plt.ylabel(<span class="string">"LogLoss"</span>)</span><br><span class="line">  plt.xlabel(<span class="string">"Periods"</span>)</span><br><span class="line">  plt.title(<span class="string">"LogLoss vs. Periods"</span>)</span><br><span class="line">  plt.plot(training_errors, label=<span class="string">"training"</span>)</span><br><span class="line">  plt.plot(validation_errors, label=<span class="string">"validation"</span>)</span><br><span class="line">  plt.legend()</span><br><span class="line">  plt.show()</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Output a plot of the confusion matrix.</span></span><br><span class="line">  cm = metrics.confusion_matrix(validation_targets, final_predictions)</span><br><span class="line">  <span class="comment"># Normalize the confusion matrix by row (i.e by the number of samples</span></span><br><span class="line">  <span class="comment"># in each class).</span></span><br><span class="line">  cm_normalized = cm.astype(<span class="string">"float"</span>) / cm.sum(axis=<span class="number">1</span>)[:, np.newaxis]</span><br><span class="line">  ax = sns.heatmap(cm_normalized, cmap=<span class="string">"bone_r"</span>)</span><br><span class="line">  ax.set_aspect(<span class="number">1</span>)</span><br><span class="line">  plt.title(<span class="string">"Confusion matrix"</span>)</span><br><span class="line">  plt.ylabel(<span class="string">"True label"</span>)</span><br><span class="line">  plt.xlabel(<span class="string">"Predicted label"</span>)</span><br><span class="line">  plt.show()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> classifier</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classifier = train_nn_classification_model(</span><br><span class="line">    learning_rate=<span class="number">0.05</span>,</span><br><span class="line">    steps=<span class="number">1000</span>,</span><br><span class="line">    batch_size=<span class="number">30</span>,</span><br><span class="line">    hidden_units=[<span class="number">100</span>, <span class="number">100</span>],</span><br><span class="line">    training_examples=training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
LogLoss error (on validation data):
  period 00 : 4.01
  period 01 : 3.45
  period 02 : 3.01
  period 03 : 3.19
  period 04 : 2.62
  period 05 : 2.29
  period 06 : 2.17
  period 07 : 2.11
  period 08 : 2.06
  period 09 : 2.00
Model training finished.
Final accuracy (on validation data): 0.94
</code></pre><p><img src="TensorFlow_9_22_1.png" alt="png"></p>
<p><img src="TensorFlow_9_22_2.png" alt="png"></p>
<p> 接下来，我们来验证测试集的准确率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mnist_test_dataframe = pd.read_csv(</span><br><span class="line">  <span class="string">"https://download.mlcc.google.cn/mledu-datasets/mnist_test.csv"</span>,</span><br><span class="line">  sep=<span class="string">","</span>,</span><br><span class="line">  header=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">test_targets, test_examples = parse_labels_and_features(mnist_test_dataframe)</span><br><span class="line">test_examples.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>...</th>
      <th>775</th>
      <th>776</th>
      <th>777</th>
      <th>778</th>
      <th>779</th>
      <th>780</th>
      <th>781</th>
      <th>782</th>
      <th>783</th>
      <th>784</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>10000.0</td>
      <td>10000.0</td>
      <td>10000.0</td>
      <td>10000.0</td>
      <td>10000.0</td>
      <td>10000.0</td>
      <td>10000.0</td>
      <td>10000.0</td>
      <td>10000.0</td>
      <td>10000.0</td>
      <td>...</td>
      <td>10000.0</td>
      <td>10000.0</td>
      <td>10000.0</td>
      <td>10000.0</td>
      <td>10000.0</td>
      <td>10000.0</td>
      <td>10000.0</td>
      <td>10000.0</td>
      <td>10000.0</td>
      <td>10000.0</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>max</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.6</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 784 columns</p>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predict_test_input_fn = create_predict_input_fn(</span><br><span class="line">    test_examples, test_targets, batch_size=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">test_predictions = classifier.predict(input_fn=predict_test_input_fn)</span><br><span class="line">test_predictions = np.array([item[<span class="string">'class_ids'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> test_predictions])</span><br><span class="line">  </span><br><span class="line">accuracy = metrics.accuracy_score(test_targets, test_predictions)</span><br><span class="line">print(<span class="string">"Accuracy on test data: %0.2f"</span> % accuracy)</span><br></pre></td></tr></table></figure>
<pre><code>Accuracy on test data: 0.95
</code></pre><h2 id="可视化第一个隐藏层的权重。"><a href="#可视化第一个隐藏层的权重。" class="headerlink" title="可视化第一个隐藏层的权重。"></a>可视化第一个隐藏层的权重。</h2><p>我们来花几分钟时间看看模型的 <code>weights_</code> 属性，以深入探索我们的神经网络，并了解它学到了哪些规律。</p>
<p>模型的输入层有 <code>784</code> 个权重，对应于 <code>28×28</code> 像素输入图片。第一个隐藏层将有 <code>784×N</code> 个权重，其中 <code>N</code> 指的是该层中的节点数。我们可以将这些权重重新变回 <code>28×28</code> 像素的图片，具体方法是将 <code>N</code> 个 <code>1×784</code> 权重数组<em>变形</em>为 <code>N</code> 个 <code>28×28</code> 大小数组。</p>
<p>运行以下单元格，绘制权重曲线图。请注意，此单元格要求名为 “classifier” 的 <code>DNNClassifier</code> 已经过训练。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(classifier.get_variable_names())</span><br><span class="line"></span><br><span class="line">weights0 = classifier.get_variable_value(<span class="string">"dnn/hiddenlayer_0/kernel"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"weights0 shape:"</span>, weights0.shape)</span><br><span class="line"></span><br><span class="line">num_nodes = weights0.shape[<span class="number">1</span>]</span><br><span class="line">num_rows = int(math.ceil(num_nodes / <span class="number">10.0</span>))</span><br><span class="line">fig, axes = plt.subplots(num_rows, <span class="number">10</span>, figsize=(<span class="number">20</span>, <span class="number">2</span> * num_rows))</span><br><span class="line"><span class="keyword">for</span> coef, ax <span class="keyword">in</span> zip(weights0.T, axes.ravel()):</span><br><span class="line">    <span class="comment"># Weights in coef is reshaped from 1x784 to 28x28.</span></span><br><span class="line">    ax.matshow(coef.reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=plt.cm.pink)</span><br><span class="line">    ax.set_xticks(())</span><br><span class="line">    ax.set_yticks(())</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;dnn/hiddenlayer_0/bias&apos;, &apos;dnn/hiddenlayer_0/bias/t_0/Adagrad&apos;, &apos;dnn/hiddenlayer_0/kernel&apos;, &apos;dnn/hiddenlayer_0/kernel/t_0/Adagrad&apos;, &apos;dnn/hiddenlayer_1/bias&apos;, &apos;dnn/hiddenlayer_1/bias/t_0/Adagrad&apos;, &apos;dnn/hiddenlayer_1/kernel&apos;, &apos;dnn/hiddenlayer_1/kernel/t_0/Adagrad&apos;, &apos;dnn/logits/bias&apos;, &apos;dnn/logits/bias/t_0/Adagrad&apos;, &apos;dnn/logits/kernel&apos;, &apos;dnn/logits/kernel/t_0/Adagrad&apos;, &apos;global_step&apos;]
weights0 shape: (784, 100)
</code></pre><p><img src="TensorFlow_9_27_1.png" alt="png"></p>
<p> 神经网络的第一个隐藏层应该会对一些级别特别低的特征进行建模，因此可视化权重可能只显示一些模糊的区域，也可能只显示数字的某几个部分。此外，您可能还会看到一些基本上是噪点（这些噪点要么不收敛，要么被更高的层忽略）的神经元。</p>
<p>在迭代不同的次数后停止训练并查看效果，可能会发现有趣的结果。</p>
<p><strong>分别用 10、100 和 1000 步训练分类器。然后重新运行此可视化。</strong></p>
<p>您看到不同级别的收敛之间有哪些直观上的差异？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classifier = train_nn_classification_model(</span><br><span class="line">    learning_rate=<span class="number">0.05</span>,</span><br><span class="line">    steps=<span class="number">10</span>,</span><br><span class="line">    batch_size=<span class="number">30</span>,</span><br><span class="line">    hidden_units=[<span class="number">100</span>, <span class="number">100</span>],</span><br><span class="line">    training_examples=training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
LogLoss error (on validation data):
  period 00 : 29.18
  period 01 : 28.72
  period 02 : 21.72
  period 03 : 27.76
  period 04 : 21.00
  period 05 : 22.31
  period 06 : 17.28
  period 07 : 15.51
  period 08 : 20.03
  period 09 : 13.90
Model training finished.
Final accuracy (on validation data): 0.60
</code></pre><p><img src="TensorFlow_9_29_1.png" alt="png"></p>
<p><img src="TensorFlow_9_29_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(classifier.get_variable_names())</span><br><span class="line"></span><br><span class="line">weights0 = classifier.get_variable_value(<span class="string">"dnn/hiddenlayer_0/kernel"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"weights0 shape:"</span>, weights0.shape)</span><br><span class="line"></span><br><span class="line">num_nodes = weights0.shape[<span class="number">1</span>]</span><br><span class="line">num_rows = int(math.ceil(num_nodes / <span class="number">10.0</span>))</span><br><span class="line">fig, axes = plt.subplots(num_rows, <span class="number">10</span>, figsize=(<span class="number">20</span>, <span class="number">2</span> * num_rows))</span><br><span class="line"><span class="keyword">for</span> coef, ax <span class="keyword">in</span> zip(weights0.T, axes.ravel()):</span><br><span class="line">    <span class="comment"># Weights in coef is reshaped from 1x784 to 28x28.</span></span><br><span class="line">    ax.matshow(coef.reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=plt.cm.pink)</span><br><span class="line">    ax.set_xticks(())</span><br><span class="line">    ax.set_yticks(())</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;dnn/hiddenlayer_0/bias&apos;, &apos;dnn/hiddenlayer_0/bias/t_0/Adagrad&apos;, &apos;dnn/hiddenlayer_0/kernel&apos;, &apos;dnn/hiddenlayer_0/kernel/t_0/Adagrad&apos;, &apos;dnn/hiddenlayer_1/bias&apos;, &apos;dnn/hiddenlayer_1/bias/t_0/Adagrad&apos;, &apos;dnn/hiddenlayer_1/kernel&apos;, &apos;dnn/hiddenlayer_1/kernel/t_0/Adagrad&apos;, &apos;dnn/logits/bias&apos;, &apos;dnn/logits/bias/t_0/Adagrad&apos;, &apos;dnn/logits/kernel&apos;, &apos;dnn/logits/kernel/t_0/Adagrad&apos;, &apos;global_step&apos;]
weights0 shape: (784, 100)
</code></pre><p><img src="TensorFlow_9_30_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classifier = train_nn_classification_model(</span><br><span class="line">    learning_rate=<span class="number">0.05</span>,</span><br><span class="line">    steps=<span class="number">100</span>,</span><br><span class="line">    batch_size=<span class="number">30</span>,</span><br><span class="line">    hidden_units=[<span class="number">100</span>, <span class="number">100</span>],</span><br><span class="line">    training_examples=training_examples,</span><br><span class="line">    training_targets=training_targets,</span><br><span class="line">    validation_examples=validation_examples,</span><br><span class="line">    validation_targets=validation_targets)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
LogLoss error (on validation data):
  period 00 : 18.72
  period 01 : 10.71
  period 02 : 7.58
  period 03 : 8.70
  period 04 : 5.79
  period 05 : 5.21
  period 06 : 5.40
  period 07 : 6.05
  period 08 : 5.75
  period 09 : 3.95
Model training finished.
Final accuracy (on validation data): 0.89
</code></pre><p><img src="TensorFlow_9_31_1.png" alt="png"></p>
<p><img src="TensorFlow_9_31_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(classifier.get_variable_names())</span><br><span class="line"></span><br><span class="line">weights0 = classifier.get_variable_value(<span class="string">"dnn/hiddenlayer_0/kernel"</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"weights0 shape:"</span>, weights0.shape)</span><br><span class="line"></span><br><span class="line">num_nodes = weights0.shape[<span class="number">1</span>]</span><br><span class="line">num_rows = int(math.ceil(num_nodes / <span class="number">10.0</span>))</span><br><span class="line">fig, axes = plt.subplots(num_rows, <span class="number">10</span>, figsize=(<span class="number">20</span>, <span class="number">2</span> * num_rows))</span><br><span class="line"><span class="keyword">for</span> coef, ax <span class="keyword">in</span> zip(weights0.T, axes.ravel()):</span><br><span class="line">    <span class="comment"># Weights in coef is reshaped from 1x784 to 28x28.</span></span><br><span class="line">    ax.matshow(coef.reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=plt.cm.pink)</span><br><span class="line">    ax.set_xticks(())</span><br><span class="line">    ax.set_yticks(())</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;dnn/hiddenlayer_0/bias&apos;, &apos;dnn/hiddenlayer_0/bias/t_0/Adagrad&apos;, &apos;dnn/hiddenlayer_0/kernel&apos;, &apos;dnn/hiddenlayer_0/kernel/t_0/Adagrad&apos;, &apos;dnn/hiddenlayer_1/bias&apos;, &apos;dnn/hiddenlayer_1/bias/t_0/Adagrad&apos;, &apos;dnn/hiddenlayer_1/kernel&apos;, &apos;dnn/hiddenlayer_1/kernel/t_0/Adagrad&apos;, &apos;dnn/logits/bias&apos;, &apos;dnn/logits/bias/t_0/Adagrad&apos;, &apos;dnn/logits/kernel&apos;, &apos;dnn/logits/kernel/t_0/Adagrad&apos;, &apos;global_step&apos;]
weights0 shape: (784, 100)
</code></pre><p><img src="TensorFlow_9_32_1.png" alt="png"></p>
]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>Titanic Machine Learning from Disaster</title>
    <url>/2019/05/28/Titanic_Machine_Learning_from_Disaster.2/</url>
    <content><![CDATA[<p>Kaggle Competition 的练习</p>
<p><a href="https://www.kaggle.com/c/titanic" target="_blank" rel="noopener">泰坦尼克号：从灾难中学习机器</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据分析库</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 机器学习库</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC, LinearSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Perceptron</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">pd.options.display.max_rows = <span class="number">10</span>  <span class="comment"># 最大显示行数</span></span><br><span class="line">pd.options.display.float_format = <span class="string">'&#123;:.5f&#125;'</span>.format  <span class="comment"># 精确度 保留一位小数</span></span><br></pre></td></tr></table></figure>
<h1 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h1><p>首先加载浏览数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df = pd.read_csv(<span class="string">'/train.csv'</span>)</span><br><span class="line">test_df = pd.read_csv(<span class="string">'/test.csv'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.00000</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.25000</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.00000</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.28330</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.00000</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.92500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>female</td>
      <td>35.00000</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.10000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>male</td>
      <td>35.00000</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.05000</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_df.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>892</td>
      <td>3</td>
      <td>Kelly, Mr. James</td>
      <td>male</td>
      <td>34.50000</td>
      <td>0</td>
      <td>0</td>
      <td>330911</td>
      <td>7.82920</td>
      <td>NaN</td>
      <td>Q</td>
    </tr>
    <tr>
      <th>1</th>
      <td>893</td>
      <td>3</td>
      <td>Wilkes, Mrs. James (Ellen Needs)</td>
      <td>female</td>
      <td>47.00000</td>
      <td>1</td>
      <td>0</td>
      <td>363272</td>
      <td>7.00000</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>2</th>
      <td>894</td>
      <td>2</td>
      <td>Myles, Mr. Thomas Francis</td>
      <td>male</td>
      <td>62.00000</td>
      <td>0</td>
      <td>0</td>
      <td>240276</td>
      <td>9.68750</td>
      <td>NaN</td>
      <td>Q</td>
    </tr>
    <tr>
      <th>3</th>
      <td>895</td>
      <td>3</td>
      <td>Wirz, Mr. Albert</td>
      <td>male</td>
      <td>27.00000</td>
      <td>0</td>
      <td>0</td>
      <td>315154</td>
      <td>8.66250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>896</td>
      <td>3</td>
      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>
      <td>female</td>
      <td>22.00000</td>
      <td>1</td>
      <td>1</td>
      <td>3101298</td>
      <td>12.28750</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>891.00000</td>
      <td>891.00000</td>
      <td>891.00000</td>
      <td>714.00000</td>
      <td>891.00000</td>
      <td>891.00000</td>
      <td>891.00000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>446.00000</td>
      <td>0.38384</td>
      <td>2.30864</td>
      <td>29.69912</td>
      <td>0.52301</td>
      <td>0.38159</td>
      <td>32.20421</td>
    </tr>
    <tr>
      <th>std</th>
      <td>257.35384</td>
      <td>0.48659</td>
      <td>0.83607</td>
      <td>14.52650</td>
      <td>1.10274</td>
      <td>0.80606</td>
      <td>49.69343</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.42000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>223.50000</td>
      <td>0.00000</td>
      <td>2.00000</td>
      <td>20.12500</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>7.91040</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>446.00000</td>
      <td>0.00000</td>
      <td>3.00000</td>
      <td>28.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>14.45420</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>668.50000</td>
      <td>1.00000</td>
      <td>3.00000</td>
      <td>38.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>31.00000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>891.00000</td>
      <td>1.00000</td>
      <td>3.00000</td>
      <td>80.00000</td>
      <td>8.00000</td>
      <td>6.00000</td>
      <td>512.32920</td>
    </tr>
  </tbody>
</table>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_df.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Pclass</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>418.00000</td>
      <td>418.00000</td>
      <td>332.00000</td>
      <td>418.00000</td>
      <td>418.00000</td>
      <td>417.00000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>1100.50000</td>
      <td>2.26555</td>
      <td>30.27259</td>
      <td>0.44737</td>
      <td>0.39234</td>
      <td>35.62719</td>
    </tr>
    <tr>
      <th>std</th>
      <td>120.81046</td>
      <td>0.84184</td>
      <td>14.18121</td>
      <td>0.89676</td>
      <td>0.98143</td>
      <td>55.90758</td>
    </tr>
    <tr>
      <th>min</th>
      <td>892.00000</td>
      <td>1.00000</td>
      <td>0.17000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>996.25000</td>
      <td>1.00000</td>
      <td>21.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>7.89580</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>1100.50000</td>
      <td>3.00000</td>
      <td>27.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>14.45420</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1204.75000</td>
      <td>3.00000</td>
      <td>39.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>31.50000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1309.00000</td>
      <td>3.00000</td>
      <td>76.00000</td>
      <td>8.00000</td>
      <td>9.00000</td>
      <td>512.32920</td>
    </tr>
  </tbody>
</table>
</div>

<p>训练集有 PassengerId    Survived    Pclass    Name    Sex    Age    SibSp    Parch    Ticket    Fare    Cabin    Embarked 共十二列数据，测试集中没有的Survived就是我们要预测的值。</p>
<h2 id="检查数据"><a href="#检查数据" class="headerlink" title="检查数据"></a>检查数据</h2><p>检查数据类型和缺失情况。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.info()</span><br><span class="line">print(<span class="string">'_'</span>*<span class="number">40</span>)</span><br><span class="line">test_df.info()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
PassengerId    891 non-null int64
Survived       891 non-null int64
Pclass         891 non-null int64
Name           891 non-null object
Sex            891 non-null object
Age            714 non-null float64
SibSp          891 non-null int64
Parch          891 non-null int64
Ticket         891 non-null object
Fare           891 non-null float64
Cabin          204 non-null object
Embarked       889 non-null object
dtypes: float64(2), int64(5), object(5)
memory usage: 83.6+ KB
________________________________________
&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;
RangeIndex: 418 entries, 0 to 417
Data columns (total 11 columns):
PassengerId    418 non-null int64
Pclass         418 non-null int64
Name           418 non-null object
Sex            418 non-null object
Age            332 non-null float64
SibSp          418 non-null int64
Parch          418 non-null int64
Ticket         418 non-null object
Fare           417 non-null float64
Cabin          91 non-null object
Embarked       418 non-null object
dtypes: float64(2), int64(4), object(5)
memory usage: 36.0+ KB
</code></pre><p>观察发现，Age Cabin Embarked 存在缺失，并且数据类型既有数字也有字符串。测试集中 Fare 缺失了一个。</p>
<h2 id="观察特征的分布"><a href="#观察特征的分布" class="headerlink" title="观察特征的分布"></a>观察特征的分布</h2><pre><code>Name 是唯一的共 891
Sex 有两种，male 占 64.7%（top=male, freq/count=64.7%）
Ticket 不同的种类比较多
Cabin 有许多乘客在同一个 cabin
Embarked 有三种大多数是 S
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.describe(include=[<span class="string">"O"</span>])</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Name</th>
      <th>Sex</th>
      <th>Ticket</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>891</td>
      <td>891</td>
      <td>891</td>
      <td>204</td>
      <td>889</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>891</td>
      <td>2</td>
      <td>681</td>
      <td>147</td>
      <td>3</td>
    </tr>
    <tr>
      <th>top</th>
      <td>Renouf, Mr. Peter Henry</td>
      <td>male</td>
      <td>1601</td>
      <td>C23 C25 C27</td>
      <td>S</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>1</td>
      <td>577</td>
      <td>7</td>
      <td>4</td>
      <td>644</td>
    </tr>
  </tbody>
</table>
</div>

<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>在训练集中存在着缺失值和错误值，所以要筛选出有价值的特征，填充缺失的数据。</p>
<h2 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h2><p>首先删除没有价值或价值比较低的特征。</p>
<p>根据我们的假设和决定，我们先放弃 Cabin 和 Ticket 。</p>
<p>我们对训练和测试数据集执行相同的操作以保持其一致。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"Before"</span>, train_df.shape, test_df.shape)</span><br><span class="line"></span><br><span class="line">train_df = train_df.drop([<span class="string">"Ticket"</span>, <span class="string">"Cabin"</span>], axis=<span class="number">1</span>)</span><br><span class="line">test_df = test_df.drop([<span class="string">"Ticket"</span>, <span class="string">"Cabin"</span>], axis=<span class="number">1</span>)</span><br><span class="line">combine = [train_df, test_df]</span><br><span class="line"></span><br><span class="line"><span class="string">"After"</span>, train_df.shape, test_df.shape, combine[<span class="number">0</span>].shape, combine[<span class="number">1</span>].shape</span><br></pre></td></tr></table></figure>
<pre><code>Before (891, 12) (418, 11)





(&apos;After&apos;, (891, 10), (418, 9), (891, 10), (418, 9))
</code></pre><h2 id="Name"><a href="#Name" class="headerlink" title="Name"></a>Name</h2><p>首先观察到 Name 都是唯一的并且在 Name 中间存在称谓，提取出名字中间的称谓。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> dataset <span class="keyword">in</span> combine:</span><br><span class="line">    dataset[<span class="string">"Title"</span>] = dataset.Name.str.extract(<span class="string">' ([A-Za-z]+)\.'</span>, expand=<span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">pd.crosstab(train_df[<span class="string">"Title"</span>], train_df[<span class="string">"Sex"</span>])</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Sex</th>
      <th>female</th>
      <th>male</th>
    </tr>
    <tr>
      <th>Title</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Capt</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>Col</th>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>Countess</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Don</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>Dr</th>
      <td>1</td>
      <td>6</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>Mr</th>
      <td>0</td>
      <td>517</td>
    </tr>
    <tr>
      <th>Mrs</th>
      <td>125</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Ms</th>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>Rev</th>
      <td>0</td>
      <td>6</td>
    </tr>
    <tr>
      <th>Sir</th>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>17 rows × 2 columns</p>
</div>

<p>把称呼替换为更为常见的，不常见的定义为 Rare</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> dataset <span class="keyword">in</span> combine:</span><br><span class="line">    dataset[<span class="string">"Title"</span>] = dataset[<span class="string">"Title"</span>].replace([<span class="string">"Lady"</span>, <span class="string">"Countess"</span>, <span class="string">"Capt"</span>, <span class="string">"Col"</span>, \</span><br><span class="line">                                                 <span class="string">"Don"</span>, <span class="string">"Dr"</span>, <span class="string">"Major"</span>, <span class="string">"Rev"</span>, <span class="string">"Sir"</span>, \</span><br><span class="line">                                                 <span class="string">"Jonkheer"</span>, <span class="string">"Dona"</span>], <span class="string">"Rare"</span>)</span><br><span class="line">    dataset[<span class="string">"Title"</span>] = dataset[<span class="string">"Title"</span>].replace(<span class="string">"Mlle"</span>, <span class="string">"Miss"</span>)</span><br><span class="line">    dataset[<span class="string">"Title"</span>] = dataset[<span class="string">"Title"</span>].replace(<span class="string">"Ms"</span>, <span class="string">"Miss"</span>)</span><br><span class="line">    dataset[<span class="string">"Title"</span>] = dataset[<span class="string">"Title"</span>].replace(<span class="string">"Mme"</span>, <span class="string">"Mrs"</span>)</span><br><span class="line"></span><br><span class="line">train_df[[<span class="string">"Title"</span>, <span class="string">"Survived"</span>]].groupby([<span class="string">"Title"</span>], as_index=<span class="keyword">False</span>).mean()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Title</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Master</td>
      <td>0.57500</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Miss</td>
      <td>0.70270</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Mr</td>
      <td>0.15667</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Mrs</td>
      <td>0.79365</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Rare</td>
      <td>0.34783</td>
    </tr>
  </tbody>
</table>
</div>

<p>把 Titles 转换为数字</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">title_mapping = &#123;<span class="string">"Mr"</span>: <span class="number">1</span>, <span class="string">"Miss"</span>: <span class="number">2</span>, <span class="string">"Mrs"</span>: <span class="number">3</span>, <span class="string">"Master"</span>: <span class="number">4</span>, <span class="string">"Rare"</span>: <span class="number">5</span>&#125;</span><br><span class="line"><span class="keyword">for</span> dataset <span class="keyword">in</span> combine:</span><br><span class="line">    dataset[<span class="string">"Title"</span>] = dataset[<span class="string">"Title"</span>].map(title_mapping)</span><br><span class="line">    dataset[<span class="string">"Title"</span>] = dataset[<span class="string">"Title"</span>].fillna(<span class="number">0</span>) <span class="comment"># 缺失值补0</span></span><br><span class="line"></span><br><span class="line">train_df.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Embarked</th>
      <th>Title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.00000</td>
      <td>1</td>
      <td>0</td>
      <td>7.25000</td>
      <td>S</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.00000</td>
      <td>1</td>
      <td>0</td>
      <td>71.28330</td>
      <td>C</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.00000</td>
      <td>0</td>
      <td>0</td>
      <td>7.92500</td>
      <td>S</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>female</td>
      <td>35.00000</td>
      <td>1</td>
      <td>0</td>
      <td>53.10000</td>
      <td>S</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>male</td>
      <td>35.00000</td>
      <td>0</td>
      <td>0</td>
      <td>8.05000</td>
      <td>S</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<p>现在可以删除 Name 和 PassengerId了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df = train_df.drop([<span class="string">"Name"</span>, <span class="string">"PassengerId"</span>], axis=<span class="number">1</span>)</span><br><span class="line">test_df = test_df.drop([<span class="string">"Name"</span>], axis=<span class="number">1</span>)</span><br><span class="line">combine = [train_df, test_df]</span><br><span class="line">train_df.shape, test_df.shape</span><br></pre></td></tr></table></figure>
<pre><code>((891, 9), (418, 9))
</code></pre><h2 id="Sex"><a href="#Sex" class="headerlink" title="Sex"></a>Sex</h2><p>将包含字符串的特征转换为数值。这是大多数模型算法所必需的。这样做也将有助于我们实现功能完成目标。</p>
<p>让我们首先将 Sex 特征转换为一个新的 feature，其中 female = 1，male = 0。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> dataset <span class="keyword">in</span> combine:</span><br><span class="line">    dataset[<span class="string">"Sex"</span>] = dataset[<span class="string">"Sex"</span>].map(&#123;<span class="string">"female"</span>: <span class="number">1</span>, <span class="string">"male"</span>: <span class="number">0</span>&#125;).astype(int)</span><br><span class="line"></span><br><span class="line">train_df.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Embarked</th>
      <th>Title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>22.00000</td>
      <td>1</td>
      <td>0</td>
      <td>7.25000</td>
      <td>S</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>38.00000</td>
      <td>1</td>
      <td>0</td>
      <td>71.28330</td>
      <td>C</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>26.00000</td>
      <td>0</td>
      <td>0</td>
      <td>7.92500</td>
      <td>S</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>35.00000</td>
      <td>1</td>
      <td>0</td>
      <td>53.10000</td>
      <td>S</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>35.00000</td>
      <td>0</td>
      <td>0</td>
      <td>8.05000</td>
      <td>S</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="Age"><a href="#Age" class="headerlink" title="Age"></a>Age</h2><p>现在处理缺少值或空值的问题。<br>我们首先处理 Age 。</p>
<p>可以考虑三种方法来完成特征的填充。</p>
<ol>
<li><p>一种简单的方法是在均值的标准差之间生成随机数。</p>
</li>
<li><p>使用其他相关特征猜测缺失值。在这个例子中，我们注意到 Age，Gender 和 Pclass 之间的相关性。用猜年龄值中位值跨越套 Pclass 和性别特征组合年龄。因此，Pclass 的中位数年龄 = 1且性别 = 0，Pclass = 1 且性别 = 1，依此类推……</p>
</li>
<li><p>结合方法1和2因此，不是基于中位数来猜测年龄值，而是根据Pclass和Gender组合的集合使用均值和标准差之间的随机数。</p>
</li>
</ol>
<p>方法1和3将随机噪声引入我们的模型。多次执行的结果可能会有所不同。我们选择方法2。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grid = sns.FacetGrid(train_df, row=<span class="string">"Pclass"</span>, col=<span class="string">"Sex"</span>, size=<span class="number">2.2</span>, aspect=<span class="number">1.6</span>)</span><br><span class="line">grid.map(plt.hist, <span class="string">"Age"</span>, alpha=<span class="number">0.5</span>, bins=<span class="number">20</span>)</span><br><span class="line">grid.add_legend()</span><br></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python3.6/dist-packages/seaborn/axisgrid.py:230: UserWarning: The `size` paramter has been renamed to `height`; please update your code.
  warnings.warn(msg, UserWarning)





&lt;seaborn.axisgrid.FacetGrid at 0x7f3b28420a90&gt;
</code></pre><p><img src="Titanic_Machine_Learning_from_Disaster_30_2.png" alt="png"></p>
<p>首先准备一个空数组，猜测 Age 和 Pclass × Geender 有关系</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">guess_ages = np.zeros((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">guess_ages</span><br></pre></td></tr></table></figure>
<pre><code>array([[0., 0., 0.],
       [0., 0., 0.]])
</code></pre><p>现在我们迭代 Sex(0,1) 和 Pclass(1,2,3) 来猜测这六种组合的 Age。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> dataset <span class="keyword">in</span> combine:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">2</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">            guess_df = dataset[(dataset[<span class="string">"Sex"</span>] == i) &amp; (dataset[<span class="string">"Pclass"</span>] == j+<span class="number">1</span>)][<span class="string">"Age"</span>].dropna()</span><br><span class="line">            age_guess = guess_df.median()</span><br><span class="line">            guess_ages[i, j] = int(age_guess/<span class="number">0.5</span> + <span class="number">0.5</span>) * <span class="number">0.5</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">2</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">            dataset.loc[(dataset.Age.isnull()) &amp; (dataset.Sex == i) &amp; (dataset.Pclass == j+<span class="number">1</span>), <span class="string">"Age"</span>] = guess_ages[i, j]</span><br><span class="line">    </span><br><span class="line">    dataset[<span class="string">"Age"</span>] = dataset[<span class="string">"Age"</span>].astype(int)</span><br><span class="line">    </span><br><span class="line">train_df.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Embarked</th>
      <th>Title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>22</td>
      <td>1</td>
      <td>0</td>
      <td>7.25000</td>
      <td>S</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>38</td>
      <td>1</td>
      <td>0</td>
      <td>71.28330</td>
      <td>C</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>26</td>
      <td>0</td>
      <td>0</td>
      <td>7.92500</td>
      <td>S</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>35</td>
      <td>1</td>
      <td>0</td>
      <td>53.10000</td>
      <td>S</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>35</td>
      <td>0</td>
      <td>0</td>
      <td>8.05000</td>
      <td>S</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<p>让我们创建 AgeBand，并确定与存活的相关性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 把Age分为5箱</span></span><br><span class="line">train_df[<span class="string">"AgeBand"</span>] = pd.cut(train_df[<span class="string">"Age"</span>], <span class="number">5</span>)</span><br><span class="line">train_df[[<span class="string">"AgeBand"</span>, <span class="string">"Survived"</span>]].groupby([<span class="string">"AgeBand"</span>], as_index=<span class="keyword">False</span>).mean().sort_values(by=<span class="string">"AgeBand"</span>, ascending=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AgeBand</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>(-0.08, 16.0]</td>
      <td>0.55000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>(16.0, 32.0]</td>
      <td>0.33737</td>
    </tr>
    <tr>
      <th>2</th>
      <td>(32.0, 48.0]</td>
      <td>0.41204</td>
    </tr>
    <tr>
      <th>3</th>
      <td>(48.0, 64.0]</td>
      <td>0.43478</td>
    </tr>
    <tr>
      <th>4</th>
      <td>(64.0, 80.0]</td>
      <td>0.09091</td>
    </tr>
  </tbody>
</table>
</div>

<p>用这个频率来把 Age 分为五箱来替代原 Age。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> dataset <span class="keyword">in</span> combine:</span><br><span class="line">    dataset.loc[dataset[<span class="string">"Age"</span>] &lt;= <span class="number">16</span>, <span class="string">"Age"</span>] = <span class="number">0</span></span><br><span class="line">    dataset.loc[(dataset[<span class="string">"Age"</span>] &gt; <span class="number">16</span>) &amp; (dataset[<span class="string">"Age"</span>] &lt;= <span class="number">32</span>), <span class="string">"Age"</span>] = <span class="number">1</span></span><br><span class="line">    dataset.loc[(dataset[<span class="string">"Age"</span>] &gt; <span class="number">32</span>) &amp; (dataset[<span class="string">"Age"</span>] &lt;= <span class="number">48</span>), <span class="string">"Age"</span>] = <span class="number">2</span></span><br><span class="line">    dataset.loc[(dataset[<span class="string">"Age"</span>] &gt; <span class="number">48</span>) &amp; (dataset[<span class="string">"Age"</span>] &lt;= <span class="number">64</span>), <span class="string">"Age"</span>] = <span class="number">3</span></span><br><span class="line">    dataset.loc[dataset[<span class="string">"Age"</span>] &gt; <span class="number">64</span>, <span class="string">"Age"</span>] = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">train_df.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Embarked</th>
      <th>Title</th>
      <th>AgeBand</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>7.25000</td>
      <td>S</td>
      <td>1</td>
      <td>(16.0, 32.0]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>71.28330</td>
      <td>C</td>
      <td>3</td>
      <td>(32.0, 48.0]</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>7.92500</td>
      <td>S</td>
      <td>2</td>
      <td>(16.0, 32.0]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>53.10000</td>
      <td>S</td>
      <td>3</td>
      <td>(32.0, 48.0]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>8.05000</td>
      <td>S</td>
      <td>1</td>
      <td>(32.0, 48.0]</td>
    </tr>
  </tbody>
</table>
</div>

<p>移除 AgeBand feature</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df = train_df.drop([<span class="string">"AgeBand"</span>], axis=<span class="number">1</span>)</span><br><span class="line">combine = [train_df, test_df]</span><br><span class="line">train_df.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Embarked</th>
      <th>Title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>7.25000</td>
      <td>S</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>71.28330</td>
      <td>C</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>7.92500</td>
      <td>S</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>53.10000</td>
      <td>S</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>8.05000</td>
      <td>S</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="Embarked"><a href="#Embarked" class="headerlink" title="Embarked"></a>Embarked</h2><p>Embarked 特征取值为 S、Q、C。我们的训练数据集有两个缺失的值。用最常见的 Embarked 来填充（众数填充）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">freq_port = train_df.Embarked.dropna().mode()[<span class="number">0</span>] <span class="comment"># 返回出现次数最多的值（众数）</span></span><br><span class="line">freq_port</span><br></pre></td></tr></table></figure>
<pre><code>&apos;S&apos;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> dataset <span class="keyword">in</span> combine:</span><br><span class="line">    dataset[<span class="string">"Embarked"</span>] = dataset[<span class="string">"Embarked"</span>].fillna(freq_port)</span><br><span class="line">    </span><br><span class="line">train_df[[<span class="string">"Embarked"</span>, <span class="string">"Survived"</span>]].groupby([<span class="string">"Embarked"</span>], as_index=<span class="keyword">False</span>).mean().sort_values(by=<span class="string">"Survived"</span>, ascending=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Embarked</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>C</td>
      <td>0.55357</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Q</td>
      <td>0.38961</td>
    </tr>
    <tr>
      <th>2</th>
      <td>S</td>
      <td>0.33901</td>
    </tr>
  </tbody>
</table>
</div>

<p>现在，我们可以把 Embarked 转换为一个新的数字序列。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> dataset <span class="keyword">in</span> combine:</span><br><span class="line">    dataset[<span class="string">"Embarked"</span>] = dataset[<span class="string">"Embarked"</span>].map( &#123;<span class="string">"S"</span>: <span class="number">0</span>, <span class="string">"C"</span>: <span class="number">1</span>, <span class="string">"Q"</span>: <span class="number">2</span>&#125; ).astype(int)</span><br><span class="line"></span><br><span class="line">train_df.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Embarked</th>
      <th>Title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>7.25000</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>71.28330</td>
      <td>1</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>7.92500</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>53.10000</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>8.05000</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="Fare"><a href="#Fare" class="headerlink" title="Fare"></a>Fare</h2><p>现在，我们可以使用 df.fillna 填充 test dataset 中 Fare 的单个缺失值，使用 median （中位数）来填充。我们只需要一行代码就可以做到这一点。</p>
<p>注意，我们并没有创建一个中间的新特性，也没有对相关性进行任何进一步的分析来猜测缺失的特性，因为我们只是替换了一个值。以达到了模型算法对非空值运算的要求。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_df[<span class="string">"Fare"</span>].fillna(test_df[<span class="string">"Fare"</span>].dropna().median(), inplace=<span class="keyword">True</span>)</span><br><span class="line">test_df.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Embarked</th>
      <th>Title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>892</td>
      <td>3</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>7.82920</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>893</td>
      <td>3</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>7.00000</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>894</td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>9.68750</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>895</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>8.66250</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>896</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>12.28750</td>
      <td>0</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>

<p>创建一个 FareBand</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df[<span class="string">"FareBand"</span>] = pd.qcut(train_df[<span class="string">"Fare"</span>], <span class="number">4</span>)</span><br><span class="line">train_df[[<span class="string">"FareBand"</span>, <span class="string">"Survived"</span>]].groupby([<span class="string">"FareBand"</span>], as_index=<span class="keyword">False</span>).mean().sort_values(by=<span class="string">"FareBand"</span>, ascending=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>FareBand</th>
      <th>Survived</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>(-0.001, 7.91]</td>
      <td>0.19731</td>
    </tr>
    <tr>
      <th>1</th>
      <td>(7.91, 14.454]</td>
      <td>0.30357</td>
    </tr>
    <tr>
      <th>2</th>
      <td>(14.454, 31.0]</td>
      <td>0.45495</td>
    </tr>
    <tr>
      <th>3</th>
      <td>(31.0, 512.329]</td>
      <td>0.58108</td>
    </tr>
  </tbody>
</table>
</div>

<p>基于 FareBand 将 Fare 转换为序列值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> dataset <span class="keyword">in</span> combine:</span><br><span class="line">    dataset.loc[dataset[<span class="string">"Fare"</span>] &lt;= <span class="number">7.91</span>, <span class="string">"Fare"</span>] = <span class="number">0</span></span><br><span class="line">    dataset.loc[(dataset[<span class="string">"Fare"</span>] &gt; <span class="number">7.91</span>) &amp; (dataset[<span class="string">"Fare"</span>] &lt;= <span class="number">14.454</span>), <span class="string">"Fare"</span>] = <span class="number">1</span></span><br><span class="line">    dataset.loc[(dataset[<span class="string">"Fare"</span>] &gt; <span class="number">14.454</span>) &amp; (dataset[<span class="string">"Fare"</span>] &lt;= <span class="number">31</span>), <span class="string">"Fare"</span>] = <span class="number">2</span></span><br><span class="line">    dataset.loc[dataset[<span class="string">"Fare"</span>] &gt; <span class="number">31</span>, <span class="string">"Fare"</span>] = <span class="number">3</span></span><br><span class="line">    dataset[<span class="string">"Fare"</span>] = dataset[<span class="string">"Fare"</span>].astype(int)</span><br><span class="line"></span><br><span class="line">train_df = train_df.drop([<span class="string">"FareBand"</span>], axis=<span class="number">1</span>)</span><br><span class="line">combine = [train_df, test_df]</span><br><span class="line">train_df.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Embarked</th>
      <th>Title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<h1 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train = train_df.drop(<span class="string">"Survived"</span>, axis=<span class="number">1</span>)</span><br><span class="line">Y_train = train_df[<span class="string">"Survived"</span>]</span><br><span class="line">X_test = test_df.drop(<span class="string">"PassengerId"</span>, axis=<span class="number">1</span>).copy()</span><br><span class="line">X_train.shape, Y_train.shape, X_test.shape</span><br></pre></td></tr></table></figure>
<pre><code>((891, 8), (891,), (418, 8))
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train.describe()</span><br><span class="line">X_test</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Embarked</th>
      <th>Title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>413</th>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>414</th>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>5</td>
    </tr>
    <tr>
      <th>415</th>
      <td>3</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>416</th>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>417</th>
      <td>3</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
<p>418 rows × 8 columns</p>
</div>

<p>现在，我们准备训练一个模型并预测。有60多种预测建模算法可供选择。我们必须了解问题的类型和解决方案的需求，以便将范围缩小到我们可以评估的几个选定的模型。</p>
<p>我们的问题是一个分类和回归问题。我们想要确定输出(Survived or not)与其他变量或特性(Gender, Age, Port……)之间的关系。这是一个监督学习。有了这两个标准 —— 监督学习加上分类和回归，我们可以把模型的选择范围缩小到几个。</p>
<p>这些包括:</p>
<ul>
<li>逻辑回归</li>
<li>KNN或k近邻</li>
<li>支持向量机</li>
<li>朴素贝叶斯分类器</li>
<li>决策树</li>
<li>随机森林</li>
<li>感知器</li>
<li>随机梯度下降</li>
<li>RVM 相关向量机</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 逻辑回归</span></span><br><span class="line"></span><br><span class="line">logreg = LogisticRegression()</span><br><span class="line">logreg.fit(X_train, Y_train)</span><br><span class="line">Y_pred = logreg.predict(X_test)</span><br><span class="line">acc_log = round(logreg.score(X_train, Y_train) * <span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">acc_log</span><br></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)





81.37
</code></pre><p>我们可以使用逻辑回归来验证和检查我们的预测。可以通过计算决策函数中特征的系数来实现。</p>
<p>正系数增加了响应的 log-odds (从而增加了概率)，负系数减少了响应的 log-odds (从而减少了概率)。<br>性别是正系数最高的，说明随着性别值的增加(男性: 0 女性:1)，存活的概率增加最多。<br>相反，随着 Pclass 的增加，生存概率下降。<br>这样，Age 是一个很好的人工特征，因为它与存活有第二高的负相关。<br>Title 也是第二高的正相关。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">coeff_df = pd.DataFrame(train_df.columns.delete(<span class="number">0</span>))</span><br><span class="line">coeff_df.columns = [<span class="string">'Feature'</span>]</span><br><span class="line">coeff_df[<span class="string">"Correlation"</span>] = pd.Series(logreg.coef_[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">coeff_df.sort_values(by=<span class="string">'Correlation'</span>, ascending=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Feature</th>
      <th>Correlation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Sex</td>
      <td>2.19360</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Title</td>
      <td>0.49431</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Fare</td>
      <td>0.31180</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Embarked</td>
      <td>0.24051</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Parch</td>
      <td>-0.25322</td>
    </tr>
    <tr>
      <th>3</th>
      <td>SibSp</td>
      <td>-0.50649</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Age</td>
      <td>-0.65716</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Pclass</td>
      <td>-0.91037</td>
    </tr>
  </tbody>
</table>
</div>

<p>其次，我们使用支持向量机建模，这是监督学习的算法，用于数据分类和回归分析。给定一组训练样本，每个样本都被标记为属于两个类别中的一个或另一个类别，SVM训练算法建立一个模型，将新的测试样本分配给其中一个类别或另一个类别，使其成为一个非概率二元线性分类器。</p>
<p>该模型生成的评分高于逻辑回归模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Support Vector Machines</span></span><br><span class="line"></span><br><span class="line">svm = SVC()</span><br><span class="line">svm.fit(X_train, Y_train)</span><br><span class="line">Y_pred = svm.predict(X_test)</span><br><span class="line">acc_svm = round(svm.score(X_train, Y_train) * <span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">acc_svm</span><br></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from &apos;auto&apos; to &apos;scale&apos; in version 0.22 to account better for unscaled features. Set gamma explicitly to &apos;auto&apos; or &apos;scale&apos; to avoid this warning.
  &quot;avoid this warning.&quot;, FutureWarning)





83.73
</code></pre><p>在模式识别中，k近邻算法(简称k-NN)是一种用于分类和回归的非参数算法。一个样本由它的邻居的多数投票来分类，这个样本被分配到它的k个最近邻居中最常见的类(k是一个正整数，通常很小)。如果 k = 1，那么对象就被简单地分配给那个最近邻居的类。</p>
<p>KNN得分优于logistic回归，但低于SVM。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># KNN</span></span><br><span class="line"></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors = <span class="number">3</span>)</span><br><span class="line">knn.fit(X_train, Y_train)</span><br><span class="line">Y_pred = knn.predict(X_test)</span><br><span class="line">acc_knn = round(knn.score(X_train, Y_train) * <span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">acc_knn</span><br></pre></td></tr></table></figure>
<pre><code>84.4
</code></pre><p>在机器学习中，朴素贝叶斯分类器是一组基于贝叶斯定理的简单概率分类器，特征之间具有强(朴素)独立性假设。朴素贝叶斯分类器是高度可伸缩的，在一个学习问题中需要一些参数在变量(特征)的数量上是线性的。</p>
<p>模型生成的置信度评分是目前评价模型中最低的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Gaussian Naive Bayes</span></span><br><span class="line"></span><br><span class="line">gaussian = GaussianNB()</span><br><span class="line">gaussian.fit(X_train, Y_train)</span><br><span class="line">Y_pred = gaussian.predict(X_test)</span><br><span class="line">acc_gaussian = round(gaussian.score(X_train, Y_train) * <span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">acc_gaussian</span><br></pre></td></tr></table></figure>
<pre><code>80.13
</code></pre><p>感知器是一种二进制分类器的监督学习算法(函数，它可以决定一个输入是否属于某个特定的类，由一个数字向量表示)。它是一种线性分类器，即基于一组权值与特征向量相结合的线性预测函数进行预测的分类算法。该算法允许在线学习，每次处理训练集中的一个元素。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Perceptron</span></span><br><span class="line"></span><br><span class="line">perceptron = Perceptron()</span><br><span class="line">perceptron.fit(X_train, Y_train)</span><br><span class="line">Y_pred = perceptron.predict(X_test)</span><br><span class="line">acc_perceptron = round(perceptron.score(X_train, Y_train) * <span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">acc_perceptron</span><br></pre></td></tr></table></figure>
<pre><code>80.58
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Linear SVC</span></span><br><span class="line"></span><br><span class="line">linear_svc = LinearSVC()</span><br><span class="line">linear_svc.fit(X_train, Y_train)</span><br><span class="line">Y_pred = linear_svc.predict(X_test)</span><br><span class="line">acc_linear_svc = round(linear_svc.score(X_train, Y_train) * <span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">acc_linear_svc</span><br></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  &quot;the number of iterations.&quot;, ConvergenceWarning)





81.14
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Stochastic Gradient Descent</span></span><br><span class="line"></span><br><span class="line">sgd = SGDClassifier()</span><br><span class="line">sgd.fit(X_train, Y_train)</span><br><span class="line">Y_pred = sgd.predict(X_test)</span><br><span class="line">acc_sgd = round(sgd.score(X_train, Y_train) * <span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">acc_sgd</span><br></pre></td></tr></table></figure>
<pre><code>80.92
</code></pre><p>使用决策树作为预测模型，是一种十分常用的分类方法。他是一种监管学习，所谓监管学习就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。</p>
<p>目前模型中最高的评分。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Decision Tree</span></span><br><span class="line"></span><br><span class="line">decision_tree = DecisionTreeClassifier()</span><br><span class="line">decision_tree.fit(X_train, Y_train)</span><br><span class="line">Y_pred = decision_tree.predict(X_test)</span><br><span class="line">acc_decision_tree = round(decision_tree.score(X_train, Y_train) * <span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">acc_decision_tree</span><br></pre></td></tr></table></figure>
<pre><code>89.0
</code></pre><p>下一个模型随机森林是最受欢迎的之一。一个包含多个决策树的分类器， 并且其输出的类别是由个别树输出的类别的众数而定。</p>
<p>模型目前评分中最高的。我们决定使用这个模型的输出(Y_pred)提交结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Random Forest</span></span><br><span class="line"></span><br><span class="line">random_forest = RandomForestClassifier(n_estimators=<span class="number">100</span>)</span><br><span class="line">random_forest.fit(X_train, Y_train)</span><br><span class="line">Y_pred = random_forest.predict(X_test)</span><br><span class="line">random_forest.score(X_train, Y_train)</span><br><span class="line">acc_random_forest = round(random_forest.score(X_train, Y_train) * <span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">acc_random_forest</span><br></pre></td></tr></table></figure>
<pre><code>89.0
</code></pre><h2 id="模型评价"><a href="#模型评价" class="headerlink" title="模型评价"></a>模型评价</h2><p>现在我们可以对所有模型的评估进行排序，以选择最适合我们问题的模型。在决策树和随机森林得分相同的情况下，我们选择随机森林来纠正决策树对训练集过度拟合的习惯。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">models = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">"Model"</span>: [<span class="string">"Support Vector Machines"</span>, <span class="string">"KNN"</span>, <span class="string">"Logistic Regression"</span>,</span><br><span class="line">              <span class="string">"Random Forest"</span>, <span class="string">"Naive Bayes"</span>, <span class="string">"Percep tron"</span>,</span><br><span class="line">              <span class="string">"Stochastic Gradient Decent"</span>, <span class="string">"Linear SVC"</span>, <span class="string">"Decision Tree"</span>],</span><br><span class="line">    <span class="string">"Score"</span>: [acc_svm, acc_knn, acc_log, acc_random_forest, acc_gaussian,</span><br><span class="line">              acc_perceptron, acc_sgd, acc_linear_svc, acc_decision_tree]</span><br><span class="line">    &#125;)</span><br><span class="line">models.sort_values(by=<span class="string">"Score"</span>, ascending=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Model</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3</th>
      <td>Random Forest</td>
      <td>89.00000</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Decision Tree</td>
      <td>89.00000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>KNN</td>
      <td>84.40000</td>
    </tr>
    <tr>
      <th>0</th>
      <td>Support Vector Machines</td>
      <td>83.73000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Logistic Regression</td>
      <td>81.37000</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Linear SVC</td>
      <td>81.14000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Stochastic Gradient Decent</td>
      <td>80.92000</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Percep tron</td>
      <td>80.58000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Naive Bayes</td>
      <td>80.13000</td>
    </tr>
  </tbody>
</table>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存结果</span></span><br><span class="line"></span><br><span class="line">submission = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">"PassengerId"</span>: test_df[<span class="string">"PassengerId"</span>],</span><br><span class="line">    <span class="string">"Survived"</span>: Y_pred</span><br><span class="line">    &#125;)</span><br><span class="line">submission.to_csv(<span class="string">"/submission.csv"</span>, index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Kaggle</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>docker + TensorFlow2.0 + jupyter lab</title>
    <url>/2020/08/20/docker%20+%20TensorFlow2.0%20+%20jupyter%20lab/</url>
    <content><![CDATA[<p>服务器为 Ubuntu</p>
<p>首先当然需要有<a href="https://docs.docker.com/get-docker/" target="_blank" rel="noopener">Docker</a></p>
<h1 id="安装docker-tensorflow-jupyter"><a href="#安装docker-tensorflow-jupyter" class="headerlink" title="安装docker tensorflow jupyter"></a>安装docker tensorflow jupyter</h1><p><a href="https://tensorflow.google.cn/install/docker" target="_blank" rel="noopener">参考tensorflow安装教程</a></p>
<p>输入一下命令：</p>
<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">$ sudo docker run -it -<span class="keyword">p</span> <span class="number">8888</span>:<span class="number">8888</span> tensorflow/tensorflo<span class="variable">w:nightly</span>-<span class="keyword">py3</span>-jupyter</span><br><span class="line">Unable <span class="keyword">to</span> <span class="keyword">find</span> image <span class="string">'tensorflow/tensorflow:nightly-py3-jupyter'</span> locally</span><br><span class="line">nightly-<span class="keyword">py3</span>-jupyter: Pulling from tensorflow/tensorflow</span><br><span class="line"><span class="number">5</span>bed26d33875: Pull <span class="built_in">complete</span></span><br><span class="line">f11b29a9c730: Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">930</span>bda195c84: Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">78</span>bf9a5ad49e: Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">84227541</span>b6b<span class="variable">b:</span> Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">4</span>f31c9672ae8: Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">3</span>ab00ff69975: Pull <span class="built_in">complete</span></span><br><span class="line">ba1f9a4960a4: Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">1</span>e2e9ebd327e: Extracting [==================================================&gt;]  <span class="number">590.2</span>MB/<span class="number">590.2</span>MB</span><br><span class="line"><span class="number">1</span>e2e9ebd327e: Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">5</span>dbeace46811: Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">236</span>b4193378<span class="variable">a:</span> Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">94</span>c8012aaf41: Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">8978</span>cb54431f: Pull <span class="built_in">complete</span></span><br><span class="line">a32047d10082: Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">4</span>bfab90cc021: Pull <span class="built_in">complete</span></span><br><span class="line">db51178e67ae: Pull <span class="built_in">complete</span></span><br><span class="line">c582e0693d6e: Pull <span class="built_in">complete</span></span><br><span class="line">d9275e9db168: Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">48835526</span>d3e2: Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">5287</span>adc8a8f2: Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">525</span>c81ec54d<span class="variable">b:</span> Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">0161</span>c3804581: Pull <span class="built_in">complete</span></span><br><span class="line">ebfa1948cb3e: Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">933</span>aa5af1920: Pull <span class="built_in">complete</span></span><br><span class="line">a600cede3739: Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">1</span>d8aeacd0c4f: Pull <span class="built_in">complete</span></span><br><span class="line"><span class="number">683</span>a7567eee<span class="variable">a:</span> Pull <span class="built_in">complete</span></span><br><span class="line">Diges<span class="variable">t:</span> <span class="built_in">sha256</span>:<span class="number">0</span>b59d7826a07049f013c171bb96c26c0d4a6222f856e6520a6cd70d7be5ecdec</span><br><span class="line">Statu<span class="variable">s:</span> Downloaded newer image <span class="keyword">for</span> tensorflow/tensorflo<span class="variable">w:nightly</span>-<span class="keyword">py3</span>-jupyter</span><br><span class="line"></span><br><span class="line">[I <span class="number">07</span>:<span class="number">24</span>:<span class="number">55.849</span> NotebookApp] Writing notebook server cookie secret <span class="keyword">to</span> /root/.local/share/jupyter/<span class="keyword">runtime</span>/notebook_cookie_secret</span><br><span class="line">jupyter_http_over_ws extension initialized. Listening <span class="keyword">on</span> /http_over_websocket</span><br><span class="line">[I <span class="number">07</span>:<span class="number">24</span>:<span class="number">56.134</span> NotebookApp] Serving notebooks from local directory: /<span class="keyword">tf</span></span><br><span class="line">[I <span class="number">07</span>:<span class="number">24</span>:<span class="number">56.134</span> NotebookApp] The Jupyter Notebook <span class="keyword">is</span> running <span class="keyword">a</span><span class="variable">t:</span></span><br><span class="line">[I <span class="number">07</span>:<span class="number">24</span>:<span class="number">56.134</span> NotebookApp] http://<span class="number">1</span>d6d334faf94:<span class="number">8888</span>/?token=ec98c98ec99956afa3a078ce9ae5e34f11a88b6bf92f0ac8</span><br><span class="line">[I <span class="number">07</span>:<span class="number">24</span>:<span class="number">56.134</span> NotebookApp]  <span class="built_in">or</span> http://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">8888</span>/?token=ec98c98ec99956afa3a078ce9ae5e34f11a88b6bf92f0ac8</span><br><span class="line">[I <span class="number">07</span>:<span class="number">24</span>:<span class="number">56.134</span> NotebookApp] Use Control-C <span class="keyword">to</span> <span class="keyword">stop</span> this server <span class="built_in">and</span> shut down <span class="keyword">all</span> kernels (twice <span class="keyword">to</span> skip confirmation).</span><br><span class="line">[C <span class="number">07</span>:<span class="number">24</span>:<span class="number">56.138</span> NotebookApp]</span><br><span class="line"></span><br><span class="line">    To access the notebook, <span class="keyword">open</span> this <span class="keyword">file</span> in <span class="keyword">a</span> browser:</span><br><span class="line">        <span class="keyword">file</span>:///root/.local/share/jupyter/<span class="keyword">runtime</span>/nbserver-<span class="number">1</span>-<span class="keyword">open</span>.html</span><br><span class="line">    Or <span class="keyword">copy</span> <span class="built_in">and</span> paste one of these URL<span class="variable">s:</span></span><br><span class="line">        http://<span class="number">1</span>d6d334faf94:<span class="number">8888</span>/?token=ec98c98ec99956afa3a078ce9ae5e34f11a88b6bf92f0ac8</span><br><span class="line">     <span class="built_in">or</span> http://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">8888</span>/?token=ec98c98ec99956afa3a078ce9ae5e34f11a88b6bf92f0ac8</span><br></pre></td></tr></table></figure>
<p>等待一会</p>
<p>安装并成功启动了jupyter notebook，在浏览器中打开提示的网址：<a href="http://127.0.0.1:8888/?token=.." target="_blank" rel="noopener">http://127.0.0.1:8888/?token=..</a>.</p>
<p>按住 ctrl + c 退出</p>
<p><strong>接下来是重点</strong></p>
<h1 id="安装jupyter-lab"><a href="#安装jupyter-lab" class="headerlink" title="安装jupyter lab"></a>安装jupyter lab</h1><p>查看docker镜像</p>
<figure class="highlight crmsh"><table><tr><td class="code"><pre><span class="line">$ sudo docker images</span><br><span class="line">REPOSITORY                                         <span class="keyword">TAG</span>                   <span class="title">IMAGE</span> ID            CREATED             SIZE</span><br><span class="line">tensorflow/tensorflow                              nightly-py3-jupyter   <span class="number">37</span>b1af999efc        <span class="number">4</span> months ago        <span class="number">2.42</span>GB</span><br></pre></td></tr></table></figure>
<p>先删除刚启动的容器</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sudo docker <span class="keyword">stop</span> <span class="number">37</span>b1af999efc  <span class="comment">#关闭</span></span><br><span class="line">sudo docker <span class="keyword">container</span> rm <span class="number">37</span>b1af999efc  <span class="comment">#删除</span></span><br></pre></td></tr></table></figure>
<p>在本机上新建一个文件夹，和一个json文件</p>
<figure class="highlight gams"><table><tr><td class="code"><pre><span class="line"><span class="symbol">$</span> mkdir jupyter_config</span><br><span class="line"><span class="symbol">$</span> cd jupyter_config/</span><br><span class="line">~/jupyter_config<span class="symbol">$</span> vim config.json</span><br></pre></td></tr></table></figure>
<p>这个文件内容是jupyter-lab的配置信息</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"NotebookApp"</span>:&#123;</span><br><span class="line">            <span class="attr">"ip"</span>:<span class="string">"*"</span>,</span><br><span class="line">            <span class="attr">"port"</span>:<span class="number">8888</span>,</span><br><span class="line">            <span class="attr">"password"</span>:<span class="string">""</span>,</span><br><span class="line">            <span class="attr">"open_browser"</span>:<span class="literal">false</span>,</span><br><span class="line">            <span class="attr">"token"</span>:<span class="string">""</span>,</span><br><span class="line">            <span class="attr">"allow_root"</span>:<span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在docker中创建挂载jupyter_config文件夹的容器</p>
<figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">$ sudo docker run -itd -<span class="keyword">p</span> <span class="number">8888</span>:<span class="number">8888</span> -v ~/【主机的目录】:/home/user【容器的目录】 tensorflow/tenso</span><br><span class="line">rflo<span class="variable">w:nightly</span>-<span class="keyword">py3</span>-jupyter bash</span><br><span class="line">$ sudo docker <span class="keyword">ps</span></span><br><span class="line">CONTAINER ID        IMAGE                                       COMMAND                  CREATED              STATUS</span><br><span class="line">          PORTS                         NAMES</span><br><span class="line">e72acb760f91        tensorflow/tensorflo<span class="variable">w:nightly</span>-<span class="keyword">py3</span>-jupyter   <span class="string">"bash"</span>                   About <span class="keyword">a</span> minute ago   Up About <span class="keyword">a</span></span><br></pre></td></tr></table></figure>
<p>进入该容器</p>
<figure class="highlight maxima"><table><tr><td class="code"><pre><span class="line">$ sudo docker exec -it e72acb760f91 bash</span><br><span class="line"></span><br><span class="line">________                               _______________</span><br><span class="line">___  <span class="symbol">__</span>/__________________________________  ____/<span class="symbol">__</span>  /________      <span class="symbol">__</span></span><br><span class="line"><span class="symbol">__</span>  /  <span class="symbol">_</span>  <span class="symbol">_</span> \<span class="symbol">_</span>  <span class="symbol">__</span> \<span class="symbol">_</span>  ___/  <span class="symbol">__</span> \<span class="symbol">_</span>  ___/<span class="symbol">_</span>  /<span class="symbol">_</span>   <span class="symbol">__</span>  /<span class="symbol">_</span>  <span class="symbol">__</span> \<span class="symbol">_</span> | /| / /</span><br><span class="line"><span class="symbol">_</span>  /   /  <span class="symbol">__</span>/  / / /(<span class="symbol">__</span>  )/ /<span class="symbol">_</span>/ /  /   <span class="symbol">_</span>  <span class="symbol">__</span>/   <span class="symbol">_</span>  / / /<span class="symbol">_</span>/ /<span class="symbol">_</span> |/ |/ /</span><br><span class="line">/<span class="symbol">_</span>/    \___//<span class="symbol">_</span>/ /<span class="symbol">_</span>//____/ \____//<span class="symbol">_</span>/    /<span class="symbol">_</span>/      /<span class="symbol">_</span>/  \____/____/|<span class="symbol">__</span>/</span><br></pre></td></tr></table></figure>
<p>在容器中安装jupyter lab</p>
<figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line">root<span class="variable">@e72acb760f91</span><span class="symbol">:/tf</span><span class="comment"># pip3 install jupyterlab</span></span><br></pre></td></tr></table></figure>
<p>切换到我们挂载的jupyter_config文件夹去</p>
<figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line">root<span class="variable">@e72acb760f91</span><span class="symbol">:/tf</span><span class="comment"># cd /home/user/</span></span><br><span class="line">root<span class="variable">@e72acb760f91</span><span class="symbol">:/home/user</span><span class="comment"># ls</span></span><br><span class="line">jupyter_config</span><br></pre></td></tr></table></figure>
<p>启动jupyter lab</p>
<figure class="highlight oxygene"><table><tr><td class="code"><pre><span class="line"># jupyter-lab --config ./jupyter_config/config.json</span><br><span class="line">[I <span class="number">08</span>:<span class="number">44</span>:<span class="number">59.650</span> LabApp] Writing notebook server cookie secret <span class="keyword">to</span> /root/.local/share/jupyter/runtime/notebook_cookie_secret</span><br><span class="line">[W <span class="number">08</span>:<span class="number">44</span>:<span class="number">59.906</span> LabApp] WARNING: The notebook server <span class="keyword">is</span> listening <span class="keyword">on</span> all IP addresses <span class="keyword">and</span> <span class="keyword">not</span> <span class="keyword">using</span> encryption. This <span class="keyword">is</span> <span class="keyword">not</span> recommended.</span><br><span class="line">[W <span class="number">08</span>:<span class="number">44</span>:<span class="number">59.906</span> LabApp] WARNING: The notebook server <span class="keyword">is</span> listening <span class="keyword">on</span> all IP addresses <span class="keyword">and</span> <span class="keyword">not</span> <span class="keyword">using</span> authentication. This <span class="keyword">is</span> highly insecure <span class="keyword">and</span> <span class="keyword">not</span> recommended.</span><br><span class="line">jupyter_http_over_ws <span class="keyword">extension</span> initialized. Listening <span class="keyword">on</span> /http_over_websocket</span><br><span class="line">[I <span class="number">08</span>:<span class="number">44</span>:<span class="number">59.917</span> LabApp] JupyterLab <span class="keyword">extension</span> loaded <span class="keyword">from</span> /usr/local/lib/python3.<span class="number">6</span>/dist-packages/jupyterlab</span><br><span class="line">[I <span class="number">08</span>:<span class="number">44</span>:<span class="number">59.917</span> LabApp] JupyterLab application directory <span class="keyword">is</span> /usr/local/share/jupyter/lab</span><br><span class="line">[I <span class="number">08</span>:<span class="number">44</span>:<span class="number">59.920</span> LabApp] Serving notebooks <span class="keyword">from</span> local directory: /home/xujie</span><br><span class="line">[I <span class="number">08</span>:<span class="number">44</span>:<span class="number">59.920</span> LabApp] The Jupyter Notebook <span class="keyword">is</span> running at:</span><br><span class="line">[I <span class="number">08</span>:<span class="number">44</span>:<span class="number">59.920</span> LabApp] http:<span class="comment">//e72acb760f91:8888/</span></span><br><span class="line">[I <span class="number">08</span>:<span class="number">44</span>:<span class="number">59.920</span> LabApp] Use Control-C <span class="keyword">to</span> stop this server <span class="keyword">and</span> shut down all kernels (twice <span class="keyword">to</span> <span class="keyword">skip</span> confirmation).</span><br></pre></td></tr></table></figure>
<p>用浏览器打开</p>
<figure class="highlight dts"><table><tr><td class="code"><pre><span class="line"><span class="symbol">http:</span><span class="comment">// docker IP :8888/</span></span><br></pre></td></tr></table></figure>
<p>ip查看方式：</p>
<figure class="highlight groovy"><table><tr><td class="code"><pre><span class="line">$ ifconfig</span><br><span class="line"></span><br><span class="line">docker0   Link <span class="string">encap:</span>Ethernet  HWaddr <span class="number">02</span>:<span class="number">42</span>:<span class="number">52</span>:<span class="number">46</span>:<span class="number">56</span>:<span class="number">2</span>f</span><br><span class="line">          inet <span class="string">addr:</span><span class="number">172.17</span><span class="number">.0</span><span class="number">.1</span>  <span class="string">Bcast:</span><span class="number">172.17</span><span class="number">.255</span><span class="number">.255</span>  <span class="string">Mask:</span><span class="number">255.255</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">          inet6 <span class="string">addr:</span> <span class="string">fe80:</span>:<span class="number">42</span>:<span class="number">52</span><span class="string">ff:</span><span class="string">fe46:</span><span class="number">562</span>f/<span class="number">64</span> <span class="string">Scope:</span>Link</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  <span class="string">MTU:</span><span class="number">1500</span>  <span class="string">Metric:</span><span class="number">1</span></span><br><span class="line">          RX <span class="string">packets:</span><span class="number">13461</span> <span class="string">errors:</span><span class="number">0</span> <span class="string">dropped:</span><span class="number">0</span> <span class="string">overruns:</span><span class="number">0</span> <span class="string">frame:</span><span class="number">0</span></span><br><span class="line">          TX <span class="string">packets:</span><span class="number">17994</span> <span class="string">errors:</span><span class="number">0</span> <span class="string">dropped:</span><span class="number">0</span> <span class="string">overruns:</span><span class="number">0</span> <span class="string">carrier:</span><span class="number">0</span></span><br><span class="line"><span class="symbol">          collisions:</span><span class="number">0</span> <span class="string">txqueuelen:</span><span class="number">0</span></span><br><span class="line">          RX <span class="string">bytes:</span><span class="number">20897178</span> (<span class="number">20.8</span> MB)  TX <span class="string">bytes:</span><span class="number">114945170</span> (<span class="number">114.9</span> MB)</span><br></pre></td></tr></table></figure>
<p>完结，开始工作。</p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>TensorFlow2</tag>
        <tag>jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title>ex1-linear regression</title>
    <url>/2018/10/08/ex1-linear%20regression/</url>
    <content><![CDATA[<p>AndrewNg 机器学习习题ex1-linear regression<br><a href="https://github.com/Voidmort/Machine-Learning-Python-Code/tree/master/Data" target="_blank" rel="noopener">练习用数据</a></p>
<p>练习数据ex1data1.txt和ex2data2.txt都是以逗号为分割符的文本文件，所以我们也可以把它们看作csv文件处理。</p>
<p>ex1data1中的第一列是一个城市的人口，第二列是这个城市中卡车司机的利润。</p>
<p>ex2data2三列分别是，一个房子的大小，房间数，售价。</p>
<h1 id="浏览数据"><a href="#浏览数据" class="headerlink" title="浏览数据"></a>浏览数据</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">path = <span class="string">'./data/ex1data1.txt'</span></span><br><span class="line">data = pd.read_csv(path, header=<span class="keyword">None</span>, names=[<span class="string">'Population'</span>, <span class="string">'Profit'</span>])</span><br><span class="line"><span class="comment"># 看一下数据的内容</span></span><br><span class="line">print(data.head())</span><br><span class="line">print(data.describe())</span><br><span class="line"><span class="comment"># 画出散点图</span></span><br><span class="line">data.plot(kind=<span class="string">'scatter'</span>, x=<span class="string">'Population'</span>, y=<span class="string">'Profit'</span>, figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>-</th>
<th>Population</th>
<th>Profit</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>6.1101</td>
<td>17.5920</td>
</tr>
<tr>
<td>1</td>
<td>5.5277</td>
<td>9.1302</td>
</tr>
<tr>
<td>2</td>
<td>8.5186</td>
<td>13.6620</td>
</tr>
<tr>
<td>3</td>
<td>7.0032</td>
<td>11.8540</td>
</tr>
<tr>
<td>4</td>
<td>5.8598</td>
<td>6.8233</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>-</th>
<th>Population</th>
<th>Profit</th>
</tr>
</thead>
<tbody>
<tr>
<td>count</td>
<td>97.000000</td>
<td>97.000000</td>
</tr>
<tr>
<td>mean</td>
<td>8.159800</td>
<td>5.839135</td>
</tr>
<tr>
<td>std</td>
<td>3.869884</td>
<td>5.510262</td>
</tr>
<tr>
<td>min</td>
<td>5.026900</td>
<td>-2.680700</td>
</tr>
<tr>
<td>25%</td>
<td>5.707700</td>
<td>1.986900</td>
</tr>
<tr>
<td>50%</td>
<td>6.589400</td>
<td>4.562300</td>
</tr>
<tr>
<td>75%</td>
<td>8.578100</td>
<td>7.046700</td>
</tr>
<tr>
<td>max</td>
<td>22.203000</td>
<td>24.147000</td>
</tr>
</tbody>
</table>
<p><img src="Figure_1.png" alt="image"></p>
<h1 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h1><p>我们将创建一个以参数θ为特征函数的代价函数 </p>
 
$J\left( \theta \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( {{h}_{\theta }}\left( {{x}^{(i)}} \right)-{{y}^{(i)}} \right)}^{2}}}$ 

<p>其中：</p>
 
${{h}{\theta }}\left( x \right)={{\theta }^{T}}X={{\theta }{0}}{{x}{0}}+{{\theta }{1}}{{x}{1}}+{{\theta }{2}}{{x}{2}}+...+{{\theta }{n}}{{x}_{n}}$

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(X, y, theta)</span>:</span></span><br><span class="line">    inner = np.power(((X * theta.T) - y), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.sum(inner) / (<span class="number">2</span> * len(X))</span><br></pre></td></tr></table></figure>
<h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 预处理</span></span><br><span class="line">data.insert(<span class="number">0</span>, <span class="string">'Ones'</span>, <span class="number">1</span>)  <span class="comment"># 添加一列1</span></span><br><span class="line">cols = data.shape[<span class="number">1</span>]</span><br><span class="line">X = data.iloc[:, :cols - <span class="number">1</span>]  <span class="comment"># 去掉最后一列</span></span><br><span class="line">Y = data.iloc[:, cols - <span class="number">1</span>: cols]  <span class="comment"># 最后一列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查X和Y 是否正确</span></span><br><span class="line">print(X.head())</span><br><span class="line">print(Y.head())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把X和Y转换为numpy的矩阵</span></span><br><span class="line">X = np.matrix(X.values)</span><br><span class="line">Y = np.matrix(Y.values)</span><br><span class="line"><span class="comment"># 初始化theta</span></span><br><span class="line">theta = np.matrix(np.array([<span class="number">0</span>, <span class="number">0</span>]))</span><br><span class="line"><span class="comment"># 检查维度</span></span><br><span class="line">print(X.shape, Y.shape, theta.shape)  <span class="comment"># (97, 2) (97, 1) (1, 2)</span></span><br></pre></td></tr></table></figure>
<h1 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h1><p>我们要这个公式来更新θ。</p>
 
${\theta_{j}}:={\theta_{j}}-\alpha \frac{\partial }{\partial {\theta_{j}}}J\left(\theta \right)$

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 梯度下降</span></span><br><span class="line"><span class="comment"># X矩阵，Y矩阵，初始的θ，学习速率，迭代次数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(X, Y, theta, alpha, iters)</span>:</span></span><br><span class="line">    temp = np.matrix(np.zeros(theta.shape))</span><br><span class="line">    parameters = int(theta.ravel().shape[<span class="number">1</span>])</span><br><span class="line">    cost =  np.zeros(iters)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(iters):</span><br><span class="line">        error = (X * theta.T) - Y</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(parameters):</span><br><span class="line">            term = np.multiply(error, X[:, j])</span><br><span class="line">            temp[<span class="number">0</span>, j] = theta[<span class="number">0</span>, j] - ((alpha / len(X)) * np.sum(term))</span><br><span class="line">        theta = temp</span><br><span class="line">        cost[i] = compute_cost(X, Y, theta)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta, cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化迭代次数和学习速率</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line">iters = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">g, cost = gradient_descent(X, Y, theta, alpha, iters)</span><br><span class="line"><span class="comment"># 用我们得到的参数g计算代价函数，查看误差</span></span><br><span class="line">print(g, compute_cost(X, Y, theta))</span><br></pre></td></tr></table></figure>
<h1 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制线性模型以及数据，查看拟合效果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_visual</span><span class="params">(data, g)</span>:</span></span><br><span class="line">    x = np.linspace(data.Population.min(), data.Population.max(), <span class="number">100</span>)</span><br><span class="line">    f = g[<span class="number">0</span>, <span class="number">0</span>] + (g[<span class="number">0</span>, <span class="number">1</span>] * x)</span><br><span class="line"></span><br><span class="line">    fig, ax = plt.subplots(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">    ax.plot(x, f, <span class="string">'g'</span>, label=<span class="string">'Prediction'</span>)</span><br><span class="line">    ax.scatter(data.Population, data.Profit, label=<span class="string">'Traning Data'</span>)</span><br><span class="line">    ax.legend(loc=<span class="number">2</span>)</span><br><span class="line">    ax.set_xlabel(<span class="string">'Population'</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">'Profit'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">data_visual(data, g)</span><br></pre></td></tr></table></figure>
<p><img src="Figure_2.png" alt="image"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制代价向量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_visual</span><span class="params">(cost)</span>:</span></span><br><span class="line">    fig, ax = plt.subplots(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">    ax.plot(np.arange(iters), cost, <span class="string">'r'</span>)</span><br><span class="line">    ax.set_xlabel(<span class="string">'Iterations'</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">'Cost'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">cost_visual(cost)</span><br></pre></td></tr></table></figure>
<p><img src="Figure_3.png" alt="image"></p>
<h1 id="多变量的线性回归"><a href="#多变量的线性回归" class="headerlink" title="多变量的线性回归"></a>多变量的线性回归</h1><p>练习1还包括一个房屋价格数据集，其中有2个变量（房子的大小，卧室的数量）和目标（房子的价格）。 我们使用我们已经应用的技术来分析数据集。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">path = <span class="string">'./data/ex1data2.txt'</span></span><br><span class="line">data2 = pd.read_csv(path, header=<span class="keyword">None</span>, names =[<span class="string">'Size'</span>, <span class="string">'Bedrooms'</span>, <span class="string">'Price'</span>])</span><br><span class="line">print(data2.head())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征归一化</span></span><br><span class="line">data2 = (data2 - data2.mean()) / data2.std()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预处理</span></span><br><span class="line"><span class="comment"># add ones column</span></span><br><span class="line">data2.insert(<span class="number">0</span>, <span class="string">'Ones'</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set X (training data) and y (target variable)</span></span><br><span class="line">cols = data2.shape[<span class="number">1</span>]</span><br><span class="line">X2 = data2.iloc[:, : cols - <span class="number">1</span>]</span><br><span class="line">Y2 = data2.iloc[:, cols - <span class="number">1</span>: cols]</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert to matrices and initialize theta</span></span><br><span class="line">X2 = np.matrix(X2.values)</span><br><span class="line">Y2 = np.matrix(Y2.values)</span><br><span class="line">theta2 = np.matrix(np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">g2, cost2 = gradient_descent(X2, Y2, theta2, alpha, iters)</span><br><span class="line">cost_visual(cost2)</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>-</th>
<th>Size</th>
<th>Bedrooms</th>
<th>Price</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>2104</td>
<td>3</td>
<td>399900</td>
</tr>
<tr>
<td>1</td>
<td>1600</td>
<td>3</td>
<td>329900</td>
</tr>
<tr>
<td>2</td>
<td>2400</td>
<td>3</td>
<td>369000</td>
</tr>
<tr>
<td>3</td>
<td>1416</td>
<td>2</td>
<td>232000</td>
</tr>
<tr>
<td>4</td>
<td>3000</td>
<td>4</td>
<td>539900</td>
</tr>
</tbody>
</table>
<p><img src="Figure_4.png" alt="image"></p>
<h1 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h1> 
$\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}y$

<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 正规方程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_func</span><span class="params">(X ,Y)</span></span><span class="symbol">:</span></span><br><span class="line">    theta = np.linalg.inv(X.T@X)@X.T@Y</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line">g = normal_func(X, Y)</span><br><span class="line">data_visual(data, g.T)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>机器学习习题</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>ex2-logistic regression</title>
    <url>/2018/10/09/ex2-logistic%20regression/</url>
    <content><![CDATA[<p>AndrewNg 机器学习习题ex2-logistic regression</p>
<p><a href="https://github.com/Voidmort/Machine-Learning-Python-Code/tree/master/Data" target="_blank" rel="noopener">练习用数据</a></p>
<p>练习数据ex2data1.txt和ex2data2.txt都是由三列数字组成的文本文件，前两列是特征，第三列是结果，结果只有0和1两种。</p>
<h1 id="浏览数据"><a href="#浏览数据" class="headerlink" title="浏览数据"></a>浏览数据</h1><p>画出散点图，观察两个不同结果的分类情况，有明显的决策边界。<br><figure class="highlight xl"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="built_in">path</span> = <span class="string">'./data/ex2data1.txt'</span></span><br><span class="line">names=[<span class="string">'exam 1'</span>, <span class="string">'exam 2'</span>, <span class="string">'admitted'</span>]</span><br><span class="line"><span class="keyword">data</span> = pd.read_csv(<span class="built_in">path</span>, header=None, names=names)</span><br><span class="line">print(<span class="keyword">data</span>.head())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 可视化</span><br><span class="line">def data_visual(<span class="keyword">data</span>, names, theta=None):</span><br><span class="line">    positive = <span class="keyword">data</span>[<span class="keyword">data</span>[names[<span class="number">2</span>]].isin([<span class="number">1</span>])]</span><br><span class="line">    negative = <span class="keyword">data</span>[<span class="keyword">data</span>[names[<span class="number">2</span>]].isin([<span class="number">0</span>])]</span><br><span class="line">    fig, ax = plt.subplots(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">    ax.scatter(positive[names[<span class="number">0</span>]], positive[names[<span class="number">1</span>]], s=<span class="number">50</span>, c=<span class="string">'b'</span>, marker=<span class="string">'o'</span>, label=<span class="string">'1'</span>)</span><br><span class="line">    ax.scatter(negative[names[<span class="number">0</span>]], negative[names[<span class="number">1</span>]], s=<span class="number">50</span>, c=<span class="string">'r'</span>, marker=<span class="string">'x'</span>, label=<span class="string">'0'</span>)</span><br><span class="line">    ax.legend()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> theta <span class="keyword">is</span> <span class="built_in">not</span> None:</span><br><span class="line">        x1 = np.arange(<span class="number">20</span>, <span class="number">100</span>, <span class="number">5</span>)</span><br><span class="line">        x2 = (- theta[<span class="number">0</span>] - theta[<span class="number">1</span>] * x1) / theta[<span class="number">2</span>]</span><br><span class="line">        plt.plot(x1, x2, <span class="built_in">color</span>=<span class="string">'black'</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_visual(<span class="keyword">data</span>, names)</span><br></pre></td></tr></table></figure></p>
<p><img src="Figure_1.png" alt="image"></p>
<h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1> 

逻辑回归模型的假设是：$h_\theta \left( x \right)=g\left(\theta^{T}X \right)$其中： $X$ 代表特征向量 $g$ 代表逻辑函数（logistic function)是一个常用的逻辑函数为S形函数（Sigmoid function），公式为： $g\left( z \right)=\frac{1}{1+{{e}^{-z}}}$。


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sigmoid函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查激活函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_visual</span><span class="params">()</span>:</span></span><br><span class="line">    nums = np.arange(<span class="number">-10</span>, <span class="number">10</span>, step=<span class="number">1</span>)</span><br><span class="line">    plt.plot(nums, sigmoid(nums))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sigmoid_visual()</span><br></pre></td></tr></table></figure>
<p><img src="Figure_2.png" alt="image"></p>
<h1 id="代价函数与预处理"><a href="#代价函数与预处理" class="headerlink" title="代价函数与预处理"></a>代价函数与预处理</h1><p>代价函数：<br> 
$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)]}$
<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta, X, Y)</span>:</span></span><br><span class="line">    theta = np.matrix(theta)</span><br><span class="line">    X = np.matrix(X)</span><br><span class="line">    Y = np.matrix(Y)</span><br><span class="line">    first = np.multiply(-Y, np.log(sigmoid(X * theta.T)))</span><br><span class="line">    second = np.multiply((<span class="number">1</span> - Y), np.log(<span class="number">1</span> - sigmoid(X * theta.T)))</span><br><span class="line">    <span class="keyword">return</span> np.sum(first - second) / len(X)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line"><span class="comment"># add a ones column - this makes the matrix multiplication work out easier</span></span><br><span class="line">data.insert(<span class="number">0</span>, <span class="string">'Ones'</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set X (training data) and y (target variable)</span></span><br><span class="line">cols = data.shape[<span class="number">1</span>]</span><br><span class="line">X = data.iloc[:, <span class="number">0</span>: cols - <span class="number">1</span>]</span><br><span class="line">Y = data.iloc[:, cols - <span class="number">1</span>: cols]</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert to numpy arrays and initalize the parameter array theta</span></span><br><span class="line">X = np.array(X.values)</span><br><span class="line">Y = np.array(Y.values)</span><br><span class="line">theta = np.zeros(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查维度</span></span><br><span class="line">print(X.shape, theta.shape, Y.shape)  <span class="comment"># (100, 3) (3,) (100, 1)</span></span><br><span class="line">print(cost(theta, X, Y))  <span class="comment"># 初始值的代价</span></span><br></pre></td></tr></table></figure></p>
<p>初始化的代价函数值为：0.6931471805599453</p>
<h1 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h1> 
$\frac{\partial J\left( \theta \right)}{\partial {{\theta }_{j}}}=\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}\left( {{x}^{(i)}} \right)-{{y}^{(i)}})x_{_{j}}^{(i)}}$

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta, X, Y)</span>:</span></span><br><span class="line">    theta = np.matrix(theta)</span><br><span class="line">    X = np.matrix(X)</span><br><span class="line">    y = np.matrix(Y)</span><br><span class="line"></span><br><span class="line">    parameters = int(theta.ravel().shape[<span class="number">1</span>])</span><br><span class="line">    grad = np.zeros(parameters)</span><br><span class="line"></span><br><span class="line">    error = sigmoid(X * theta.T) - Y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(parameters):</span><br><span class="line">        term = np.multiply(error, X[:, i])</span><br><span class="line">        grad[i] = np.sum(term) / len(X)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>
<h1 id="训练数据与决策边界"><a href="#训练数据与决策边界" class="headerlink" title="训练数据与决策边界"></a>训练数据与决策边界</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用SciPy's truncated newton（TNC）实现寻找最优参数</span></span><br><span class="line">result = opt.fmin_tnc(func=cost, x0=theta, fprime=gradient, args=(X, Y))</span><br><span class="line">print(result)</span><br><span class="line">print(cost(result[<span class="number">0</span>], X, Y))</span><br><span class="line"></span><br><span class="line">theta = result[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 画出决策边界</span></span><br><span class="line">data_visual(data, names, theta)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算预测效果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(theta, X)</span>:</span></span><br><span class="line">    probability = sigmoid(X * theta.T)</span><br><span class="line">    <span class="keyword">return</span> [<span class="number">1</span> <span class="keyword">if</span> x &gt;= <span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> probability]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">theta_min = np.matrix(result[<span class="number">0</span>])</span><br><span class="line">predictions = predict(theta_min, X)</span><br><span class="line">correct = [<span class="number">1</span> <span class="keyword">if</span> ((a == <span class="number">1</span> <span class="keyword">and</span> b == <span class="number">1</span>) <span class="keyword">or</span> (a == <span class="number">0</span> <span class="keyword">and</span> b == <span class="number">0</span>)) <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> (a, b) <span class="keyword">in</span> zip(predictions, Y)]</span><br><span class="line">accuracy = (sum(map(int, correct)) % len(correct))</span><br><span class="line">print(<span class="string">'accuracy = &#123;&#125;%'</span>.format(accuracy))</span><br></pre></td></tr></table></figure>
<p>accuracy = 89%</p>
<p><img src="Figure_3.png" alt="image"></p>
<h1 id="逻辑回归正则化"><a href="#逻辑回归正则化" class="headerlink" title="逻辑回归正则化"></a>逻辑回归正则化</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">path2 = <span class="string">'./data/ex2data2.txt'</span></span><br><span class="line">names = [<span class="string">'test1'</span>, <span class="string">'test2'</span>, <span class="string">'accepted'</span>]</span><br><span class="line">data2 = pd.read_csv(path2, header=<span class="keyword">None</span>, names=names)</span><br><span class="line">print(data2.head())</span><br><span class="line"></span><br><span class="line"><span class="comment">#data_visual(data2, names)</span></span><br><span class="line"></span><br><span class="line">degree = <span class="number">5</span></span><br><span class="line">x1 = data2[<span class="string">'test1'</span>]</span><br><span class="line">x2 = data2[<span class="string">'test2'</span>]</span><br><span class="line"></span><br><span class="line">data2.insert(<span class="number">3</span>, <span class="string">'Ones'</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, degree):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, i):</span><br><span class="line">        data2[<span class="string">'F'</span> + str(i) + str(j)] = np.power(x1, i-j) * np.power(x2, j)</span><br><span class="line"></span><br><span class="line">data2.drop(<span class="string">'test1'</span>, axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">data2.drop(<span class="string">'test2'</span>, axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">print(data2.head())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正则化代价函数 learng_rate = λ lambda</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost_reg</span><span class="params">(theta, X, Y, learng_rate)</span>:</span></span><br><span class="line">    theta = np.matrix(theta)</span><br><span class="line">    X = np.matrix(X)</span><br><span class="line">    Y = np.matrix(Y)</span><br><span class="line"></span><br><span class="line">    first = np.multiply(-Y, np.log(sigmoid(X * theta.T)))</span><br><span class="line">    second = np.multiply((<span class="number">1</span> - Y), np.log(<span class="number">1</span> - sigmoid(X * theta.T)))</span><br><span class="line">    reg = (learng_rate / (<span class="number">2</span> * len(X))) * np.sum(np.power(theta[:, <span class="number">1</span>: theta.shape[<span class="number">1</span>]], <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.sum(first - second) / len(X) + reg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_reg</span><span class="params">(theta, X, Y, learng_rate)</span>:</span></span><br><span class="line">    theta = np.matrix(theta)</span><br><span class="line">    X = np.matrix(X)</span><br><span class="line">    Y = np.matrix(Y)</span><br><span class="line"></span><br><span class="line">    parameters = int(theta.ravel().shape[<span class="number">1</span>])</span><br><span class="line">    grad = np.zeros(parameters)</span><br><span class="line"></span><br><span class="line">    error = sigmoid(X * theta.T) - Y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(parameters):</span><br><span class="line">        term = np.multiply(error, X[:, i])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(i == <span class="number">0</span>):</span><br><span class="line">            grad[i] = np.sum(term) / len(X)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            grad[i] = (np.sum(term) / len(X)) + ((learng_rate / len(X)) * theta[:, i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grad</span><br><span class="line"></span><br><span class="line"><span class="comment"># set X and y (remember from above that we moved the label to column 0)</span></span><br><span class="line">cols = data2.shape[<span class="number">1</span>]</span><br><span class="line">X2 = data2.iloc[:,<span class="number">1</span>:cols]</span><br><span class="line">Y2 = data2.iloc[:, :<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert to numpy arrays and initalize the parameter array theta</span></span><br><span class="line">X2 = np.array(X2.values)</span><br><span class="line">Y2 = np.array(Y2.values)</span><br><span class="line">theta2 = np.zeros(<span class="number">11</span>)</span><br><span class="line"></span><br><span class="line">learng_rate = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(cost_reg(theta2, X2, Y2, learng_rate))</span><br><span class="line">print(gradient_reg(theta2, X2, Y2, learng_rate))</span><br><span class="line"></span><br><span class="line">result2 = opt.fmin_tnc(func=cost_reg, x0=theta2, fprime=gradient_reg, args=(X2, Y2, learng_rate))</span><br><span class="line">print(result2)</span><br><span class="line"></span><br><span class="line">theta_min = np.matrix(result2[<span class="number">0</span>])</span><br><span class="line">predictions = predict(theta_min, X2)</span><br><span class="line">correct = [<span class="number">1</span> <span class="keyword">if</span> ((a == <span class="number">1</span> <span class="keyword">and</span> b == <span class="number">1</span>) <span class="keyword">or</span> (a == <span class="number">0</span> <span class="keyword">and</span> b == <span class="number">0</span>)) <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> (a, b) <span class="keyword">in</span> zip(predictions, Y2)]</span><br><span class="line">accuracy = (sum(map(int, correct)) % len(correct))</span><br><span class="line">print(<span class="string">'accuracy = &#123;0&#125;%'</span>.format(accuracy))</span><br></pre></td></tr></table></figure>
<p>accuracy = 78%</p>
<h1 id="正则化画出决策边界"><a href="#正则化画出决策边界" class="headerlink" title="正则化画出决策边界"></a>正则化画出决策边界</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy.optimize <span class="keyword">as</span> opt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_y</span><span class="params">(df)</span>:</span>  <span class="comment"># 读取标签</span></span><br><span class="line"><span class="comment">#     '''assume the last column is the target'''</span></span><br><span class="line">    <span class="keyword">return</span> np.array(df.iloc[:, <span class="number">-1</span>])  <span class="comment"># df.iloc[:, -1]是指df的最后一列</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line"><span class="comment">#     '''just 1 batch gradient'''</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span> / len(X)) * X.T @ (sigmoid(X @ theta) - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="string">''' cost fn is -l(theta) for you to minimize'''</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(-y * np.log(sigmoid(X @ theta)) - (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - sigmoid(X @ theta)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">'./data/ex2data2.txt'</span>, names=[<span class="string">'test1'</span>, <span class="string">'test2'</span>, <span class="string">'accepted'</span>])</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_mapping</span><span class="params">(x, y, power, as_ndarray=False)</span>:</span></span><br><span class="line"><span class="comment">#     """return mapped features as ndarray or dataframe"""</span></span><br><span class="line">    <span class="comment"># data = &#123;&#125;</span></span><br><span class="line">    <span class="comment"># # inclusive</span></span><br><span class="line">    <span class="comment"># for i in np.arange(power + 1):</span></span><br><span class="line">    <span class="comment">#     for p in np.arange(i + 1):</span></span><br><span class="line">    <span class="comment">#         data["f&#123;&#125;&#123;&#125;".format(i - p, p)] = np.power(x, i - p) * np.power(y, p)</span></span><br><span class="line"></span><br><span class="line">    data = &#123;<span class="string">"f&#123;&#125;&#123;&#125;"</span>.format(i - p, p): np.power(x, i - p) * np.power(y, p)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(power + <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> np.arange(i + <span class="number">1</span>)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> as_ndarray:</span><br><span class="line">        <span class="keyword">return</span> pd.DataFrame(data).as_matrix()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x1 = np.array(df.test1)</span><br><span class="line">x2 = np.array(df.test2)</span><br><span class="line">data = feature_mapping(x1, x2, power=<span class="number">6</span>)</span><br><span class="line">print(data.shape)</span><br><span class="line">print(data.head())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">theta = np.zeros(data.shape[<span class="number">1</span>])</span><br><span class="line">X = feature_mapping(x1, x2, power=<span class="number">6</span>, as_ndarray=<span class="keyword">True</span>)</span><br><span class="line">print(X.shape)</span><br><span class="line"></span><br><span class="line">y = get_y(df)</span><br><span class="line">print(y.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_cost</span><span class="params">(theta, X, y, l=<span class="number">1</span>)</span>:</span></span><br><span class="line"><span class="comment">#     '''you don't penalize theta_0'''</span></span><br><span class="line">    theta_j1_to_n = theta[<span class="number">1</span>:]</span><br><span class="line">    regularized_term = (l / (<span class="number">2</span> * len(X))) * np.power(theta_j1_to_n, <span class="number">2</span>).sum()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost(theta, X, y) + regularized_term</span><br><span class="line"><span class="comment"># 正则化代价函数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">regularized_cost(theta, X, y, l=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_gradient</span><span class="params">(theta, X, y, l=<span class="number">1</span>)</span>:</span></span><br><span class="line"><span class="comment">#     '''still, leave theta_0 alone'''</span></span><br><span class="line">    theta_j1_to_n = theta[<span class="number">1</span>:]</span><br><span class="line">    regularized_theta = (l / len(X)) * theta_j1_to_n</span><br><span class="line"></span><br><span class="line">    <span class="comment"># by doing this, no offset is on theta_0</span></span><br><span class="line">    regularized_term = np.concatenate([np.array([<span class="number">0</span>]), regularized_theta])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gradient(theta, X, y) + regularized_term</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">'init cost = &#123;&#125;'</span>.format(regularized_cost(theta, X, y)))</span><br><span class="line"></span><br><span class="line">res = opt.minimize(fun=regularized_cost, x0=theta, args=(X, y), method=<span class="string">'Newton-CG'</span>, jac=regularized_gradient)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw_boundary</span><span class="params">(power, l)</span>:</span></span><br><span class="line"><span class="comment">#     """</span></span><br><span class="line"><span class="comment">#     power: polynomial power for mapped feature</span></span><br><span class="line"><span class="comment">#     l: lambda constant</span></span><br><span class="line"><span class="comment">#     """</span></span><br><span class="line">    density = <span class="number">1000</span></span><br><span class="line">    threshhold = <span class="number">2</span> * <span class="number">10</span>**<span class="number">-3</span></span><br><span class="line"></span><br><span class="line">    final_theta = feature_mapped_logistic_regression(power, l)</span><br><span class="line">    x, y = find_decision_boundary(density, power, final_theta, threshhold)</span><br><span class="line"></span><br><span class="line">    df = pd.read_csv(<span class="string">'./data/ex2data2.txt'</span>, names=[<span class="string">'test1'</span>, <span class="string">'test2'</span>, <span class="string">'accepted'</span>])</span><br><span class="line">    sns.lmplot(<span class="string">'test1'</span>, <span class="string">'test2'</span>, hue=<span class="string">'accepted'</span>, data=df, size=<span class="number">6</span>, fit_reg=<span class="keyword">False</span>, scatter_kws=&#123;<span class="string">"s"</span>: <span class="number">100</span>&#125;)</span><br><span class="line"></span><br><span class="line">    plt.scatter(x, y, c=<span class="string">'R'</span>, s=<span class="number">10</span>)</span><br><span class="line">    plt.title(<span class="string">'Decision boundary'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_mapped_logistic_regression</span><span class="params">(power, l)</span>:</span></span><br><span class="line"><span class="comment">#     """for drawing purpose only.. not a well generealize logistic regression</span></span><br><span class="line"><span class="comment">#     power: int</span></span><br><span class="line"><span class="comment">#         raise x1, x2 to polynomial power</span></span><br><span class="line"><span class="comment">#     l: int</span></span><br><span class="line"><span class="comment">#         lambda constant for regularization term</span></span><br><span class="line"><span class="comment">#     """</span></span><br><span class="line">    df = pd.read_csv(<span class="string">'./data/ex2data2.txt'</span>, names=[<span class="string">'test1'</span>, <span class="string">'test2'</span>, <span class="string">'accepted'</span>])</span><br><span class="line">    x1 = np.array(df.test1)</span><br><span class="line">    x2 = np.array(df.test2)</span><br><span class="line">    y = get_y(df)</span><br><span class="line"></span><br><span class="line">    X = feature_mapping(x1, x2, power, as_ndarray=<span class="keyword">True</span>)</span><br><span class="line">    theta = np.zeros(X.shape[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    res = opt.minimize(fun=regularized_cost,</span><br><span class="line">                       x0=theta,</span><br><span class="line">                       args=(X, y, l),</span><br><span class="line">                       method=<span class="string">'TNC'</span>,</span><br><span class="line">                       jac=regularized_gradient)</span><br><span class="line">    final_theta = res.x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> final_theta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_decision_boundary</span><span class="params">(density, power, theta, threshhold)</span>:</span></span><br><span class="line">    t1 = np.linspace(<span class="number">-1</span>, <span class="number">1.5</span>, density)</span><br><span class="line">    t2 = np.linspace(<span class="number">-1</span>, <span class="number">1.5</span>, density)</span><br><span class="line"></span><br><span class="line">    cordinates = [(x, y) <span class="keyword">for</span> x <span class="keyword">in</span> t1 <span class="keyword">for</span> y <span class="keyword">in</span> t2]</span><br><span class="line">    x_cord, y_cord = zip(*cordinates)</span><br><span class="line">    mapped_cord = feature_mapping(x_cord, y_cord, power)  <span class="comment"># this is a dataframe</span></span><br><span class="line"></span><br><span class="line">    inner_product = mapped_cord.as_matrix() @ theta</span><br><span class="line"></span><br><span class="line">    decision = mapped_cord[np.abs(inner_product) &lt; threshhold]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> decision.f10, decision.f01</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 寻找决策边界函数</span></span><br><span class="line">draw_boundary(power=<span class="number">6</span>, l=<span class="number">1</span>)  <span class="comment"># lambda=1</span></span><br><span class="line">draw_boundary(power=<span class="number">6</span>, l=<span class="number">0</span>)  <span class="comment"># lambda=1 过拟合</span></span><br><span class="line">draw_boundary(power=<span class="number">6</span>, l=<span class="number">100</span>)  <span class="comment"># lambda=1 欠拟合</span></span><br></pre></td></tr></table></figure>
<p><img src="Figure_5.png" alt="image"></p>
<p><img src="Figure_6.png" alt="image"></p>
<p><img src="Figure_7.png" alt="image"></p>
]]></content>
      <categories>
        <category>机器学习习题</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>ex4-NN back propagation</title>
    <url>/2018/09/29/ex4-NN%20back%20propagation/</url>
    <content><![CDATA[<p>AndrewNg 机器学习习题ex4-NN back propagation</p>
<p><a href="https://github.com/Voidmort/Machine-Learning-Python-Code/tree/master/Data" target="_blank" rel="noopener">练习用数据</a></p>
<p>需要的头：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> scipy.optimize <span class="keyword">as</span> opt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report <span class="comment"># 这个包是评价报告</span></span><br></pre></td></tr></table></figure></p>
<h1 id="Visualizing-the-data"><a href="#Visualizing-the-data" class="headerlink" title="Visualizing the data"></a>Visualizing the data</h1><p>载入数据：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path, transpose=True)</span>:</span></span><br><span class="line">    data = sio.loadmat(path)</span><br><span class="line">    y = data.get(<span class="string">'y'</span>)</span><br><span class="line">    y = y.reshape(y.shape[<span class="number">0</span>])</span><br><span class="line">    X = data.get(<span class="string">'X'</span>)</span><br><span class="line">    <span class="keyword">if</span> transpose:</span><br><span class="line">        X = np.array([im.reshape((<span class="number">20</span>, <span class="number">20</span>)).T <span class="keyword">for</span> im <span class="keyword">in</span> X])</span><br><span class="line">        X = np.array([im.reshape(<span class="number">400</span>) <span class="keyword">for</span> im <span class="keyword">in</span> X])</span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X, y = load_data(<span class="string">'./data/ex4data1.mat'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_100_image</span><span class="params">(X)</span>:</span></span><br><span class="line">    size = int(np.sqrt(X.shape[<span class="number">1</span>]))</span><br><span class="line">    sample_idx = np.random.choice(np.array(X.shape[<span class="number">0</span>]), <span class="number">100</span>)</span><br><span class="line">    sample_images = X[sample_idx, :]</span><br><span class="line"></span><br><span class="line">    fig, ax_array = plt.subplots(nrows=<span class="number">10</span>, ncols=<span class="number">10</span>, sharey=<span class="keyword">True</span>, sharex=<span class="keyword">True</span>, figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            ax_array[r, c].matshow(sample_images[<span class="number">10</span> * r + c].reshape((size, size)), cmap=matplotlib.cm.binary)</span><br><span class="line">            plt.xticks(np.array([]))</span><br><span class="line">            plt.yticks(np.array([]))</span><br><span class="line">    plt.show()</span><br><span class="line">plot_100_image(X)</span><br></pre></td></tr></table></figure></p>
<p><img src="Figure_1.png" alt="image"></p>
<h1 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h1><p>特征集合X添加一列全为1的偏差向量，把目标向量y进行OneHot编码。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_raw, y_raw = load_data(<span class="string">'./data/ex4data1.mat'</span>, transpose=<span class="keyword">False</span>) <span class="comment"># 这里转置</span></span><br><span class="line">X = np.insert(X_raw, <span class="number">0</span>, np.ones(X_raw.shape[<span class="number">0</span>]), axis=<span class="number">1</span>) <span class="comment"># 增加全为1的一列</span></span><br><span class="line">print(y.shape) <span class="comment"># (5000,)</span></span><br><span class="line">y = np.array([y_raw]).T</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">encoder = OneHotEncoder(sparse=<span class="keyword">False</span>)</span><br><span class="line">y_onehot = encoder.fit_transform(y)</span><br><span class="line">print(y_onehot.shape) <span class="comment"># (5000, 10)</span></span><br></pre></td></tr></table></figure></p>
<h1 id="读取权重"><a href="#读取权重" class="headerlink" title="读取权重"></a>读取权重</h1><p>先读取出ex4weights.mat中的theta1和theta2，把theta展开后进行扁平化处理。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_weight</span><span class="params">(path)</span>:</span></span><br><span class="line">    data = sio.loadmat(path)</span><br><span class="line">    <span class="keyword">return</span> data[<span class="string">'Theta1'</span>], data[<span class="string">'Theta2'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t1, t2 = load_weight(<span class="string">'./data/ex4weights.mat'</span>)</span><br><span class="line">print(t1.shape, t2.shape) <span class="comment"># (25, 401) (10, 26)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">serialize</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="comment"># np.ravel() 降维</span></span><br><span class="line">    <span class="comment"># np.concatenate() 拼接</span></span><br><span class="line">    <span class="keyword">return</span> np.concatenate((np.ravel(a), np.ravel(b)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deserialize</span><span class="params">(seq)</span>:</span></span><br><span class="line">    <span class="comment"># 解开为两个theta</span></span><br><span class="line">    <span class="keyword">return</span> seq[:<span class="number">25</span> * <span class="number">401</span>].reshape(<span class="number">25</span>, <span class="number">401</span>), seq[<span class="number">25</span> * <span class="number">401</span>:].reshape(<span class="number">10</span>, <span class="number">26</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">theta = serialize(t1, t2)</span><br><span class="line">print(theta.shape)  <span class="comment"># (25 * 401) + (10 * 26) = 10285</span></span><br></pre></td></tr></table></figure></p>
<h1 id="前向传播-feed-forward"><a href="#前向传播-feed-forward" class="headerlink" title="前向传播 feed forward"></a>前向传播 feed forward</h1><p>（400 + 1） -&gt; (25 + 1) -&gt; (1)<br><img src="Figure_2.jpg" alt="image"><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_forward</span><span class="params">(theta, X)</span>:</span></span><br><span class="line">    t1, t2 = deserialize(theta)</span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    a1 = X <span class="comment"># 5000 * 401</span></span><br><span class="line">    z2 = a1 @ t1.T</span><br><span class="line">    a2 = np.insert(sigmoid(z2), <span class="number">0</span>, np.ones(m), axis=<span class="number">1</span>)  <span class="comment"># 5000*26 第一列加一列一</span></span><br><span class="line">    z3 = a2 @ t2.T  <span class="comment"># 5000 * 100</span></span><br><span class="line">    h = sigmoid(z3)  <span class="comment"># 5000 * 10 这是 h_theta(X)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a1, z2, a2, z3, h  <span class="comment"># 把每一层的计算都返回</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#_, _, _, _, h = feed_forward(theta, X)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#print(h.shape) # (5000, 10)</span></span><br></pre></td></tr></table></figure></p>
<h1 id="代价函数与正则化"><a href="#代价函数与正则化" class="headerlink" title="代价函数与正则化"></a>代价函数与正则化</h1><p><img src="Figure_4" alt="image"></p>
<p><img src="Figure_5" alt="image"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    _, _, _, _, h = feed_forward(theta, X)</span><br><span class="line">    pair_computation = -np.multiply(y, np.log(h)) - np.multiply((<span class="number">1</span> - y), np.log(<span class="number">1</span> - h))</span><br><span class="line">    <span class="keyword">return</span> pair_computation.sum() / m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cost_res = cost(theta, X, y)</span><br><span class="line">print(<span class="string">"cost:"</span>,cost_res)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_cost</span><span class="params">(theta, X, y, l=<span class="number">1</span>)</span>:</span></span><br><span class="line">    t1, t2 = deserialize(theta)  <span class="comment"># t1: (25,401) t2: (10,26)</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    reg_t1 = np.power(t1[:, <span class="number">1</span>:], <span class="number">2</span>).sum()</span><br><span class="line">    reg_t2 = np.power(t2[:, <span class="number">1</span>:], <span class="number">2</span>).sum()</span><br><span class="line">    reg = (<span class="number">1</span> / (<span class="number">2</span> * m)) * (reg_t1 + reg_t2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost(theta, X, y) + reg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">regularized_cost_res = regularized_cost(theta, X, y)</span><br><span class="line">print(<span class="string">"reg cost:"</span>,regularized_cost_res)</span><br></pre></td></tr></table></figure>
<h1 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_gradient</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.multiply(sigmoid(z), <span class="number">1</span> - sigmoid(z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(sigmoid_gradient(<span class="number">0</span>))  <span class="comment">#0.25 </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    t1, t2 = deserialize(theta)</span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    deltal = np.zeros(t1.shape)</span><br><span class="line">    delta2 = np.zeros(t2.shape)</span><br><span class="line"></span><br><span class="line">    a1, z2, a2, z3, h = feed_forward(theta, X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        a1i = a1[i, :]</span><br><span class="line">        z2i = z2[i, :]</span><br><span class="line">        a2i = a2[i, :]</span><br><span class="line"></span><br><span class="line">        hi = h[i, :]</span><br><span class="line">        yi = y[i, :]</span><br><span class="line"></span><br><span class="line">        d3i = hi - yi</span><br><span class="line"></span><br><span class="line">        z2i = np.insert(z2i, <span class="number">0</span>, np.ones(<span class="number">1</span>))</span><br><span class="line">        d2i = np.multiply(t2.T @ d3i, sigmoid_gradient(z2i))</span><br><span class="line"></span><br><span class="line">        delta2 += np.matrix(d3i).T @ np.matrix(a2i)</span><br><span class="line">        deltal += np.matrix(d2i[<span class="number">1</span>:]).T @ np.matrix(a1i)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    delta1 = deltal / m</span><br><span class="line">    delta2 = delta2 / m</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> serialize(delta1, delta2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">d1, d2 = deserialize(gradient(theta, X, y))</span><br><span class="line">print(d1.shape, d2.shape) <span class="comment"># (25, 401) (10, 26)</span></span><br></pre></td></tr></table></figure>
<h1 id="梯度校验"><a href="#梯度校验" class="headerlink" title="梯度校验"></a>梯度校验</h1><p><img src="Figure_7.png" alt="image"></p>
<p>梯度正则化：</p>
<p><img src="Figure_6.png" alt="image"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_gradient</span><span class="params">(theta, X, y, l=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""don't regularize theta of bias terms"""</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    delta1, delta2 = deserialize(gradient(theta, X, y))</span><br><span class="line">    t1, t2 = deserialize(theta)</span><br><span class="line"></span><br><span class="line">    t1[:, <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    reg_term_d1 = (l / m) * t1</span><br><span class="line">    delta1 = delta1 + reg_term_d1</span><br><span class="line"></span><br><span class="line">    t2[:, <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    reg_term_d2 = (l / m) * t2</span><br><span class="line">    delta2 = delta2 + reg_term_d2</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> serialize(delta1, delta2)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expand_array</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="string">"""replicate array into matrix</span></span><br><span class="line"><span class="string">    [1, 2, 3]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    [[1, 2, 3],</span></span><br><span class="line"><span class="string">     [1, 2, 3],</span></span><br><span class="line"><span class="string">     [1, 2, 3]]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># turn matrix back to ndarray</span></span><br><span class="line">    <span class="keyword">return</span> np.array(np.matrix(np.ones(arr.shape[<span class="number">0</span>])).T @ np.matrix(arr))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_checking</span><span class="params">(theta, X, y, epsilon, regularized=False)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">a_numeric_grad</span><span class="params">(plus, minus, regularized=False)</span>:</span></span><br><span class="line">        <span class="string">"""calculate a partial gradient with respect to 1 theta"""</span></span><br><span class="line">        <span class="keyword">if</span> regularized:</span><br><span class="line">            <span class="keyword">return</span> (regularized_cost(plus, X, y) - regularized_cost(minus, X, y)) / (epsilon * <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (cost(plus, X, y) - cost(minus, X, y)) / (epsilon * <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    theta_matrix = expand_array(theta)  <span class="comment"># expand to (10285, 10285)</span></span><br><span class="line">    epsilon_matrix = np.identity(len(theta)) * epsilon</span><br><span class="line"></span><br><span class="line">    plus_matrix = theta_matrix + epsilon_matrix</span><br><span class="line">    minus_matrix = theta_matrix - epsilon_matrix</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calculate numerical gradient with respect to all theta</span></span><br><span class="line">    numeric_grad = np.array([a_numeric_grad(plus_matrix[i], minus_matrix[i], regularized)</span><br><span class="line">                                    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(theta))])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># analytical grad will depend on if you want it to be regularized or not</span></span><br><span class="line">    analytic_grad = regularized_gradient(theta, X, y) <span class="keyword">if</span> regularized <span class="keyword">else</span> gradient(theta, X, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If you have a correct implementation, and assuming you used EPSILON = 0.0001</span></span><br><span class="line">    <span class="comment"># the diff below should be less than 1e-9</span></span><br><span class="line">    <span class="comment"># this is how original matlab code do gradient checking</span></span><br><span class="line">    diff = np.linalg.norm(numeric_grad - analytic_grad) / np.linalg.norm(numeric_grad + analytic_grad)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'If your backpropagation implementation is correct,\nthe relative difference will be smaller than 10e-9 (assume epsilon=0.0001).\nRelative Difference: &#123;&#125;\n'</span>.format(diff))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># gradient_checking(theta, X, y, epsilon= 0.0001)#这个运行很慢，谨慎运行</span></span><br></pre></td></tr></table></figure>
<p>If your backpropagation implementation is correct,<br>the relative difference will be smaller than 10e-9 (assume epsilon=0.0001).<br>Relative Difference: 2.1466000818218673e-09</p>
<h1 id="准备训练模型"><a href="#准备训练模型" class="headerlink" title="准备训练模型"></a>准备训练模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_init</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.random.uniform(<span class="number">-0.12</span>, <span class="number">0.12</span>, size)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_training</span><span class="params">(X, y)</span>:</span></span><br><span class="line">    init_theta = random_init(<span class="number">10285</span>) <span class="comment"># 25 * 401 + 10 * 26</span></span><br><span class="line"></span><br><span class="line">    res = opt.minimize(fun=regularized_cost,</span><br><span class="line">                       x0=init_theta,</span><br><span class="line">                       args=(X ,y, <span class="number">1</span>),</span><br><span class="line">                       method=<span class="string">'TNC'</span>,</span><br><span class="line">                       jac=regularized_gradient,</span><br><span class="line">                       options=&#123;<span class="string">'maxiter'</span>: <span class="number">400</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">res = nn_training(X, y) <span class="comment"># 慢</span></span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure>
<p>Out put：<br><figure class="highlight subunit"><table><tr><td class="code"><pre><span class="line">    fun: 0.32211992072588747</span><br><span class="line">    jac: array([ 2.15004329e<span class="string">-04</span>,  3.88985627e<span class="string">-08</span>, <span class="string">-3</span>.33174201e<span class="string">-08</span>, ...,</span><br><span class="line">       3.15328424e<span class="string">-05</span>,  2.82831419e<span class="string">-05</span>, <span class="string">-1</span>.68082404e<span class="string">-05</span>])</span><br><span class="line">message: 'Max. number of function evaluations reached'</span><br><span class="line">   nfev: 400</span><br><span class="line">    nit: 26</span><br><span class="line"> status: 3</span><br><span class="line"><span class="keyword">success: </span>False</span><br><span class="line">      x: array([ 0.00000000e<span class="string">+00</span>,  1.94492814e<span class="string">-04</span>, <span class="string">-1</span>.66587101e<span class="string">-04</span>, ...,</span><br><span class="line">      <span class="string">-7</span>.15493763e<span class="string">-01</span>, <span class="string">-1</span>.36561388e<span class="string">+00</span>, <span class="string">-2</span>.90127262e<span class="string">+00</span>])</span><br></pre></td></tr></table></figure></p>
<h1 id="显示准确率"><a href="#显示准确率" class="headerlink" title="显示准确率"></a>显示准确率</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_, y_answer = load_data(<span class="string">'./data/ex4data1.mat'</span>)</span><br><span class="line"></span><br><span class="line">final_theta = res.x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_accuracy</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    _, _, _, _, h = feed_forward(theta, X)</span><br><span class="line"></span><br><span class="line">    y_pred = np.argmax(h, axis=<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    print(classification_report(y, y_pred))</span><br><span class="line"></span><br><span class="line">show_accuracy(final_theta, X, y_answer)</span><br></pre></td></tr></table></figure>
<p>Out Put:<br><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">             precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">          <span class="number">1</span>       <span class="number">1.00</span>      <span class="number">0.79</span>      <span class="number">0.88</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">2</span>       <span class="number">0.73</span>      <span class="number">1.00</span>      <span class="number">0.85</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">3</span>       <span class="number">0.82</span>      <span class="number">0.99</span>      <span class="number">0.89</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">4</span>       <span class="number">1.00</span>      <span class="number">0.89</span>      <span class="number">0.94</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">5</span>       <span class="number">1.00</span>      <span class="number">0.86</span>      <span class="number">0.92</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">6</span>       <span class="number">0.94</span>      <span class="number">0.99</span>      <span class="number">0.97</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">7</span>       <span class="number">0.99</span>      <span class="number">0.81</span>      <span class="number">0.89</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">8</span>       <span class="number">0.94</span>      <span class="number">0.95</span>      <span class="number">0.95</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">9</span>       <span class="number">0.96</span>      <span class="number">0.95</span>      <span class="number">0.95</span>       <span class="number">500</span></span><br><span class="line">         <span class="number">10</span>       <span class="number">0.96</span>      <span class="number">0.98</span>      <span class="number">0.97</span>       <span class="number">500</span></span><br><span class="line"></span><br><span class="line">avg / total       <span class="number">0.93</span>      <span class="number">0.92</span>      <span class="number">0.92</span>      <span class="number">5000</span></span><br></pre></td></tr></table></figure></p>
<h1 id="显示隐藏层"><a href="#显示隐藏层" class="headerlink" title="显示隐藏层"></a>显示隐藏层</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_hidden_layer</span><span class="params">(theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    theta: (10285, )</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    final_theta1, _ = deserialize(theta)</span><br><span class="line">    hidden_layer = final_theta1[:, <span class="number">1</span>:]  <span class="comment"># ger rid of bias term theta</span></span><br><span class="line"></span><br><span class="line">    fig, ax_array = plt.subplots(nrows=<span class="number">5</span>, ncols=<span class="number">5</span>, sharey=<span class="keyword">True</span>, sharex=<span class="keyword">True</span>, figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">            ax_array[r, c].matshow(hidden_layer[<span class="number">5</span> * r + c].reshape((<span class="number">20</span>, <span class="number">20</span>)),</span><br><span class="line">                                   cmap=matplotlib.cm.binary)</span><br><span class="line">            plt.xticks(np.array([]))</span><br><span class="line">            plt.yticks(np.array([]))</span><br><span class="line"></span><br><span class="line">plot_hidden_layer(final_theta)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Figure_3.png" alt="image"></p>
]]></content>
      <categories>
        <category>机器学习习题</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>ex3-neural network</title>
    <url>/2018/09/28/ex3-neural%20network/</url>
    <content><![CDATA[<p>AndrewNg 机器学习习题ex3-neural network</p>
<p><a href="https://github.com/Voidmort/Machine-Learning-Python-Code/tree/master/Data" target="_blank" rel="noopener">练习用数据</a></p>
<p>ex3data1.mat是一个matlab文件，储存了5000个图像的数据，每个图像是一个20像素×20像素的灰度图，展开后为一个400维的向量，每一个向量都储存为矩阵X的行，所以X的维度是（5000，400）<br>y的每一行代表X所对应的手写数字，y的维度是（5000，1）</p>
<p>需要的头：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> scipy.optimize <span class="keyword">as</span> opt</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report <span class="comment"># 这个包是评价报告</span></span><br></pre></td></tr></table></figure></p>
<h1 id="Visualizing-the-data"><a href="#Visualizing-the-data" class="headerlink" title="Visualizing the data"></a>Visualizing the data</h1><p>载入数据：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(path, transpose=True)</span>:</span></span><br><span class="line">    data = sio.loadmat(path)</span><br><span class="line">    y = data.get(<span class="string">'y'</span>)</span><br><span class="line">    y = y.reshape(y.shape[<span class="number">0</span>])</span><br><span class="line">    X = data.get(<span class="string">'X'</span>)</span><br><span class="line">    <span class="keyword">if</span> transpose:</span><br><span class="line">        X = np.array([im.reshape((<span class="number">20</span>, <span class="number">20</span>)).T <span class="keyword">for</span> im <span class="keyword">in</span> X])</span><br><span class="line">        X = np.array([im.reshape(<span class="number">400</span>) <span class="keyword">for</span> im <span class="keyword">in</span> X])</span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line">X, y = load_data(<span class="string">'./data/ex3data1.mat'</span>)</span><br></pre></td></tr></table></figure></p>
<p>画一个图<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_an_image</span><span class="params">(image)</span>:</span></span><br><span class="line">    fig, ax = plt.subplots(figsize=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    ax.matshow(image.reshape((<span class="number">20</span>, <span class="number">20</span>)), cmap=matplotlib.cm.binary)</span><br><span class="line">    plt.xticks(np.array([]))</span><br><span class="line">    plt.yticks(np.array([]))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">pick_one = np.random.randint(<span class="number">0</span>, <span class="number">5000</span>)</span><br><span class="line">plot_an_image(X[pick_one, :])</span><br><span class="line">print(<span class="string">'this should be &#123;&#125;'</span>.format(y[pick_one]))</span><br></pre></td></tr></table></figure></p>
<p><img src="Figure_1.png" alt="image"></p>
<p>画一百个图<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_100_image</span><span class="params">(X)</span>:</span></span><br><span class="line">    size = int(np.sqrt(X.shape[<span class="number">1</span>]))</span><br><span class="line">    sample_idx = np.random.choice(np.array(X.shape[<span class="number">0</span>]), <span class="number">100</span>)</span><br><span class="line">    sample_images = X[sample_idx, :]</span><br><span class="line"></span><br><span class="line">    fig, ax_array = plt.subplots(nrows=<span class="number">10</span>, ncols=<span class="number">10</span>, sharey=<span class="keyword">True</span>, sharex=<span class="keyword">True</span>, figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            ax_array[r, c].matshow(sample_images[<span class="number">10</span> * r + c].reshape((size, size)), cmap=matplotlib.cm.binary)</span><br><span class="line">            plt.xticks(np.array([]))</span><br><span class="line">            plt.yticks(np.array([]))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">plot_100_image(X)</span><br></pre></td></tr></table></figure></p>
<p><img src="Figure_2.png" alt="image"></p>
<h1 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h1><p>加载好ex3data1.mat文件后我们需要处理一下，首先X是一个(5000,400)的矩阵，我们在第一列加上一列全为1的矩阵为偏差量，y是一个(5000,)的矩阵，需要注意的是，为了兼容Oxtave和matlab，y中0的被标记为了10。我们把y分成10类整理y数据为(10,5000)的一个矩阵。</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">扩展 5000<span class="number">*1</span> 到 5000<span class="number">*10</span></span><br><span class="line">     比如 <span class="attribute">y</span>=10 -&gt; [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]: ndarray</span><br><span class="line">     比如 <span class="attribute">y</span>=1 -&gt; [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]: ndarray</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_X, raw_y = load_data(<span class="string">'./data/ex3data1.mat'</span>)</span><br><span class="line">X = np.insert(raw_X, <span class="number">0</span>, values=np.ones(raw_X.shape[<span class="number">0</span>]), axis = <span class="number">1</span>) <span class="comment"># 插入了第一列 全为1</span></span><br><span class="line">y_matrix = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</span><br><span class="line">    y_matrix.append((raw_y == k).astype(int))</span><br><span class="line">y_matrix = [y_matrix[<span class="number">-1</span>]] + y_matrix[:<span class="number">-1</span>]</span><br><span class="line">y = np.array(y_matrix)</span><br></pre></td></tr></table></figure>
<h1 id="训练一维模型"><a href="#训练一维模型" class="headerlink" title="训练一维模型"></a>训练一维模型</h1><p>处理好数据后接着写，激活函数和代价函数，代价函数的偏导数就是梯度函数，我们期望这个函数最小。给梯度函数和代价函数加入正则项。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(-y * np.log(sigmoid(X @ theta)) - (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - sigmoid(X @ theta)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度就是jθ的在θ偏导</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="comment"># @ 对应元素相乘求和</span></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span> / len(X)) * X.T @ (sigmoid(X @ theta) - y)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_cost</span><span class="params">(theta, X, y, l=<span class="number">1</span>)</span>:</span></span><br><span class="line">    theta_j1_to_n = theta[<span class="number">1</span>:]</span><br><span class="line">    regularized_term = (<span class="number">1</span> / (<span class="number">2</span> * len(X))) * np.power(theta_j1_to_n, <span class="number">2</span>).sum()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost(theta, X, y) + regularized_term</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_gradient</span><span class="params">(theta, X, y, l=<span class="number">1</span>)</span>:</span></span><br><span class="line">    theta_j1_to_n = theta[<span class="number">1</span>:]</span><br><span class="line">    regularized_theta = (l / len(X)) * theta_j1_to_n</span><br><span class="line">    regularized_term = np.concatenate([np.array([<span class="number">0</span>]), regularized_theta]) <span class="comment"># 在theta矩阵前接一个[0]</span></span><br><span class="line">    <span class="keyword">return</span> gradient(theta, X, y) + regularized_term</span><br></pre></td></tr></table></figure>
<p>运用minimize()函数开始迭代，计算出theta，然后验证theta的准确性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic_regression</span><span class="params">(X, y, l=<span class="number">1</span>)</span>:</span></span><br><span class="line">    theta = np.zeros(X.shape[<span class="number">1</span>])</span><br><span class="line">    res = opt.minimize(fun=regularized_cost, x0=theta, args=(X, y, l), method=<span class="string">'TNC'</span>, jac=regularized_gradient, options=&#123;<span class="string">'disp'</span>: <span class="keyword">True</span>&#125;)</span><br><span class="line">    final_theta = res.x</span><br><span class="line">    <span class="keyword">return</span> final_theta</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(x, theta)</span>:</span></span><br><span class="line">    prob = sigmoid(x @ theta)</span><br><span class="line">    <span class="keyword">return</span> (prob &gt;= <span class="number">0.5</span>).astype(int)</span><br><span class="line"></span><br><span class="line">t0 = logistic_regression(X, y[<span class="number">0</span>])</span><br><span class="line">y_pred = predict(X, t0)</span><br><span class="line">print(<span class="string">'Accuracy=&#123;&#125;'</span>.format(np.mean(y[<span class="number">0</span>] == y_pred)))</span><br></pre></td></tr></table></figure>
<p>最终求得结果为 Accuracy=0.9974</p>
<h1 id="训练K维模型"><a href="#训练K维模型" class="headerlink" title="训练K维模型"></a>训练K维模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">k_theta = np.array([logistic_regression(X, y[k]) <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">10</span>)])</span><br><span class="line">print(k_theta.shape) <span class="comment"># (10, 401)</span></span><br><span class="line"></span><br><span class="line">prob_matrix = sigmoid(X @ k_theta.T)</span><br><span class="line">np.set_printoptions(suppress=<span class="keyword">True</span>) <span class="comment"># 科学计数法表示</span></span><br><span class="line">print(prob_matrix.shape) <span class="comment"># (5000, 10)</span></span><br><span class="line"></span><br><span class="line">y_pred = np.argmax(prob_matrix, axis=<span class="number">1</span>)</span><br><span class="line">print(y_pred.shape) <span class="comment"># (5000,)</span></span><br><span class="line"></span><br><span class="line">y_answer = raw_y.copy()</span><br><span class="line">y_answer[y_answer==<span class="number">10</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">print(classification_report(y_answer, y_pred))</span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">             precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">          <span class="number">0</span>       <span class="number">0.97</span>      <span class="number">0.99</span>      <span class="number">0.98</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">1</span>       <span class="number">0.95</span>      <span class="number">0.99</span>      <span class="number">0.97</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">2</span>       <span class="number">0.95</span>      <span class="number">0.92</span>      <span class="number">0.93</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">3</span>       <span class="number">0.95</span>      <span class="number">0.91</span>      <span class="number">0.93</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">4</span>       <span class="number">0.95</span>      <span class="number">0.95</span>      <span class="number">0.95</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">5</span>       <span class="number">0.92</span>      <span class="number">0.92</span>      <span class="number">0.92</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">6</span>       <span class="number">0.97</span>      <span class="number">0.98</span>      <span class="number">0.97</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">7</span>       <span class="number">0.95</span>      <span class="number">0.95</span>      <span class="number">0.95</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">8</span>       <span class="number">0.93</span>      <span class="number">0.92</span>      <span class="number">0.92</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">9</span>       <span class="number">0.92</span>      <span class="number">0.92</span>      <span class="number">0.92</span>       <span class="number">500</span></span><br><span class="line"></span><br><span class="line">avg / total       <span class="number">0.94</span>      <span class="number">0.94</span>      <span class="number">0.94</span>      <span class="number">5000</span></span><br></pre></td></tr></table></figure>
<p>如ex3.pdf中所说，我们成功的分类出94%的例子。</p>
<h1 id="Feedforward-Propagation-and-Prediction"><a href="#Feedforward-Propagation-and-Prediction" class="headerlink" title="Feedforward Propagation and Prediction"></a>Feedforward Propagation and Prediction</h1><p><img src="Figure_3.jpg" alt="image"></p>
<p>我们的神经网路如上图所示，它有3层构成（一个输入层，一个隐藏层a，一个输出层。）已经提供了一组训练参数（Θ1，Θ2）储存在ex3weights.mat中</p>
<figure class="highlight erlang-repl"><table><tr><td class="code"><pre><span class="line"><span class="comment">% Load saved matrices from file</span></span><br><span class="line">load(<span class="string">'ex3weights.mat'</span>);</span><br><span class="line"><span class="comment">% The matrices Theta1 and Theta2 will now be in your Octave</span></span><br><span class="line"><span class="comment">% environment</span></span><br><span class="line"><span class="comment">% Theta1 has size 25 x 401</span></span><br><span class="line"><span class="comment">% Theta2 has size 10 x 26</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_weight</span><span class="params">(path)</span>:</span></span><br><span class="line">    data = sio.loadmat(path)</span><br><span class="line">    <span class="keyword">return</span> data[<span class="string">'Theta1'</span>], data[<span class="string">'Theta2'</span>]</span><br><span class="line"></span><br><span class="line">theta1, theta2 = load_weight(<span class="string">'./data/ex3weights.mat'</span>)</span><br><span class="line"></span><br><span class="line">X, y = load_data(<span class="string">'./data/ex3data1.mat'</span>,transpose=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">X = np.insert(X, <span class="number">0</span>, values=np.ones(X.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)  <span class="comment"># intercept</span></span><br><span class="line"></span><br><span class="line">a1 = X</span><br><span class="line"></span><br><span class="line">z2 = a1 @ theta1.T <span class="comment"># (5000, 401) @ (25,401).T = (5000, 25)</span></span><br><span class="line">print(z2.shape)</span><br><span class="line"></span><br><span class="line">z2 = np.insert(z2, <span class="number">0</span>, values=np.ones(z2.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">a2 = sigmoid(z2)</span><br><span class="line"></span><br><span class="line">z3 = a2 @ theta2.T</span><br><span class="line"></span><br><span class="line">a3 = sigmoid(z3)</span><br><span class="line"></span><br><span class="line">y_pred = np.argmax(a3, axis=<span class="number">1</span>) + <span class="number">1</span>  <span class="comment"># numpy is 0 base index, +1 for matlab convention，返回沿轴axis最大值的索引，axis=1代表行</span></span><br><span class="line"></span><br><span class="line">print(classification_report(y, y_pred))</span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">             precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">          <span class="number">1</span>       <span class="number">0.97</span>      <span class="number">0.98</span>      <span class="number">0.97</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">2</span>       <span class="number">0.98</span>      <span class="number">0.97</span>      <span class="number">0.97</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">3</span>       <span class="number">0.98</span>      <span class="number">0.96</span>      <span class="number">0.97</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">4</span>       <span class="number">0.97</span>      <span class="number">0.97</span>      <span class="number">0.97</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">5</span>       <span class="number">0.98</span>      <span class="number">0.98</span>      <span class="number">0.98</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">6</span>       <span class="number">0.97</span>      <span class="number">0.99</span>      <span class="number">0.98</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">7</span>       <span class="number">0.98</span>      <span class="number">0.97</span>      <span class="number">0.97</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">8</span>       <span class="number">0.98</span>      <span class="number">0.98</span>      <span class="number">0.98</span>       <span class="number">500</span></span><br><span class="line">          <span class="number">9</span>       <span class="number">0.97</span>      <span class="number">0.96</span>      <span class="number">0.96</span>       <span class="number">500</span></span><br><span class="line">         <span class="number">10</span>       <span class="number">0.98</span>      <span class="number">0.99</span>      <span class="number">0.99</span>       <span class="number">500</span></span><br><span class="line"></span><br><span class="line">avg / total       <span class="number">0.98</span>      <span class="number">0.98</span>      <span class="number">0.98</span>      <span class="number">5000</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>机器学习习题</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>ex6-SVM</title>
    <url>/2018/11/01/ex6-SVM/</url>
    <content><![CDATA[<p>AndrewNg 机器学习习题ex6-SVM</p>
<p><a href="https://github.com/Voidmort/Machine-Learning-Python-Code/tree/master/Data" target="_blank" rel="noopener">练习用数据</a></p>
<h1 id="线性SVM"><a href="#线性SVM" class="headerlink" title="线性SVM"></a>线性SVM</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sb</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在一个简单的二位数据集中 SVM中不同的C处理结果</span></span><br><span class="line">raw_data = loadmat(<span class="string">'data/ex6data1.mat'</span>)</span><br><span class="line">print(raw_data)</span><br><span class="line"></span><br><span class="line">data = pd.DataFrame(raw_data[<span class="string">'X'</span>], columns=[<span class="string">'X1'</span>, <span class="string">'X2'</span>])</span><br><span class="line">data[<span class="string">'y'</span>] = raw_data[<span class="string">'y'</span>]</span><br><span class="line"></span><br><span class="line">positive = data[data[<span class="string">'y'</span>].isin([<span class="number">1</span>])]</span><br><span class="line">negative = data[data[<span class="string">'y'</span>].isin([<span class="number">0</span>])]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">ax.scatter(positive[<span class="string">'X1'</span>], positive[<span class="string">'X2'</span>], s=<span class="number">50</span>, marker=<span class="string">'x'</span>, label=<span class="string">'Positive'</span>)</span><br><span class="line">ax.scatter(negative[<span class="string">'X1'</span>], negative[<span class="string">'X2'</span>], s=<span class="number">50</span>, marker=<span class="string">'o'</span>, label=<span class="string">'Negative'</span>)</span><br><span class="line">ax.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">svc = svm.LinearSVC(C=<span class="number">1</span>, loss=<span class="string">'hinge'</span>, max_iter=<span class="number">1000</span>)</span><br><span class="line">print(svc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先看下C=1的结果</span></span><br><span class="line">svc.fit(data[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], data[<span class="string">'y'</span>])</span><br><span class="line">score = svc.score(data[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], data[<span class="string">'y'</span>])</span><br><span class="line">print(score) <span class="comment"># 0.9803921568627451</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当C=100的时候</span></span><br><span class="line">svc2 = svm.LinearSVC(C=<span class="number">100</span>, loss=<span class="string">'hinge'</span>, max_iter=<span class="number">1000</span>)</span><br><span class="line">svc2.fit(data[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], data[<span class="string">'y'</span>])</span><br><span class="line">score2 = svc2.score(data[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], data[<span class="string">'y'</span>])</span><br><span class="line">print(score2) <span class="comment"># 0.9411764705882353 每次执行的结果可能不同 </span></span><br><span class="line"></span><br><span class="line">data[<span class="string">'SVM 1 Confidence'</span>] = svc.decision_function(data[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]])</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">ax.scatter(data[<span class="string">'X1'</span>], data[<span class="string">'X2'</span>], s=<span class="number">50</span>, c=data[<span class="string">'SVM 1 Confidence'</span>], cmap=<span class="string">'seismic'</span>)</span><br><span class="line">ax.set_title(<span class="string">'SVM (C=1) Decision Confidence'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">data[<span class="string">'SVM 2 Confidence'</span>] = svc2.decision_function(data[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]])</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax.scatter(data[<span class="string">'X1'</span>], data[<span class="string">'X2'</span>], s=<span class="number">50</span>, c=data[<span class="string">'SVM 2 Confidence'</span>], cmap=<span class="string">'seismic'</span>)</span><br><span class="line">ax.set_title(<span class="string">'SVM (C=100) Decision Confidence'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Figure_1.png" alt="image"></p>
<p><img src="Figure_1-1.png" alt="image"></p>
<p><img src="Figure_1-2.png" alt="image"></p>
<h1 id="高斯核函数"><a href="#高斯核函数" class="headerlink" title="高斯核函数"></a>高斯核函数</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 核函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian_kernel</span><span class="params">(x1, x2, sigma)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.exp(-(np.sum((x1 - x2) ** <span class="number">2</span>) / (<span class="number">2</span> * (sigma ** <span class="number">2</span>))))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x1 = np.array([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">1.0</span>])</span><br><span class="line">x2 = np.array([<span class="number">0.0</span>, <span class="number">4.0</span>, <span class="number">-1.0</span>])</span><br><span class="line">sigma = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">gaussian_kernel(x1, x2, sigma)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.32465246735834974</span></span><br></pre></td></tr></table></figure>
<h1 id="非线性决策边界"><a href="#非线性决策边界" class="headerlink" title="非线性决策边界"></a>非线性决策边界</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_data = loadmat(<span class="string">'data/ex6data2.mat'</span>)</span><br><span class="line">data = pd.DataFrame(raw_data[<span class="string">'X'</span>], columns=[<span class="string">'X1'</span>, <span class="string">'X2'</span>])</span><br><span class="line">data[<span class="string">'y'</span>] = raw_data[<span class="string">'y'</span>]</span><br><span class="line"></span><br><span class="line">positive = data[data[<span class="string">'y'</span>].isin([<span class="number">1</span>])]</span><br><span class="line">negative = data[data[<span class="string">'y'</span>].isin([<span class="number">0</span>])]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">ax.scatter(positive[<span class="string">'X1'</span>], positive[<span class="string">'X2'</span>], s=<span class="number">30</span>, marker=<span class="string">'x'</span>, label=<span class="string">'Positive'</span>)</span><br><span class="line">ax.scatter(negative[<span class="string">'X1'</span>], negative[<span class="string">'X2'</span>], s=<span class="number">30</span>, marker=<span class="string">'o'</span>, label=<span class="string">'Negative'</span>)</span><br><span class="line">ax.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Figure_1-3.png" alt="image"></p>
<p>对于该数据集，我们将使用内置的RBF内核构建支持向量机分类器，并检查其对训练数据的准确性。 为了可视化决策边界，这一次我们将根据实例具有负类标签的预测概率来对点做阴影。 从结果可以看出，它们大部分是正确的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">svc = svm.SVC(C=<span class="number">100</span>, gamma=<span class="number">10</span>, probability=<span class="keyword">True</span>)</span><br><span class="line">print(svc)</span><br><span class="line"></span><br><span class="line">svc.fit(data[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], data[<span class="string">'y'</span>])</span><br><span class="line">svc.score(data[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]], data[<span class="string">'y'</span>])</span><br><span class="line">data[<span class="string">'Probability'</span>] = svc.predict_proba(data[[<span class="string">'X1'</span>, <span class="string">'X2'</span>]])[:,<span class="number">0</span>]</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">ax.scatter(data[<span class="string">'X1'</span>], data[<span class="string">'X2'</span>], s=<span class="number">30</span>, c=data[<span class="string">'Probability'</span>], cmap=<span class="string">'Reds'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Figure_1-4.png" alt="image"></p>
<h1 id="搜索最佳参数"><a href="#搜索最佳参数" class="headerlink" title="搜索最佳参数"></a>搜索最佳参数</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 搜索最佳参数</span></span><br><span class="line">raw_data = loadmat(<span class="string">'data/ex6data3.mat'</span>)</span><br><span class="line">X = raw_data[<span class="string">'X'</span>]</span><br><span class="line">Xval = raw_data[<span class="string">'Xval'</span>]</span><br><span class="line">y = raw_data[<span class="string">'y'</span>].ravel()</span><br><span class="line">yval = raw_data[<span class="string">'yval'</span>]. ravel()</span><br><span class="line"></span><br><span class="line">C_values = [<span class="number">0.001</span>, <span class="number">0.003</span>, <span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">100</span>]</span><br><span class="line">gamma_values = [<span class="number">0.01</span>, <span class="number">0.03</span>, <span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">30</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">best_score = <span class="number">0</span></span><br><span class="line">best_params = &#123;<span class="string">'C'</span>: <span class="keyword">None</span>, <span class="string">'gamma'</span>:<span class="keyword">None</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> C <span class="keyword">in</span> C_values:</span><br><span class="line">    <span class="keyword">for</span> gamma <span class="keyword">in</span> gamma_values:</span><br><span class="line">        svc = svm.SVC(C=C, gamma=gamma)</span><br><span class="line">        svc.fit(X, y)</span><br><span class="line">        score = svc.score(Xval, yval)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">            best_score = score</span><br><span class="line">            best_params[<span class="string">'C'</span>] = C</span><br><span class="line">            best_params[<span class="string">'gamma'</span>] = gamma</span><br><span class="line"></span><br><span class="line">print(best_params, best_score)</span><br></pre></td></tr></table></figure>
<p>{‘C’: 0.3, ‘gamma’: 100} 0.965</p>
<h1 id="垃圾邮件过滤"><a href="#垃圾邮件过滤" class="headerlink" title="垃圾邮件过滤"></a>垃圾邮件过滤</h1><p>现在，我们将进行第二部分的练习。 在这一部分中，我们的目标是使用SVM来构建垃圾邮件过滤器。 在练习文本中，有一个任务涉及一些文本预处理，以获得适合SVM处理的格式的数据。 然而，这个任务很简单（将字词映射到为练习提供的字典中的ID），而其余的预处理步骤（如HTML删除，词干，标准化等）已经完成。 我将跳过机器学习任务，而不是重现这些预处理步骤，其中包括从预处理过的训练集构建分类器，以及将垃圾邮件和非垃圾邮件转换为单词出现次数的向量的测试数据集。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 垃圾邮件过滤</span></span><br><span class="line">mat_tr = loadmat(<span class="string">'data/spamTrain.mat'</span>)</span><br><span class="line">X, y = mat_tr.get(<span class="string">'X'</span>), mat_tr.get(<span class="string">'y'</span>).ravel()</span><br><span class="line">print(X.shape, y.shape)  <span class="comment"># ((4000, 1899), (4000,))</span></span><br><span class="line"></span><br><span class="line">mat_test = loadmat(<span class="string">'data/spamTest.mat'</span>)</span><br><span class="line">test_X, test_y = mat_test.get(<span class="string">'Xtest'</span>), mat_test.get(<span class="string">'ytest'</span>).ravel()</span><br><span class="line">print(test_X.shape, test_y.shape)  <span class="comment"># ((1000, 1899), (1000,))</span></span><br><span class="line"></span><br><span class="line">svc = svm.SVC()</span><br><span class="line">svc.fit(X, y)</span><br><span class="line">pred = svc.predict(test_X)</span><br><span class="line">print(metrics.classification_report(test_y, pred))</span><br></pre></td></tr></table></figure></p>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">             precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">          <span class="number">0</span>       <span class="number">0.94</span>      <span class="number">0.99</span>      <span class="number">0.97</span>       <span class="number">692</span></span><br><span class="line">          <span class="number">1</span>       <span class="number">0.98</span>      <span class="number">0.87</span>      <span class="number">0.92</span>       <span class="number">308</span></span><br><span class="line"></span><br><span class="line">avg / total       <span class="number">0.95</span>      <span class="number">0.95</span>      <span class="number">0.95</span>      <span class="number">1000</span></span><br></pre></td></tr></table></figure>
<p>这个结果是使用使用默认参数的。 。</p>
<p>然后用逻辑回归来计算后精确的达到了99%<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果是逻辑回归呢？</span></span><br><span class="line">logit = LogisticRegression()</span><br><span class="line">logit.fit(X, y)</span><br><span class="line">pred = logit.predict(test_X)</span><br><span class="line">print(metrics.classification_report(test_y, pred))</span><br></pre></td></tr></table></figure></p>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">             precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">          <span class="number">0</span>       <span class="number">1.00</span>      <span class="number">0.99</span>      <span class="number">0.99</span>       <span class="number">692</span></span><br><span class="line">          <span class="number">1</span>       <span class="number">0.97</span>      <span class="number">0.99</span>      <span class="number">0.98</span>       <span class="number">308</span></span><br><span class="line"></span><br><span class="line">avg / total       <span class="number">0.99</span>      <span class="number">0.99</span>      <span class="number">0.99</span>      <span class="number">1000</span></span><br></pre></td></tr></table></figure>
<p>调整参数后也可以达到和逻辑回归一样的精确度<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">svc = svm.SVC(C=<span class="number">100</span>)</span><br><span class="line">svc.fit(X, y)</span><br><span class="line">pred = svc.predict(test_X)</span><br><span class="line">print(metrics.classification_report(test_y, pred))</span><br></pre></td></tr></table></figure></p>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">             precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">          <span class="number">0</span>       <span class="number">1.00</span>      <span class="number">0.99</span>      <span class="number">0.99</span>       <span class="number">692</span></span><br><span class="line">          <span class="number">1</span>       <span class="number">0.97</span>      <span class="number">0.99</span>      <span class="number">0.98</span>       <span class="number">308</span></span><br><span class="line"></span><br><span class="line">avg / total       <span class="number">0.99</span>      <span class="number">0.99</span>      <span class="number">0.99</span>      <span class="number">1000</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>机器学习习题</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>ex5-bias vs variance</title>
    <url>/2018/09/29/ex5-bias%20vs%20variance/</url>
    <content><![CDATA[<p>AndrewNg 机器学习习题ex5-bias vs variance</p>
<p><a href="https://github.com/Voidmort/Machine-Learning-Python-Code/tree/master/Data" target="_blank" rel="noopener">练习用数据</a></p>
<p>ex5data1.mat文件储存了大坝出水量的数据，由三部分组成：</p>
<ul>
<li>训练集：X，y</li>
<li>交叉验证集：Xval，yval</li>
<li>测试集：Xtest，ytest</li>
</ul>
<p>需要的头：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"><span class="keyword">import</span> scipy.optimize <span class="keyword">as</span> opt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br></pre></td></tr></table></figure></p>
<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>画出训练集的散点图，给特征集加一列1.<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">    d = sio.loadmat(<span class="string">'./data/ex5data1.mat'</span>)</span><br><span class="line">    <span class="keyword">return</span> map(np.ravel, [d[<span class="string">'X'</span>], d[<span class="string">'y'</span>], d[<span class="string">'Xval'</span>], d[<span class="string">'yval'</span>], d[<span class="string">'Xtest'</span>], d[<span class="string">'ytest'</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X, y, Xval, yval, Xtest, ytest = load_data()</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">'water_level'</span>: X, <span class="string">'flow'</span>: y&#125;)</span><br><span class="line">print(df.shape)</span><br><span class="line">sns.lmplot(<span class="string">'water_level'</span>, <span class="string">'flow'</span>, data=df, fit_reg=<span class="keyword">False</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">X, Xval, Xtest = [np.insert(x.reshape(x.shape[<span class="number">0</span>], <span class="number">1</span>), <span class="number">0</span>, np.ones(x.shape[<span class="number">0</span>]), axis=<span class="number">1</span>) <span class="keyword">for</span> x <span class="keyword">in</span> (X, Xval, Xtest)]</span><br><span class="line"><span class="comment"># print(X, Xval, Xtest )</span></span><br></pre></td></tr></table></figure></p>
<p><img src="Figure_1.png" alt="image"></p>
<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>代价函数是：</p>
<p><img src="Figure_3.png" alt="image"></p>
<p>梯度下降：<br> 

${\theta_{j}}:={\theta_{j}}-\alpha \frac{\partial }{\partial {\theta_{j}}}J\left(\theta \right)$

</p>
<p>正则化线性回归的代价函数为：<br> 

$J\left( \theta \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{[({{({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})}^{2}}+\lambda \sum\limits_{j=1}^{n}{\theta _{j}^{2}})]}$
</p>
<p>如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对$\theta_0$进行正则化，所以梯度下降算法将分两种情形：<br> 

$Repeat$ $until$ $convergence${

 ${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{0}^{(i)}})$

 ${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \right)}}+\frac{\lambda }{m}{\theta_j}]$

 $for$ $j=1,2,...n$

 }

<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    inner = X @ theta - y  <span class="comment"># R(m+1)</span></span><br><span class="line">    <span class="comment"># 1*m @ m*1 = 1*1 矩阵乘法</span></span><br><span class="line">    <span class="comment"># 一维矩阵的转置乘以它自己等于每个元素的平方和</span></span><br><span class="line">    <span class="keyword">return</span> inner.T @ inner / (<span class="number">2</span> * m)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(cost(theta, X, y,))</span><br><span class="line"><span class="comment"># 303.9515255535976</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> X.T @ (X @ theta - y) / m  <span class="comment"># (m, n).T @ (m, 1) -&gt; (n, 1)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(gradient(theta, X, y,))</span><br><span class="line"><span class="comment"># [-15.30301567 598.16741084]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 正则化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_cost</span><span class="params">(theta, X, y, l=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> cost(theta, X, y) + (l / (<span class="number">2</span> * X.shape[<span class="number">0</span>])) * np.power(theta[<span class="number">1</span>:], <span class="number">2</span>).sum()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_gradient</span><span class="params">(theta, X, y, l=<span class="number">1</span>)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    regularized_term = theta.copy()</span><br><span class="line">    regularized_term[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    regularized_term = (l / m) * regularized_term</span><br><span class="line">    <span class="keyword">return</span> gradient(theta, X, y) + regularized_term</span><br><span class="line"></span><br><span class="line">print(regularized_gradient(theta, X, y, l=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># [-15.30301567 598.25074417]</span></span><br></pre></td></tr></table></figure></p>
<h1 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h1><p>正则化项 $\lambda=0$<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_regression_np</span><span class="params">(theta, X, y, l=<span class="number">1</span>)</span>:</span></span><br><span class="line">    res = opt.fmin_tnc(func=regularized_cost, x0=theta, fprime=regularized_gradient, args=(X, y))</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">final_theta = linear_regression_np(theta, X, y)[<span class="number">0</span>]</span><br><span class="line">b = final_theta[<span class="number">0</span>]</span><br><span class="line">m = final_theta[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:, <span class="number">1</span>], y, label=<span class="string">"Training data"</span>)</span><br><span class="line">plt.plot(X[:, <span class="number">1</span>], X[:, <span class="number">1</span>]*m + b, label=<span class="string">'Prediction'</span>)</span><br><span class="line">plt.legend(loc=<span class="number">2</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="Figure_2.png" alt="image"></p>
<h1 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curve</span><span class="params">(X, y, Xval, yval, l=<span class="number">0</span>)</span>:</span></span><br><span class="line">    training_cost, cv_cost = [], []  <span class="comment"># 计算训练集的代价和交叉验证（cross validation）集的代价</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">        res = linear_regression_np(theta, X[:i, :], y[:i], l=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        tc = regularized_cost(res[<span class="number">0</span>], X[:i, :], y[:i], l=<span class="number">0</span>)</span><br><span class="line">        cv = regularized_cost(res[<span class="number">0</span>], Xval, yval, l=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        training_cost.append(tc)</span><br><span class="line">        cv_cost.append(cv)</span><br><span class="line"></span><br><span class="line">    plt.plot(np.arange(<span class="number">1</span>, m + <span class="number">1</span>), training_cost, label=<span class="string">'training cost'</span>)</span><br><span class="line">    plt.plot(np.arange(<span class="number">1</span>, m + <span class="number">1</span>), cv_cost, label=<span class="string">'cv cost'</span>)</span><br><span class="line">    plt.legend(loc=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plot_learning_curve(X, y, Xval, yval, l=<span class="number">0</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Figure_4.png" alt="image"></p>
<p>观察学习曲线发现拟合的不太好，欠拟合。很显然我们的模型不优秀，改为多项式特征尝试。</p>
<h1 id="多项式特征"><a href="#多项式特征" class="headerlink" title="多项式特征"></a>多项式特征</h1><p>把特征扩展到8阶，然后归一化特征值。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">poly_features</span><span class="params">(x, power, as_ndarray=False)</span>:</span></span><br><span class="line">    data = &#123;<span class="string">'f&#123;&#125;'</span>.format(i): np.power(x, i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, power + <span class="number">1</span>)&#125;</span><br><span class="line">    df = pd.DataFrame(data)</span><br><span class="line">    <span class="keyword">return</span> df.as_matrix() <span class="keyword">if</span> as_ndarray <span class="keyword">else</span> df</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 归一化特征值，减去平均数除以标准差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_feature</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="string">"""Applies function along input axis(default 0) of DataFrame."""</span></span><br><span class="line">    <span class="keyword">return</span> df.apply(<span class="keyword">lambda</span> column: (column - column.mean()) / column.std())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_poly_data</span><span class="params">(*args, power)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    args: keep feeding in X, Xval, or Xtest</span></span><br><span class="line"><span class="string">        will return in the same order</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare</span><span class="params">(x)</span>:</span></span><br><span class="line">        <span class="comment"># expand feature</span></span><br><span class="line">        df = poly_features(x, power=power)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># normalization</span></span><br><span class="line">        ndarr = normalize_feature(df).as_matrix()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># add intercept term</span></span><br><span class="line">        <span class="keyword">return</span> np.insert(ndarr, <span class="number">0</span>, np.ones(ndarr.shape[<span class="number">0</span>]), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [prepare(x) <span class="keyword">for</span> x <span class="keyword">in</span> args]</span><br></pre></td></tr></table></figure></p>
<p>尝试不同的λ来观察学习曲线<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y, Xval, yval, Xtest, ytest = load_data()</span><br><span class="line">X_poly, Xval_poly, Xtest_poly= prepare_poly_data(X, Xval, Xtest, power=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">plot_learning_curve(X_poly, y, Xval_poly, yval, l=<span class="number">0</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plot_learning_curve(X_poly, y, Xval_poly, yval, l=<span class="number">1</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plot_learning_curve(X_poly, y, Xval_poly, yval, l=<span class="number">100</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>当λ取0时，也就是没有正则项时，可以看到训练的代价太低了，不真实. 这是 过拟合了</p>
<p><img src="Figure_5.png" alt="image"></p>
<p>当训练代价增加了些，不再是0了。 稍减轻了过拟合</p>
<p><img src="Figure_6.png" alt="image"></p>
<p>当λ取100时，正则化过多，变成了欠拟合。</p>
<p><img src="Figure_7.png" alt="image"></p>
<h1 id="最优λ"><a href="#最优λ" class="headerlink" title="最优λ"></a>最优λ</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 找到最佳拟合</span></span><br><span class="line">l_candidate = [<span class="number">0</span>, <span class="number">0.001</span>, <span class="number">0.003</span>, <span class="number">0.01</span>, <span class="number">0.03</span>, <span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>]</span><br><span class="line">training_cost, cv_cost = [], []</span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> l_candidate:</span><br><span class="line">    theta = np.ones(X_poly.shape[<span class="number">1</span>])</span><br><span class="line">    theta = linear_regression_np(theta, X_poly, y, l)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    tc = cost(theta, X_poly, y)</span><br><span class="line">    cv = cost(theta, Xval_poly, yval)</span><br><span class="line">    training_cost.append(tc)</span><br><span class="line">    cv_cost.append(cv)</span><br><span class="line"></span><br><span class="line">plt.plot(l_candidate, training_cost, label=<span class="string">'training'</span>)</span><br><span class="line">plt.plot(l_candidate, cv_cost, label=<span class="string">'cross validation'</span>)</span><br><span class="line">plt.legend(loc=<span class="number">2</span>)</span><br><span class="line">plt.xlabel(<span class="string">'lambda'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># best cv I got from all those candidates</span></span><br><span class="line">l_candidate[np.argmin(cv_cost)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># use test data to compute the cost</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> l_candidate:</span><br><span class="line">    theta = np.ones(X_poly.shape[<span class="number">1</span>])</span><br><span class="line">    theta = linear_regression_np(theta, X_poly, y, l)[<span class="number">0</span>]</span><br><span class="line">    print(<span class="string">'test cost(l=&#123;&#125;) = &#123;&#125;'</span>.format(l, cost(theta, Xtest_poly, ytest)))</span><br></pre></td></tr></table></figure>
<p><img src="Figure_8.png" alt="image"></p>
<figure class="highlight subunit"><table><tr><td class="code"><pre><span class="line"><span class="keyword">test </span>cost(l=0) = 9.799399498688892</span><br><span class="line"><span class="keyword">test </span>cost(l=0.001) = 11.054987989655938</span><br><span class="line"><span class="keyword">test </span>cost(l=0.003) = 11.249198861537238</span><br><span class="line"><span class="keyword">test </span>cost(l=0.01) = 10.879605199670008</span><br><span class="line"><span class="keyword">test </span>cost(l=0.03) = 10.022734920552129</span><br><span class="line"><span class="keyword">test </span>cost(l=0.1) = 8.632060998872074</span><br><span class="line"><span class="keyword">test </span>cost(l=0.3) = 7.336602384055533</span><br><span class="line"><span class="keyword">test </span>cost(l=1) = 7.46630349664086</span><br><span class="line"><span class="keyword">test </span>cost(l=3) = 11.643928200535115</span><br><span class="line"><span class="keyword">test </span>cost(l=10) = 27.715080216719304</span><br></pre></td></tr></table></figure>
<p>调参后， lambda = 0.3 是最优选择，这个时候测试代价最小</p>
]]></content>
      <categories>
        <category>机器学习习题</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>ex7-k means and PCA</title>
    <url>/2018/11/07/ex7-k%20means%20and%20PCA/</url>
    <content><![CDATA[<p>AndrewNg 机器学习习题ex7-k means and PCA</p>
<p><a href="https://github.com/Voidmort/Machine-Learning-Python-Code/tree/master/Data" target="_blank" rel="noopener">练习用数据</a></p>
<p>在本练习中，我们将实现K-means聚类，并使用它来压缩图像。 我们将从一个简单的2D数据集开始，以了解K-means是如何工作的，然后我们将其应用于图像压缩。 我们还将对主成分分析进行实验，并了解如何使用它来找到面部图像的低维表示。</p>
<h1 id="Implementing-K-means"><a href="#Implementing-K-means" class="headerlink" title="Implementing K-means"></a>Implementing K-means</h1><p>我们将实施和应用K-means到一个简单的二维数据集，以获得一些直观的工作原理。 K-means是一个迭代的，无监督的聚类算法，将类似的实例组合成簇。 该算法通过猜测每个簇的初始聚类中心开始，然后重复将实例分配给最近的簇，并重新计算该簇的聚类中心。 我们要实现的第一部分是找到数据中每个实例最接近的聚类中心的函数。</p>
<h2 id="可视化数据"><a href="#可视化数据" class="headerlink" title="可视化数据"></a>可视化数据</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sb</span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line">data = loadmat(<span class="string">'data/ex7data2.mat'</span>)</span><br><span class="line">X = data[<span class="string">'X'</span>]</span><br><span class="line">data2 = pd.DataFrame(data.get(<span class="string">'X'</span>), columns=[<span class="string">'X1'</span>, <span class="string">'X2'</span>])</span><br><span class="line">print(data2.head())</span><br><span class="line">sb.set(context=<span class="string">"notebook"</span>, style=<span class="string">"white"</span>)</span><br><span class="line">sb.lmplot(<span class="string">'X1'</span>, <span class="string">'X2'</span>, data=data2, fit_reg=<span class="keyword">False</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Figure_1.png" alt="image"></p>
<h2 id="Finding-closest-centroids"><a href="#Finding-closest-centroids" class="headerlink" title="Finding closest centroids"></a>Finding closest centroids</h2><p>$c^{(i)} := j\ that\ minimizes\ ||x^i - u_j||^2 $</p>
<p>计算每一个特征值到所选取的聚类中心的距离，纪录最短距离的聚类中心编号。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_closest_centroids</span><span class="params">(X, centroids)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">0</span>]</span><br><span class="line">    k = centroids.shape[<span class="number">0</span>]</span><br><span class="line">    idx = np.zeros(m)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        min_dist = <span class="number">1000000</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">            dist = np.sum((X[i, :] - centroids[j, :]) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> dist &lt; min_dist:</span><br><span class="line">                min_dist = dist</span><br><span class="line"></span><br><span class="line">                idx[i] = j</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> idx</span><br><span class="line">  </span><br><span class="line">    </span><br><span class="line">initial_centroids = initial_centroids = np.array([[<span class="number">3</span>, <span class="number">3</span>], [<span class="number">6</span>, <span class="number">2</span>], [<span class="number">8</span>, <span class="number">5</span>]])</span><br><span class="line">idx = find_closest_centroids(X, initial_centroids)</span><br><span class="line">print(idx[: <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<p>[0. 2. 1.]</p>
<p>输出与文本中的预期值匹配（记住我们的数组是从零开始索引的，而不是从一开始索引的，所以值比练习中的值低一个）。</p>
<h2 id="Computing-centroid-means"><a href="#Computing-centroid-means" class="headerlink" title="Computing centroid means"></a>Computing centroid means</h2><p>接下来，我们需要一个函数来计算簇的聚类中心。 聚类中心只是当前分配给簇的所有样本的平均值。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_centroids</span><span class="params">(X, idx, k)</span>:</span></span><br><span class="line">    m, n = X.shape</span><br><span class="line">    centroids = np.zeros((k, n))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        indices = np.where(idx == i)</span><br><span class="line">        centroids[i, :] = (np.sum(X[indices, :], axis=<span class="number">1</span>) / len(indices[<span class="number">0</span>])).ravel()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> centroids</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(compute_centroids(X, idx, <span class="number">3</span>))</span><br></pre></td></tr></table></figure></p>
<p>[[2.42830111 3.15792418]<br> [5.81350331 2.63365645]<br> [7.11938687 3.6166844 ]]<br> 此输出也符合练习中的预期值。 下一部分涉及实际运行该算法的一些迭代次数和可视化结果。 这个步骤是由于并不复杂，我将从头开始构建它。 为了运行算法，我们只需要在将样本分配给最近的簇并重新计算簇的聚类中心。</p>
<h2 id="K-means-on-example-dataset"><a href="#K-means-on-example-dataset" class="headerlink" title="K-means on example dataset"></a>K-means on example dataset</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">centroids_trace = np.empty(shape=[<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_k_means</span><span class="params">(X, initial_centroids, max_iters)</span>:</span></span><br><span class="line">    m, n = X.shape</span><br><span class="line">    k = initial_centroids.shape[<span class="number">0</span>]</span><br><span class="line">    idx = np.zeros(m)</span><br><span class="line">    centroids = initial_centroids</span><br><span class="line">    <span class="keyword">global</span> centroids_trace</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_iters):</span><br><span class="line">        idx = find_closest_centroids(X, centroids)</span><br><span class="line">        centroids = compute_centroids(X, idx, k)</span><br><span class="line">        centroids_trace = np.append(centroids_trace, centroids, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> idx, centroids</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_centroids</span><span class="params">(X, k)</span>:</span></span><br><span class="line">    m, n = X.shape</span><br><span class="line">    centroids = np.zeros((k, n))</span><br><span class="line">    idx = np.random.randint(<span class="number">0</span>, m, k)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        centroids[i, :] = X[idx[i], :]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> centroids</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">initial_centroids = init_centroids(X, <span class="number">3</span>)</span><br><span class="line">idx, centroids = run_k_means(X, initial_centroids, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">cluster1 = X[np.where(idx == <span class="number">0</span>)[<span class="number">0</span>], :]</span><br><span class="line">cluster2 = X[np.where(idx == <span class="number">1</span>)[<span class="number">0</span>], :]</span><br><span class="line">cluster3 = X[np.where(idx == <span class="number">2</span>)[<span class="number">0</span>], :]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">ax.scatter(cluster1[:, <span class="number">0</span>], cluster1[:, <span class="number">1</span>], s=<span class="number">30</span>, color=<span class="string">'r'</span>, label=<span class="string">'Cluster 1'</span>)</span><br><span class="line">ax.scatter(cluster2[:, <span class="number">0</span>], cluster2[:, <span class="number">1</span>], s=<span class="number">30</span>, color=<span class="string">'g'</span>, label=<span class="string">'Cluster 2'</span>)</span><br><span class="line">ax.scatter(cluster3[:, <span class="number">0</span>], cluster3[:, <span class="number">1</span>], s=<span class="number">30</span>, color=<span class="string">'b'</span>, label=<span class="string">'Cluster 3'</span>)</span><br><span class="line">ax.legend()</span><br><span class="line"></span><br><span class="line">x = centroids_trace[:, <span class="number">0</span>]</span><br><span class="line">y = centroids_trace[:, <span class="number">1</span>]</span><br><span class="line">ax.scatter(x, y, color=<span class="string">'black'</span>, s=<span class="number">50</span>, zorder=<span class="number">2</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Figure_2.png" alt="image"></p>
<h1 id="Image-compression-with-K-means"><a href="#Image-compression-with-K-means" class="headerlink" title="Image compression with K-means"></a>Image compression with K-means</h1><p>我们的下一个任务是将K-means应用于图像压缩。 从下面的演示可以看到，我们可以使用聚类来找到最具代表性的少数颜色，并使用聚类分配将原始的24位颜色映射到较低维的颜色空间。</p>
<p>下面是我们要压缩的图像。</p>
<p><img src="bird_small.png" alt="image"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">image_data = loadmat(<span class="string">'data/bird_small.mat'</span>)</span><br><span class="line">print(image_data)</span><br><span class="line">A = image_data[<span class="string">'A'</span>]</span><br><span class="line">print(A.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line"><span class="comment"># normalize value ranges</span></span><br><span class="line">A = A / <span class="number">255</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># reshape the array</span></span><br><span class="line">print((A.shape[<span class="number">0</span>] * A.shape[<span class="number">1</span>], A.shape[<span class="number">2</span>]))</span><br><span class="line">X = np.reshape(A, (<span class="number">128</span>*<span class="number">128</span>, <span class="number">3</span>))</span><br><span class="line">print(X.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># randomly initalize the centroids</span></span><br><span class="line">initial_centroids = init_centroids(X, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run the algorithm</span></span><br><span class="line">idx, centroids = run_k_means(X, initial_centroids, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># gor the closet centroids one last time</span></span><br><span class="line">idx = find_closest_centroids(X, centroids)</span><br><span class="line"></span><br><span class="line"><span class="comment"># map each poxel to the centroid value</span></span><br><span class="line">X_recovered = centroids[idx.astype(int), :]</span><br><span class="line">print(X_recovered.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># reshape to the original dimensions</span></span><br><span class="line">X_recovered = np.reshape(X_recovered, (A.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>], A.shape[<span class="number">2</span>]))</span><br></pre></td></tr></table></figure>
<p>用scikit-learn来实现K-means<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans  <span class="comment"># 导入kmeans库</span></span><br><span class="line"></span><br><span class="line">model = KMeans(n_clusters=<span class="number">16</span>, n_init=<span class="number">100</span>, n_jobs=<span class="number">1</span>)</span><br><span class="line">model.fit(X)</span><br><span class="line">centroids = model.cluster_centers_</span><br><span class="line">print(centroids.shape)</span><br><span class="line">C = model.predict(X)</span><br><span class="line">print(C.shape)</span><br><span class="line">print(centroids[C].shape)</span><br><span class="line"></span><br><span class="line">compressed_pic = centroids[C].reshape((<span class="number">128</span>, <span class="number">128</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">ax[<span class="number">0</span>].imshow(A)</span><br><span class="line">ax[<span class="number">1</span>].imshow(X_recovered)</span><br><span class="line">ax[<span class="number">2</span>].imshow(compressed_pic)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="Figure_3.png" alt="image"></p>
<p>通过K-means算法，我们让图像以更少的色彩来显示实现压缩，但是图像的主要特征仍然存在。</p>
<h1 id="Principal-component-analysis（主成分分析）"><a href="#Principal-component-analysis（主成分分析）" class="headerlink" title="Principal component analysis（主成分分析）"></a>Principal component analysis（主成分分析）</h1><p>PCA是在数据集中找到“主成分”或最大方差方向的线性变换。 它可以用于降维。 在本练习中，我们首先负责实现PCA并将其应用于一个简单的二维数据集，以了解它是如何工作的。 我们从加载和可视化数据集开始。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = loadmat(<span class="string">'data/ex7data1.mat'</span>)</span><br><span class="line">X = data[<span class="string">'X'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="comment"># normalize the feature</span></span><br><span class="line">    X = (X - X.mean()) / X.std()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the covariance matrix</span></span><br><span class="line">    X = np.matrix(X)</span><br><span class="line">    cov = (X.T * X) / X.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># perform SVD</span></span><br><span class="line">    U, S, V = np.linalg.svd(cov)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> U, S, V</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">U, S, V = pca(X)</span><br><span class="line">print(U, S, V)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">project_data</span><span class="params">(X, U, k)</span>:</span></span><br><span class="line">    U_reduced = U[:, :k]</span><br><span class="line">    <span class="keyword">return</span> np.dot(X, U_reduced)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Z = project_data(X, U, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recover_data</span><span class="params">(Z, U, k)</span>:</span></span><br><span class="line">    U_reduced = U[:, :k]</span><br><span class="line">    <span class="keyword">return</span> np.dot(Z, U_reduced.T)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X_recovered = recover_data(Z, U, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">ax[<span class="number">0</span>].scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>])</span><br><span class="line">ax[<span class="number">1</span>].scatter(list(X_recovered[:, <span class="number">0</span>]), list(X_recovered[:, <span class="number">1</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="Figure_4.png" alt="image"></p>
<p>请注意，第一主成分的投影轴基本上是数据集中的对角线。 当我们将数据减少到一个维度时，我们失去了该对角线周围的变化，所以在我们的再现中，一切都沿着该对角线。</p>
<p>我们在此练习中的最后一个任务是将PCA应用于脸部图像。 通过使用相同的降维技术，我们可以使用比原始图像少得多的数据来捕获图像的“本质”</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_n_image</span><span class="params">(X, n)</span>:</span></span><br><span class="line">    <span class="string">""" plot first n images</span></span><br><span class="line"><span class="string">    n has to be a square number</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    pic_size = int(np.sqrt(X.shape[<span class="number">1</span>]))</span><br><span class="line">    grid_size = int(np.sqrt(n))</span><br><span class="line">    first_n_images = X[:n, :]</span><br><span class="line"></span><br><span class="line">    fig, ax_array = plt.subplots(nrows=grid_size, ncols=grid_size,</span><br><span class="line">                                    sharey=<span class="keyword">True</span>, sharex=<span class="keyword">True</span>, figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> range(grid_size):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> range(grid_size):</span><br><span class="line">            ax_array[r, c].imshow(first_n_images[grid_size * r + c].reshape((pic_size, pic_size)).T, cmap=plt.cm.gray)</span><br><span class="line">            plt.xticks(np.array([]))</span><br><span class="line">            plt.yticks(np.array([]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">faces = loadmat(<span class="string">'data/ex7faces.mat'</span>)</span><br><span class="line">X = faces[<span class="string">'X'</span>]</span><br><span class="line">print(X.shape)</span><br><span class="line">plot_n_image(X, <span class="number">100</span>)</span><br><span class="line">face1 = np.reshape(X[<span class="number">1</span>,:], (<span class="number">32</span>, <span class="number">32</span>)).T</span><br><span class="line"></span><br><span class="line">U, S, V = pca(X)</span><br><span class="line">Z = project_data(X, U, <span class="number">100</span>)</span><br><span class="line">print(Z.shape)</span><br><span class="line">X_recovered = recover_data(Z, U, <span class="number">100</span>)</span><br><span class="line">face2 = np.reshape(X_recovered[<span class="number">1</span>,:], (<span class="number">32</span>, <span class="number">32</span>)).T</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">ax[<span class="number">0</span>].imshow(face1, cmap=plt.cm.gray)</span><br><span class="line">ax[<span class="number">1</span>].imshow(face2, cmap=plt.cm.gray)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 计算平均均方差误差与训练集方差的比例</span></span><br><span class="line">print(np.sum(S[:<span class="number">100</span>]) / np.sum(S))  <span class="comment"># 0.9434273519364477</span></span><br></pre></td></tr></table></figure>
<p><img src="Figure_5.png" alt="image"></p>
<p>我们把1024个特征缩减到100个时还保留了94%的差异值。</p>
]]></content>
      <categories>
        <category>机器学习习题</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>ex8-anomaly detection and recommendation</title>
    <url>/2018/11/20/ex8-anomaly%20detection%20and%20recommendation/</url>
    <content><![CDATA[<p>AndrewNg 机器学习习题ex6-anomaly detection and recommendation</p>
<p>这是最后一个练习了，共有两个算法，第一个是异常检测，第二个是推荐系统。</p>
<h1 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h1><p>之前写过了这里就不再重复了：Python实现异常检测算法</p>
<h1 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h1><p>推荐系统使用的算法就是协同过滤（collaborative ltering learning algorithm）</p>
<p>首先来看提供的数据都有些什么，更具PDF可知，有5个文件是我们需要的数据集合。</p>
<table>
<thead>
<tr>
<th>数据集名称</th>
<th>内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>movie_ids.txt</td>
<td>电影的列表</td>
</tr>
<tr>
<td>ex8data1.mat</td>
<td>用于异常检测的第一个示例数据集</td>
</tr>
<tr>
<td>ex8data2.mat</td>
<td>用于异常检测的第二个示例数据集</td>
</tr>
<tr>
<td>ex8_movies.mat</td>
<td>电影评论数据集</td>
</tr>
<tr>
<td>ex8_movieParams.mat</td>
<td>为调试提供的参数</td>
</tr>
</tbody>
</table>
<h1 id="导入库和检查数据集"><a href="#导入库和检查数据集" class="headerlink" title="导入库和检查数据集"></a>导入库和检查数据集</h1><p>ex8_movies.mat中有两个标签的数据，Y是1682个电影的评分，每个电影有943条五个级别的评分，R是一个和Y相同维度的二进制数组，0代表评过分，1代表没评分。</p>
<p>% Notes: X - num_movies (1682)  x num_features (10) matrix of movie features<br>%        Theta - num_users (943)  x num_features (10) matrix of user features<br>%        Y - num_movies x num_users matrix of user ratings of movies<br>%        R - num_movies x num_users matrix, where R(i, j) = 1 if the<br>%            i-th movie was rated by the j-th user</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">sns.set(context=<span class="string">"notebook"</span>, style=<span class="string">"white"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Y是包含从1到5的等级的（数量的电影x数量的用户）数组.R是包含指示用户是否给电影评分的二进制值的“指示符”数组。</span></span><br><span class="line">movies_mat = sio.loadmat(<span class="string">'./data/ex8_movies.mat'</span>);</span><br><span class="line">Y, R = movies_mat.get(<span class="string">'Y'</span>), movies_mat.get(<span class="string">'R'</span>)</span><br><span class="line">print(Y.shape, R.shape)</span><br><span class="line"><span class="comment"># (1682, 943) (1682, 943)</span></span><br><span class="line"></span><br><span class="line">m, u = Y.shape</span><br><span class="line"><span class="comment"># m: how many movies</span></span><br><span class="line"><span class="comment"># u: how many users</span></span><br><span class="line">n = <span class="number">10</span></span><br><span class="line"><span class="comment"># how many features for a movie</span></span><br><span class="line"></span><br><span class="line">param_mat = sio.loadmat(<span class="string">'./data/ex8_movieParams.mat'</span>)</span><br><span class="line">theta, X = param_mat.get(<span class="string">'Theta'</span>), param_mat.get(<span class="string">'X'</span>)</span><br><span class="line">print(theta.shape, X.shape)</span><br><span class="line"><span class="comment"># (943, 10) (1682, 10)</span></span><br></pre></td></tr></table></figure>
<h1 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h1><p><img src="rcmd_cost.png" alt="Cost"></p>
<p>在对feature运算时，我们先把params serialize为只有一个维度的数组，通过deserialize函数来恢复为原状。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">serialize</span><span class="params">(X, theta)</span>:</span></span><br><span class="line">    <span class="comment"># serialize 2 matrix</span></span><br><span class="line">    <span class="comment"># X(move, feature), (1682, 10): movie features</span></span><br><span class="line">    <span class="comment"># theta (user, feature), (943, 10): user preference</span></span><br><span class="line">    <span class="comment"># 1682*10 + 943*10 = (26250,)</span></span><br><span class="line">    <span class="keyword">return</span> np.concatenate((X.ravel(), theta.ravel()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deserialize</span><span class="params">(param, n_movie, n_user, n_featuers)</span>:</span></span><br><span class="line">    <span class="comment"># into ndarray of X(1682, 10), theta(943, 10)</span></span><br><span class="line">    <span class="keyword">return</span> param[:n_movie * n_featuers].reshape(n_movie, n_featuers),\</span><br><span class="line">            param[n_movie * n_featuers:].reshape(n_user, n_featuers)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># recomendation fn</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(param, Y, R, n_features)</span>:</span></span><br><span class="line">    <span class="string">"""compute cost for every r(i, j) = 1</span></span><br><span class="line"><span class="string">        arg:</span></span><br><span class="line"><span class="string">            param: serialized X, theta</span></span><br><span class="line"><span class="string">            Y (movie, user), (1682, 943): (movie, user) rating</span></span><br><span class="line"><span class="string">            R (movie, user), (1682, 943): (movie, user) has rating</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># theta (user, feat)</span></span><br><span class="line">    <span class="comment"># X(movie, feature), (1682, 10): movie features</span></span><br><span class="line">    n_movie, n_user = Y.shape</span><br><span class="line">    X, theta = deserialize(param, n_movie, n_user, n_features)</span><br><span class="line">    inner = np.multiply(X @ theta.T - Y, R)</span><br><span class="line">    <span class="keyword">return</span> np.power(inner, <span class="number">2</span>).sum() / <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span><span class="params">(param, Y, R, n_features)</span>:</span></span><br><span class="line">    <span class="comment"># theta (user, feature), (943, 10): user preference</span></span><br><span class="line">    <span class="comment"># X(movie, feature), (1682, 10): movie features</span></span><br><span class="line">    n_movies, n_user = Y.shape</span><br><span class="line">    X, theta = deserialize(param, n_movies, n_user, n_features)</span><br><span class="line"></span><br><span class="line">    inner = np.multiply(X @ theta.T - Y, R)  <span class="comment"># (1682, 943)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># X_grad (1682, 10)</span></span><br><span class="line">    X_grad = inner @ theta</span><br><span class="line"></span><br><span class="line">    <span class="comment"># theta_grad (943, 10)</span></span><br><span class="line">    theta_grad = inner.T @ X</span><br><span class="line"></span><br><span class="line">    <span class="comment"># roll them together and return</span></span><br><span class="line">    <span class="keyword">return</span> serialize(X_grad, theta_grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_cost</span><span class="params">(param, Y, R, n_features, l=<span class="number">1</span>)</span>:</span></span><br><span class="line">    reg_term = np.power(param, <span class="number">2</span>).sum() * (<span class="number">1</span>/<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> cost(param, Y, R, n_features) + reg_term</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularized_gradient</span><span class="params">(param, Y, R, n_features, l=<span class="number">1</span>)</span>:</span></span><br><span class="line">    grad = gradient(param, Y, R, n_features)</span><br><span class="line">    reg_term = l * param</span><br><span class="line">    <span class="keyword">return</span> grad + reg_term</span><br></pre></td></tr></table></figure>
<p>按照练习8中的参数cost输出为22，验证结果“</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按照练习中给出计算结果为22</span></span><br><span class="line">users = <span class="number">4</span></span><br><span class="line">movies = <span class="number">5</span></span><br><span class="line">features = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">X_sub = X[:movies, :features]</span><br><span class="line">theta_sub = theta[:users, :features]</span><br><span class="line">Y_sub = Y[:movies, :users]</span><br><span class="line">R_sub = R[:movies, :users]</span><br><span class="line"></span><br><span class="line">param_sub = serialize(X_sub, theta_sub)</span><br><span class="line">c = cost(param_sub, Y_sub, R_sub, features)</span><br><span class="line">print(c)  <span class="comment"># 22.224603725685675</span></span><br></pre></td></tr></table></figure>
<p>计算一下总的cost</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># total readl params</span></span><br><span class="line">param = serialize(X, theta)</span><br><span class="line"><span class="comment"># total cost</span></span><br><span class="line">total_cost = cost(param, Y, R, <span class="number">10</span>)</span><br><span class="line">print(total_cost)  <span class="comment"># 27918.64012454421</span></span><br></pre></td></tr></table></figure>
<h1 id="gradient-function"><a href="#gradient-function" class="headerlink" title="gradient function"></a>gradient function</h1><p><img src="rcmd_gradient.png" alt="Gradient"></p>
<p><img src="rcmd_vectorized_grad.png" alt="Gradient"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_movie, n_user = Y.shape</span><br><span class="line">X_grad, theta_grad = deserialize(gradient(param, Y, R, <span class="number">10</span>),</span><br><span class="line">                                n_movie, n_user, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> X_grad.shape == X.shape</span><br><span class="line"><span class="keyword">assert</span> theta_grad.shape == theta.shape</span><br></pre></td></tr></table></figure>
<h1 id="regularized-cost-and-gradient"><a href="#regularized-cost-and-gradient" class="headerlink" title="regularized cost and gradient"></a>regularized cost and gradient</h1><p><img src="rcmd_reg_gradient.png" alt="Gradient"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># regularized cost</span></span><br><span class="line"><span class="comment"># in the ex8_confi.m, lambda = 1.5, and it's using sub data set</span></span><br><span class="line">reg_cost = regularized_cost(param_sub, Y_sub, R_sub, features, l=<span class="number">1.5</span>)</span><br><span class="line">print(reg_cost)  <span class="comment"># 28.304238738078038</span></span><br><span class="line"><span class="comment"># total regularized cost</span></span><br><span class="line">total_cost = regularized_cost(param, Y, R, <span class="number">10</span>, l=<span class="number">1</span>)</span><br><span class="line">print(total_cost)   <span class="comment"># 32520.682450229557</span></span><br><span class="line"></span><br><span class="line">n_movie, n_user = Y.shape</span><br><span class="line"></span><br><span class="line">X_grad, theta_grad = deserialize(regularized_gradient(param, Y, R, <span class="number">10</span>),</span><br><span class="line">                                                      n_movie, n_user, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> X_grad.shape == X.shape</span><br><span class="line"><span class="keyword">assert</span> theta_grad.shape == theta.shape</span><br></pre></td></tr></table></figure>
<h1 id="parse-movie-id-txt"><a href="#parse-movie-id-txt" class="headerlink" title="parse movie_id.txt"></a>parse movie_id.txt</h1><figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line"><span class="comment"># parse movie_id.txt</span></span><br><span class="line">movie_list = []</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'./data/movie_ids.txt'</span>, encoding=<span class="string">'latin-1'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">line</span> <span class="keyword">in</span> f:</span><br><span class="line">        tokens = <span class="built_in">line</span>.strip().<span class="built_in">split</span>(<span class="string">' '</span>)</span><br><span class="line">        movie_list.append(<span class="string">' '</span>.join(tokens[<span class="number">1</span>:]))</span><br><span class="line"></span><br><span class="line">movie_list = np.array(movie_list)</span><br></pre></td></tr></table></figure>
<h1 id="给电影打分"><a href="#给电影打分" class="headerlink" title="给电影打分"></a>给电影打分</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># reproduce my ratings</span></span><br><span class="line">ratings = np.zeros(<span class="number">1682</span>)</span><br><span class="line">ratings[<span class="number">0</span>] = <span class="number">4</span></span><br><span class="line">ratings[<span class="number">6</span>] = <span class="number">3</span></span><br><span class="line">ratings[<span class="number">11</span>] = <span class="number">5</span></span><br><span class="line">ratings[<span class="number">53</span>] = <span class="number">4</span></span><br><span class="line">ratings[<span class="number">63</span>] = <span class="number">5</span></span><br><span class="line">ratings[<span class="number">65</span>] = <span class="number">3</span></span><br><span class="line">ratings[<span class="number">68</span>] = <span class="number">5</span></span><br><span class="line">ratings[<span class="number">97</span>] = <span class="number">2</span></span><br><span class="line">ratings[<span class="number">182</span>] = <span class="number">4</span></span><br><span class="line">ratings[<span class="number">225</span>] = <span class="number">5</span></span><br><span class="line">ratings[<span class="number">354</span>] = <span class="number">5</span></span><br></pre></td></tr></table></figure>
<h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>把我们的评价插入到所有电影的评分中去，把参数theta和X处理为正态分布。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># prepare data</span></span><br><span class="line"><span class="comment"># now I become user 0</span></span><br><span class="line">Y, R = movies_mat.get(<span class="string">'Y'</span>), movies_mat.get(<span class="string">'R'</span>)</span><br><span class="line">Y = np.insert(Y, <span class="number">0</span>, ratings, axis=<span class="number">1</span>)</span><br><span class="line">R = np.insert(R, <span class="number">0</span>, ratings != <span class="number">0</span>, axis=<span class="number">1</span>)</span><br><span class="line">print(Y.shape)  <span class="comment"># (1682, 944)</span></span><br><span class="line">print(R.shape)  <span class="comment"># (1682, 944)</span></span><br><span class="line"></span><br><span class="line">n_features = <span class="number">50</span></span><br><span class="line">n_movie, n_user = Y.shape</span><br><span class="line">l = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为正态分布</span></span><br><span class="line">X = np.random.standard_normal((n_movie, n_features))</span><br><span class="line">theta = np.random.standard_normal((n_user, n_features))</span><br><span class="line"></span><br><span class="line">print(X.shape, theta.shape)  <span class="comment"># (1682, 50) (944, 50)</span></span><br><span class="line"></span><br><span class="line">param = serialize(X, theta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># normalized ratings</span></span><br><span class="line">Y_norm = Y - Y.mean()</span><br><span class="line">print(Y_norm.mean())  <span class="comment"># 4.6862111343939375e-17</span></span><br></pre></td></tr></table></figure>
<h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><figure class="highlight oxygene"><table><tr><td class="code"><pre><span class="line"># training</span><br><span class="line">import scipy.optimize <span class="keyword">as</span> opt</span><br><span class="line">res = opt.minimize(fun=regularized_cost,</span><br><span class="line">                   x0=param,</span><br><span class="line">                   args=(Y_norm, R, n_features, l),</span><br><span class="line">                   <span class="function"><span class="keyword">method</span>='<span class="title">TNC</span>',</span></span><br><span class="line"><span class="function">                   <span class="title">jac</span>=<span class="title">regularized_gradient</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(res)</span></span></span><br></pre></td></tr></table></figure>
<p>稍等一会儿得到一下结果<br><figure class="highlight subunit"><table><tr><td class="code"><pre><span class="line">    fun: 24268.448311691616</span><br><span class="line">    jac: array([<span class="string">-12</span>.49378802,  14.209063  ,  <span class="string">-6</span>.75343791, ...,   0.61519582,</span><br><span class="line">       <span class="string">-1</span>.32599207,   0.58813019])</span><br><span class="line">message: 'Converged (|f_n-f_(n<span class="string">-1</span>)| ~= 0)'</span><br><span class="line">   nfev: 219</span><br><span class="line">    nit: 14</span><br><span class="line"> status: 1</span><br><span class="line"><span class="keyword">success: </span>True</span><br><span class="line">      x: array([<span class="string">-0</span>.30795529,  0.88620348, <span class="string">-0</span>.10899471, ...,  0.18986581,</span><br><span class="line">      <span class="string">-0</span>.28537047, <span class="string">-0</span>.11540767])</span><br></pre></td></tr></table></figure></p>
<p>检查推荐结果<br>y=np.argsort(x)将x中的元素从小到大排列，提取其对应的index(索引)，然后输出到y<br><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">X<span class="emphasis">_trained, theta_</span>trained = deserialize(res.x, n<span class="emphasis">_movie, n_</span>user, n_features)</span><br><span class="line">print(X<span class="emphasis">_trained.shape, theta_</span>trained.shape)</span><br><span class="line"></span><br><span class="line">prediction = X<span class="emphasis">_trained @ theta_</span>trained.T</span><br><span class="line">my_preds = prediction[:, 0] + Y.mean()</span><br><span class="line"></span><br><span class="line">idx = np.argsort(my_preds)[::-1]  # descending order</span><br><span class="line">print(idx.shape)</span><br><span class="line"></span><br><span class="line"><span class="section"># top ten idx</span></span><br><span class="line">my_preds[<span class="string">idx</span>][<span class="symbol">:10</span>]</span><br><span class="line"></span><br><span class="line">for m in movie_list[<span class="string">idx</span>][<span class="symbol">:10</span>]:</span><br><span class="line"><span class="code">    print(m)</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight less"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">Godfather</span>, <span class="selector-tag">The</span> (<span class="number">1972</span>)</span><br><span class="line"><span class="selector-tag">Forrest</span> <span class="selector-tag">Gump</span> (<span class="number">1994</span>)</span><br><span class="line"><span class="selector-tag">Star</span> <span class="selector-tag">Wars</span> (<span class="number">1977</span>)</span><br><span class="line"><span class="selector-tag">Titanic</span> (<span class="number">1997</span>)</span><br><span class="line"><span class="selector-tag">Shawshank</span> <span class="selector-tag">Redemption</span>, <span class="selector-tag">The</span> (<span class="number">1994</span>)</span><br><span class="line"><span class="selector-tag">Raiders</span> <span class="selector-tag">of</span> <span class="selector-tag">the</span> <span class="selector-tag">Lost</span> <span class="selector-tag">Ark</span> (<span class="number">1981</span>)</span><br><span class="line"><span class="selector-tag">Return</span> <span class="selector-tag">of</span> <span class="selector-tag">the</span> <span class="selector-tag">Jedi</span> (<span class="number">1983</span>)</span><br><span class="line"><span class="selector-tag">Usual</span> <span class="selector-tag">Suspects</span>, <span class="selector-tag">The</span> (<span class="number">1995</span>)</span><br><span class="line"><span class="selector-tag">Braveheart</span> (<span class="number">1995</span>)</span><br><span class="line"><span class="selector-tag">Empire</span> <span class="selector-tag">Strikes</span> <span class="selector-tag">Back</span>, <span class="selector-tag">The</span> (<span class="number">1980</span>)</span><br></pre></td></tr></table></figure>
<p>每次得到的结果有个别差别，但是七成时没有变化的。</p>
]]></content>
      <categories>
        <category>机器学习习题</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>gRPC</title>
    <url>/2020/07/24/gRPC/</url>
    <content><![CDATA[<h1 id="什么是RPC"><a href="#什么是RPC" class="headerlink" title="什么是RPC"></a>什么是RPC</h1><p><a href="https://www.zhihu.com/question/25536695/answer/221638079" target="_blank" rel="noopener">参考知乎 谁能用通俗的语言解释一下什么是 RPC 框架？</a></p>
<p>之前只知道IPC，是指进程间通信(Inter Process Communication)，至少两个进程或线程间传送数据或信号的一些技术或方法，RPC和IPC类似，百科解释RPC（Remote Procedure Call）为远程过程调用，简单理解为，就是像调用本地函数一样调用远程函数，例如，有两台服务器A,B，一个应用部署在A服务器上，想要调用B服务器上的函数和方法，由于不在一个内存空间，不能直接调用，需要通过网络来表达调用的语义和传达调用的数据。</p>
<h1 id="RPC原理"><a href="#RPC原理" class="headerlink" title="RPC原理"></a>RPC原理</h1><p>首先客户端需要告诉服务器，需要调用的函数，这里函数和进程ID存在一个映射，客户端远程调用时，需要查一下函数，找到对应的ID，然后执行函数的代码。</p>
<p>客户端需要把本地参数传给远程函数，本地调用的过程中，直接压栈即可，但是在远程调用过程中不再同一个内存里，无法直接传递函数的参数，因此需要客户端把参数转换成字节流，传给服务端，然后服务端将字节流转换成自身能读取的格式，是一个序列化和反序列化的过程。</p>
<p>数据准备好了之后，网络传输层需要把调用的ID和序列化后的参数传给服务端，然后把计算好的结果序列化传给客户端，因此TCP层即可完成上述过程，gRPC中采用的是HTTP2协议。</p>
<p><img src="image3.png" alt="image"></p>
<h1 id="RPC框架对比"><a href="#RPC框架对比" class="headerlink" title="RPC框架对比"></a>RPC框架对比</h1><p>Dubbo 是阿里巴巴公司开源的一个Java高性能优秀的服务框架，使得应用可通过高性能的RPC 实现服务的输出和输入功能，可以和Spring框架无缝集成。不过，略有遗憾的是，据说在淘宝内部，dubbo由于跟淘宝另一个类似的框架HSF（非开源）有竞争关系，导致dubbo团队已经解散，反到是当当网的扩展版本Dubbox仍在持续发展，墙内开花墙外香。Dubbox和Dubbo本质上没有区别，名字的含义扩展了Dubbo而已，以下扩展出来的功能，也是选择Dubbox很重要的考察点。</p>
<p>Motan 是新浪微博开源的一个Java框架。它诞生的比较晚，起于2013年，2016年5月开源。Motan 在微博平台中已经广泛应用，每天为数百个服务完成近千亿次的调用。与Dubbo相比，Motan在功能方面并没有那么全面，也没有实现特别多的扩展。用的人比较少，功能和稳定性有待观望。对跨语言调用支持较差，主要支持java。</p>
<p>Hessian 采用的是二进制RPC协议，适用于发送二进制数据。但本身也是一个Web Service框架对RPC调用提供支持，功能简单，使用起来也方便。基于Http协议进行传输。通过Servlet提供远程服务。通过Hessain本身提供的API来发起请求。响应端根据Hessian提供的API来接受请求。</p>
<p>rpcx 是Go语言生态圈的Dubbo， 比Dubbo更轻量，实现了Dubbo的许多特性，借助于Go语言优秀的并发特性和简洁语法，可以使用较少的代码实现分布式的RPC服务。</p>
<p>gRPC 是Google开发的高性能、通用的开源RPC框架，其由Google主要面向移动应用开发并基于HTTP/2协议标准而设计，基于ProtoBuf(Protocol Buffers)序列化协议开发，且支持众多开发语言。本身它不是分布式的，所以要实现上面的框架的功能需要进一步的开发。</p>
<p>thrift 是Apache的一个跨语言的高性能的服务框架，也得到了广泛的应用。</p>
<table>
<thead>
<tr>
<th>功能</th>
<th>Hessian</th>
<th>Montan</th>
<th>rpcx</th>
<th>gRPC</th>
<th>Thrift</th>
<th>Dubbo</th>
<th>Dubbox</th>
<th>Spring Cloud</th>
</tr>
</thead>
<tbody>
<tr>
<td>开发语言</td>
<td>跨语言</td>
<td>Java</td>
<td>Go</td>
<td>跨语言</td>
<td>跨语言</td>
<td>Java</td>
<td>Java</td>
<td>Java</td>
</tr>
<tr>
<td>分布式(服务治理)</td>
<td>×</td>
<td>√</td>
<td>√</td>
<td>×</td>
<td>×</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>多序列化框架支持</td>
<td>hessian</td>
<td>√(支持Hessian2、Json,可扩展)</td>
<td>√</td>
<td>× 只支持protobuf)</td>
<td>×(thrift格式)</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>多种注册中心</td>
<td>×</td>
<td>√</td>
<td>√</td>
<td>×</td>
<td>×</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>管理中心</td>
<td>×</td>
<td>√</td>
<td>√</td>
<td>×</td>
<td>×</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>跨编程语言</td>
<td>√</td>
<td>×(支持php client和C server)</td>
<td>×</td>
<td>√</td>
<td>√</td>
<td>×</td>
<td>×</td>
<td>×</td>
</tr>
<tr>
<td>支持REST</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>关注度</td>
<td>低</td>
<td>中</td>
<td>低</td>
<td>中</td>
<td>中</td>
<td>中</td>
<td>高</td>
<td>中</td>
</tr>
<tr>
<td>上手难度</td>
<td>低</td>
<td>低</td>
<td>中</td>
<td>中</td>
<td>中</td>
<td>低</td>
<td>低</td>
<td>中</td>
</tr>
<tr>
<td>运维成本</td>
<td>低</td>
<td>中</td>
<td>中</td>
<td>中</td>
<td>低</td>
<td>中</td>
<td>中</td>
<td>中</td>
</tr>
<tr>
<td>开源机构</td>
<td>Caucho</td>
<td>Weibo</td>
<td>Apache</td>
<td>Google</td>
<td>Apache</td>
<td>Alibaba</td>
<td>Dangdang</td>
<td>Apache</td>
</tr>
</tbody>
</table>
<p><a href="https://blog.csdn.net/u013452337/article/details/86593291" target="_blank" rel="noopener">参考</a></p>
<h1 id="gRPC简介"><a href="#gRPC简介" class="headerlink" title="gRPC简介"></a>gRPC简介</h1><p><a href="https://grpc.io/" target="_blank" rel="noopener">https://grpc.io/</a></p>
<p>gRPC是可以在任何环境中运行的现代开源高性能RPC框架。它可以通过可插拔的支持来有效地连接数据中心内和跨数据中心的服务，以实现负载平衡，跟踪，运行状况检查和身份验证。它也适用于分布式计算的最后一英里，以将设备，移动应用程序和浏览器连接到后端服务。</p>
<p>在gRPC中，客户端应用程序可以直接在其他计算机上的服务器应用程序上调用方法，就好像它是本地对象一样，从而使您更轻松地创建分布式应用程序和服务。与许多RPC系统一样，gRPC围绕定义服务的思想，指定可通过其参数和返回类型远程调用的方法。在服务器端，服务器实现此接口并运行gRPC服务器以处理客户端调用。在客户端，客户端具有一个存根（在某些语言中仅称为客户端），提供与服务器相同的方法。</p>
<p><img src="https://grpc.io/img/landing-2.svg" alt="image"></p>
<p>从Google内部的服务器到您自己的台式机，gRPC客户端和服务器都可以在各种环境中运行并相互通信，并且可以使用gRPC支持的任何语言编写。因此，例如，您可以使用Go，Python或Ruby的客户端轻松地用Java创建gRPC服务器。此外，最新的Google API的接口将具有gRPC版本，可让您轻松地在应用程序中内置Google功能。</p>
<h1 id="Protobuf"><a href="#Protobuf" class="headerlink" title="Protobuf"></a>Protobuf</h1><p>在写gPRC前还需要知道<a href="https://developers.google.com/protocol-buffers" target="_blank" rel="noopener">Protobuf</a>，Protocol Buffer的翻译为协议缓冲区， 是Google的与语言无关，与平台无关，可扩展的机制，用于对结构化数据进行序列化（例如XML），但更小，更快，更简单。您定义要一次构造数据的方式，然后可以使用生成的特殊源代码轻松地使用各种语言在各种数据流中写入和读取结构化数据。</p>
<p>看一个非常简单的例子。假设要定义一个搜索请求消息格式，其中每个搜索请求都有一个查询字符串，您感兴趣的特定结果页面以及每页结果数量。这是.proto用于定义消息类型的文件。</p>
<figure class="highlight protobuf"><table><tr><td class="code"><pre><span class="line">syntax = <span class="string">"proto3"</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">SearchRequest</span> </span>&#123;</span><br><span class="line">  <span class="built_in">string</span> query = <span class="number">1</span>;</span><br><span class="line">  <span class="built_in">int32</span> page_number = <span class="number">2</span>;</span><br><span class="line">  <span class="built_in">int32</span> result_per_page = <span class="number">3</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>文件的第一行指定正在使用proto3语法：如果不这样做，则协议缓冲区编译器将假定您正在使用proto2。这必须是文件的第一行，非空，非注释行。</p>
</blockquote>
<blockquote>
<p>所述SearchRequest消息定义指定了三个字段（名称/值对），一个用于每条数据要在此类型的消息包括。每个字段都有一个名称和类型。</p>
</blockquote>
<p>.proto文件最终生成什么</p>
<p>当你使用protoc来编译一个.proto文件的时候，编译器将利用你在文件中定义的类型生成你打算使用的语言的代码文件。生成的代码包括getting setting 接口和序列化，反序列化接口。</p>
<ul>
<li>对于C ++，编译器会从每个.proto文件生成一个.h和一个.cc文件，并为您文件中描述的每种消息类型提供一个类。</li>
<li>对于Java，编译器生成一个.java文件，其中包含每种消息类型的类，以及Builder用于创建消息类实例的特殊类。</li>
<li>Python有点不同 - Python编译器生成一个模块，其中包含每个消息类型的静态描述符，然后，用一个元类在运行时创建必要的Python数据访问类。</li>
<li>对于Go，编译器会为.pb.go文件中的每种消息类型生成一个类型的文件。</li>
<li>对于Ruby，编译器生成一个.rb包含消息类型的Ruby模块的文件。</li>
<li>对于Objective-C，编译器从每个.proto文件生成一个pbobjc.h和一个pbobjc.m文件，其中包含文件中描述的每种消息类型的类。</li>
<li>对于C＃，编译器会从每个.proto文件生成一个.cs文件，其中包含文件中描述的每种消息类型的类。</li>
</ul>
<h1 id="gPRC的HelloWord"><a href="#gPRC的HelloWord" class="headerlink" title="gPRC的HelloWord"></a>gPRC的HelloWord</h1><p>记录使用python的gRPC</p>
<p>安装gPRC和gRPC工具</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line">$ python -m pip <span class="keyword">install</span> grpcio</span><br><span class="line">$ python -m pip <span class="keyword">install</span> grpcio-tools</span><br></pre></td></tr></table></figure>
<p>gRPC工具包括协议缓冲区编译器protoc和用于根据.proto服务定义生成服务器和客户端代码的特殊插件。</p>
<p>首先新建一个protos的文件夹编写helloworld.proto<br><figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment">#protos\helloworld.proto</span></span><br><span class="line">syntax = <span class="string">"proto3"</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">option</span> java_multiple_files = <span class="keyword">true</span>;</span><br><span class="line"><span class="keyword">option</span> java_package = <span class="string">"io.grpc.examples.helloworld"</span>;</span><br><span class="line"><span class="keyword">option</span> java_outer_classname = <span class="string">"HelloWorldProto"</span>;</span><br><span class="line"><span class="keyword">option</span> objc_class_prefix = <span class="string">"HLW"</span>;</span><br><span class="line"></span><br><span class="line">package helloworld;</span><br><span class="line"></span><br><span class="line">// The greeting service definition.</span><br><span class="line">service Greeter &#123;</span><br><span class="line">  // Sends a greeting</span><br><span class="line">  rpc SayHello (HelloRequest) returns (HelloReply) &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// The request <span class="keyword">message</span> containing the user's name.</span><br><span class="line"><span class="keyword">message</span> HelloRequest &#123;</span><br><span class="line">  <span class="keyword">string</span> name = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// The response <span class="keyword">message</span> containing the greetings</span><br><span class="line"><span class="keyword">message</span> HelloReply &#123;</span><br><span class="line">  <span class="keyword">string</span> <span class="keyword">message</span> = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>接下来，我们需要生成应用程序使用的gRPC代码。</p>
<figure class="highlight jboss-cli"><table><tr><td class="code"><pre><span class="line">$ python -m grpc_tools.protoc -I.<span class="string">/protos</span> <span class="params">--python_out=</span>. <span class="params">--grpc_python_out=</span>. <span class="string">./protos/helloworld.proto</span></span><br></pre></td></tr></table></figure>
<p>目录下会生成两个文件</p>
<blockquote>
<p>helloworld_pb2.py</p>
</blockquote>
<blockquote>
<p>helloworld_pb2_grpc.py</p>
</blockquote>
<p>然后我们就可以调用这两个文件的生成的接口了，</p>
<p>在Server端定义接口的实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># greeter_server.py</span></span><br><span class="line"><span class="string">"""The Python implementation of the GRPC helloworld.Greeter server."""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> concurrent <span class="keyword">import</span> futures</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> grpc</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> helloworld_pb2</span><br><span class="line"><span class="keyword">import</span> helloworld_pb2_grpc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Greeter</span><span class="params">(helloworld_pb2_grpc.GreeterServicer)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SayHello</span><span class="params">(self, request, context)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> helloworld_pb2.HelloReply(message=<span class="string">'Hello, %s!'</span> % request.name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">serve</span><span class="params">()</span>:</span></span><br><span class="line">    server = grpc.server(futures.ThreadPoolExecutor(max_workers=<span class="number">10</span>))</span><br><span class="line">    helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(), server)</span><br><span class="line">    server.add_insecure_port(<span class="string">'[::]:50051'</span>)</span><br><span class="line">    server.start()</span><br><span class="line">    server.wait_for_termination()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    logging.basicConfig()</span><br><span class="line">    serve()</span><br></pre></td></tr></table></figure>
<p>Client端调用远程方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># greeter_client.py</span></span><br><span class="line"><span class="string">"""The Python implementation of the GRPC helloworld.Greeter client."""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> grpc</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> helloworld_pb2</span><br><span class="line"><span class="keyword">import</span> helloworld_pb2_grpc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># NOTE(gRPC Python Team): .close() is possible on a channel and should be</span></span><br><span class="line">    <span class="comment"># used in circumstances in which the with statement does not fit the needs</span></span><br><span class="line">    <span class="comment"># of the code.</span></span><br><span class="line">    <span class="keyword">with</span> grpc.insecure_channel(<span class="string">'localhost:50051'</span>) <span class="keyword">as</span> channel:</span><br><span class="line">        stub = helloworld_pb2_grpc.GreeterStub(channel)</span><br><span class="line">        response = stub.SayHello(helloworld_pb2.HelloRequest(name=<span class="string">'you'</span>))</span><br><span class="line">    print(<span class="string">"Greeter client received: "</span> + response.message)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    logging.basicConfig()</span><br><span class="line">    run()</span><br></pre></td></tr></table></figure>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">首先启动server</span><br><span class="line">$ python greeter_server.py</span><br><span class="line"></span><br><span class="line">运行client</span><br><span class="line">$ python greeter_client.py</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line">Greeter<span class="built_in"> client </span>received: Hello, you!</span><br></pre></td></tr></table></figure>
<h1 id="跨语言调用"><a href="#跨语言调用" class="headerlink" title="跨语言调用"></a>跨语言调用</h1><p>gRPC是支持不同语言调用的，C#写一个Server，用python的Client调用。</p>
<p>首先用Visual Studio新建一个工程，添加Greeter和GreeterServer两个项目</p>
<p><img src="image1.png" alt="image"></p>
<p>Greeter添加Grpc的NuGet包</p>
<blockquote>
<p>Goggle.Protobuf</p>
</blockquote>
<blockquote>
<p>Grpc</p>
</blockquote>
<blockquote>
<p>Grpc.Tools</p>
</blockquote>
<p>选择.proto的properties，build改为Protobuf</p>
<p><img src="image2.png" alt="image"></p>
<p>现在选择Greeter项目，生成会build出 <strong>Helloworld.cs</strong> 和 <strong>HelloworldGrpc.cs</strong>两个文件。</p>
<p><a href="https://github.com/grpc/grpc/blob/master/src/csharp/BUILD-INTEGRATION.md" target="_blank" rel="noopener">详情</a></p>
<p>编写GreeterServer的Program.cs，记得要引用Greeter</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">// GreeterServer/Program.cs</span><br><span class="line">using System;</span><br><span class="line">using System.Threading.Tasks;</span><br><span class="line">using Grpc.Core;</span><br><span class="line">using Helloworld;</span><br><span class="line"></span><br><span class="line">namespace GreeterServer</span><br><span class="line">&#123;</span><br><span class="line">    class GreeterImpl : Greeter.GreeterBase</span><br><span class="line">    &#123;</span><br><span class="line">        // Server side handler of the SayHello RPC</span><br><span class="line">        public override Task&lt;HelloReply&gt; SayHello(HelloRequest request, ServerCallContext context)</span><br><span class="line">        &#123;</span><br><span class="line">            return Task.FromResult(new HelloReply &#123; Message = &quot;Hello &quot; + request.Name &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    class Program</span><br><span class="line">    &#123;</span><br><span class="line">        const int Port = 50051;</span><br><span class="line"></span><br><span class="line">        public static void Main(string[] args)</span><br><span class="line">        &#123;</span><br><span class="line">            Server server = new Server</span><br><span class="line">            &#123;</span><br><span class="line">                Services = &#123; Greeter.BindService(new GreeterImpl()) &#125;,</span><br><span class="line">                Ports = &#123; new ServerPort(&quot;localhost&quot;, Port, ServerCredentials.Insecure) &#125;</span><br><span class="line">            &#125;;</span><br><span class="line">            server.Start();</span><br><span class="line"></span><br><span class="line">            Console.WriteLine(&quot;Greeter server listening on port &quot; + Port);</span><br><span class="line">            Console.WriteLine(&quot;Press any key to stop the server...&quot;);</span><br><span class="line">            Console.ReadKey();</span><br><span class="line"></span><br><span class="line">            server.ShutdownAsync().Wait();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后就可以构建并执行C#的Server了</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">&gt; dotnet build Greeter.sln</span><br><span class="line">&gt; cd GreeterServer\</span><br><span class="line">&gt; dotnet run</span><br><span class="line">Greeter<span class="built_in"> server </span>listening on<span class="built_in"> port </span>51062</span><br><span class="line">Press any key <span class="keyword">to</span> stop the server<span class="built_in">..</span>.</span><br></pre></td></tr></table></figure>
<p>再执行之前用python写的client<br><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">python greeter_client.py</span><br><span class="line">Greeter<span class="built_in"> client </span>received: Hello you</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>gRPC</category>
      </categories>
  </entry>
  <entry>
    <title>TensorFlow学习前的准备工作</title>
    <url>/2018/12/24/tensorflow%E5%AD%A6%E4%B9%A0%E5%89%8D%E7%9A%84%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C/</url>
    <content><![CDATA[<p>工欲善其事必先利其器，在学习tensor flow之前需要先学会使用一些工具，首先是jupyter，这之后关于tensor flow的blog也都会写成jupyter格式的。</p>
<h1 id="Jupyter"><a href="#Jupyter" class="headerlink" title="Jupyter"></a>Jupyter</h1><p>Jupyter Notebook 的本质是一个 Web 应用程序，便于创建和共享文学化程序文档，支持实时代码，数学方程，可视化和 markdown。 用途包括：数据清理和转换，数值模拟，统计建模，机器学习等等。</p>
<p>官网：<a href="http://jupyter.org/" target="_blank" rel="noopener">Jupyter</a></p>
<p>Installing Jupyter with pip<br><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">As an existing or experienced Python user, you may wish to <span class="keyword">install</span> Jupyter <span class="keyword">using</span> Python’s <span class="keyword">package</span> manager, pip, instead <span class="keyword">of</span> Anaconda.</span><br><span class="line"></span><br><span class="line"><span class="keyword">If</span> you have Python <span class="number">3</span> installed (which <span class="keyword">is</span> recommended):</span><br><span class="line"></span><br><span class="line">python3 -m pip <span class="keyword">install</span> <span class="comment">--upgrade pip</span></span><br><span class="line">python3 -m pip <span class="keyword">install</span> jupyter</span><br><span class="line"><span class="keyword">If</span> you have Python <span class="number">2</span> installed:</span><br><span class="line"></span><br><span class="line">python -m pip <span class="keyword">install</span> <span class="comment">--upgrade pip</span></span><br><span class="line">python -m pip <span class="keyword">install</span> jupyter</span><br><span class="line">Congratulations, you have installed Jupyter Notebook! <span class="keyword">To</span> run the notebook, run the <span class="keyword">following</span> command <span class="keyword">at</span> the Terminal (Mac/Linux) <span class="keyword">or</span> Command <span class="keyword">Prompt</span> (Windows):</span><br><span class="line"></span><br><span class="line">run: jupyter notebook</span><br></pre></td></tr></table></figure></p>
<p>安装成功后执行 jupyter notebook 后会打开一个web，通过网页就可以执行python程序。</p>
<p>还可以把ipynb文件转换为html，md，pdf等格式</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">ipython nbconvert --to markdown  filename.ipynb</span><br><span class="line">ipython nbconvert --to <span class="selector-tag">html</span>  filename.ipynb</span><br></pre></td></tr></table></figure>
<p>ipynb转换为html、md、pdf等格式，还有另一种更简单的方法：在jupyter notebook中，选择File-&gt;Download as，直接选择需要转换的格式就可以了。需要注意的是，转换为pdf格式之前，同样要保证已经安装了xelatex。</p>
<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h2 id="基础数学"><a href="#基础数学" class="headerlink" title="基础数学"></a>基础数学</h2><figure class="highlight excel"><table><tr><td class="code"><pre><span class="line">代数</span><br><span class="line">变量、系数和函数</span><br><span class="line">线性方程式，例如 </span><br><span class="line">对数和对数方程式，例如 </span><br><span class="line">S 型函数</span><br><span class="line">线性代数</span><br><span class="line">张量和张量等级</span><br><span class="line">矩阵乘法</span><br><span class="line">三角学</span><br><span class="line"><span class="built_in">Tanh</span>（作为激活函数进行讲解，无需提前掌握相关知识）</span><br><span class="line">统计信息</span><br><span class="line">均值、中间值、离群值和标准偏差</span><br><span class="line">能够读懂直方图</span><br><span class="line">微积分（可选，适合高级主题）</span><br><span class="line">导数概念（您不必真正计算导数）</span><br><span class="line">梯度或斜率</span><br><span class="line">偏导数（与梯度紧密相关）</span><br><span class="line">链式法则（带您全面了解用于训练神经网络的反向传播算法）</span><br></pre></td></tr></table></figure>
<h2 id="基础-Python"><a href="#基础-Python" class="headerlink" title="基础 Python"></a>基础 Python</h2><figure class="highlight cs"><table><tr><td class="code"><pre><span class="line">Python 教程中介绍了以下 Python 基础知识：</span><br><span class="line"></span><br><span class="line">定义和调用函数：使用位置和关键字参数</span><br><span class="line"></span><br><span class="line">字典、列表、集合（创建、访问和迭代）</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> 循环：包含多个迭代器变量的 <span class="keyword">for</span> 循环（例如 <span class="keyword">for</span> a, b <span class="keyword">in</span> [(<span class="number">1</span>,<span class="number">2</span>), (<span class="number">3</span>,<span class="number">4</span>)]）</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>/<span class="keyword">else</span> 条件块和条件表达式</span><br><span class="line"></span><br><span class="line">字符串格式（例如 <span class="string">'%.2f'</span> % <span class="number">3.14</span>）</span><br><span class="line"></span><br><span class="line">变量、赋值、基本数据类型（<span class="keyword">int</span>、<span class="keyword">float</span>、<span class="keyword">bool</span>、str）</span><br><span class="line"></span><br><span class="line">pass 语句</span><br></pre></td></tr></table></figure>
<h2 id="Python-库"><a href="#Python-库" class="headerlink" title="Python 库"></a>Python 库</h2><figure class="highlight smali"><table><tr><td class="code"><pre><span class="line">机器学习速成课程代码示例使用了第三方库提供的以下功能。无需提前熟悉这些库；您可以在需要时查询相关内容。</span><br><span class="line"></span><br><span class="line">Matplotlib（适合数据可视化）</span><br><span class="line">pyplot 模块</span><br><span class="line">cm 模块</span><br><span class="line">gridspec 模块</span><br><span class="line">Seaborn（适合热图）</span><br><span class="line">heatmap 函数</span><br><span class="line">Pandas（适合数据处理）</span><br><span class="line">DataFrame 类</span><br><span class="line">NumPy（适合低阶数学运算）</span><br><span class="line">linspace 函数</span><br><span class="line">random 函数</span><br><span class="line">array 函数</span><br><span class="line">arange 函数</span><br><span class="line">scikit-learn（适合评估指标）</span><br><span class="line">metrics 模块</span><br></pre></td></tr></table></figure>
<h1 id="Bash-shell"><a href="#Bash-shell" class="headerlink" title="Bash shell"></a>Bash shell</h1><p>知道命令行，会敲命令。</p>
<h1 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h1><p>最后要知道Pandas库中DataFrame的数据结构，看另一篇博客，Pandas简介。</p>
<p>以上都不会也没有关系，毕竟tensor flow是给学龄前儿童玩耍的。</p>
<h1 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h1><p><a href="https://www.tensorflow.org/?hl=zh-cn" target="_blank" rel="noopener">官网</a><br>这里提供了非常详细的中文教程</p>
<p>我没有搭建实体环境，是通过<a href="https://colab.research.google.com" target="_blank" rel="noopener">Colaboratory</a>来实验代码。</p>
<h2 id="Colaboratory"><a href="#Colaboratory" class="headerlink" title="Colaboratory"></a>Colaboratory</h2><p>Colaboratory是Google的一个研究项目，旨在提供开发者一个云端训练神经网络的工具。它是Jupyter一个笔记本环境，不用做任何配置，完全运行在云端。Colaboratory存储在Google Drive中，可以进行共享。Colaboratory向开发者提供了免费的Tesla K80 GPU使用。</p>
<h3 id="实用的键盘快捷键"><a href="#实用的键盘快捷键" class="headerlink" title="实用的键盘快捷键"></a>实用的键盘快捷键</h3><ul>
<li><strong>⌘/Ctrl+m,b：</strong>在当前选择的单元格下方创建一个空白代码单元格</li>
<li><strong>⌘/Ctrl+m,i：</strong>中断单元格的运行</li>
<li><strong>⌘/Ctrl+m,h：</strong>显示所有键盘快捷键列表</li>
<li><p>要查看关于任何 TensorFlow API 方法的文档，请将光标放置在其左括号的正后方，然后按 <strong>Tab</strong> 键：</p>
<p><img src="https://download.mlcc.google.cn/mledu-images/tf_pop_up_doc_example.png" alt="TensorFlow tf.constant 方法的弹出式文档"></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>博客搭建笔记</title>
    <url>/2018/04/01/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="搭建环境"><a href="#搭建环境" class="headerlink" title="搭建环境"></a>搭建环境</h1><p>这次尝试在Ubuntu环境下搭建github+hexo博客<br>软件需要以下四个:</p>
<p><a href="https://www.ubuntu.com/download/desktop" title="下载地址" target="_blank" rel="noopener">Ubuntu 16.04</a></p>
<p><a href="https://nodejs.org/en/download/" title="Node" target="_blank" rel="noopener">Node.js</a></p>
<p><a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a></p>
<p><a href="https://github.com/" target="_blank" rel="noopener">GitHub</a></p>
<h1 id="Ubuntu安装"><a href="#Ubuntu安装" class="headerlink" title="Ubuntu安装"></a>Ubuntu安装</h1><p>略</p>
<h1 id="Node环境安装"><a href="#Node环境安装" class="headerlink" title="Node环境安装"></a>Node环境安装</h1><p>Hexo博客系统是静态网页的形似，依赖Node.js，简单的说 Node.js 就是运行在服务端的 JavaScript。Node.js 是一个基于Chrome JavaScript 运行时建立的一个平台。（其实我并不懂这个玩意）只是Hexo需要使用npm安装，npm是依托于node的安装软件管理系统</p>
<h2 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h2><p>Windowns下直接下载安装，Ubuntu 我刚开始使用了apt-get install结果装完后版本过低使后面的搭建过程接连出错，直接使用编译好的文件安装，首先官网下载最新tar包然后链接为全局<br><figure class="highlight crmsh"><table><tr><td class="code"><pre><span class="line">tar  xf <span class="keyword">node</span><span class="title">-v5</span>.<span class="number">10.1</span>-linux-x64.tar.gz -C /usr/local/</span><br><span class="line">cd /usr/local/</span><br><span class="line">mv <span class="keyword">node</span><span class="title">-v5</span>.<span class="number">10.1</span>-linux-x64/ nodejs</span><br><span class="line">ln -s /usr/local/nodejs/bin/<span class="keyword">node</span> <span class="title">/usr</span>/local/bin</span><br><span class="line">ln -s /usr/local/nodejs/bin/npm /usr/local/bin</span><br></pre></td></tr></table></figure></p>
<h2 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h2><p>源码安装<br>参考了<a href="http://www.runoob.com/nodejs/nodejs-install-setup.html" target="_blank" rel="noopener">菜鸟教程</a></p>
<figure class="highlight crmsh"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Node</span>.<span class="title">js</span> 源码安装</span><br><span class="line">以下部分我们将介绍在Ubuntu Linux下安装 <span class="keyword">Node</span>.<span class="title">js</span> 。 其他的Linux系统，如Centos等类似如下安装步骤。</span><br><span class="line">在 Github 上获取 <span class="keyword">Node</span>.<span class="title">js</span> 源码：</span><br><span class="line">$ sudo git <span class="keyword">clone</span> <span class="title">https</span>://github.com/nodejs/<span class="keyword">node</span>.<span class="title">git</span></span><br><span class="line">Cloning into '<span class="keyword">node</span><span class="title">'...</span></span><br><span class="line"><span class="title">修改目录权限：</span></span><br><span class="line"><span class="title">$</span> sudo chmod -R <span class="number">755</span> <span class="keyword">node</span><span class="title"></span></span><br><span class="line"><span class="title">使用 ./configure</span> 创建编译文件，并按照：</span><br><span class="line">$ cd <span class="keyword">node</span><span class="title"></span></span><br><span class="line"><span class="title">$</span> sudo ./configure</span><br><span class="line">$ sudo make</span><br><span class="line">$ sudo make install</span><br><span class="line">查看 <span class="keyword">node</span> <span class="title">版本：</span></span><br><span class="line"><span class="title"> </span></span><br><span class="line"><span class="title">$</span> <span class="keyword">node</span> <span class="title">--version</span></span><br><span class="line">v10.<span class="number">0.0</span>-pre</span><br><span class="line">$npm -v</span><br><span class="line"><span class="number">5.6</span>.<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>源码编译的时间比我想象中的长啊</p>
<h2 id="方法三"><a href="#方法三" class="headerlink" title="方法三"></a>方法三</h2><p><a href="https://github.com/nodesource/distributions" target="_blank" rel="noopener">nodesource</a><br><figure class="highlight vala"><table><tr><td class="code"><pre><span class="line"><span class="meta"># Using Ubuntu</span></span><br><span class="line">curl -sL https:<span class="comment">//deb.nodesource.com/setup_11.x | sudo -E bash -</span></span><br><span class="line">sudo apt-<span class="keyword">get</span> install -y nodejs</span><br></pre></td></tr></table></figure></p>
<h1 id="注册一个GitHub账号"><a href="#注册一个GitHub账号" class="headerlink" title="注册一个GitHub账号"></a>注册一个GitHub账号</h1><p>关于Git的学习使看了廖雪峰老师的博客，好久之前看的都忘了，哎<br><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000</a></p>
<p>Github账户注册和新建项目，项目必须要遵守格式：<code>账户名.github.io</code>，不然接下来会有很多麻烦。并且需要勾选Initialize this repository with a README</p>
<p>在建好的项目右侧有个settings按钮，点击它，向下拉到GitHub Pages，你会看到那边有个网址，访问它，你将会惊奇的发现该项目已经被部署到网络上，能够通过外网来访问它。 </p>
<h1 id="Hexo安装"><a href="#Hexo安装" class="headerlink" title="Hexo安装"></a>Hexo安装</h1><p>找个合适的地方</p>
<pre><code>sudo npm npm install hexo-cli -g
</code></pre><figure class="highlight avrasm"><table><tr><td class="code"><pre><span class="line">国内上npm很慢或失败，尝试淘宝源的cnpm</span><br><span class="line"><span class="symbol">http:</span>//npm.taobao<span class="meta">.org</span>/</span><br></pre></td></tr></table></figure>
<p>输入<code>hexo -v</code>检查hexo是否安装成功</p>
<p>输入<code>hexo init</code>,初始化项目，npm国外的源有点慢啊，可以修改使用淘宝源</p>
<p>输入<code>npm install</code>， 安装所有组件</p>
<p>输入<code>hexo g</code>，创建静态网页</p>
<p>输入<code>hexo s</code>，开启服务器，访问<a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a></p>
<p>中途出现了好多次error，除了要加sudo，还有一些奇怪的错误，但是重复了几遍就只剩warn了。。。</p>
<blockquote>
<p>总结 使用最新的软件和节点能减少出错几率</p>
</blockquote>
<p>将Hexo与Github page联系起来，设置Git的user name和email（如果是第一次的话）</p>
<blockquote>
<p>git方面另写一篇博客</p>
</blockquote>
<pre><code>测试：
在终端 ssh -T git@github.com

Hi Voidmort! You&apos;ve successfully authenticated, but GitHub does not provide shell access.

成功！
</code></pre><p>配置Deployment，在其文件夹中，找到_config.yml文件，修改repo值（在末尾）<br><figure class="highlight dts"><table><tr><td class="code"><pre><span class="line"><span class="meta"># Deployment</span></span><br><span class="line"><span class="meta">## Docs: https:<span class="comment">//hexo.io/docs/deployment.html</span></span></span><br><span class="line"><span class="symbol">deploy:</span></span><br><span class="line"><span class="symbol">  type:</span> git</span><br><span class="line"><span class="symbol">  repository:</span> git@github.com:Voidmort/Voidmort.github.io.git</span><br><span class="line"><span class="symbol">  branch:</span> master</span><br></pre></td></tr></table></figure></p>
<p>repo值是github项目里的ssh（右下角）</p>
<p>部署到git之前要装一个扩展：</p>
<blockquote>
<p>npm install hexo-deployer-git –save</p>
</blockquote>
<p><a href="https://hexo.io/zh-cn/docs/commands.html" target="_blank" rel="noopener">hexo指令</a></p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">hexo n <span class="string">"我的博客"</span> == hexo new <span class="string">"我的博客"</span> #新建文章</span><br><span class="line">hexo p == hexo publish</span><br><span class="line">hexo g == hexo generate#生成</span><br><span class="line">hexo s == hexo<span class="built_in"> server </span>#启动服务预览</span><br><span class="line">hexo d == hexo deploy#部署</span><br><span class="line">hexo clean</span><br></pre></td></tr></table></figure>
<h1 id="域名绑定"><a href="#域名绑定" class="headerlink" title="域名绑定"></a>域名绑定</h1><p>我购买了阿里的域名，首先ping voidmort.github.io,查看IP地址然后直接在阿里域名管理里点新手引导写上IP地址，然后使用新域名登陆，发现上不去。。。<br>在GitHub setting中找到Custom domain 写上刚购买的域名</p>
<blockquote>
<p>OK！ 博客搭建完成</p>
</blockquote>
<h1 id="Next-主题晋级"><a href="#Next-主题晋级" class="headerlink" title="Next 主题晋级"></a>Next 主题晋级</h1><p>主题地址：<br>theme-next.iissnan.com</p>
<p>搜索服务<br>微搜索 由 lzlun129 贡献</p>
<p>npm install swig-templates</p>
<p>TBD</p>
<p>Local Search 由 flashlab 贡献</p>
<p>添加百度/谷歌/本地 自定义站点内容搜索</p>
<p>安装 hexo-generator-searchdb，在站点的根目录下执行以下命令：</p>
<p>$ npm install hexo-generator-searchdb –save</p>
<h1 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h1><p>$ hexo d<br>ERROR Deployer not found: git</p>
<p>npm install –save hexo-deployer-git</p>
<p>search:<br>  path: search.xml<br>  field: post<br>  format: html<br>  limit: 10000</p>
<p>编辑 主题配置文件，启用本地搜索功能：</p>
<h1 id="Local-search"><a href="#Local-search" class="headerlink" title="Local search"></a>Local search</h1><p>local_search:<br>  enable: true</p>
<p>RSS：</p>
<p>需要先安装 hexo-generator-feed 插件。<br><a href="https://github.com/hexojs/hexo-generator-feed" target="_blank" rel="noopener">https://github.com/hexojs/hexo-generator-feed</a></p>
<p>Live2D：<br><a href="https://www.npmjs.com/package/hexo-helper-live2d" target="_blank" rel="noopener">https://www.npmjs.com/package/hexo-helper-live2d</a></p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>hexo</tag>
        <tag>github</tag>
        <tag>node.js</tag>
      </tags>
  </entry>
  <entry>
    <title>埃及分数</title>
    <url>/2019/12/23/%E5%9F%83%E5%8F%8A%E5%88%86%E6%95%B0/</url>
    <content><![CDATA[<p>在看科普视频的时候学到了埃及分数和贪婪算法，这里用code实现一下。</p>
<h1 id="埃及分数"><a href="#埃及分数" class="headerlink" title="埃及分数"></a>埃及分数</h1><p>古埃人使用的是象形文字，他们用这样的符号表示分数</p>
<p><img src="aijifenshu0.png" alt="image"></p>
<p><img src="aijifenshu1.png" alt="image"></p>
<p>他们在圈圈下面画几个竖线代表几分之一，还有几个规定好的分数有特殊的符号，分数有无数多个，不可能给所有分数都画上符号，所以古埃及人把任意分数都表示为不同的单位分数的和，就是分子为1，分母为各不相同的正整数。任何正有理数都能表达成这一个形式。</p>
<p>例如：</p>
<p>$1=\frac12+\frac13+\frac16$</p>
<p>1还可以表示为：</p>
<p>$1=\frac12+\frac13+\frac19+\frac1{18}$</p>
<p>接下使用贪婪算法来生成任意一个正有理数的埃及分数形式。</p>
<h1 id="贪婪算法"><a href="#贪婪算法" class="headerlink" title="贪婪算法"></a>贪婪算法</h1><p>贪婪算法：将一项分数分解成若干项单分子分数后的项数最少，称为第一种好算法；最大的分母数值最小，称为第二种好算法。 例如：</p>
<p>${\displaystyle {\frac {2}{7}}={\frac {1}{4}}+{\frac {1}{28}}}$。共2项，是第一种好算法，比${\displaystyle {\frac {2}{7}}={\frac {1}{5}}+{\frac {1}{20}}+{\frac {1}{28}}}$的项数要少。</p>
<p>又例如，${\displaystyle {\frac {5}{121}}={\frac {1}{33}}+{\frac {1}{121}}+{\frac {1}{363}}}比 {\displaystyle {\frac {5}{121}}={\frac {1}{25}}+{\frac {1}{759}}+{\frac {1}{208725}}}$ 的最大分母要小，所以是第二种好算法。</p>
<p>找出仅小于${\displaystyle r={\frac {a}{b}}}$的最大单位分数。这个分数的分母的计算方法是：即用${\displaystyle b}$除以${\displaystyle a}$，舍去余数，再加1。（如果没有余数，则${\displaystyle r}$已是单位分数。）<br>把${\displaystyle r}$减去单位分数，以这个新的、更小的${\displaystyle r}$重复步骤1。</p>
<p>例子：把${\displaystyle {\frac {19}{20}}}$转成单位分数。</p>
<p>${\displaystyle \lfloor 20\div 19\rfloor =1}$，所以第1个单位分数是${\frac {1}{2}}$；</p>
<p>${\displaystyle {\frac {19}{20}}-{\frac {1}{2}}={\frac {9}{20}}}$；</p>
<p>${\displaystyle \lfloor 20\div 9\rfloor =2}$，所以第2个单位分数是${\displaystyle {\frac {1}{3}}}$；</p>
<p>${\displaystyle {\frac {9}{20}}-{\frac {1}{3}}={\frac {7}{60}}}$；</p>
<p>${\displaystyle \lfloor 60\div 7\rfloor =8}$，所以第3个单位分数是${\displaystyle {\frac {1}{9}}}$；</p>
<p>${\displaystyle {\frac {7}{60}}-{\frac {1}{9}}={\frac {1}{180}}}$已是单位分数。</p>
<p>所以结果是：<br>${\displaystyle {\frac {19}{20}}={\frac {1}{2}}+{\frac {1}{3}}+{\frac {1}{9}}+{\frac {1}{180}}}$。</p>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 寻找最大公约数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_gcd</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="comment"># Supports non-integers for backward compatibility.</span></span><br><span class="line">    <span class="keyword">while</span> b:</span><br><span class="line">        a, b = b, a%b</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="comment"># 约分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReduceFraction</span><span class="params">(numerator, denominator)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> type(numerator) <span class="keyword">is</span> int <span class="keyword">is</span> type(denominator):</span><br><span class="line">        g = _gcd(numerator, denominator)</span><br><span class="line">        <span class="keyword">if</span> denominator &lt; <span class="number">0</span>:</span><br><span class="line">            g = -g</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            g = _gcd(numerator, denominator)</span><br><span class="line">        numerator //= g</span><br><span class="line">        denominator //= g</span><br><span class="line">    <span class="keyword">return</span> (numerator, denominator)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分数减法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SubFraction</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    an, ad = a</span><br><span class="line">    bn, bd = b</span><br><span class="line">    numerator = an*bd - ad*bn</span><br><span class="line">    denominator = ad * bd</span><br><span class="line">    <span class="keyword">return</span> ReduceFraction(numerator, denominator)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 埃及分数生成器返回分母的list</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">EgpytFraction</span><span class="params">(numerator=<span class="number">0</span>, denominator=None, ret=[])</span>:</span></span><br><span class="line">    a = (denominator // numerator) + <span class="number">1</span></span><br><span class="line">    ret.append(a)</span><br><span class="line">    t = SubFraction((numerator, denominator), (<span class="number">1</span>, a))</span><br><span class="line">    <span class="keyword">if</span> t[<span class="number">0</span>] == <span class="number">1</span>:</span><br><span class="line">        ret.append(t[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> ret</span><br><span class="line">    <span class="keyword">return</span> EgpytFraction(t[<span class="number">0</span>], t[<span class="number">1</span>], ret=ret)</span><br></pre></td></tr></table></figure>
<pre><code>EgpytFraction(5, 121, ret)
[25, 757, 763309, 873960180913, 1527612795642093418846225]
</code></pre><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>埃及分数的表示不是唯一的，但应该有一个项数最少的表达式，我们把这个叫做最优的，但目前还没有一个算法可以求出最优的埃及分数。</p>
]]></content>
      <categories>
        <category>算法练习</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>微软笔试算法（一）</title>
    <url>/2018/05/29/%E5%BE%AE%E8%BD%AF%E7%AC%94%E8%AF%95%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<p>最近求职中接到的好多外包公司的电话，一般我都是拒绝的，其中告知外包到微软的就有三家，有一家说是微软小冰项目，给了我八道算法题，都不是太难，有的在LeetCode上还做过，在这里记录一下，正好也整理下LeetCode上刷过的题。</p>
<p>这里记录的算法是只用python实现，也是我目前能想到的最优解，一共八个问题。</p>
<ol>
<li>完成一个函数：def Add(num1, num2):其中，两个输入都是数字，都是字符串（如：“12345678986543210123456789”），要求计算两个数字的和，返回一个字符串，不能使用内置函数，如int，long等。例如，输入两个数字是：“1000000000000000”和“-1”，返回“999999999999999”。</li>
</ol>
<p>这个问题比较简单了，主要就是三个小问题：</p>
<ol>
<li>字符串和数字的互转</li>
<li>加法的进位</li>
<li>减法的借位</li>
</ol>
<p>字符串和数字的互转不让用int函数，这个简单，python只要写个字典就解决了，C的话就要写switch case来一个一个比较了。这里就解决了字符转数字的问题了。</p>
<p>到的数字开始计算，因为要设计借位和进位，答案会有四种情况</p>
<ol>
<li>正数 + 正数 = 正数   2 + 3 = 5</li>
<li>负数 + 负数 = 负数   -2 + -3 = -5</li>
<li>大正数 + 负数 = 正数 -2 + 3 = 1</li>
<li>正数 + 大负数 = 负数 2 + -3 = -1</li>
</ol>
<p>貌似就是这样，但是观察结果除了负号就是两种，所以程序一开始应该先分析负号，进位的情况有1,2，借位情况是3,4，正好这两个结果除了负号答案一样，所以分两类就OK</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 问题 1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">question1_add</span><span class="params">(num1, num2)</span>:</span></span><br><span class="line">    <span class="comment"># 创建个字典用来把字符转为int便于计算</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">strtoint</span><span class="params">(str)</span>:</span></span><br><span class="line">        numdict = &#123;<span class="string">"0"</span>: <span class="number">0</span>, <span class="string">"1"</span>: <span class="number">1</span>, <span class="string">"2"</span>: <span class="number">2</span>, <span class="string">"3"</span>: <span class="number">3</span>, <span class="string">"4"</span>: <span class="number">4</span>, <span class="string">"5"</span>: <span class="number">5</span>, <span class="string">"6"</span>: <span class="number">6</span>, <span class="string">"7"</span>: <span class="number">7</span>, <span class="string">"8"</span>: <span class="number">8</span>, <span class="string">"9"</span>: <span class="number">9</span>&#125;</span><br><span class="line">        <span class="keyword">return</span> numdict.get(str)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 把列表中的数字转会字符串 </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inttostr</span><span class="params">(lt)</span>:</span></span><br><span class="line">        ls2 = [str(i) <span class="keyword">for</span> i <span class="keyword">in</span> lt]</span><br><span class="line">        <span class="keyword">return</span> <span class="string">''</span>.join(ls2)</span><br><span class="line"></span><br><span class="line">    lt, x, y = [], [], []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 两个数是否为负数的标记</span></span><br><span class="line">    minusflag1, minusflag2 = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 首先先提取出负号，记录负号的情况 </span></span><br><span class="line">    <span class="keyword">if</span> num1[<span class="number">0</span>] <span class="keyword">is</span> <span class="string">"-"</span>:</span><br><span class="line">        x = list(map(strtoint, num1[<span class="number">1</span>:]))</span><br><span class="line">        minusflag1 = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = list(map(strtoint, num1))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> num2[<span class="number">0</span>] <span class="keyword">is</span> <span class="string">"-"</span>:</span><br><span class="line">        y = list(map(strtoint, num2[<span class="number">1</span>:]))</span><br><span class="line">        minusflag2 = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        y = list(map(strtoint, num2))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把 x作为最长的那列，y是短的那列</span></span><br><span class="line">    <span class="keyword">if</span> len(x) &lt; len(y):</span><br><span class="line">        x, y = y, x</span><br><span class="line"></span><br><span class="line">    maxlen, minlen = len(x), len(y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 在短的那列前面插上0让两列一样齐以便于计算</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(maxlen-minlen):</span><br><span class="line">        y.insert(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算通常是从最低位开始，所以i从最低位开始</span></span><br><span class="line">    <span class="comment"># carry记录借位和进位情况</span></span><br><span class="line">    i, carry = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 两个数都是正数或负数</span></span><br><span class="line">    <span class="keyword">if</span> (minusflag1 == <span class="number">0</span> <span class="keyword">and</span> minusflag2 == <span class="number">0</span>) <span class="keyword">or</span> (minusflag1 == <span class="number">1</span> <span class="keyword">and</span> minusflag2 == <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># +1是为了防止最高位有进位的情况，给最高位加一个0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(maxlen+<span class="number">1</span>):</span><br><span class="line">            i -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> j &lt; maxlen:</span><br><span class="line">                sum = x[i] + y[i] + carry</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                sum = carry</span><br><span class="line">            <span class="comment"># 两个数相加大于十代表有进位， 把结果和10的商给新列表，余给进位标志   </span></span><br><span class="line">            <span class="keyword">if</span> sum &gt;= <span class="number">10</span>:</span><br><span class="line">                lt.insert(<span class="number">0</span>, sum%<span class="number">10</span>)</span><br><span class="line">                carry = sum//<span class="number">10</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                lt.insert(<span class="number">0</span>, sum)</span><br><span class="line">                carry = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 如果最高位没有进位 把0删除       </span></span><br><span class="line">        <span class="keyword">if</span> lt[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">            lt.pop(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果都为负数 加 -</span></span><br><span class="line">        <span class="keyword">if</span> minusflag1 == <span class="number">1</span> <span class="keyword">and</span> minusflag2 == <span class="number">1</span>:</span><br><span class="line">            lt.insert(<span class="number">0</span>, <span class="string">"-"</span>)</span><br><span class="line">            <span class="keyword">return</span> inttostr(lt)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> inttostr(lt)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 一正一负</span></span><br><span class="line">    <span class="keyword">if</span> (minusflag1 == <span class="number">1</span> <span class="keyword">and</span> minusflag2 == <span class="number">0</span>) <span class="keyword">or</span> (minusflag1 == <span class="number">0</span> <span class="keyword">and</span> minusflag2 == <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(maxlen):</span><br><span class="line">            i -= <span class="number">1</span></span><br><span class="line">            sum = x[i] - y[i] + carry</span><br><span class="line">            <span class="keyword">if</span> sum &lt; <span class="number">0</span>:</span><br><span class="line">                lt.insert(<span class="number">0</span>, sum%<span class="number">10</span>)</span><br><span class="line">                carry = sum//<span class="number">10</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                lt.insert(<span class="number">0</span>, sum)</span><br><span class="line">                carry = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> lt[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">            lt.pop(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">if</span> len(lt) == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 判断是否应该加 - 号</span></span><br><span class="line">        <span class="keyword">if</span> (num1[<span class="number">0</span>] <span class="keyword">is</span> <span class="string">"-"</span> <span class="keyword">and</span> len(num1[<span class="number">1</span>:]) &gt; len(num2)) <span class="keyword">or</span> num2[<span class="number">0</span>] <span class="keyword">is</span> <span class="string">"-"</span> <span class="keyword">and</span> len(num2[<span class="number">1</span>:]) &gt; len(num1) :</span><br><span class="line">            lt.insert(<span class="number">0</span>, <span class="string">"-"</span>)</span><br><span class="line">            <span class="keyword">return</span> inttostr(lt)</span><br><span class="line">        <span class="keyword">return</span> inttostr(lt)</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>给定一个数组nums，然后对其排序，使得排序结果满足nums[0] &lt; nums[1] &gt; nums[2] &lt; nums[3]…。 例如给定数组nums=[1,2,3,4,5,6,7,8,9],其中一个满足条件的结果是1<6>2<7>3<8>4<9>5.给出一个结果即可（可能无解）。最优解法是O(n)时间复杂度和O(1)空间复杂度。</9></8></7></6></li>
</ol>
<p>原题位置：<a href="https://leetcode.com/problems/wiggle-sort-ii/" target="_blank" rel="noopener">https://leetcode.com/problems/wiggle-sort-ii/</a></p>
<p>这道题就比较有趣了，LeetCode上大神给出的算法挺多，但是最优解不是O(n)，用python实现的话还能比O(n)小。</p>
<p>先说我看到题第一眼的想法和看了LeetCode上大神的方法吧，很简单就三行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums.sort()</span><br><span class="line">median = len(nums[::<span class="number">2</span>]) - <span class="number">1</span></span><br><span class="line">nums[::<span class="number">2</span>], nums[<span class="number">1</span>::<span class="number">2</span>] = nums[median::<span class="number">-1</span>], nums[:median:<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>
<p>首先排序，找到中位数，然后把中位数左右两边的交叉相排就OK了。</p>
<p>好吧，详细一点吧，S代表比中位数小的，L代表比中位数大的，M代表中位数</p>
<figure class="highlight groovy"><table><tr><td class="code"><pre><span class="line">Small <span class="string">half:</span>  M . S . S . S      Small <span class="string">half:</span>  M . S . S . S .</span><br><span class="line">Large <span class="string">half:</span>  . L . L . M .      Large <span class="string">half:</span>  . L . L . L . M</span><br><span class="line">--------------------------      --------------------------</span><br><span class="line"><span class="string">Together:</span>    M L S L S M S      <span class="string">Together:</span>    M L S L S L S M</span><br></pre></td></tr></table></figure>
<p>根据这个原理还可以写成其他形式<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums.sort()</span><br><span class="line">median = len(nums[::<span class="number">2</span>])</span><br><span class="line">nums[<span class="number">1</span>::<span class="number">2</span>], nums[::<span class="number">2</span>] = nums[median:],nums[:median]</span><br></pre></td></tr></table></figure></p>
<p>这个方式简单但是不满足O(n)时间复杂度的要求，这个方法是LeetCode上大神想出的叫virtual indexing，地址<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">解释一下site()函数的作用</span><br><span class="line"></span><br><span class="line">为了防止median的元素挨在一起，也就是说奇数位置上的值是median，同时与他相邻的某个偶数位置上的值也是median导致排序失败</span><br><span class="line"></span><br><span class="line">为了避免这个问题，可以采用一种方法，即另j 每次移动两步，也就是说先另j直线奇数位置，再另j指向偶数位置，所以对于大小为10的序列，j的变化可能像是这样</span><br></pre></td></tr></table></figure></p>
<p>1 -&gt; 3 -&gt; 5 -&gt; 7 -&gt; 9 -&gt; 0 -&gt; 2 -&gt; 4 -&gt; 6 -&gt; 8<br><figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line">暂且先不考虑怎么实现这样的改变，先说一下这样做带来的好处</span><br><span class="line"></span><br><span class="line">由上面j的变化可知，j的改变是每次移动两步，所以，根据算法描述。所有和<span class="built_in">median</span>相等的元素一定是最后才固定位置，又因为当j指向的值等于<span class="built_in">median</span>时，是不与i和k指向的任何一个元素交换的。所以，每次移动两步带来的结果是这些<span class="built_in">median</span>永远不可能相邻。换句话说就是永远不会出现两个<span class="built_in">median</span>挨着的情况</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">def question2_sort2(nums):</span><br><span class="line">    def site(n):</span><br><span class="line">        <span class="literal">return</span> (<span class="number">1</span> + <span class="number">2</span> * n) % (<span class="built_in">len</span>(nums) | <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    nums.<span class="built_in">sort</span>()</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(nums) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">median</span> = (nums[<span class="built_in">len</span>(nums)<span class="comment"> // 2] + nums[len(nums) // 2 - 1]) / 2</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">median</span> = nums[<span class="built_in">len</span>(nums)<span class="comment"> // 2]</span></span><br><span class="line">    i, j, k = <span class="number">0</span>, <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> j &lt;= k:</span><br><span class="line">        <span class="keyword">if</span> nums[site(j)] &gt; <span class="built_in">median</span>:</span><br><span class="line">            nums[site(i)], nums[site(j)] = nums[site(j)], nums[site(i)]</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">        elif nums[site(j)] &lt; <span class="built_in">median</span>:</span><br><span class="line">            nums[site(j)], nums[site(k)] = nums[site(k)], nums[site(j)]</span><br><span class="line">            k -= <span class="number">1</span></span><br><span class="line">            j += <span class="number">1</span> <span class="comment"># 没错就是这里，我认为执行过交换后j就在正确的位置上了，所以我让j+1结果就是循环次数减少了，如果进行j+1则会循环N次，两种的结果不同但都复合条件</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">    <span class="literal">return</span> nums</span><br></pre></td></tr></table></figure></p>
<ol start="3">
<li>写一个函数，输入是两个int数组A和B。要求从A和B中分别取出一个数，使他们的和为20。打印出所有的组合。要求数字在数组中的位置和数字本身。比如输入为 A = [18, 2, 7, 8, 3], B = [17, 1, 19]，输出为 3 (A4) + 17 (B0) = 20，表示A的第4个元素是3，B的第0个元素是17</li>
</ol>
<p>这就是道送分题了，嵌套循环就OK了，没什么好说的。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">question3_take</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(a)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(b)):</span><br><span class="line">            <span class="keyword">if</span> a[i] + b[j] == <span class="number">20</span>:</span><br><span class="line">                print(<span class="string">"%d (A%d) + %d (B%d) = 20"</span> % (a[i], i, b[j], j))</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">    print(<span class="string">" No solution!"</span>)</span><br></pre></td></tr></table></figure></p>
<ol start="4">
<li>写一个函数，输入一个随机的01序列，打印出这个序列中最长的01交替出现的序列的起始位置和结束位置。例如：输入“000101010101101”，输出起始位置2, 结束位置10</li>
</ol>
<p>这题让我纠结了下，因为需要记录的位置有点多</p>
<p>因为我们要遍历这个序列中所有的01所以要两个指针同时移动才能确保01的出现，当出现01我们计数器n +1，然后判断是否到结尾或者下一个不是01的情况，我们就结束这段，记录这段的长度和结束开始的位置，如果下一段出现的01长我们就替换掉上次的记录。</p>
<p>值得注意的是01可能出现在序列的奇数或偶数位上，所以要遍历两次，再比较奇偶上哪个长，返回长的那段。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(i, j, num)</span>:</span></span><br><span class="line">    start, end, n, count = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> j &lt; len(num):</span><br><span class="line">        <span class="keyword">if</span> num[i] <span class="keyword">is</span> <span class="string">'0'</span> <span class="keyword">and</span> num[j] <span class="keyword">is</span> <span class="string">'1'</span>:</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> j+<span class="number">1</span> &gt;= len(num) - <span class="number">1</span> <span class="keyword">or</span> num[i+<span class="number">2</span>] <span class="keyword">is</span> <span class="string">'1'</span> <span class="keyword">or</span> num[j+<span class="number">2</span>] <span class="keyword">is</span> <span class="string">'0'</span>:</span><br><span class="line">            <span class="keyword">if</span> n &gt; count:</span><br><span class="line">                count = n</span><br><span class="line">                n = <span class="number">0</span></span><br><span class="line">                end = j + <span class="number">1</span></span><br><span class="line">                start = end - count*<span class="number">2</span></span><br><span class="line">        i += <span class="number">2</span></span><br><span class="line">        j += <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> start, end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">question4</span><span class="params">(num)</span>:</span></span><br><span class="line">    i, j = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    start1, end1 = func(i, j, num)</span><br><span class="line">    i, j = <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">    start2, end2 = func(i, j, num)</span><br><span class="line">    <span class="keyword">if</span> end1 - start1 &gt; end2- start2:</span><br><span class="line">        <span class="keyword">return</span> start1, end1</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> start2, end2</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>算法练习</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>文本相似度</title>
    <url>/2019/07/18/%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6/</url>
    <content><![CDATA[<p>如何对比两个文本的相似度，这里记录几个简单计算文本距离的方法。</p>
<p>规定，结果为0-1的浮点数，0为完全不相关，1为完全正相关</p>
<h1 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h1><p>使用 <code>jieba.cut</code> 等工具对文本进行分词处理，获得到两个字符串序列。</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">import jieba</span><br><span class="line">seq1 = [<span class="selector-tag">i</span> <span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> jieba.cut(text1, cut_all=True) <span class="keyword">if</span> <span class="selector-tag">i</span> != <span class="string">''</span>]</span><br><span class="line">seq2 = [<span class="selector-tag">i</span> <span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> jieba.cut(text2, cut_all=True) <span class="keyword">if</span> <span class="selector-tag">i</span> != <span class="string">''</span>]</span><br></pre></td></tr></table></figure>
<h1 id="共有词比率"><a href="#共有词比率" class="headerlink" title="共有词比率"></a>共有词比率</h1><p>比较简单的算法就是看两句话中相同的汉字数，如果有较多的相同汉字，则认为它们是比较相似的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">similartiy_rate</span><span class="params">(seq1, seq2)</span>:</span></span><br><span class="line">    set1, set2 = set(seq1), set(seq2)</span><br><span class="line">    <span class="keyword">return</span> len(set1 &amp; set2) / len(set2)</span><br></pre></td></tr></table></figure>
<h1 id="编辑距离"><a href="#编辑距离" class="headerlink" title="编辑距离"></a>编辑距离</h1><p>编辑距离（Edit Distance），又称Levenshtein距离，是指两个字串之间，由一个转成另一个所需的最少编辑操作次数。编辑操作包括将一个字符替换成另一个字符，插入一个字符，删除一个字符。一般来说，编辑距离越小，两个串的相似度越大。由俄罗斯科学家Vladimir Levenshtein 在1965年提出这个概念。</p>
<p>例如将<code>abcdef</code>转换为<code>abcdgh</code>,需要改变两个字符(<code>ef-&gt;gh</code>)，所以编辑距离就是2</p>
<p>然后拿编辑距离去除以两者之间的最大长度，2/6≈0.333，意味着只要变动这么多就可以从A变成B，所以不用变动的字符便代表了相似度，1-0.333＝0.667。</p>
<pre><code>参数 method：
1:序列间对齐最短长度的序列
2:序列间对齐最长长度的序列
</code></pre><figure class="highlight maxima"><table><tr><td class="code"><pre><span class="line">from <span class="built_in">array</span> import <span class="built_in">array</span></span><br><span class="line"></span><br><span class="line">def levenshtein(seq1, seq2, <span class="built_in">method</span>=<span class="number">1</span>):</span><br><span class="line">	<span class="keyword">if</span> seq1 == seq2:</span><br><span class="line">		<span class="built_in">return</span> <span class="number">0.0</span></span><br><span class="line">	len1, len2 = len(seq1), len(seq2)</span><br><span class="line">	<span class="keyword">if</span> len1 == <span class="number">0</span> <span class="keyword">or</span> len2 == <span class="number">0</span>:</span><br><span class="line">		<span class="built_in">return</span> <span class="number">1.0</span></span><br><span class="line">	<span class="keyword">if</span> len1 &lt; len2: # minimize the <span class="built_in">arrays</span> size</span><br><span class="line">		len1, len2 = len2, len1</span><br><span class="line">		seq1, seq2 = seq2, seq1</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span> <span class="built_in">method</span> == <span class="number">1</span>:</span><br><span class="line">		<span class="built_in">return</span> <span class="number">1</span> - levenshtein(seq1, seq2) / <span class="built_in">float</span>(len1)</span><br><span class="line">	<span class="keyword">if</span> <span class="built_in">method</span> != <span class="number">2</span>:</span><br><span class="line">		raise ValueError(<span class="string">"expected either 1 or 2 for `method` parameter"</span>)</span><br><span class="line">	</span><br><span class="line">	column = <span class="built_in">array</span>('L', <span class="built_in">range</span>(len2 + <span class="number">1</span>))</span><br><span class="line">	<span class="built_in">length</span> = <span class="built_in">array</span>('L', <span class="built_in">range</span>(len2 + <span class="number">1</span>))</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, len1 + <span class="number">1</span>):</span><br><span class="line">	</span><br><span class="line">		column[<span class="number">0</span>] = <span class="built_in">length</span>[<span class="number">0</span>] = x</span><br><span class="line">		<span class="built_in">last</span> = llast = x - <span class="number">1</span></span><br><span class="line">		</span><br><span class="line">		<span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, len2 + <span class="number">1</span>):</span><br><span class="line">		</span><br><span class="line">			# dist</span><br><span class="line">			old = column[y]</span><br><span class="line">			ic = column[y - <span class="number">1</span>] + <span class="number">1</span></span><br><span class="line">			dc = column[y] + <span class="number">1</span></span><br><span class="line">			rc = <span class="built_in">last</span> + (seq1[x - <span class="number">1</span>] != seq2[y - <span class="number">1</span>])</span><br><span class="line">			column[y] = <span class="built_in">min</span>(ic, dc, rc)</span><br><span class="line">			<span class="built_in">last</span> = old</span><br><span class="line">			</span><br><span class="line">			# <span class="built_in">length</span></span><br><span class="line">			lold = <span class="built_in">length</span>[y]</span><br><span class="line">			lic = <span class="built_in">length</span>[y - <span class="number">1</span>] + <span class="number">1</span> <span class="keyword">if</span> ic == column[y] <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">			ldc = <span class="built_in">length</span>[y] + <span class="number">1</span> <span class="keyword">if</span> dc == column[y] <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">			lrc = llast + <span class="number">1</span> <span class="keyword">if</span> rc == column[y] <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">			<span class="built_in">length</span>[y] = <span class="built_in">max</span>(ldc, lic, lrc)</span><br><span class="line">			llast = lold</span><br><span class="line">	</span><br><span class="line">	<span class="built_in">return</span> <span class="number">1</span> - column[y] / <span class="built_in">float</span>(<span class="built_in">length</span>[y])</span><br></pre></td></tr></table></figure>
<h1 id="余弦相似度"><a href="#余弦相似度" class="headerlink" title="余弦相似度"></a>余弦相似度</h1><p>余弦相似度度是常用的计算距离的公式，通常用来比较文本的相似性</p>
<p>首先看一下公式</p>
<p><img src="1.jpg" alt="image"></p>
<p>基本步骤：</p>
<p><img src="2.png" alt="image"></p>
<pre><code>1.通过中文分词，把完整的句子根据分词算法分为独立的词集合
2.求出两个词集合的并集(词包)
3.计算各自词集的词频并把词频向量化
4.带入向量计算模型就可以求出文本相似度
5.套用余弦函数计量两个句子的相似度。
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 统计每个元素出现的次数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">all_list</span><span class="params">(arr)</span>:</span></span><br><span class="line">    result = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> set(arr):</span><br><span class="line">        result[i] = arr.count(i) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 词频编码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_frequency</span><span class="params">(word_list1, word_list2)</span>:</span></span><br><span class="line">    word_list = word_list1 + word_list2</span><br><span class="line">    dict = all_list(word_list)</span><br><span class="line">    word_vector1 = map(<span class="keyword">lambda</span> x: dict[x], word_list1)</span><br><span class="line">    word_vector2 = map(<span class="keyword">lambda</span> x: dict[x], word_list2)</span><br><span class="line">    <span class="keyword">return</span> list(word_vector1), list(word_vector2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算余弦</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_cosine</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    sum_list = map(<span class="keyword">lambda</span> x, y: x*y, a, b)</span><br><span class="line">    p = reduce(<span class="keyword">lambda</span> x, y: x+y, sum_list)</span><br><span class="line"></span><br><span class="line">    q1 = map(<span class="keyword">lambda</span> x: x*x, a)</span><br><span class="line">    q1_sum = reduce(<span class="keyword">lambda</span> x, y: x+y, q1)</span><br><span class="line"></span><br><span class="line">    q2 = map(<span class="keyword">lambda</span> x: x*x, b)</span><br><span class="line">    q2_sum = reduce(<span class="keyword">lambda</span> x, y: x+y, q2)</span><br><span class="line"></span><br><span class="line">    q = math.sqrt(q1_sum) * math.sqrt(q2_sum)</span><br><span class="line">    <span class="keyword">if</span> q != <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> p/q</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 余弦相似度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cosine_similartiy</span><span class="params">(seq1, seq2)</span>:</span></span><br><span class="line">    p1, p2 = word_frequency(seq1, seq2)</span><br><span class="line">    <span class="keyword">return</span> calculate_cosine(p1, p2)</span><br></pre></td></tr></table></figure>
<h1 id="汉明距离"><a href="#汉明距离" class="headerlink" title="汉明距离"></a>汉明距离</h1><p>汉明距离是使用在数据传输差错控制编码里的，它表示两个（相同长度）字对应位不同的数量，我们以d(x,y)表示两个字x,y之间的汉明距离。对两个字符串进行异或运算，并统计结果为1的个数，那么这个数就是汉明距离。</p>
<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 汉明距离</span></span><br><span class="line">def hamming(seq1, seq2):</span><br><span class="line">    L = <span class="built_in">len</span>(seq1)</span><br><span class="line">    <span class="keyword">if</span> L != <span class="built_in">len</span>(seq2):</span><br><span class="line">        seq2.extend([<span class="number">0</span>] * (<span class="built_in">len</span>(seq1) - <span class="built_in">len</span>(seq2)))</span><br><span class="line">    <span class="keyword">if</span> L == <span class="number">0</span>:</span><br><span class="line">        <span class="literal">return</span> <span class="number">0</span> <span class="comment"># equal</span></span><br><span class="line">    dist = <span class="built_in">sum</span>(c1 != c2 <span class="keyword">for</span> c1, c2 <span class="keyword">in</span> zip(seq1, seq2))</span><br><span class="line">    <span class="literal">return</span> <span class="number">1</span> - dist / float(L)</span><br></pre></td></tr></table></figure>
<h1 id="Jaccard系数"><a href="#Jaccard系数" class="headerlink" title="Jaccard系数"></a>Jaccard系数</h1><p>定义</p>
<p>给定两个集合A,B，Jaccard 系数定义为A与B交集的大小与A与B并集的大小的比值，定义如下：</p>
<p><img src="3.png" alt="image"></p>
<p>当集合A，B都为空时，J(A,B)定义为1。<br>与Jaccard 系数相关的指标叫做Jaccard 距离，用于描述集合之间的不相似度。Jaccard 距离越大，样本相似度越低。公式定义如下：</p>
<p><img src="4.png" alt="image"></p>
<p>其中对参差（symmetric difference）  <img src="5.png" alt="image"></p>
<p>性质</p>
<p><img src="6.png" alt="image"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 杰卡德系数</span></span><br><span class="line">def jaccard(seq1, seq2):</span><br><span class="line">    set1, set2 = <span class="keyword">set</span>(seq1), <span class="keyword">set</span>(seq2)</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">len</span>(set1 &amp; set2) / <span class="built_in">float</span>(<span class="keyword">len</span>(set1 | set2))</span><br></pre></td></tr></table></figure>
<h1 id="Dice系数"><a href="#Dice系数" class="headerlink" title="Dice系数"></a>Dice系数</h1><p>Dice 系数可以计算两个字符串的相似度：Dice（s1,s2）=2*comm(s1,s2)/(leng(s1)+leng(s2))。<br>其中，comm (s1,s2)是s1、s2 中相同字符的个数leng(s1)，leng(s2)是字符串s1、s2 的长度。</p>
<p>Dice 系数是一种集合相似度度量函数，与相似度指数相同，也被称为系数，形式上也和杰卡德系数没有多大区别，但还是有些不同的性质。</p>
<p><img src="7.png" alt="image"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Dice系数</span></span><br><span class="line">def sorensen(seq1, seq2):</span><br><span class="line">    set1, set2 = <span class="keyword">set</span>(seq1), <span class="keyword">set</span>(seq2)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * <span class="keyword">len</span>(set1 &amp; set2) / <span class="built_in">float</span>(<span class="keyword">len</span>(set1) + <span class="keyword">len</span>(set2))</span><br></pre></td></tr></table></figure>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>简单的几个计算文本相似度的方法就是这些了，常用的为余弦相似度，稍复杂的可以尝试多个算法的叠加，例如还有欧几里得距离，曼哈顿距离，SimHash等方法可以计算文本之间的相似度。7</p>
]]></content>
      <categories>
        <category>机器学习代码实现</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实战中的函数学习记录</title>
    <url>/2020/05/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E4%B8%AD%E7%9A%84%E5%87%BD%E6%95%B0%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<p>记录机器学习实战中遇到的函数<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure></p>
<h1 id="tile"><a href="#tile" class="headerlink" title="tile()"></a>tile()</h1><p>tile(A, reps)</p>
<p>tile函数的作用是让某个数组或矩阵A，以reps的维度重复，构造出新的数组，所以返回值也是个数组。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = array([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">b = np.tile(a, <span class="number">2</span>)</span><br><span class="line">c = np.tile(a, (<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">b,c</span><br></pre></td></tr></table></figure></p>
<pre><code>(array([0, 1, 0, 1]),
 array([[0, 1, 0, 1],
        [0, 1, 0, 1]]))
</code></pre><h1 id="argsort"><a href="#argsort" class="headerlink" title="argsort()"></a>argsort()</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = array([<span class="number">1</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">-1</span>,<span class="number">6</span>,<span class="number">9</span>])</span><br><span class="line">x.argsort()</span><br></pre></td></tr></table></figure>
<pre><code>array([3, 0, 2, 1, 4, 5], dtype=int64)
</code></pre><p>argsort()函数是将x中的元素从小到大排列，提取其对应的index(索引)，然后输出。例如：x[3]=-1最小，所以y[0]=3,x[5]=9最大，所以y[5]=5。</p>
<h1 id="operator-itemgetter函数"><a href="#operator-itemgetter函数" class="headerlink" title="operator.itemgetter函数"></a>operator.itemgetter函数</h1><p>operator模块提供的itemgetter函数用于获取对象的哪些维的数据，参数为一些序号。</p>
<p>要注意，operator.itemgetter函数获取的不是值，而是定义了一个函数，通过该函数作用到对象上才能获取值。</p>
<p>sorted函数用来排序，sorted(iterable[, cmp[, key[, reverse]]])</p>
<p>其中key的参数为一个函数或者lambda函数。所以itemgetter可以用来当key的参数<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> operator</span><br><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b=operator.itemgetter(<span class="number">1</span>) </span><br><span class="line">b(a)</span><br></pre></td></tr></table></figure></p>
<pre><code>2
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b=operator.itemgetter(<span class="number">1</span>,<span class="number">0</span>)  <span class="comment">#定义函数b，获取对象的第1个域和第0个的值</span></span><br><span class="line">b(a)</span><br></pre></td></tr></table></figure>
<pre><code>(2, 1)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">students = [(<span class="string">'john'</span>, <span class="string">'A'</span>, <span class="number">15</span>), (<span class="string">'jane'</span>, <span class="string">'B'</span>, <span class="number">12</span>), (<span class="string">'dave'</span>, <span class="string">'B'</span>, <span class="number">10</span>)]</span><br><span class="line"><span class="comment">#根据第二个域进行排序</span></span><br><span class="line">sorted(students, key=operator.itemgetter(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<pre><code>[(&apos;dave&apos;, &apos;B&apos;, 10), (&apos;jane&apos;, &apos;B&apos;, 12), (&apos;john&apos;, &apos;A&apos;, 15)]
</code></pre><h1 id="pickle模块"><a href="#pickle模块" class="headerlink" title="pickle模块"></a>pickle模块</h1><p>该pickle模块实现了用于序列化和反序列化Python对象结构的二进制协议。 “Pickling”是将Python对象层次结构转换为字节流的过程， “unpickling”是反向操作，从而将字节流（来自二进制文件或类似字节的对象）转换回对象层次结构。pickle模块对于错误或恶意构造的数据是不安全的。</p>
<p>pickle协议和JSON（JavaScript Object Notation）的区别 ：</p>
<p>　　1. JSON是一种文本序列化格式（它输出unicode文本，虽然大部分时间它被编码utf-8），而pickle是二进制序列化格式;</p>
<p>　　2. JSON是人类可读的，而pickle则不是;</p>
<p>　　3. JSON是可互操作的，并且在Python生态系统之外广泛使用，而pickle是特定于Python的;</p>
<p>默认情况下，JSON只能表示Python内置类型的子集，而不能表示自定义类; pickle可以表示极其庞大的Python类型（其中许多是自动的，通过巧妙地使用Python的内省工具;复杂的案例可以通过实现特定的对象API来解决）。</p>
<p>pickle 数据格式是特定于Python的。它的优点是没有外部标准强加的限制，例如JSON或XDR（不能代表指针共享）; 但是这意味着非Python程序可能无法重建pickled Python对象。</p>
<p>默认情况下，pickle数据格式使用相对紧凑的二进制表示。如果您需要最佳尺寸特征，则可以有效地压缩数据。</p>
<p>模块接口</p>
<p>要序列化对象层次结构，只需调用该dumps()函数即可。同样，要对数据流进行反序列化，请调用该loads()函数。但是，如果您想要更多地控制序列化和反序列化，则可以分别创建一个Pickler或一个Unpickler对象。</p>
<p>pickle模块提供以下常量：</p>
<p>pickle.HIGHEST_PROTOCOL<br>整数， 可用的最高协议版本。这个值可以作为一个被传递协议的价值函数 dump()和dumps()以及该Pickler 构造函数。</p>
<p>pickle.DEFAULT_PROTOCOL<br>整数，用于编码的默认协议版本。可能不到HIGHEST_PROTOCOL。目前，默认协议是3，这是为Python 3设计的新协议。</p>
<p>pickle模块提供以下功能，使酸洗过程更加方便：</p>
<p>pickle.dump（obj，file，protocol = None，*，fix_imports = True ）<br>将obj对象的编码pickle编码表示写入到文件对象中，相当于Pickler(file,protocol).dump(obj)</p>
<p>可供选择的协议参数是一个整数，指定pickler使用的协议版本，支持的协议是0到HIGHEST_PROTOCOL。如果未指定，则默认为DEFAULT_PROTOCOL。如果指定为负数，则选择HIGHEST_PROTOCOL。</p>
<p>文件参数必须具有接受单个字节的参数写方法。因此，它可以是为二进制写入打开的磁盘文件， io.BytesIO实例或满足此接口的任何其他自定义对象。</p>
<p>如果fix_imports为true且protocol小于3，则pickle将尝试将新的Python 3名称映射到Python 2中使用的旧模块名称，以便使用Python 2可读取pickle数据流。</p>
<p>pickle.dumps（obj，protocol = None，*，fix_imports = True ）<br>将对象的pickled表示作为bytes对象返回，而不是将其写入文件。</p>
<p>参数protocol和fix_imports具有与in中相同的含义 dump()。</p>
<p>pickle.load（file，*，fix_imports = True，encoding =“ASCII”，errors =“strict” ）<br>从打开的文件对象 文件中读取pickle对象表示，并返回其中指定的重构对象层次结构。这相当于Unpickler(file).load()。</p>
<p>pickle的协议版本是自动检测的，因此不需要协议参数。超过pickle对象的表示的字节将被忽略。</p>
<p>参数文件必须有两个方法，一个采用整数参数的read()方法和一个不需要参数的readline()方法。两种方法都应返回字节。因此，文件可以是为二进制读取而打开的磁盘文件，io.BytesIO对象或满足此接口的任何其他自定义对象。</p>
<p>可选的关键字参数是fix_imports，encoding和errors，用于控制Python 2生成的pickle流的兼容性支持。如果fix_imports为true，则pickle将尝试将旧的Python 2名称映射到Python 3中使用的新名称。编码和 错误告诉pickle如何解码Python 2编码的8位字符串实例; 这些默认分别为’ASCII’和’strict’。该编码可以是“字节”作为字节对象读取这些8位串的实例。使用encoding=’latin1’所需的取储存NumPy的阵列和实例datetime，date并且time被Python 2解码。</p>
<p>pickle.loads（bytes_object，*，fix_imports = True，encoding =“ASCII”，errors =“strict” ）<br>从bytes对象读取pickle对象层次结构并返回其中指定的重构对象层次结构。</p>
<p>pickle的协议版本是自动检测的，因此不需要协议参数。超过pickle对象的表示的字节将被忽略。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"></span><br><span class="line">path = <span class="string">'test'</span></span><br><span class="line">f = open(path, <span class="string">'wb'</span>)</span><br><span class="line">data = &#123;<span class="string">'a'</span>:<span class="number">123</span>, <span class="string">'b'</span>:<span class="string">'ads'</span>, <span class="string">'c'</span>:[[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]&#125;</span><br><span class="line">pickle.dump(data, f)</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line">f1 = open(path, <span class="string">'rb'</span>)</span><br><span class="line">data1 = pickle.load(f1)</span><br><span class="line">print(data1)</span><br></pre></td></tr></table></figure></p>
<pre><code>{&apos;a&apos;: 123, &apos;b&apos;: &apos;ads&apos;, &apos;c&apos;: [[1, 2], [3, 4]]}
</code></pre><h1 id="feedparser-模块"><a href="#feedparser-模块" class="headerlink" title="feedparser 模块"></a>feedparser 模块</h1><p>feedparser是一个Python的Feed解析库，可以处理RSS ，CDF，Atom 。使用它我们可从任何 RSS 或 Atom 订阅源得到标题、链接和文章的条目了。<br>RSS(Really Simple Syndication,简易信息聚合)是一种描述和同步网站内容的格式你可以认为是一种定制个性化推送信息的服务。它能够解决你漫无目的的浏览网页的问题。它不会过时，信息越是过剩，它的意义也越加彰显。网络中充斥着大量的信息垃圾，每天摄入了太多自己根本不关心的信息。让自己关注的信息主动来找自己，且这些信息都是用户自己所需要的，这就是RSS的意义。</p>
<p>parse() 方法<br>feedparser 最为核心的函数自然是 parse() 解析 URL 地址的函数。<br>我们知道，每个RSS和Atom订阅源都包含一个标题（d.feed.title）和一组文章条目(d.entries)<br>通常每个文章条目都有一段摘要（d.entries[i].summary）,或者是包含了条目中实际文本的描述性标签（d.entries[i].description）<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> feedparser</span><br><span class="line">d=feedparser.parse(<span class="string">'http://xvjie.wang/atom.xml'</span>)</span><br><span class="line">d.feed <span class="comment"># 对应的值也是一个字典</span></span><br></pre></td></tr></table></figure></p>
<pre><code>{&apos;title&apos;: &apos;Voidmort&apos;,
 &apos;title_detail&apos;: {&apos;type&apos;: &apos;text/plain&apos;,
  &apos;language&apos;: None,
  &apos;base&apos;: &apos;http://xvjie.wang/atom.xml&apos;,
  &apos;value&apos;: &apos;Voidmort&apos;},
 &apos;links&apos;: [{&apos;href&apos;: &apos;http://xvjie.wang/atom.xml&apos;,
   &apos;rel&apos;: &apos;self&apos;,
   &apos;type&apos;: &apos;application/atom+xml&apos;},
  {&apos;href&apos;: &apos;https://xvjie.wang/&apos;, &apos;rel&apos;: &apos;alternate&apos;, &apos;type&apos;: &apos;text/html&apos;}],
 &apos;link&apos;: &apos;https://xvjie.wang/&apos;,
 &apos;updated&apos;: &apos;2020-03-15T06:43:28.902Z&apos;,
 &apos;updated_parsed&apos;: time.struct_time(tm_year=2020, tm_mon=3, tm_mday=15, tm_hour=6, tm_min=43, tm_sec=28, tm_wday=6, tm_yday=75, tm_isdst=0),
 &apos;id&apos;: &apos;https://xvjie.wang/&apos;,
 &apos;guidislink&apos;: False,
 &apos;authors&apos;: [{&apos;name&apos;: &apos;Voidmort&apos;}],
 &apos;author_detail&apos;: {&apos;name&apos;: &apos;Voidmort&apos;},
 &apos;author&apos;: &apos;Voidmort&apos;,
 &apos;generator_detail&apos;: {&apos;href&apos;: &apos;http://hexo.io/&apos;, &apos;name&apos;: &apos;Hexo&apos;},
 &apos;generator&apos;: &apos;Hexo&apos;}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d[<span class="string">'feed'</span>][<span class="string">'title'</span>]</span><br></pre></td></tr></table></figure>
<pre><code>&apos;Voidmort&apos;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d.feed.title    <span class="comment">#通过属性的方式访问</span></span><br></pre></td></tr></table></figure>
<pre><code>&apos;Voidmort&apos;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d.feed.title_detail</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;type&apos;: &apos;text/plain&apos;,
 &apos;language&apos;: None,
 &apos;base&apos;: &apos;http://xvjie.wang/atom.xml&apos;,
 &apos;value&apos;: &apos;Voidmort&apos;}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d.feed.link</span><br></pre></td></tr></table></figure>
<pre><code>&apos;https://xvjie.wang/&apos;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 该属性类型为列表，表示一组文章的条目</span></span><br><span class="line">d.entries[:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[{&apos;title&apos;: &apos;机器学习实战（三）&apos;,
  &apos;title_detail&apos;: {&apos;type&apos;: &apos;text/plain&apos;,
   &apos;language&apos;: None,
   &apos;base&apos;: &apos;http://xvjie.wang/atom.xml&apos;,
   &apos;value&apos;: &apos;机器学习实战（三）&apos;},
  &apos;links&apos;: [{&apos;href&apos;: &apos;https://xvjie.wang/2020/03/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/&apos;,
    &apos;rel&apos;: &apos;alternate&apos;,
    &apos;type&apos;: &apos;text/html&apos;}],
  &apos;link&apos;: &apos;https://xvjie.wang/2020/03/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/&apos;,
  &apos;id&apos;: &apos;https://xvjie.wang/2020/03/06/机器学习实战（三）/&apos;,
  &apos;guidislink&apos;: False,
  &apos;published&apos;: &apos;2020-03-06T02:15:50.000Z&apos;,
  &apos;published_parsed&apos;: time.struct_time(tm_year=2020, tm_mon=3, tm_mday=6, tm_hour=2, tm_min=15, tm_sec=50, tm_wday=4, tm_yday=66, tm_isdst=0),
  &apos;updated&apos;: &apos;2020-03-15T06:43:28.902Z&apos;,
  &apos;updated_parsed&apos;: time.struct_time(tm_year=2020, tm_mon=3, tm_mday=15, tm_hour=6, tm_min=43, tm_sec=28, tm_wday=6, tm_yday=75, tm_isdst=0),
  &apos;summary&apos;: &apos;&lt;h1 id=&quot;决策树的简介&quot;&gt;&lt;a href=&quot;#决策树的简介&quot; class=&quot;headerlink&quot;&apos;,
  &apos;summary_detail&apos;: {&apos;type&apos;: &apos;text/html&apos;,
   &apos;language&apos;: None,
   &apos;base&apos;: &apos;http://xvjie.wang/atom.xml&apos;,
   &apos;value&apos;: &apos;&lt;h1 id=&quot;决策树的简介&quot;&gt;&lt;a href=&quot;#决策树的简介&quot; class=&quot;headerlink&quot;&apos;},
  &apos;tags&apos;: [{&apos;term&apos;: &apos;机器学习实战&apos;,
    &apos;scheme&apos;: &apos;https://xvjie.wang/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/&apos;,
    &apos;label&apos;: None},
   {&apos;term&apos;: &apos;决策树&apos;,
    &apos;scheme&apos;: &apos;https://xvjie.wang/tags/%E5%86%B3%E7%AD%96%E6%A0%91/&apos;,
    &apos;label&apos;: None},
   {&apos;term&apos;: &apos;ID3&apos;, &apos;scheme&apos;: &apos;https://xvjie.wang/tags/ID3/&apos;, &apos;label&apos;: None}]},
 {&apos;title&apos;: &apos;Python虚拟环境的搭建&apos;,
  &apos;title_detail&apos;: {&apos;type&apos;: &apos;text/plain&apos;,
   &apos;language&apos;: None,
   &apos;base&apos;: &apos;http://xvjie.wang/atom.xml&apos;,
   &apos;value&apos;: &apos;Python虚拟环境的搭建&apos;},
  &apos;links&apos;: [{&apos;href&apos;: &apos;https://xvjie.wang/2020/02/19/Python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E7%9A%84%E6%90%AD%E5%BB%BA/&apos;,
    &apos;rel&apos;: &apos;alternate&apos;,
    &apos;type&apos;: &apos;text/html&apos;}],
  &apos;link&apos;: &apos;https://xvjie.wang/2020/02/19/Python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E7%9A%84%E6%90%AD%E5%BB%BA/&apos;,
  &apos;id&apos;: &apos;https://xvjie.wang/2020/02/19/Python虚拟环境的搭建/&apos;,
  &apos;guidislink&apos;: False,
  &apos;published&apos;: &apos;2020-02-19T03:03:50.000Z&apos;,
  &apos;published_parsed&apos;: time.struct_time(tm_year=2020, tm_mon=2, tm_mday=19, tm_hour=3, tm_min=3, tm_sec=50, tm_wday=2, tm_yday=50, tm_isdst=0),
  &apos;updated&apos;: &apos;2020-03-06T09:01:06.144Z&apos;,
  &apos;updated_parsed&apos;: time.struct_time(tm_year=2020, tm_mon=3, tm_mday=6, tm_hour=9, tm_min=1, tm_sec=6, tm_wday=4, tm_yday=66, tm_isdst=0),
  &apos;summary&apos;: &apos;&lt;p&gt;我使用的Ubuntu18已经自带了pyhon3.6，现在我想用pip安装一些其它的应用的版本和现有的有冲突，为了防止冲突，我需要另一个python环境。&lt;/p&gt;\n&lt;h1 id=&quot;python的安装&quot;&gt;&lt;a href=&quot;#python的安装&quot;&apos;,
  &apos;summary_detail&apos;: {&apos;type&apos;: &apos;text/html&apos;,
   &apos;language&apos;: None,
   &apos;base&apos;: &apos;http://xvjie.wang/atom.xml&apos;,
   &apos;value&apos;: &apos;&lt;p&gt;我使用的Ubuntu18已经自带了pyhon3.6，现在我想用pip安装一些其它的应用的版本和现有的有冲突，为了防止冲突，我需要另一个python环境。&lt;/p&gt;\n&lt;h1 id=&quot;python的安装&quot;&gt;&lt;a href=&quot;#python的安装&quot;&apos;},
  &apos;tags&apos;: [{&apos;term&apos;: &apos;virtualenv&apos;,
    &apos;scheme&apos;: &apos;https://xvjie.wang/categories/virtualenv/&apos;,
    &apos;label&apos;: None}]}]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">len(d.entries)   <span class="comment">#一共20篇文章</span></span><br></pre></td></tr></table></figure>
<pre><code>20
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[e.title <span class="keyword">for</span> e <span class="keyword">in</span> d.entries][:<span class="number">5</span>]         <span class="comment">#列出前5篇文章的标题</span></span><br></pre></td></tr></table></figure>
<pre><code>[&apos;机器学习实战（三）&apos;, &apos;Python虚拟环境的搭建&apos;, &apos;机器学习实战（二）&apos;, &apos;机器学习实战（一）&apos;, &apos;Django&apos;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d.entries[<span class="number">0</span>].summary   <span class="comment">#第一篇文章的摘要  和d.entries[0].description功能一样</span></span><br></pre></td></tr></table></figure>
<pre><code>&apos;&lt;h1 id=&quot;决策树的简介&quot;&gt;&lt;a href=&quot;#决策树的简介&quot; class=&quot;headerlink&quot;&apos;
</code></pre><h1 id="sign"><a href="#sign" class="headerlink" title="sign()"></a>sign()</h1><p>sign()是Python的Numpy中的取数字符号（数字前的正负号）的函数。</p>
<p><img src="20190125144039645.png" alt=""><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#导入numpy库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line"><span class="comment">#输入数据</span></span><br><span class="line">dataArr = [<span class="number">-0.2</span>, <span class="number">-1.1</span>, <span class="number">0</span>, <span class="number">2.3</span>, <span class="number">4.5</span>, <span class="number">0.0</span>]</span><br><span class="line">print(<span class="string">"输入数据为："</span>)</span><br><span class="line">print(dataArr)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#使用numpy的sign(x)函数求输入数据的符号</span></span><br><span class="line">signResult = np.sign(dataArr)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#打印出sign()的输出结果</span></span><br><span class="line">print(<span class="string">"\n使用sign函数的输出符号为："</span>,signResult)</span><br></pre></td></tr></table></figure></p>
<pre><code>输入数据为：
[-0.2, -1.1, 0, 2.3, 4.5, 0.0]

使用sign函数的输出符号为： [-1. -1.  0.  1.  1.  0.]
</code></pre><h1 id="numpy-linalg"><a href="#numpy-linalg" class="headerlink" title="numpy.linalg"></a>numpy.linalg</h1><p>numpy.linalg模块包含线性代数的函数。使用这个模块，可以计算逆矩阵、求特征值、解线性方程组以及求解行列式等。</p>
<h2 id="求矩阵的逆"><a href="#求矩阵的逆" class="headerlink" title="求矩阵的逆"></a>求矩阵的逆</h2><p>注：矩阵必须是方阵且可逆，否则会抛出LinAlgError异常。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">A = np.mat(<span class="string">"0 1 2;1 0 3;4 -3 8"</span>)</span><br><span class="line">A</span><br></pre></td></tr></table></figure></p>
<pre><code>matrix([[ 0,  1,  2],
        [ 1,  0,  3],
        [ 4, -3,  8]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用inv函数计算逆矩阵</span></span><br><span class="line">inv = np.linalg.inv(A)</span><br><span class="line">inv</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[-4.5,  7. , -1.5],
        [-2. ,  4. , -1. ],
        [ 1.5, -2. ,  0.5]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 检查原矩阵和求得的逆矩阵相乘的结果为单位矩阵</span></span><br><span class="line">A * inv</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]])
</code></pre><h2 id="求解线性方程组"><a href="#求解线性方程组" class="headerlink" title="求解线性方程组"></a>求解线性方程组</h2><p>numpy.linalg中的函数solve可以求解形如 Ax = b 的线性方程组，其中 A 为矩阵，b 为一维或二维的数组，x 是未知变量<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#创建矩阵和数组</span></span><br><span class="line">B = np.mat(<span class="string">"1 -2 1;0 2 -8;-4 5 9"</span>)</span><br><span class="line">b = np.array([<span class="number">0</span>,<span class="number">8</span>,<span class="number">-9</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用solve函数求解线性方程</span></span><br><span class="line">x = np.linalg.solve(B,b)</span><br><span class="line">x</span><br></pre></td></tr></table></figure></p>
<pre><code>array([29., 16.,  3.])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用dot函数检查求得的解是否正确</span></span><br><span class="line">np.dot(B , x)</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[ 0.,  8., -9.]])
</code></pre><h2 id="特征值和特征向量"><a href="#特征值和特征向量" class="headerlink" title="特征值和特征向量"></a>特征值和特征向量</h2><p>特征值（eigenvalue）即方程 Ax = ax 的根，是一个标量。</p>
<p>其中，A 是一个二维矩阵，x 是一个一维向量。特征向量（eigenvector）是关于特征值的向量</p>
<p>numpy.linalg模块中，eigvals函数可以计算矩阵的特征值，而eig函数可以返回一个包含特征值和对应的特征向量的元组<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建一个矩阵</span></span><br><span class="line">C = np.mat(<span class="string">"3 -2;1 0"</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 调用eigvals函数求解特征值</span></span><br><span class="line">c0 = np.linalg.eigvals(C)</span><br><span class="line">c0</span><br></pre></td></tr></table></figure></p>
<pre><code>array([2., 1.])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用eig函数求解特征值和特征向量 </span></span><br><span class="line"><span class="comment">#(该函数将返回一个元组，按列排放着特征值和对应的特征向量，其中第一列为特征值，第二列为特征向量)</span></span><br><span class="line">c1,c2 = np.linalg.eig(C)</span><br><span class="line">c1, c2</span><br></pre></td></tr></table></figure>
<pre><code>(array([2., 1.]),
 matrix([[0.89442719, 0.70710678],
         [0.4472136 , 0.70710678]]))
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用dot函数验证求得的解是否正确</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(c1)):</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"left:"</span>,np.dot(C,c2[:,i]))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"right:"</span>,c1[i] * c2[:,i])</span><br></pre></td></tr></table></figure>
<pre><code>left: [[1.78885438]
 [0.89442719]]
right: [[1.78885438]
 [0.89442719]]
left: [[0.70710678]
 [0.70710678]]
right: [[0.70710678]
 [0.70710678]]
</code></pre><h2 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h2><p>SVD（Singular Value Decomposition，奇异值分解）是一种因子分解运算，将一个矩阵分解为3个矩阵的乘积</p>
<p>numpy.linalg模块中的svd函数可以对矩阵进行奇异值分解。该函数返回3个矩阵——U、Sigma和V，其中U和V是正交矩阵，Sigma包含输入矩阵的奇异值。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 分解矩阵</span></span><br><span class="line">D = np.mat(<span class="string">"4 11 14;8 7 -2"</span>)</span><br><span class="line"><span class="comment"># 使用svd函数分解矩阵</span></span><br><span class="line">U,Sigma,V = np.linalg.svd(D,full_matrices=<span class="keyword">False</span>)</span><br><span class="line">U, Sigma, V</span><br></pre></td></tr></table></figure></p>
<pre><code>(matrix([[-0.9486833 , -0.31622777],
         [-0.31622777,  0.9486833 ]]),
 array([18.97366596,  9.48683298]),
 matrix([[-0.33333333, -0.66666667, -0.66666667],
         [ 0.66666667,  0.33333333, -0.66666667]]))
</code></pre><p>结果包含等式中左右两端的两个正交矩阵U和V，以及中间的奇异值矩阵Sigma<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用diag函数生成完整的奇异值矩阵。将分解出的3个矩阵相乘</span></span><br><span class="line">U * np.diag(Sigma) * V</span><br></pre></td></tr></table></figure></p>
<pre><code>matrix([[ 4., 11., 14.],
        [ 8.,  7., -2.]])
</code></pre><h2 id="广义逆矩阵"><a href="#广义逆矩阵" class="headerlink" title="广义逆矩阵"></a>广义逆矩阵</h2><p>使用numpy.linalg模块中的pinv函数进行求解,</p>
<p>注：inv函数只接受方阵作为输入矩阵，而pinv函数则没有这个限制<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建一个矩阵</span></span><br><span class="line">E = np.mat(<span class="string">"4 11 14;8 7 -2"</span>)</span><br><span class="line"><span class="comment"># 使用pinv函数计算广义逆矩阵</span></span><br><span class="line">pseudoinv = np.linalg.pinv(E)</span><br><span class="line">pseudoinv</span><br></pre></td></tr></table></figure></p>
<pre><code>matrix([[-0.00555556,  0.07222222],
        [ 0.02222222,  0.04444444],
        [ 0.05555556, -0.05555556]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将原矩阵和得到的广义逆矩阵相乘</span></span><br><span class="line">E * pseudoinv</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[ 1.00000000e+00, -9.29811783e-16],
        [-1.66533454e-16,  1.00000000e+00]])
</code></pre><h2 id="行列式"><a href="#行列式" class="headerlink" title="行列式"></a>行列式</h2><p>numpy.linalg模块中的det函数可以计算矩阵的行列式<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算矩阵的行列式</span></span><br><span class="line">F = np.mat(<span class="string">"3.0 4.0;5.0 6.0"</span>)</span><br><span class="line"><span class="comment"># 使用det函数计算行列式</span></span><br><span class="line">np.linalg.det(F)</span><br></pre></td></tr></table></figure></p>
<pre><code>-1.9999999999999971
</code></pre><p>3×6-4×5=-2 ?<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.eye((<span class="number">3</span>))</span><br></pre></td></tr></table></figure></p>
<pre><code>array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]])
</code></pre><h1 id="BeautifulSoup"><a href="#BeautifulSoup" class="headerlink" title="BeautifulSoup"></a>BeautifulSoup</h1><p>Beautiful Soup 是一个可以从HTML或XML文件中提取数据的Python库.它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式.Beautiful Soup会帮你节省数小时甚至数天的工作时间.</p>
<p><a href="https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/" target="_blank" rel="noopener">https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/</a></p>
<h1 id="power"><a href="#power" class="headerlink" title="power()"></a>power()</h1><p>函数解释：</p>
<p>power(A,B) ：求A的B次方，数学等价于$A^B$</p>
<p>其中A和B既可以是数字(标量),也可以是列表(向量)<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a , b = <span class="number">3</span>, <span class="number">4</span></span><br><span class="line">np.power(a, b)</span><br></pre></td></tr></table></figure></p>
<pre><code>81
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A, B = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="number">3</span></span><br><span class="line">np.power(A, B)</span><br></pre></td></tr></table></figure>
<pre><code>array([ 1,  8, 27], dtype=int32)
</code></pre><p>A B都是列表(向量)时候，必须len(A)=len(B)<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A, B = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">np.power(A, B)</span><br></pre></td></tr></table></figure></p>
<pre><code>array([  1,  32, 729], dtype=int32)
</code></pre><h1 id="nonzero"><a href="#nonzero" class="headerlink" title="nonzero()"></a>nonzero()</h1><p>nonzero(a)</p>
<p>nonzero函数是numpy中用于得到数组array中非零元素的位置（数组索引）的函数。它的返回值是一个长度为a.ndim(数组a的轴数)的元组，元组的每个元素都是一个整数数组，其值为非零元素的下标在对应轴上的值。</p>
<p>（1）只有a中非零元素才会有索引值，那些零值元素没有索引值；</p>
<p>（2）返回的索引值数组是一个2维tuple数组，该tuple数组中包含一维的array数组。其中，一维array向量的个数与a的维数是一致的。</p>
<p>（3）索引值数组的每一个array均是从一个维度上来描述其索引值。比如，如果a是一个二维数组，则索引值数组有两个array，第一个array从行维度来描述索引值；第二个array从列维度来描述索引值。</p>
<p>（4）transpose(np.nonzero(x))函数能够描述出每一个非零元素在不同维度的索引值。</p>
<p>（5）通过a[nonzero(a)]得到所有a中的非零值</p>
<h2 id="a是一维数组"><a href="#a是一维数组" class="headerlink" title="a是一维数组"></a>a是一维数组</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = [<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b = np.nonzero(a)</span><br><span class="line">b</span><br></pre></td></tr></table></figure>
<pre><code>(array([1, 2], dtype=int64),)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.array(b).ndim</span><br></pre></td></tr></table></figure>
<pre><code>2
</code></pre><h2 id="a是二维数组"><a href="#a是二维数组" class="headerlink" title="a是二维数组"></a>a是二维数组</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.array([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">3</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">9</span>]])</span><br><span class="line">b = np.nonzero(a)</span><br><span class="line">a, b</span><br></pre></td></tr></table></figure>
<pre><code>(array([[0, 0, 3],
        [0, 0, 0],
        [0, 0, 9]]),
 (array([0, 2], dtype=int64), array([2, 2], dtype=int64)))
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.array(b).ndim</span><br></pre></td></tr></table></figure>
<pre><code>2
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.transpose(np.nonzero(a))</span><br></pre></td></tr></table></figure>
<pre><code>array([[0, 2],
       [2, 2]], dtype=int64)
</code></pre><h1 id="frozenset"><a href="#frozenset" class="headerlink" title="frozenset()"></a>frozenset()</h1><h2 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h2><p>frozenset() 返回一个冻结的集合，冻结后集合不能再添加或删除任何元素。</p>
<h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><p>frozenset() 函数语法：</p>
<p>class frozenset([iterable])</p>
<h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><p>iterable – 可迭代的对象，比如列表、字典、元组等等。<br>返回值<br>返回新的 frozenset 对象，如果不提供任何参数，默认会生成空集合。</p>
<p>实例<br>以下实例展示了 frozenset() 的使用方法：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = frozenset(range(<span class="number">10</span>))     <span class="comment"># 生成一个新的不可变集合</span></span><br><span class="line">a</span><br></pre></td></tr></table></figure></p>
<pre><code>frozenset({0, 1, 2, 3, 4, 5, 6, 7, 8, 9})
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b = frozenset(<span class="string">'voidmort'</span>)</span><br><span class="line">b <span class="comment"># 创建不可变集合</span></span><br></pre></td></tr></table></figure>
<pre><code>frozenset({&apos;d&apos;, &apos;i&apos;, &apos;m&apos;, &apos;o&apos;, &apos;r&apos;, &apos;t&apos;, &apos;v&apos;})
</code></pre><h1 id="apriori-关联分析"><a href="#apriori-关联分析" class="headerlink" title="apriori 关联分析"></a>apriori 关联分析</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%pip install efficient-apriori</span><br></pre></td></tr></table></figure>
<pre><code>  Downloading https://files.pythonhosted.org/packages/5a/c6/ecdf3a32d23cada466634c649cf4f50fefe76f56eae53ecceff688b306be/efficient_apriori-1.1.1-py3-none-any.whl
Installing collected packages: efficient-apriori
Successfully installed efficient-apriori-1.1.1
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> efficient_apriori <span class="keyword">import</span> apriori</span><br><span class="line">transactions = [(<span class="string">'eggs'</span>, <span class="string">'bacon'</span>, <span class="string">'soup'</span>),</span><br><span class="line">                (<span class="string">'eggs'</span>, <span class="string">'bacon'</span>, <span class="string">'apple'</span>),</span><br><span class="line">                (<span class="string">'soup'</span>, <span class="string">'bacon'</span>, <span class="string">'banana'</span>)]</span><br><span class="line">itemsets, rules = apriori(transactions, min_support=<span class="number">0.5</span>, min_confidence=<span class="number">1</span>)</span><br><span class="line">print(rules)</span><br></pre></td></tr></table></figure>
<pre><code>[{eggs} -&gt; {bacon}, {soup} -&gt; {bacon}]
</code></pre><h1 id="NumPy-corrcoef"><a href="#NumPy-corrcoef" class="headerlink" title="NumPy-corrcoef()"></a>NumPy-corrcoef()</h1><p>numpy.corrcoef(x, y=None, rowvar=True, bias=<no value="">, ddof=<no value="">)</no></no></p>
<p>返回皮尔逊积矩相关系数。</p>
<p>相关系数矩阵之间的关系，</p>
<p>返回值r介于-1和1之间（含1）。r=0,没有相关性。</p>
<p>参数:</p>
<pre><code>x : array_like
包含多个变量和观测值的一维或二维数组。每行 x 表示一个变量，每列都是对所有这些变量的单个观察。也看到 rowvar 下面。

y : 阵列式，可选
一组附加的变量和观察值。 y 形状与 x .

罗瓦尔 : 可选的布尔
如果 rowvar 为真（默认值），则每行代表一个变量，列中包含观测值。否则，关系将被转置：每列表示一个变量，而行包含观测值。
</code></pre><p>返回:</p>
<pre><code>R : 变量的相关系数矩阵。
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">b = np.array([<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">correlation</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (((x-x.mean())/(x.std(ddof=<span class="number">0</span>)))*((y-y.mean())/(y.std(ddof=<span class="number">0</span>)))).mean() </span><br><span class="line"> </span><br><span class="line">correlation(a,b)</span><br></pre></td></tr></table></figure>
<pre><code>0.9999999999999999
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.corrcoef(a, b)</span><br></pre></td></tr></table></figure>
<pre><code>array([[1., 1.],
       [1., 1.]])
</code></pre>]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>python函数</tag>
      </tags>
  </entry>
  <entry>
    <title>我的流浪地球</title>
    <url>/2019/02/10/%E6%88%91%E7%9A%84%E6%B5%81%E6%B5%AA%E5%9C%B0%E7%90%83/</url>
    <content><![CDATA[<pre><code>二〇一九年二月十号，星期六，农历正月初六，假期的最后一天，明天就要奔赴帝都，开始我的流浪生活。
</code></pre><p><img src="https://s2.ax1x.com/2019/02/17/kyqmMF.jpg" alt="image"></p>
<p>流浪地球是我看过大刘的第二本书，而且看过两遍，第一遍是大约两年前年从北京回家的火车上，那一年三体火了，那一年也是我在帝都的第一年，第二遍是是去年我在从家到北京的火车上，看第二遍的理由是我忘记我看过了，读到一半才想起看过。为什么要说原著，因为相比较流浪地球的电影，我更想聊聊原著。</p>
<p>先说说电影吧，刚上映就被吵得火热，一片好评，但我是一向不看好中国影视对原著的改编，所以我怀着八分的心情去看，去掉男女主的台词，加上情怀给六分及格，看完的瞬间心情是担心三体的台词别这么糟糕，因为原著是叙述形式的，想通过影视作品表现原著的思想基本不可能了，能把故事讲明白的导演就能给十分了，这一次的电影很明显，讲的和原著中的故事情节没什么关系，好吧，吐槽也就这么多了，对于中国的第一部正真意义上的科幻大片，行星发动机的特效给9分，虽然看多了科幻大片和游戏，也没啥感觉，但是这个行星发动机的确是我想象中的样子，我本来是去看剧情的你给我放特效？电影剧情比较简单没啥可聊的，可吐槽的地方太多，懒得写了。</p>
<p>最后再说说原著吧，这是个中篇小说，已第一视角我来叙述，故事内容不多，也就讲了这几件事</p>
<pre><code>幼年时地球停转，进入地下城生活
初二时，地下城岩浆侵入，排队救援，因为主角年轻所以排前面获救，母亲和老人排后面被烧死
青年时参加奥运会找了个日本媳妇，并抽到了生育指标
主角爸在摧毁小行星的行动中殉职
高潮：一部分人认为太阳不会爆炸，开始起义，媳妇也加入了起义军，最后起义军攻入总部，主角也加入起义军，原政府领导被处死，然后太阳爆炸了，证明起义军是错的
结尾，主角老去，地球继续流浪中
</code></pre><p>至于电影中的过木星，就一笔带过了，貌似没出啥事，原著故事想说的是，到底谁对谁错，没有办法衡量，主角一直坚守的信念也在起义军攻入总部时崩塌了，他也不在乎太阳是否会爆炸了，随着起义军一起疯狂了，这是一个人一直坚持但是看不到希望的结果，地球上的人也都疯了，尽情的狂欢，其实这大概才是正真的末日吧，当太阳爆炸后，所有人又成为了同一根绳上的蚂蚱，继续流浪地球计划。</p>
<p>对了，疯狂外星人是改编大刘的《乡村教师》，郝建的作品还是比较喜剧的，但这个貌似和原著完全没有什么关系了。。。</p>
<p>最后.</p>
<p><strong><em>期待《三体》中</em></strong></p>
]]></content>
      <categories>
        <category>影评</category>
      </categories>
      <tags>
        <tag>影评</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实战（一）</title>
    <url>/2020/02/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<p>此blog是机器学习实战这本书的读书笔记</p>
<h1 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h1><p>用计算机来彰显数据背后真正的意义，这才是机器学习的真正含义。</p>
<p>在分类算法中目标变量的类型通常是标称型的，而在回归算法中通常是连续型的。</p>
<p>训练样本必须知道目标变量的值，以便机器学习算法可以发现特征和目标变量之间的关系。</p>
<p>特征或者属性通常是训练样本集的列，它们是独立测量的结果，多个特征联系在一起共同组成一个训练样本。</p>
<p>监督学习：</p>
<pre><code>k-邻近算法  线性回归    朴素贝叶斯算法  局部加权线性回归
支持向量机  Ridge回归   决策树  Lasso最小回归系数
</code></pre><p>无监督学习：</p>
<pre><code>K-均值  最大期望算法    DBSCAN  Parzen窗设计
</code></pre><h1 id="如何选择合适的算法"><a href="#如何选择合适的算法" class="headerlink" title="如何选择合适的算法"></a>如何选择合适的算法</h1><p>如果要预测目标变量的值，选择监督学习算法，否则无监督学习算法。</p>
<p>确定目标变量类型：</p>
<p>离散型：True/False，1/2/3，A/B/C等，选择分类算法</p>
<p>连续型：0.0 ~ 100， -99 ~ 99， +∞ ~ -∞等，选择回归算法</p>
<p>一般来说发现最好的算法的关键是反复试错迭代。</p>
<h1 id="开发机器学习应用程序的步骤"><a href="#开发机器学习应用程序的步骤" class="headerlink" title="开发机器学习应用程序的步骤"></a>开发机器学习应用程序的步骤</h1><ol>
<li>收集数据</li>
<li>准备输入数据</li>
<li>分析输入数据</li>
<li>训练算法</li>
<li>测试算法</li>
<li>使用算法</li>
</ol>
]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实战</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实战（七）</title>
    <url>/2020/04/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%83%EF%BC%89/</url>
    <content><![CDATA[<h1 id="利用AdaBoost元算法提高分类性能"><a href="#利用AdaBoost元算法提高分类性能" class="headerlink" title="利用AdaBoost元算法提高分类性能"></a>利用AdaBoost元算法提高分类性能</h1><p>在做决定时，大家可能会吸取多个专家而不是一个人的意见，机器学习也有类似的算法，这就是元算法（meta-algorithm）。<br>元算法是对其他算法进行组合的一种方式。</p>
<h1 id="基于数据集多重抽样的分类器"><a href="#基于数据集多重抽样的分类器" class="headerlink" title="基于数据集多重抽样的分类器"></a>基于数据集多重抽样的分类器</h1><p>前面已经学习了五种不同的分类算法，它们各有优缺点，我们可以将不同的分类器组合起来，这种组合结果则呗称为集成方法（ensemble method）或者元算法（meta-algorithm）。<br>集成方法有多种形式：不同算法的集成，同一种算法不同设置的集成，数据集不同部分分配给不同分类器之后的集成，同一种分类器多个不同实例的两种计算方法。</p>
<p>AdaBoost</p>
<pre><code>优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整
缺点：对离群点敏感
适用数据类型：数值型和标称型数据
</code></pre><h2 id="bagging：基于数据随机抽样的分类器构建方法"><a href="#bagging：基于数据随机抽样的分类器构建方法" class="headerlink" title="bagging：基于数据随机抽样的分类器构建方法"></a>bagging：基于数据随机抽样的分类器构建方法</h2><p>自举汇聚法（bootstrap aggregating），也称为bagging方法，从原始集合随机选择一个样本，然后随机选择的样本来代替这个样本，意为有放回的取样得到的。</p>
<p>在有放回抽样得到S个数据集后，将某个学习算法分别作用在每个数据集上，就得到了S个分类器，对新数据进行分类时，用这个S个分类器进行投票表决的方法决定分类结果。</p>
<p>其他bagging的方法：随机森林（random forest）</p>
<h2 id="boosting"><a href="#boosting" class="headerlink" title="boosting"></a>boosting</h2><p>boosting是一种类似bagging的方法。bagging的分类器时通过串行训练而获得的，每个新分类器都根据已训练处的分类器的性能来进行训练。boosting是通过集中关注被已有分类器错分的那些数据来获得新的分类器。</p>
<p>由于boosting分类结果是基于所有分类器的加权求和结果的，因此boosting与bagging不一样。<br>bagging中的分类权重是相等的，boosting中的分类器权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。</p>
<p>AdaBoost的一般流程</p>
<ol>
<li>收集数据</li>
<li>准备数据：依赖于所有使用的弱分类器类型，本章使用的是单层决策树，这种分类器可以处理任何数据类型。作为弱分类器，简单的分类器效果更好。</li>
<li>分析数据</li>
<li>训练算法：AdaBoost的大部分时间都在训练上，分类器将多次在同一数据集上训练弱分类器</li>
<li>测试算法：计算分类的错误率</li>
<li>使用算法：同SVM一样，AdaBoost预测两个类别中的一个，如果要应用在多分类问题，要进行修改。</li>
</ol>
<h1 id="训练算法：基于错误提升分类器的性能"><a href="#训练算法：基于错误提升分类器的性能" class="headerlink" title="训练算法：基于错误提升分类器的性能"></a>训练算法：基于错误提升分类器的性能</h1><p>AdaBoost是adaptive boosting（自适应boosting）的缩写，其运行过程如下：训练数据中的每个样本，并赋予其中一个权重，这些权重构成向量D。<br>一开始，这些权重都初始化成相等值，首先在训练数据上训练出一个弱分类器并计算该分类器的错误率，然后在同一数据集上再次训练弱分类器。<br>在分类器的第二次训练中，将重新调整每个样本的权重，其中第一次分对的样本权重将会降低，而第一次分错的样本权重将会提高。为了从所有弱分类器中得到最终的分类结果<br>AdaBoost为每个分类器都分配一个权重值alpha，这些alpha值是基于每个弱分类器的错误率进行计算的。<br>其中错误率ε的定义为：</p>
<p>$$\epsilon=\frac{未正确分类的样本数目}{所有样本数目}$$</p>
<p>alpha的计算公式如下：</p>
<p>$$\alpha=\frac{1}{2}ln(\frac{1-\epsilon}{\epsilon})$$</p>
<p>计算流程图如下：</p>
<p><img src="07_fig01.jpg" alt=""></p>
<p>左边是数据集，其中直方图的不同宽度表示样本的权重。在经过一个分类器后，加权的预测结果会通过三角形的alpha进行加权。每个三角形中输出的加权结果在圆形中求和，得到输出结果。</p>
<p>计算出alpha值后，可以对权重向量D进行更新。</p>
<p>如果样本被正确分类，权重降低</p>
<p><img src="07_fig02.jpg" alt=""></p>
<p>如果样本被错分，权重增加</p>
<p><img src="07_fig03.jpg" alt=""></p>
<p>在计算出D之后，AdaBoost又开始进行下一轮迭代。AdaBoost算法会不断的重复训练和调整权重，直到训练错误率为0或者达到指定次数。</p>
<h1 id="基于单层决策树构建弱分类器"><a href="#基于单层决策树构建弱分类器" class="headerlink" title="基于单层决策树构建弱分类器"></a>基于单层决策树构建弱分类器</h1><p>单层决策树（decision stump，也称决策树桩）是一种简单的决策树。先构建一个简单的数据集。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadSimpData</span><span class="params">()</span>:</span></span><br><span class="line">    datMat = matrix([[<span class="number">1.</span>, <span class="number">2.1</span>],</span><br><span class="line">                     [<span class="number">2.</span>, <span class="number">1.1</span>],</span><br><span class="line">                     [<span class="number">1.3</span>, <span class="number">1.</span>],</span><br><span class="line">                     [<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">                     [<span class="number">2.</span>, <span class="number">1.</span>]])</span><br><span class="line">    classLabels = [<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">-1.0</span>, <span class="number">-1.0</span>, <span class="number">1.0</span>]</span><br><span class="line">    <span class="keyword">return</span> datMat, classLabels</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.patches <span class="keyword">import</span> Circle</span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>]=<span class="keyword">False</span> <span class="comment"># 用来正常显示负号</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotSupportVectors</span><span class="params">()</span>:</span></span><br><span class="line">    xcord0 = []</span><br><span class="line">    ycord0 = []</span><br><span class="line">    xcord1 = []</span><br><span class="line">    ycord1 = []</span><br><span class="line">    </span><br><span class="line">    datMat, classLabels = loadSimpData()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classLabels)):</span><br><span class="line">        <span class="keyword">if</span> (classLabels[i] == <span class="number">-1</span>):</span><br><span class="line">            xcord0.append(datMat[i, <span class="number">0</span>])</span><br><span class="line">            ycord0.append(datMat[i, <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            xcord1.append(datMat[i, <span class="number">0</span>])</span><br><span class="line">            ycord1.append(datMat[i, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    ax.scatter(xcord0, ycord0, marker=<span class="string">'s'</span>, s=<span class="number">90</span>)</span><br><span class="line">    ax.scatter(xcord1, ycord1, marker=<span class="string">'o'</span>, s=<span class="number">50</span>, c=<span class="string">'red'</span>)</span><br><span class="line">    plt.title(<span class="string">'单层决策树测试数据'</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plotSupportVectors()</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%83%EF%BC%89_9_0.png" alt="png"></p>
<p>如果想要选择一个与坐标轴平行的线来把圆点和方形分开是不可能的，这就是单层决策树难以处理的一个著名问题。通过使用多棵单层决策树我们就可以构建出一个能够正确处理该数据集的分类器。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datMat, classLabels = loadSimpData()</span><br></pre></td></tr></table></figure>
<p>接下来构建单层决策树</p>
<p>第一个函数用来测试是否有某个值小于或者大于我们正在测试的阈值。</p>
<p>第二个函数是在一个加权数据集中循环，并找到具有低错误率的单层决策树</p>
<p>伪代码：</p>
<pre><code>将最小错误率minError设备+∞
对数据集中的每一个特征（第一层循环）：
    对每个步长（第二层循环）：
        对每个不等号（第三层循环）：
            建立一棵单层决策树并利用加权数据集对它测试
            如果错误率低于minError，则将当前单层决策树设为最佳单层决策树
返回最佳单层决策树
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stumpClassify</span><span class="params">(dataMatrix, dimen, threshVal, threshIneq)</span>:</span></span><br><span class="line">    retArray = ones((shape(dataMatrix)[<span class="number">0</span>], <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">if</span> threshIneq == <span class="string">'lt'</span>:</span><br><span class="line">        retArray[dataMatrix[:, dimen] &lt;= threshVal] = <span class="number">-1.0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        retArray[dataMatrix[:, dimen] &gt; threshVal] = <span class="number">-1.0</span></span><br><span class="line">    <span class="keyword">return</span> retArray</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildStump</span><span class="params">(dataArr, classLabels, D)</span>:</span></span><br><span class="line">    dataMatrix = mat(dataArr)</span><br><span class="line">    labelMat = mat(classLabels).T</span><br><span class="line">    m, n = shape(dataMatrix)</span><br><span class="line">    numSteps = <span class="number">10.0</span></span><br><span class="line">    bestStump = &#123;&#125;</span><br><span class="line">    bestClasEst = mat(zeros((m, <span class="number">1</span>)))</span><br><span class="line">    minError = inf</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        rangeMin = dataMatrix[:, i].min()</span><br><span class="line">        rangeMax = dataMatrix[:, i].max()</span><br><span class="line">        stepSize = (rangeMax-rangeMin)/numSteps</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">-1</span>, int(numSteps)+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> inequal <span class="keyword">in</span> [<span class="string">'lt'</span>, <span class="string">'gt'</span>]:</span><br><span class="line">                threshVal = (rangeMin + float(j) * stepSize)</span><br><span class="line">                predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)</span><br><span class="line">                errArr = mat(ones((m, <span class="number">1</span>)))</span><br><span class="line">                errArr[predictedVals == labelMat] = <span class="number">0</span></span><br><span class="line">                weightedError = D.T*errArr</span><br><span class="line">                <span class="comment">#print("split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f" % (i, threshVal, inequal, weightedError))</span></span><br><span class="line">                <span class="keyword">if</span> weightedError &lt; minError:</span><br><span class="line">                    minError = weightedError</span><br><span class="line">                    bestClasEst = predictedVals.copy()</span><br><span class="line">                    bestStump[<span class="string">'dim'</span>] = i</span><br><span class="line">                    bestStump[<span class="string">'thresh'</span>] = threshVal</span><br><span class="line">                    bestStump[<span class="string">'ineq'</span>] = inequal</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> bestStump, minError, bestClasEst</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">D = mat(ones((<span class="number">5</span>, <span class="number">1</span>))/<span class="number">5</span>)</span><br><span class="line">buildStump(datMat, classLabels, D)</span><br></pre></td></tr></table></figure>
<pre><code>({&apos;dim&apos;: 0, &apos;thresh&apos;: 1.3, &apos;ineq&apos;: &apos;lt&apos;},
 matrix([[0.2]]),
 array([[-1.],
        [ 1.],
        [-1.],
        [-1.],
        [ 1.]]))
</code></pre><p>第一个函数stumpClassify()是通过阈值比较对数据进行分类的。所有在阈值一边的数据会分到类别-1，而在另一边的数据分到类别+1。</p>
<p>第二个函数buildStump()将会遍历stumpClassify()函数所有的可能性，并找到数据集上最佳的单层决策树。这里的最佳是基于数据权重向量D来定义的。<br>bestStump的字典用于储存给定权重向量D时所得到的最佳单层决策树的相关信息。<br>变量numSteps用于在特征的所有可能值上进行遍历。而变量minError则在开始初始化为无穷大，之后寻找可能的最小错误率。</p>
<p>这个单层决策树的核心思想就是找到一个数，通过比较这个数，找到拟合正确率最大的。</p>
<p>结果dim=0，代表用第一列比较，thresh=1.3，ineq=lt，代表小于等于1.3的结果</p>
<p>第一列：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datMat[:,<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[1. ],
        [2. ],
        [1.3],
        [1. ],
        [2. ]])
</code></pre><p>这一列大于1.3的为1，小于等于1.3的为-1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datMat[:,<span class="number">0</span>] &lt;= <span class="number">1.3</span></span><br></pre></td></tr></table></figure>
<pre><code>matrix([[ True],
        [False],
        [ True],
        [ True],
        [False]])
</code></pre><p>所以预测结果为：[-1,1-1,-1,1] 正确率为4/5，所以错误率为0.2。</p>
<p>这就是一个单层决策树，只考虑一列数的影响，而非全局。</p>
<h1 id="完整AdaBoost"><a href="#完整AdaBoost" class="headerlink" title="完整AdaBoost"></a>完整AdaBoost</h1><p>我们利用上面的单层决策树来实现完整的Adaboost，伪代码如下：</p>
<pre><code>对每次迭代：
    利用buildStump()函数找到最佳的单层决策树
    将最佳单层决策树加入到单层决策树数组
    计算alpha
    计算新的权重向量D
    更新累计类别估计值
    如果错误率等于0.0则退出循环
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaBoostTrainDS</span><span class="params">(dataArr, classLabels, numIt=<span class="number">40</span>)</span>:</span></span><br><span class="line">    weakClassArr = []</span><br><span class="line">    m = shape(dataArr)[<span class="number">0</span>]</span><br><span class="line">    D = mat(ones((m,<span class="number">1</span>))/m)</span><br><span class="line">    aggClassEst = mat(zeros((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numIt):</span><br><span class="line">        bestStump, error, classEst = buildStump(dataArr, classLabels, D)</span><br><span class="line">        <span class="comment">#print("D:",D.T)</span></span><br><span class="line">        alpha = float(<span class="number">0.5</span>*log((<span class="number">1.0</span>-error)/max(error, <span class="number">1e-16</span>)))</span><br><span class="line">        bestStump[<span class="string">'alpha'</span>] = alpha</span><br><span class="line">        weakClassArr.append(bestStump) </span><br><span class="line">        <span class="comment">#print("classEst: ", classEst.T)</span></span><br><span class="line">        expon = multiply(<span class="number">-1</span>*alpha*mat(classLabels).T, classEst) <span class="comment"># 混淆矩阵</span></span><br><span class="line">        D = multiply(D, exp(expon))</span><br><span class="line">        D = D/D.sum()</span><br><span class="line">        aggClassEst += alpha*classEst</span><br><span class="line">        <span class="comment">#print("aggClassEst: ", aggClassEst.T)</span></span><br><span class="line">        aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T, ones((m, <span class="number">1</span>)))</span><br><span class="line">        <span class="comment">#print("predictedVals: ", sign(aggClassEst).T)</span></span><br><span class="line">        errorRate = aggErrors.sum()/m</span><br><span class="line">        <span class="comment">#print("total error: ", errorRate, "\n")</span></span><br><span class="line">        <span class="keyword">if</span> errorRate == <span class="number">0.0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> weakClassArr, aggClassEst</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">adaBoostTrainDS(datMat, classLabels, <span class="number">9</span>)</span><br></pre></td></tr></table></figure>
<pre><code>D: [[0.2 0.2 0.2 0.2 0.2]]
classEst:  [[-1.  1. -1. -1.  1.]]
aggClassEst:  [[-0.69314718  0.69314718 -0.69314718 -0.69314718  0.69314718]]
predictedVals:  [[-1.  1. -1. -1.  1.]]
total error:  0.2 

D: [[0.5   0.125 0.125 0.125 0.125]]
classEst:  [[ 1.  1. -1. -1. -1.]]
aggClassEst:  [[ 0.27980789  1.66610226 -1.66610226 -1.66610226 -0.27980789]]
predictedVals:  [[ 1.  1. -1. -1. -1.]]
total error:  0.2 

D: [[0.28571429 0.07142857 0.07142857 0.07142857 0.5       ]]
classEst:  [[1. 1. 1. 1. 1.]]
aggClassEst:  [[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]
predictedVals:  [[ 1.  1. -1. -1.  1.]]
total error:  0.0 

[{&apos;dim&apos;: 0, &apos;thresh&apos;: 1.3, &apos;ineq&apos;: &apos;lt&apos;, &apos;alpha&apos;: 0.6931471805599453},
 {&apos;dim&apos;: 1, &apos;thresh&apos;: 1.0, &apos;ineq&apos;: &apos;lt&apos;, &apos;alpha&apos;: 0.9729550745276565},
 {&apos;dim&apos;: 0, &apos;thresh&apos;: 0.9, &apos;ineq&apos;: &apos;lt&apos;, &apos;alpha&apos;: 0.8958797346140273}]
</code></pre><p>结果包含三个字典，其中包含了分类所需要的所有信息。</p>
<h1 id="测试算法：基于AdaBoost的分类"><a href="#测试算法：基于AdaBoost的分类" class="headerlink" title="测试算法：基于AdaBoost的分类"></a>测试算法：基于AdaBoost的分类</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaClassify</span><span class="params">(datToClass, classifierArr)</span>:</span></span><br><span class="line">    dataMatrix = mat(datToClass)</span><br><span class="line">    m = shape(dataMatrix)[<span class="number">0</span>]</span><br><span class="line">    aggClassEst = mat(zeros((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classifierArr)):</span><br><span class="line">        classEst = stumpClassify(dataMatrix, classifierArr[i][<span class="string">'dim'</span>], classifierArr[i][<span class="string">'thresh'</span>], classifierArr[i][<span class="string">'ineq'</span>])</span><br><span class="line">        aggClassEst += classifierArr[i][<span class="string">'alpha'</span>]*classEst</span><br><span class="line">        <span class="comment">#print(aggClassEst.T)</span></span><br><span class="line">    <span class="keyword">return</span> sign(aggClassEst)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classifierArr = adaBoostTrainDS(datMat, classLabels, <span class="number">30</span>)</span><br><span class="line">adaClassify([<span class="number">0</span>, <span class="number">0</span>], classifierArr)</span><br></pre></td></tr></table></figure>
<pre><code>D: [[0.2 0.2 0.2 0.2 0.2]]
classEst:  [[-1.  1. -1. -1.  1.]]
aggClassEst:  [[-0.69314718  0.69314718 -0.69314718 -0.69314718  0.69314718]]
predictedVals:  [[-1.  1. -1. -1.  1.]]
total error:  0.2 

D: [[0.5   0.125 0.125 0.125 0.125]]
classEst:  [[ 1.  1. -1. -1. -1.]]
aggClassEst:  [[ 0.27980789  1.66610226 -1.66610226 -1.66610226 -0.27980789]]
predictedVals:  [[ 1.  1. -1. -1. -1.]]
total error:  0.2 

D: [[0.28571429 0.07142857 0.07142857 0.07142857 0.5       ]]
classEst:  [[1. 1. 1. 1. 1.]]
aggClassEst:  [[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]
predictedVals:  [[ 1.  1. -1. -1.  1.]]
total error:  0.0 

[[-0.69314718]]
[[-1.66610226]]
[[-2.56198199]]


matrix([[-1.]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">adaClassify([[<span class="number">5</span>, <span class="number">5</span>], [<span class="number">0</span>, <span class="number">0</span>]], classifierArr)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 0.69314718 -0.69314718]]
[[ 1.66610226 -1.66610226]]
[[ 2.56198199 -2.56198199]]


matrix([[ 1.],
        [-1.]])
</code></pre><h1 id="实例：在一个复杂数据集上应用AdaBoost"><a href="#实例：在一个复杂数据集上应用AdaBoost" class="headerlink" title="实例：在一个复杂数据集上应用AdaBoost"></a>实例：在一个复杂数据集上应用AdaBoost</h1><ol>
<li>收集数据</li>
<li>准备数据：确保类别标签是+1和-1而非0和1</li>
<li>分析数据</li>
<li>训练算法：在数据上，利用adaBoostTrainDS()函数训练出一系列分类器</li>
<li>测试算法：用AdaBoost和Logistic回归对比</li>
<li>使用算法：观察该例子上的错误率</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    numFeat = len(open(fileName).readline().split(<span class="string">'\t'</span>))</span><br><span class="line">    dataMat = []; labelMat = []</span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr =[]</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeat<span class="number">-1</span>):</span><br><span class="line">            lineArr.append(float(curLine[i]))</span><br><span class="line">        dataMat.append(lineArr)</span><br><span class="line">        labelMat.append(float(curLine[<span class="number">-1</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat,labelMat</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataArr, labelArr = loadDataSet(<span class="string">'MLiA_SourceCode/machinelearninginaction/Ch07/horseColicTraining2.txt'</span>)</span><br><span class="line">classifierArray = adaBoostTrainDS(dataArr, labelArr, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">testArr, testlabelArr = loadDataSet(<span class="string">'MLiA_SourceCode/machinelearninginaction/Ch07/horseColicTest2.txt'</span>)</span><br><span class="line">prediction10 = adaClassify(testArr, classifierArray)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">errArr=mat(ones((<span class="number">67</span>, <span class="number">1</span>)))</span><br><span class="line">errArr[prediction10 != mat(testlabelArr).T].sum()</span><br></pre></td></tr></table></figure>
<pre><code>16.0
</code></pre><p>分类错误为16个，错误率23%，明显优于逻辑回归的33%。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classifierNum = [<span class="number">1</span>, <span class="number">10</span> , <span class="number">50</span>, <span class="number">100</span>, <span class="number">500</span>, <span class="number">1000</span>, <span class="number">10000</span>]</span><br><span class="line">trainerrArr=mat(ones((<span class="number">299</span>, <span class="number">1</span>)))</span><br><span class="line">testerrArr=mat(ones((<span class="number">67</span>, <span class="number">1</span>)))</span><br><span class="line">print(<span class="string">"分类器数目\t训练错误率\t测试错误率"</span>)</span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> classifierNum:</span><br><span class="line">    classifierArray = adaBoostTrainDS(dataArr, labelArr, n)</span><br><span class="line">    trainPrediction = adaClassify(dataArr, classifierArray)</span><br><span class="line">    testPrediction = adaClassify(testArr, classifierArray)</span><br><span class="line">    trainErrRate = trainerrArr[trainPrediction != mat(labelArr).T].sum()/<span class="number">299</span></span><br><span class="line">    testErrRate = testerrArr[testPrediction != mat(testlabelArr).T].sum()/<span class="number">67</span></span><br><span class="line">    print(<span class="string">"%d\t\t%.2f\t\t%.2f"</span> % (n, trainErrRate, testErrRate))</span><br></pre></td></tr></table></figure>
<pre><code>分类器数目    训练错误率    测试错误率
1        0.28        0.27
10        0.23        0.24
50        0.19        0.21
100        0.19        0.22
500        0.16        0.25
1000        0.14        0.31
10000        0.11        0.33
</code></pre><p>观察上表发现，随着分类器的增加训练错误率在减小，测试错误率在达到一个最小值后又开始上升，这种现象称为过拟合（overfitting）。</p>
<p>AdaBoost和SVM有很多相似之处，我们可以把弱分类器想象成SVM的一个核函数，也可以按照最大化某个最小间隔的方式重写AdaBoost算法。而它们的不同之处在于其所定义的间隔计算方式有所不同，因此导致结果也不同。特别是在高纬度空间下，这两者之间的差异就更加明显。</p>
<h1 id="非均衡问题分类"><a href="#非均衡问题分类" class="headerlink" title="非均衡问题分类"></a>非均衡问题分类</h1><p>之前我们假设类别的分类代价是一样的，这样就会出现一系列问题，例如：我们预测马会死，人们就可能给马实施安乐死而不是通过治疗来避免死亡，我们的预测也许是错误的，马本来可以继续活着，毕竟我们的分类器只有80%的精确率。<br>如果过滤垃圾邮件，合法的邮件也被认为是垃圾邮件呢，癌症检测情愿误判也不能漏判。</p>
<p>在大多数情况下不同类别的分类代价并不相等。接下来讨论一种新的分类器度量方法。</p>
<h2 id="其他分类性能度量指标：正确率、召回率、及ROC曲线"><a href="#其他分类性能度量指标：正确率、召回率、及ROC曲线" class="headerlink" title="其他分类性能度量指标：正确率、召回率、及ROC曲线"></a>其他分类性能度量指标：正确率、召回率、及ROC曲线</h2><p>错误率指的是在所有测试样例中错分的样例比例，这样的度量错误掩盖了样例如何被分错的事实。</p>
<p>在机器学习中有一个普遍适用的称为混淆矩阵（confusion matrix）的工具，它可以帮助人们更好的了解分类中的错误，有这样一个关于在房子周围可能发现的动物类型的预测。</p>
<table>
<thead>
<tr>
<th>.</th>
<th>预</th>
<th>测</th>
<th>结</th>
<th>果</th>
</tr>
</thead>
<tbody>
<tr>
<td>真</td>
<td></td>
<td>狗</td>
<td>猫</td>
<td>鼠 </td>
</tr>
<tr>
<td>实</td>
<td>狗</td>
<td>24</td>
<td>2</td>
<td>5</td>
</tr>
<tr>
<td>结</td>
<td>猫</td>
<td>2</td>
<td>27</td>
<td>0</td>
</tr>
<tr>
<td>果</td>
<td>鼠</td>
<td>4</td>
<td>2</td>
<td>30</td>
</tr>
</tbody>
</table>
<p>利用混淆矩阵就可以更好地理解分类中的错误了。如果矩阵非对角元素均为0，就会得到一个完美的分类器。</p>
<p>接下来考虑另外一个混淆矩阵。</p>
<table>
<thead>
<tr>
<th>.</th>
<th></th>
<th>预测结果</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>真</td>
<td></td>
<td>+1</td>
<td>-1</td>
</tr>
<tr>
<td>实</td>
<td>+1</td>
<td>真正例（TP）</td>
<td>伪反例（FN） </td>
</tr>
<tr>
<td>结</td>
<td>-1</td>
<td>伪证例（FP）</td>
<td>真反例（TN）</td>
</tr>
<tr>
<td>果</td>
<td></td>
</tr>
</tbody>
</table>
<p>正确率（precision） = TP/(TP+FP)</p>
<p>召回率（Recall） = TP/(TP+FN)</p>
<p>我们可以构造一个高正确率或搞召回率的分类器，但是奋男保证两者都成立。</p>
<p>另一种度量分类中的非均衡性的工具是ROC曲线（ROC curve）,ROC代表接收者操作特征（receiver operating characteristic），他最早在二战期间由电器工程师构建雷达系统时使用过。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotROC</span><span class="params">(predStrengths, classLabels)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    cur = (<span class="number">1.0</span>,<span class="number">1.0</span>) <span class="comment"># cursor</span></span><br><span class="line">    ySum = <span class="number">0.0</span> <span class="comment"># variable to calculate AUC</span></span><br><span class="line">    numPosClas = sum(array(classLabels)==<span class="number">1.0</span>)</span><br><span class="line">    yStep = <span class="number">1</span>/float(numPosClas); xStep = <span class="number">1</span>/float(len(classLabels)-numPosClas)</span><br><span class="line">    sortedIndicies = predStrengths.argsort() <span class="comment"># get sorted index, it's reverse</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    fig.clf()</span><br><span class="line">    ax = plt.subplot(<span class="number">111</span>)</span><br><span class="line">    <span class="comment"># loop through all the values, drawing a line segment at each point</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> sortedIndicies.tolist()[<span class="number">0</span>]:</span><br><span class="line">        <span class="keyword">if</span> classLabels[index] == <span class="number">1.0</span>:</span><br><span class="line">            delX = <span class="number">0</span></span><br><span class="line">            delY = yStep</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            delX = xStep</span><br><span class="line">            delY = <span class="number">0</span></span><br><span class="line">            ySum += cur[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># draw line from cur to (cur[0]-delX,cur[1]-delY)</span></span><br><span class="line">        ax.plot([cur[<span class="number">0</span>],cur[<span class="number">0</span>]-delX],[cur[<span class="number">1</span>],cur[<span class="number">1</span>]-delY], c=<span class="string">'b'</span>)</span><br><span class="line">        cur = (cur[<span class="number">0</span>]-delX,cur[<span class="number">1</span>]-delY)</span><br><span class="line">    ax.plot([<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>],<span class="string">'b--'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'False positive rate'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'True positive rate'</span>)</span><br><span class="line">    plt.title(<span class="string">'ROC curve for AdaBoost horse colic detection system'</span>)</span><br><span class="line">    ax.axis([<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"the Area Under the Curve is: "</span>,ySum*xStep)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataArr, labelArr = loadDataSet(<span class="string">'MLiA_SourceCode/machinelearninginaction/Ch07/horseColicTraining2.txt'</span>)</span><br><span class="line">classifierArray, aggClassEst = adaBoostTrainDS(dataArr, labelArr, <span class="number">10</span>)</span><br><span class="line">plotROC(aggClassEst.T, labelArr)</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%83%EF%BC%89_40_0.png" alt="png"></p>
<pre><code>the Area Under the Curve is:  0.8582969635063604
</code></pre><p>上图给出两条线，一条实线一条虚线，图中的横轴是伪证例的比例（FP/(FP+TN)），纵轴是真正例的比例（TP/(TP+FN)）。ROC曲线给出的是当阈值变化时假阳率和真阳率的变化情况。</p>
<p>ROC曲线不但可以用于比较分类器，还可以基于成本效益（cost versus benefit）分析来做出决策。由于在不同的阈值下，不同的分类器的表现情况可能各不相同，因此一某种方式将它们组合起来获取更有意义。</p>
<p>在理想情况下，最佳的分类器应该尽可能处于左上角，这意味着分类器在假阳率很低的同时获得了很高的真阳率。例如过滤了所有的垃圾邮件，没有将合法邮件误识别为垃圾邮件。</p>
<p>ROC曲线下下的面积（Are unser the curve，AUC）AUC给出的时分类器的平均性能值，一个完美的分类器AUC为1，随机猜测的AUC为0.5。</p>
<h2 id="基于代价函数的分类器决策控制"><a href="#基于代价函数的分类器决策控制" class="headerlink" title="基于代价函数的分类器决策控制"></a>基于代价函数的分类器决策控制</h2><h2 id="处理非均衡问题的数据抽样方法"><a href="#处理非均衡问题的数据抽样方法" class="headerlink" title="处理非均衡问题的数据抽样方法"></a>处理非均衡问题的数据抽样方法</h2><p>欠抽样（undersampling）删除样例</p>
<p>过抽样（oversampling）复制样例</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>集成方法通过组合多个分类器的分类结果，获得了比简单的分类器更好的分类结果。</p>
<p>多个分类器组合可能会进一步凸显出单分类器的不足，比如过拟合问题。如果分类器之间差别显著，那么多个分类器组合就可能会缓解这一问题。</p>
<p>在bagging中，是通过随机抽样的替换方式得到与原始数据集规模一样的数据集，而boosting在数据集上顺序应用了多个不同的分类器。</p>
<p>非均衡分类问题是指在分类器训练时正例数目和反例数目不相等（相差很大）。该问题在错分正例和反例的代价不同时也存在</p>
]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>AdaBoost</tag>
        <tag>bagging</tag>
        <tag>boosting</tag>
        <tag>ROC</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实战（三）</title>
    <url>/2020/03/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[<h1 id="决策树的简介"><a href="#决策树的简介" class="headerlink" title="决策树的简介"></a>决策树的简介</h1><p>你是否玩过二十个问题的游戏，就是你在脑海中想某个事物，向你提问二十个问题推测出你想的东西。这个游戏的原理和决策树类似，下面是一个判断垃圾邮件的决策树。</p>
<p><img src="0.png" alt="image"></p>
<h1 id="决策树的构造"><a href="#决策树的构造" class="headerlink" title="决策树的构造"></a>决策树的构造</h1><pre><code>决策树
优点：计算复杂度不高，输出的结果易于理解，对中间值的缺失不敏感，可以处理不相关特征的数据
缺点：可能会产生过度匹配的问题
适用数据类型：数值型和标称型
</code></pre><p>在构造决策树时，首先要确定哪些特征在划分数据分类时起到决定性的作用，为了划分出最好的结果，我们必须评估每个特征，创建分支的伪代码createBranch()函数如下</p>
<figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> so returen 类标签</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    寻找划分数据集的最好特征</span><br><span class="line">    划分数据集</span><br><span class="line">    创建分支节点</span><br><span class="line">        <span class="keyword">for</span> 每个划分的子集</span><br><span class="line">            调用函数createBranch并增加返回结果到分支节点中</span><br><span class="line">    <span class="keyword">return</span> 分支节点</span><br></pre></td></tr></table></figure>
<p>上面的伪代码createBranch()是一个递归函数，在倒数第二行直接调用自己。</p>
<p>决策树的一般流程：</p>
<ol>
<li>收集数据</li>
<li>准备数据</li>
<li>分析数据</li>
<li>训练算法</li>
<li>测试算法</li>
<li>使用算法</li>
</ol>
<h2 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h2><p>划分数据集的大原则是：将无序的数据变得更加有序，如何能知道数据是向有序的方向划分呢？方法有很多，这里的方法为香浓熵（其它方法还有基尼系数）。</p>
<p>熵的定义为信息的期望值，如果待分类的事物可能划分在多个分类之中，则符号$x_i$的信息定义为</p>
<p>$$l(x_i)=-\log_2p(x_i)$$</p>
<p>其中$p(x_i)$是选择分类的概率</p>
<p>为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过下面公式得到：</p>
<p>$$H = -\sum_{i=1}^n p(x_i)\log_2p(x_i)$$</p>
<p>其中n是分类的数目，下面用python计算信息熵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    numEntries = len(dataSet)</span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    <span class="comment"># 为所有可能分的类创建字典，如果类别已经记录，则记录当前类别出现的次数</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        currentLabel = featVec[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># 计算每个类别的出现的概率，然后套入公式求出熵</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        prob = float(labelCounts[key]) / numEntries</span><br><span class="line">        shannonEnt -= prob * log(prob, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br></pre></td></tr></table></figure>
<p>下面创建了一个数据集测试一下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    dataSet = [[<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">               [<span class="number">1</span>, <span class="number">1</span>, <span class="string">'yes'</span>],</span><br><span class="line">               [<span class="number">1</span>, <span class="number">0</span>, <span class="string">'no'</span>],</span><br><span class="line">               [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>],</span><br><span class="line">               [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'no'</span>],</span><br><span class="line">              ]</span><br><span class="line">    labels = [<span class="string">'no surfacing'</span>, <span class="string">'flippers'</span>]</span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myDat, labels = createDataSet()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myDat</span><br></pre></td></tr></table></figure>
<pre><code>[[1, 1, &apos;yes&apos;], [1, 1, &apos;yes&apos;], [1, 0, &apos;no&apos;], [0, 1, &apos;no&apos;], [0, 1, &apos;no&apos;]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">calcShannonEnt(myDat)</span><br></pre></td></tr></table></figure>
<pre><code>0.9709505944546686
</code></pre><p>熵越高，则混合的数据也越多，我们可以在数据集中添加更多的分类，观察熵是如何变化的，添加一个maybe的类别</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myDat[<span class="number">0</span>][<span class="number">-1</span>]=<span class="string">'maybe'</span></span><br><span class="line">myDat</span><br></pre></td></tr></table></figure>
<pre><code>[[1, 1, &apos;maybe&apos;], [1, 1, &apos;yes&apos;], [1, 0, &apos;no&apos;], [0, 1, &apos;no&apos;], [0, 1, &apos;no&apos;]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">calcShannonEnt(myDat)</span><br></pre></td></tr></table></figure>
<pre><code>1.3709505944546687
</code></pre><h2 id="划分数据集"><a href="#划分数据集" class="headerlink" title="划分数据集"></a>划分数据集</h2><p>得到熵后我们就可以按照获取最大信息增益的方法划分数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, axis, value)</span>:</span></span><br><span class="line">    retDataSet = []</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value:</span><br><span class="line">            reducedFeatVec = featVec[:axis]</span><br><span class="line">            reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">            retDataSet.append(reducedFeatVec)</span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br></pre></td></tr></table></figure>
<p>splitDatSet()有三个参数：待划分的数据集，划分数据集的特征列，需要返回的特征值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 注意append和extend的不同</span></span><br><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">b = [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">a.append(b)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>
<pre><code>[1, 2, 3, [4, 5, 6]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">a.extend(b)</span><br><span class="line">a</span><br></pre></td></tr></table></figure>
<pre><code>[1, 2, 3, 4, 5, 6]
</code></pre><p>用前面简单的数据集测试一下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myDat, labels = createDataSet()</span><br><span class="line">myDat</span><br></pre></td></tr></table></figure>
<pre><code>[[1, 1, &apos;yes&apos;], [1, 1, &apos;yes&apos;], [1, 0, &apos;no&apos;], [0, 1, &apos;no&apos;], [0, 1, &apos;no&apos;]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">splitDataSet(myDat, <span class="number">1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[[1, &apos;yes&apos;], [1, &apos;yes&apos;], [0, &apos;no&apos;], [0, &apos;no&apos;]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">splitDataSet(myDat, <span class="number">0</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[[1, &apos;no&apos;], [1, &apos;no&apos;]]
</code></pre><p>接下来遍历整个数据集，循环计算香农熵splitDataSet()函数，找到最好的特征划分方式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">    bestInfoGain = <span class="number">0.0</span></span><br><span class="line">    bestFeature = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">        uniqueVals = set(featList)</span><br><span class="line">        newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)</span><br><span class="line">            prob = len(subDataSet)/float(len(dataSet))</span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</span><br><span class="line">        infoGain = baseEntropy - newEntropy</span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):</span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">chooseBestFeatureToSplit(myDat)</span><br></pre></td></tr></table></figure>
<pre><code>0
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myDat</span><br></pre></td></tr></table></figure>
<pre><code>[[1, 1, &apos;yes&apos;], [1, 1, &apos;yes&apos;], [1, 0, &apos;no&apos;], [0, 1, &apos;no&apos;], [0, 1, &apos;no&apos;]]
</code></pre><p>代码运行告诉我们第0个特征划分最好</p>
<h2 id="递归构建决策树"><a href="#递归构建决策树" class="headerlink" title="递归构建决策树"></a>递归构建决策树</h2><p>目前我们已经构建好所有决策树算法所需的子功能模块，其工作原理如下：<br>得到原始数据集，然后基于最好的属性划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分后，数据将被向下传递到树分支的下一节点，在这个节点上，我们可以再次划分数据，因此我们可以采用递归的原则处理数据集。</p>
<p>递归结束的条件是：程序遍历完所有的划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。如果所有实列具有相同的分类，则得到一个叶子节点或者终止块。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span></span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys():</span><br><span class="line">            classCount[vote] = <span class="number">0</span></span><br><span class="line">            classCount[vote] += <span class="number">1</span></span><br><span class="line">        sortedClassCount = sorted(classCount,iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>上面代码的作用市，当遍历完所有的特征时，我们用投票表决的方法，返回出现次数最多的类别</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet, labels)</span>:</span></span><br><span class="line">    classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    uniqueVals = set(featValues)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        subLabels = labels[:]</span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)</span><br><span class="line">    <span class="keyword">return</span> myTree</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myDat, labels = createDataSet()</span><br><span class="line">myTree = createTree(myDat, labels)</span><br><span class="line">myTree</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;no surfacing&apos;: {0: &apos;no&apos;, 1: {&apos;flippers&apos;: {0: &apos;no&apos;, 1: &apos;yes&apos;}}}}
</code></pre><h1 id="绘制树形图"><a href="#绘制树形图" class="headerlink" title="绘制树形图"></a>绘制树形图</h1><p>为了更清晰的看出我们创建的树，可以用matplotlib绘图</p>
<h2 id="使用文本注解绘制树节点"><a href="#使用文本注解绘制树节点" class="headerlink" title="使用文本注解绘制树节点"></a>使用文本注解绘制树节点</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义文本框和箭头格式</span></span><br><span class="line">decisionNode = dict(boxstyle=<span class="string">"sawtooth"</span>, fc=<span class="string">"0.8"</span>)</span><br><span class="line">leafNode = dict(boxstyle=<span class="string">"round4"</span>, fc=<span class="string">"0.8"</span>)</span><br><span class="line">arrow_args = dict(arrowstyle=<span class="string">"&lt;-"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制带箭头的注解</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotNode</span><span class="params">(nodeTxt, centerPt, parentPt, nodeType)</span>:</span></span><br><span class="line">    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords=<span class="string">'axes fraction'</span>,</span><br><span class="line">                            xytext=centerPt, textcoords=<span class="string">'axes fraction'</span>,</span><br><span class="line">                            va=<span class="string">'center'</span>, ha=<span class="string">'center'</span>, bbox=nodeType, arrowprops=arrow_args)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createPlot</span><span class="params">()</span>:</span></span><br><span class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">'white'</span>)</span><br><span class="line">    fig.clf()</span><br><span class="line">    createPlot.ax1 = plt.subplot(<span class="number">111</span>, frameon=<span class="keyword">False</span>)</span><br><span class="line">    plotNode(<span class="string">'决策节点'</span>, (<span class="number">0.5</span>, <span class="number">0.1</span>), (<span class="number">0.1</span>, <span class="number">0.5</span>), decisionNode)</span><br><span class="line">    plotNode(<span class="string">'叶节点'</span>, (<span class="number">0.8</span>, <span class="number">0.1</span>), (<span class="number">0.3</span>, <span class="number">0.8</span>), leafNode)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">createPlot()</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89_35_0.png" alt="png"></p>
<h2 id="构造注解树"><a href="#构造注解树" class="headerlink" title="构造注解树"></a>构造注解树</h2><p>绘制一颗完整的树需要一些技巧，我们必须有x,y坐标，知道多少个叶节点，确定x轴的长度，树的深度，确定y轴的高度，这里编写两个函数getNumLeafs()和getTreeDepth()，来获取叶节点数和树的深度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getNumLeafs</span><span class="params">(myTree)</span>:</span></span><br><span class="line">    numLeafs = <span class="number">0</span></span><br><span class="line">    firstStr = list(myTree.keys())[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> list(secondDict.keys()):</span><br><span class="line">        <span class="keyword">if</span> type(secondDict[key]).__name__==<span class="string">'dict'</span>:</span><br><span class="line">            numLeafs += getNumLeafs(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            numLeafs += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> numLeafs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getTreeDepth</span><span class="params">(myTree)</span>:</span></span><br><span class="line">    maxDepth = <span class="number">0</span></span><br><span class="line">    firstStr = list(myTree.keys())[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> list(secondDict.keys()):</span><br><span class="line">        <span class="keyword">if</span> type(secondDict[key]).__name__==<span class="string">'dict'</span>:</span><br><span class="line">            thisDepth = <span class="number">1</span> + getTreeDepth(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            thisDepth = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> thisDepth &gt; maxDepth: maxDepth = thisDepth</span><br><span class="line">    <span class="keyword">return</span> maxDepth</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">retrieveTree</span><span class="params">(i)</span>:</span></span><br><span class="line">    listOfTrees =[&#123;<span class="string">'no surfacing'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: &#123;<span class="string">'flippers'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: <span class="string">'yes'</span>&#125;&#125;&#125;&#125;,</span><br><span class="line">                  &#123;<span class="string">'no surfacing'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: &#123;<span class="string">'flippers'</span>: &#123;<span class="number">0</span>: &#123;<span class="string">'head'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: <span class="string">'yes'</span>&#125;&#125;, <span class="number">1</span>: <span class="string">'no'</span>&#125;&#125;&#125;&#125;</span><br><span class="line">                  ]</span><br><span class="line">    <span class="keyword">return</span> listOfTrees[i]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">retrieveTree(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;no surfacing&apos;: {0: &apos;no&apos;,
  1: {&apos;flippers&apos;: {0: {&apos;head&apos;: {0: &apos;no&apos;, 1: &apos;yes&apos;}}, 1: &apos;no&apos;}}}}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myTree = retrieveTree(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">getNumLeafs(myTree)</span><br></pre></td></tr></table></figure>
<pre><code>3
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">getTreeDepth(myTree)</span><br></pre></td></tr></table></figure>
<pre><code>2
</code></pre><p>接下来我们画出这颗树</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotMidText</span><span class="params">(cntrPt, parentPt, txtString)</span>:</span></span><br><span class="line">    xMid = (parentPt[<span class="number">0</span>]-cntrPt[<span class="number">0</span>])/<span class="number">2.0</span> + cntrPt[<span class="number">0</span>]</span><br><span class="line">    yMid = (parentPt[<span class="number">1</span>]-cntrPt[<span class="number">1</span>])/<span class="number">2.0</span> + cntrPt[<span class="number">1</span>]</span><br><span class="line">    createPlot.ax1.text(xMid, yMid, txtString)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotTree</span><span class="params">(myTree, parentPt, nodeTxt)</span>:</span></span><br><span class="line">    numLeafs = getNumLeafs(myTree)</span><br><span class="line">    depth = getTreeDepth(myTree)</span><br><span class="line">    firstStr = list(myTree.keys())[<span class="number">0</span>]</span><br><span class="line">    cntrPt = (plotTree.xOff + (<span class="number">1.0</span> + float(numLeafs))/<span class="number">2.0</span>/plotTree.totalW, plotTree.yOff)</span><br><span class="line">    plotMidText(cntrPt, parentPt, nodeTxt)</span><br><span class="line">    plotNode(firstStr, cntrPt, parentPt, decisionNode)</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    plotTree.yOff = plotTree.yOff - <span class="number">1.0</span>/plotTree.totalD</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> list(secondDict.keys()):</span><br><span class="line">        <span class="keyword">if</span> type(secondDict[key]).__name__ == <span class="string">'dict'</span>:</span><br><span class="line">            plotTree(secondDict[key], cntrPt, str(key))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            plotTree.xOff = plotTree.xOff +  <span class="number">1.0</span>/plotTree.totalW</span><br><span class="line">            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)</span><br><span class="line">            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))</span><br><span class="line">    plotTree.yOff = plotTree.yOff + <span class="number">1.0</span>/plotTree.totalD</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createPlot</span><span class="params">(inTree)</span>:</span></span><br><span class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">'white'</span>)</span><br><span class="line">    fig.clf()</span><br><span class="line">    axprops = dict(xticks=[], yticks=[])</span><br><span class="line">    createPlot.ax1 = plt.subplot(<span class="number">111</span>, frameon=<span class="keyword">False</span>, **axprops)</span><br><span class="line">    plotTree.totalW = float(getNumLeafs(inTree))</span><br><span class="line">    plotTree.totalD = float(getTreeDepth(inTree))</span><br><span class="line">    plotTree.xOff = <span class="number">-0.5</span>/plotTree.totalW</span><br><span class="line">    plotTree.yOff = <span class="number">1.0</span></span><br><span class="line">    plotTree(inTree, (<span class="number">0.5</span>, <span class="number">1.0</span>), <span class="string">''</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myTree = retrieveTree(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">createPlot(myTree)</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89_46_0.png" alt="png"></p>
<p>添加一组数据再绘制</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myTree[<span class="string">'no surfacing'</span>][<span class="number">3</span>]=<span class="string">'maybe'</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">createPlot(myTree)</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89_49_0.png" alt="png"></p>
<h1 id="测试和存储分类器"><a href="#测试和存储分类器" class="headerlink" title="测试和存储分类器"></a>测试和存储分类器</h1><h2 id="测试算法：使用决策树执行分类"><a href="#测试算法：使用决策树执行分类" class="headerlink" title="测试算法：使用决策树执行分类"></a>测试算法：使用决策树执行分类</h2><p>依靠训练数据构造了决策树之后，我们可以将它用于实际数据的分类，在执行数据分类时，需要使用决策树以及用于构造决策树的标签向量。然后程序比较测试数据与决策树上的数值，递归执行该过程，直到叶子节点；最后将测试数据定义为叶子节点所属的类型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inputTree, featLabels, testVec)</span>:</span></span><br><span class="line">    firstStr = list(inputTree.keys())[<span class="number">0</span>]</span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> list(secondDict.keys()):</span><br><span class="line">        <span class="keyword">if</span> testVec[featIndex] == key:</span><br><span class="line">            <span class="keyword">if</span> type(secondDict[key]).__name__ == <span class="string">'dict'</span>:</span><br><span class="line">                classLables = classify(secondDict[key], featLabels, testVec)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                classLables = secondDict[key]</span><br><span class="line">    <span class="keyword">return</span> classLables</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myDat, labels = createDataSet()</span><br><span class="line">labels</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;no surfacing&apos;, &apos;flippers&apos;]
</code></pre><p>myTree = retrieveTree(0)<br>myTree</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classify(myTree, labels, [<span class="number">1</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>&apos;no&apos;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classify(myTree, labels, [<span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>&apos;yes&apos;
</code></pre><h2 id="使用算法：决策树的存储"><a href="#使用算法：决策树的存储" class="headerlink" title="使用算法：决策树的存储"></a>使用算法：决策树的存储</h2><p>决策树的构造是很耗时的任务，但如果使用创建好的决策树则可以很快的解决分类问题，这里需要使用pickle模块把决策树保存到本地。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span><span class="params">(inputTree, filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fw = open(filename, <span class="string">'wb'</span>)</span><br><span class="line">    pickle.dump(inputTree, fw)</span><br><span class="line">    fw.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grabTree</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fr = open(filename, <span class="string">'rb'</span>)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">storeTree(myTree, <span class="string">'classifierStorage.txt'</span>)</span><br><span class="line">grabTree(<span class="string">'classifierStorage.txt'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;no surfacing&apos;: {0: &apos;no&apos;, 1: {&apos;flippers&apos;: {0: &apos;no&apos;, 1: &apos;yes&apos;}}, 3: &apos;maybe&apos;}}
</code></pre><p>通过上述的代码，我们将分类器储存在本地，则不用每一次分类都重新学习一遍</p>
<h1 id="实例：使用决策树预测隐形眼镜的类型"><a href="#实例：使用决策树预测隐形眼镜的类型" class="headerlink" title="实例：使用决策树预测隐形眼镜的类型"></a>实例：使用决策树预测隐形眼镜的类型</h1><p>根据隐形眼镜的材质等信息预测患者需要的眼镜类型，流程如下</p>
<ol>
<li>收集数据：提供的文本文件</li>
<li>准备数据：解析tab键分割的数据行</li>
<li>分析数据：快速检查数据正确性，使用createPlot()函数绘制树形图</li>
<li>训练算法：使用creatTree()函数</li>
<li>测试算法：编写测试函数验证决策树的正确率</li>
<li>使用算法：储存树的数据结构，以便下次使用</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fr = open(<span class="string">'./MLiA_SourceCode/machinelearninginaction/Ch03/lenses.txt'</span>)</span><br><span class="line">lenses = [inst.strip().split(<span class="string">'\t'</span>) <span class="keyword">for</span> inst <span class="keyword">in</span> fr.readlines()]</span><br><span class="line">lensesLabels = [<span class="string">'age'</span>, <span class="string">'prescript'</span>, <span class="string">'astigmatic'</span>, <span class="string">'tearRate'</span>]</span><br><span class="line">lensesTree = createTree(lenses, lensesLabels)</span><br><span class="line">lensesTree</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;tearRate&apos;: {&apos;reduced&apos;: &apos;no lenses&apos;,
  &apos;normal&apos;: {&apos;astigmatic&apos;: {&apos;yes&apos;: {&apos;prescript&apos;: {&apos;myope&apos;: &apos;hard&apos;,
      &apos;hyper&apos;: {&apos;age&apos;: {&apos;pre&apos;: &apos;no lenses&apos;,
        &apos;young&apos;: &apos;hard&apos;,
        &apos;presbyopic&apos;: &apos;no lenses&apos;}}}},
    &apos;no&apos;: {&apos;age&apos;: {&apos;pre&apos;: &apos;soft&apos;,
      &apos;young&apos;: &apos;soft&apos;,
      &apos;presbyopic&apos;: {&apos;prescript&apos;: {&apos;myope&apos;: &apos;no lenses&apos;,
        &apos;hyper&apos;: &apos;soft&apos;}}}}}}}}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">createPlot(lensesTree)</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89_62_0.png" alt="png"></p>
<p>通过观察树我们知道，医生最多只需要问四个问题就能确定患者需要佩戴的眼镜。</p>
<p>虽然决策树非常好的匹配了实验数据，但匹配的选项太多了，我们将这种问题称为过度匹配（overfitting），为了减少过度匹配问题，我们可以裁剪决策树，去掉一些不必要的叶子节点。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本章使用的算法称为ID3，它无法处理数值型数据，如果特征太多，也会面临其它问题。</p>
<p>决策树分类器就像带有终止块的流程图，终止块表示分类结果。开始处理数据集时，我们首先需要测量集合中数据的不一致性，也就是熵，然后寻找最优方案划分数据集，知道数据集中的所有数据属于同一分类。ID3可以划分标称型数据集。</p>
]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>决策树</tag>
        <tag>ID3</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实战（九）</title>
    <url>/2020/05/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B9%9D%EF%BC%89/</url>
    <content><![CDATA[<h1 id="数回归"><a href="#数回归" class="headerlink" title="数回归"></a>数回归</h1><p>分类回归树 Classification And Regression Trees 分类回归树。该算法既可以用于回归还可以用于分类。</p>
<h1 id="复杂数据的局部性建模"><a href="#复杂数据的局部性建模" class="headerlink" title="复杂数据的局部性建模"></a>复杂数据的局部性建模</h1><p>数回归</p>
<pre><code>优点：可以对复杂和线性的数据建模
缺点：结果不易理解
适用数据类型：数值型和标称型数据
</code></pre><p>第三章使用的树构建的算法是ID3。ID3的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。也就取值，那么数据将被切分成4份，一但按某种特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以有观点认为这种切分方式过于迅速。另一种方法是二元切分发，即每次吧数据集切分成两份。如果数据的某个特征等于切分所要求的值，那么这些数据就进入树的左子树，反之则进入树的右子树。</p>
<p>除了切分过于迅速外，ID3算法还存在另一个问题，它不能直接处理连续型特征。只有事先将连续型特征转换成离散型，才能在ID3算法中使用。但这种转换过程会破坏连续型变量的内在性质。而使用二元切分法则对于树构建过程进行调整以处理连续型特征。</p>
<p>具体处理方法是：</p>
<pre><code>如果特征值大于给定值就走左子树，否则就走右子树。
</code></pre><p>另外，二元切分法也节省了树的构建时间，但这点意义也不是特别大因为这些树的构建一般是离线完成的。</p>
<p>CART是十分著名且广泛记载的树构建算法，它使用二元切分来处理连续型变量。对CART稍作修改就可以处理回归问题。</p>
<p>回归树的一般方法：</p>
<ol>
<li>收集数据</li>
<li>准备数据：需要数值型的数据，标称型数据应该映射成二值型数据</li>
<li>分析数据：绘出数据的二维可视化显示结果，以字典方式生成树</li>
<li>训练算法：大部分时间都花费在叶节点树模型的构建上</li>
<li>测试算法：使用测试数据上的R^2值来分析模型的效果</li>
<li>使用算法：使用训练出的树做预测</li>
</ol>
<h2 id="连续和离散型特征的树的构建"><a href="#连续和离散型特征的树的构建" class="headerlink" title="连续和离散型特征的树的构建"></a>连续和离散型特征的树的构建</h2><p>在树的构建过程中，需要解决多种类型数据的存储问题。这里将使用字典来存储树的数据结构，该字典将包含以下四种元素。</p>
<pre><code>待切分的特征
待切分的特征值
右子树。当不需要切分时，也可以是单值
左子树。与右子树类似
</code></pre><p>CART算法只做二元切分，所以这里可以固定树的数据结构。树包含左键和右键，可以存储另一颗树或者单个值。字典还包含特征和特征值这两个键，它们给出的切分算法所有的特征和特征值。</p>
<p>接下来构建两种树，第一种是回归树（regression tree）其中每个叶节点包含单个值，第二种是是模型树（model tree）其中每个叶节点包含一个线性方程。</p>
<p>createTree()的伪代码大致如下：</p>
<pre><code>找到最佳的待切分特征：
    如果该节点不能再分，将该节点存为叶节点
    执行二元切分
    在右子树调用createTree()方法
    在左子树调用createTree()方法
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        fltLine = map(float, curLine)</span><br><span class="line">        fltLine = list(fltLine)</span><br><span class="line">        dataMat.append(fltLine)</span><br><span class="line">    fr.close()</span><br><span class="line">    <span class="keyword">return</span> dataMat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regLeaf</span><span class="params">(dataSet)</span>:</span> <span class="comment"># returns the value used for each leaf</span></span><br><span class="line">    <span class="keyword">return</span> mean(dataSet[:,<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regErr</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> var(dataSet[:,<span class="number">-1</span>]) * shape(dataSet)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet, leafType=regLeaf, errType=regErr, ops=<span class="params">(<span class="number">1</span>, <span class="number">4</span>)</span>)</span>:</span></span><br><span class="line">    feat, val = chooseBestSplit(dataSet, leafType, errType, ops)</span><br><span class="line">    <span class="comment"># 满足条件时返回叶节点值</span></span><br><span class="line">    <span class="keyword">if</span> feat == <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> val</span><br><span class="line">    retTree = &#123;&#125;</span><br><span class="line">    retTree[<span class="string">'spInd'</span>] = feat</span><br><span class="line">    retTree[<span class="string">'spVal'</span>] = val</span><br><span class="line">    lSet, rSet = binSplitDataSet(dataSet, feat, val)</span><br><span class="line">    retTree[<span class="string">'left'</span>] = createTree(lSet, leafType, errType, ops)</span><br><span class="line">    retTree[<span class="string">'right'</span>] = createTree(rSet, leafType, errType, ops)</span><br><span class="line">    <span class="keyword">return</span> retTree</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binSplitDataSet</span><span class="params">(dataSet, feature, value)</span>:</span></span><br><span class="line">    mat0 = dataSet[nonzero(dataSet[:, feature] &gt; value)[<span class="number">0</span>], :]</span><br><span class="line">    mat1 = dataSet[nonzero(dataSet[:, feature] &lt;= value)[<span class="number">0</span>], :]</span><br><span class="line">    <span class="keyword">return</span> mat0, mat1</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">testMat = mat(eye(<span class="number">4</span>))</span><br><span class="line">testMat</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mat0, mat1 = binSplitDataSet(testMat, <span class="number">1</span>, <span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mat0</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[0., 1., 0., 0.]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mat1</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[1., 0., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nonzero(testMat[:, <span class="number">1</span>] &gt; <span class="number">0.5</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>1
</code></pre><h1 id="将CART算法用于回归"><a href="#将CART算法用于回归" class="headerlink" title="将CART算法用于回归"></a>将CART算法用于回归</h1><p>要对数据的复杂关系建模，我们已经决定借用树结构来帮助切分数据，那么如何实现数据的切分呢？</p>
<p>为成功构建以分段常数为叶节点的树，需要度量出数据的一致性。在给定的节点计算数据的混乱度，计算数据混乱度的方法，首先计算所有数据的均值，然后计算每条数据的值到均值的差值，为了对正负差值同等看待，一般使用绝对值或平方值来代替上述差值。类似方差的计算，唯一不同是方差是平方误差的均值，而这里需要的是平方误差的总值，总方差可以通过均方差乘以数据集中样本点的个数来得到。</p>
<h2 id="构建树"><a href="#构建树" class="headerlink" title="构建树"></a>构建树</h2><p>构建回归树，需要补充一些新的代码，首先要做的就是实现chooseBestSplit()函数，给定某个误差计算方法，该函数会找到数据集上最佳的二元切分方式。另外该函数还要确定什么时候停止切分，一旦停止切分会生成一个叶节点。因此chooseBestSplit()函数需要完成两件事：用最佳方式切分数据集和生成相应的叶节点。</p>
<p>leafType是对创建叶节点的函数的引用，errType是对前面介绍的总方差计算函数的引用，而ops是一个用户定义的参数构成的元组，用已完成树的构建。</p>
<p>伪代码如下：</p>
<pre><code>对每个特征：
    对每个特征值：
        将数据集切分成两份
        计算切分的误差
        如果当前误差小于当前最小误差，那么将当前切分设定为最佳切分并更新最小误差
返回最佳切分的特征和阈值
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestSplit</span><span class="params">(dataSet, leafType=regLeaf, errType=regErr, ops=<span class="params">(<span class="number">1</span>, <span class="number">4</span>)</span>)</span>:</span></span><br><span class="line">    tolS = ops[<span class="number">0</span>]</span><br><span class="line">    tolN = ops[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> len(set(dataSet[:, <span class="number">-1</span>].T.tolist()[<span class="number">0</span>])) == <span class="number">1</span>: <span class="comment"># 如果所有值相等则退出</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span>, leafType(dataSet)</span><br><span class="line">    </span><br><span class="line">    m, n = shape(dataSet)</span><br><span class="line">    S = errType(dataSet)</span><br><span class="line">    bestS = inf</span><br><span class="line">    bestIndex = <span class="number">0</span></span><br><span class="line">    bestValue = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> featIndex <span class="keyword">in</span> range(n<span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">for</span> splitVal <span class="keyword">in</span> set(dataSet[:, featIndex].tolist()[<span class="number">0</span>]):</span><br><span class="line">            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)</span><br><span class="line">            <span class="keyword">if</span> (shape(mat0)[<span class="number">0</span>] &lt; tolN) <span class="keyword">or</span> (shape(mat1)[<span class="number">0</span>] &lt; tolN) :</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            newS = errType(mat0) + errType(mat1)</span><br><span class="line">            <span class="keyword">if</span> newS &lt; bestS:</span><br><span class="line">                bestIndex = featIndex</span><br><span class="line">                bestValue = splitVal</span><br><span class="line">                bestS = newS</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (S - bestS) &lt; tolS: <span class="comment"># 如果误差减小不大则退出</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span>, leafType(dataSet)</span><br><span class="line">    </span><br><span class="line">    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (shape(mat0)[<span class="number">0</span>] &lt; tolN) <span class="keyword">or</span> (shape(mat1)[<span class="number">0</span>] &lt; tolN): <span class="comment"># 如果切分出的数据集很小则退出</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span>, leafType(dataSet)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> bestIndex, bestValue</span><br></pre></td></tr></table></figure>
<p>regLeaf()它负责生成叶节点。当chooseBestSplit()函数确定不再对数据进行切分时，将调用该regLeaf()函数来得到叶节点的模型，在回归树中，该模型就是目标变量的均值。</p>
<p>regErr()是误差估计函数，该函数在给定数据上计算目标变量的平方误差，当然也可以先计算出均值，然后计算每个差值再平方。因为这里需要总方差，所以用均方差函数var()的结果乘以数据集中的样本个数。</p>
<p>chooseBestSplit()该函数的目的是找到数据的最佳二元切分方式。如果找不到一个好的二元切分，该函数返回None并同时调用createTree()方法来产生叶节点，叶节点的值也将返回None。ops设定了tolS和tolN两个值，tolS是容许的误差下降值，tolN是切分的最少样本数。</p>
<h2 id="运行代码"><a href="#运行代码" class="headerlink" title="运行代码"></a>运行代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myDat = loadDataSet(<span class="string">'MLiA_SourceCode/Ch09/ex00.txt'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myMat = mat(myDat)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">createTree(myMat)</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;spInd&apos;: 0,
 &apos;spVal&apos;: 0.036098,
 &apos;left&apos;: 0.5878577680412371,
 &apos;right&apos;: 0.050698999999999994}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotScatter</span><span class="params">(data)</span>:</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    <span class="comment">#print(myMat)</span></span><br><span class="line">    ax.scatter(data[:, <span class="number">0</span>].T.tolist()[<span class="number">0</span>], data[:, <span class="number">1</span>].T.tolist()[<span class="number">0</span>], <span class="number">5</span>, c=<span class="string">'red'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plotScatter(myMat)</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B9%9D%EF%BC%89_19_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myDat = loadDataSet(<span class="string">'MLiA_SourceCode/Ch09/ex0.txt'</span>)</span><br><span class="line">myMat = mat(myDat)</span><br><span class="line">createTree(myMat)</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;spInd&apos;: 1,
 &apos;spVal&apos;: 0.409175,
 &apos;left&apos;: {&apos;spInd&apos;: 1,
  &apos;spVal&apos;: 0.663687,
  &apos;left&apos;: {&apos;spInd&apos;: 1,
   &apos;spVal&apos;: 0.725426,
   &apos;left&apos;: 3.7206952592592595,
   &apos;right&apos;: 2.998615611111111},
  &apos;right&apos;: 2.2076016800000002},
 &apos;right&apos;: 0.45470547435897446}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plotScatter(myMat[:, <span class="number">1</span>:])</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B9%9D%EF%BC%89_21_0.png" alt="png"></p>
<h1 id="树剪枝"><a href="#树剪枝" class="headerlink" title="树剪枝"></a>树剪枝</h1><p>通过降低决策树的复杂度来避免过拟合的过程称为剪枝（pruning）。</p>
<h2 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h2><p>树构建算法其实对输入的参数tolS和tolN非常敏感，对ops参数调整就是预剪枝。</p>
<h2 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h2><p>利用测试集来对树进行剪枝，不需要用户指定参数，为后剪枝。</p>
<p>prune()伪代码如下</p>
<pre><code>基于已有的树切分测试数据：
    如果存在任一子集是一棵树，则在该子集递归剪枝过程
    计算将当前两个叶节点合并后的误差
    计算不合并的误差
    如果合并降低误差，就将叶节点合并
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isTree</span><span class="params">(obj)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (type(obj).__name__==<span class="string">'dict'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getMean</span><span class="params">(tree)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">'right'</span>]):</span><br><span class="line">        tree[<span class="string">'right'</span>] = getMean(tree[<span class="string">'right'</span>])</span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">'left'</span>]):</span><br><span class="line">        tree[<span class="string">'left'</span>] = getMean(tree[<span class="string">'left'</span>])</span><br><span class="line">    <span class="keyword">return</span> (tree[<span class="string">'left'</span>]+tree[<span class="string">'right'</span>])/<span class="number">2.0</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prune</span><span class="params">(tree, testData)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> shape(testData)[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> getMean(tree)  <span class="comment"># if we have no test data collapse the tree</span></span><br><span class="line">    <span class="keyword">if</span> (isTree(tree[<span class="string">'right'</span>]) <span class="keyword">or</span> isTree(tree[<span class="string">'left'</span>])): <span class="comment"># if the branches are not trees try to prune them</span></span><br><span class="line">        lSet, rSet = binSplitDataSet(testData, tree[<span class="string">'spInd'</span>], tree[<span class="string">'spVal'</span>])</span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">'left'</span>]): tree[<span class="string">'left'</span>] = prune(tree[<span class="string">'left'</span>], lSet)</span><br><span class="line">    <span class="keyword">if</span> isTree(tree[<span class="string">'right'</span>]): tree[<span class="string">'right'</span>] =  prune(tree[<span class="string">'right'</span>], rSet)</span><br><span class="line">    <span class="comment"># if they are now both leafs, see if we can merge them</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isTree(tree[<span class="string">'left'</span>]) <span class="keyword">and</span> <span class="keyword">not</span> isTree(tree[<span class="string">'right'</span>]):</span><br><span class="line">        lSet, rSet = binSplitDataSet(testData, tree[<span class="string">'spInd'</span>], tree[<span class="string">'spVal'</span>])</span><br><span class="line">        errorNoMerge = sum(power(lSet[:,<span class="number">-1</span>] - tree[<span class="string">'left'</span>],<span class="number">2</span>)) +\</span><br><span class="line">            sum(power(rSet[:,<span class="number">-1</span>] - tree[<span class="string">'right'</span>],<span class="number">2</span>))</span><br><span class="line">        treeMean = (tree[<span class="string">'left'</span>]+tree[<span class="string">'right'</span>])/<span class="number">2.0</span></span><br><span class="line">        errorMerge = sum(power(testData[:,<span class="number">-1</span>] - treeMean,<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">if</span> errorMerge &lt; errorNoMerge: </span><br><span class="line">            <span class="keyword">return</span> treeMean</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">return</span> tree</span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">return</span> tree</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myDat2 = loadDataSet(<span class="string">'MLiA_SourceCode/Ch09/ex2.txt'</span>)</span><br><span class="line">myMat2 = mat(myDat2)</span><br><span class="line">myTree = createTree(myMat2, ops=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">myDatTest = loadDataSet(<span class="string">'MLiA_SourceCode/Ch09/ex2test.txt'</span>)</span><br><span class="line">myMat2Test = mat(myDatTest)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prune(myTree, myMat2Test)</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;spInd&apos;: 0,
 &apos;spVal&apos;: 0.228628,
 &apos;left&apos;: {&apos;spInd&apos;: 0,
  &apos;spVal&apos;: 0.965969,
  &apos;left&apos;: 92.5239915,
  &apos;right&apos;: 65.53919801898735},
 &apos;right&apos;: -1.1055498250000002}
</code></pre><h1 id="模型树"><a href="#模型树" class="headerlink" title="模型树"></a>模型树</h1><p>用树来对数据建模，除了吧叶节点简单地设定为常数值之外，还有一种方法是把叶节点设定为分段线性函数，这里所谓的分段性（piecewise linear）是指模型由多个线性片段组成。</p>
<p>决策树相比其他机器学习算法的优势之一在于结果更易理解。很显然，两条直线比很多节点组成一棵大树更容易解释。模型树的可解释性是它优于回归树的特点之一。另外，模型树也具有更高的预测准确度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linearSolve</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    m, n = shape(dataSet)</span><br><span class="line">    X = mat(ones((m, n)))</span><br><span class="line">    Y = mat(ones((m, <span class="number">1</span>)))</span><br><span class="line">    X[:, <span class="number">1</span>:n] = dataSet[:, <span class="number">0</span>:n<span class="number">-1</span>]</span><br><span class="line">    Y = dataSet[:, <span class="number">-1</span>]</span><br><span class="line">    xTx = X.T*X</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">        <span class="keyword">raise</span> NameError(<span class="string">'This matrix is singular, cannot do inverse,\</span></span><br><span class="line"><span class="string">                         try increasing the second value of ops'</span>)</span><br><span class="line">    </span><br><span class="line">    ws = xTx.I * (X.T * Y)</span><br><span class="line">    <span class="keyword">return</span> ws, X, Y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">modelLeaf</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    ws, X, Y = linearSolve(dataSet)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">modelErr</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    ws, X, Y = linearSolve(dataSet)</span><br><span class="line">    yHat = X * ws</span><br><span class="line">    <span class="keyword">return</span> sum(power(Y - yHat, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">createTree(myMat2, modelLeaf, modelErr, (<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;spInd&apos;: 0, &apos;spVal&apos;: 0.228628, &apos;left&apos;: None, &apos;right&apos;: None}
</code></pre><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这一章提供的code有很多错误，修正后并不能得到书中的答案。如果要使用树算法，还是建议使用sklearn，而非自己编写。</p>
<p>CART算法可以用于构建二元树并处理离散型或连续型数据的切分。若使用不同的误差准则，就可以通过CART算法构建模型树和回归树。该算法构建出的树会倾向于对数据过拟合。过拟合的树十分复杂，剪枝可以解决这个问题。</p>
]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>树回归</tag>
        <tag>CSRT算法</tag>
        <tag>树剪枝算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实战（二）</title>
    <url>/2020/02/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<h1 id="k-邻近算法概述"><a href="#k-邻近算法概述" class="headerlink" title="k-邻近算法概述"></a>k-邻近算法概述</h1><p>k-邻近算法采用测量不同特征之间的距离方法进行分类。</p>
<pre><code>优点：精度高，对异常值不敏感，无数据输入假定
缺点：计算复杂度高，空间复杂度高
适用数据范围：数值型和标称型
</code></pre><h1 id="准备使用Python导入数据"><a href="#准备使用Python导入数据" class="headerlink" title="准备使用Python导入数据"></a>准备使用Python导入数据</h1><p>首先写一个简单的程序来理解python是如何解析和加载数据的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    group = array([[<span class="number">1.0</span>, <span class="number">1.1</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0.1</span>]])</span><br><span class="line">    labels = [<span class="string">'A'</span>, <span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'B'</span>]</span><br><span class="line">    <span class="keyword">return</span> group, labels</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">group, labels = createDataSet()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">group</span><br></pre></td></tr></table></figure>
<pre><code>array([[1. , 1.1],
       [1. , 1. ],
       [0. , 0. ],
       [0. , 0.1]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">labels</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;A&apos;, &apos;A&apos;, &apos;B&apos;, &apos;B&apos;]
</code></pre><h1 id="实施KNN分类算法"><a href="#实施KNN分类算法" class="headerlink" title="实施KNN分类算法"></a>实施KNN分类算法</h1><p>对未知类别属性的数据集中的每个点依次执行以下操作：</p>
<ol>
<li>计算已知类别数据集中的点与当前点之间的距离</li>
<li>按照距离递增次序排序</li>
<li>选取与当前距离最小的k个点</li>
<li>确定前k个点所在类别的出现频率</li>
<li>返回前k个点出现频率最高的类别当作当前点的预测分类</li>
</ol>
<p>计算距离的方法使用欧氏距离计算公式:</p>
<p>$$d=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$$</p>
<p>具体程序如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX, dataSet, labels, k)</span>:</span></span><br><span class="line">    dataSetSize = dataSet.shape[<span class="number">0</span>]</span><br><span class="line">    diffMat = tile(inX, (dataSetSize, <span class="number">1</span>)) - dataSet</span><br><span class="line">    sqDiffMat = diffMat**<span class="number">2</span></span><br><span class="line">    sqDistances = sqDiffMat.sum(axis=<span class="number">1</span>)</span><br><span class="line">    distances = sqDistances ** <span class="number">0.5</span></span><br><span class="line">    sortedDistIndicies = distances.argsort()</span><br><span class="line">    classCount = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        voteIlabe1 = labels[sortedDistIndicies[i]]</span><br><span class="line">        classCount[voteIlabe1] = classCount.get(voteIlabe1, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">    sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>classify0 有四个参数</p>
<pre><code>inX：需要分类的向量
dataSet：训练集的特征
labels：训练集的标签
k：最近邻居的数目
</code></pre><p>测试下我们写的算法是否正确，我们传入（0，0）的点，正确的分类点应该是B点</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">classify0([<span class="number">0</span>,<span class="number">0</span>], group, labels, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&apos;B&apos;
</code></pre><h1 id="使用k-邻近算法改进约会网站的配对结果"><a href="#使用k-邻近算法改进约会网站的配对结果" class="headerlink" title="使用k-邻近算法改进约会网站的配对结果"></a>使用k-邻近算法改进约会网站的配对结果</h1><p>约会网站通过三个特征把样本分为3类</p>
<table>
<thead>
<tr>
<th>每年飞行里程</th>
<th>玩游戏消耗的时间百分比</th>
<th>每周消耗的冰淇淋公升数</th>
<th>样本分类</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>(largeDoses,smallDoses,didntLike)</td>
</tr>
</tbody>
</table>
<h2 id="准备数据：从文本文件中解析数据"><a href="#准备数据：从文本文件中解析数据" class="headerlink" title="准备数据：从文本文件中解析数据"></a>准备数据：从文本文件中解析数据</h2><p>共有1000行数据存放在datingTestSet2.txt中，行之间用\n分割，列之间\t分割，现在需要把这个文件里的数据转换为一个向量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file2matrix</span><span class="params">(filename)</span>:</span></span><br><span class="line">    fr = open(filename, <span class="string">'r'</span>)</span><br><span class="line">    arrayOLines = fr.readlines()</span><br><span class="line">    numberOfLines = len(arrayOLines)</span><br><span class="line">    returnMat = zeros((numberOfLines, <span class="number">3</span>))</span><br><span class="line">    classLabelVector = []</span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> arrayOLines:</span><br><span class="line">        line = line.strip()</span><br><span class="line">        listFromLine = line.split(<span class="string">'\t'</span>)</span><br><span class="line">        returnMat[index, :] = listFromLine[<span class="number">0</span>: <span class="number">3</span>]</span><br><span class="line">        classLabelVector.append(int(listFromLine[<span class="number">-1</span>]))</span><br><span class="line">        index += <span class="number">1</span></span><br><span class="line">    fr.close()</span><br><span class="line">    <span class="keyword">return</span> returnMat, classLabelVector</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datingDataMat, datingLabels = file2matrix(<span class="string">'./MLiA_SourceCode/machinelearninginaction/Ch02/datingTestSet2.txt'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datingDataMat</span><br></pre></td></tr></table></figure>
<pre><code>array([[4.0920000e+04, 8.3269760e+00, 9.5395200e-01],
       [1.4488000e+04, 7.1534690e+00, 1.6739040e+00],
       [2.6052000e+04, 1.4418710e+00, 8.0512400e-01],
       ...,
       [2.6575000e+04, 1.0650102e+01, 8.6662700e-01],
       [4.8111000e+04, 9.1345280e+00, 7.2804500e-01],
       [4.3757000e+04, 7.8826010e+00, 1.3324460e+00]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datingLabels[:<span class="number">20</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[3, 2, 1, 1, 1, 1, 3, 3, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3]
</code></pre><h2 id="分析数据：使用matplotlib绘制散点图"><a href="#分析数据：使用matplotlib绘制散点图" class="headerlink" title="分析数据：使用matplotlib绘制散点图"></a>分析数据：使用matplotlib绘制散点图</h2><p>数据使用第二列和第三列的值分别是</p>
<pre><code>x轴：玩游戏消耗的时间百分比
y轴：每周消费的冰淇淋公升数
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.scatter(datingDataMat[:, <span class="number">1</span>], datingDataMat[:, <span class="number">2</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89_19_0.png" alt="png"></p>
<p>从上图很难看到有用的信息，我们通过颜色标记不同的样本</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.scatter(datingDataMat[:, <span class="number">1</span>], datingDataMat[:, <span class="number">2</span>], <span class="number">15.0</span>*array(datingLabels), <span class="number">15.0</span>*array(datingLabels))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%8C%EF%BC%89_21_0.png" alt="png"></p>
<p>利用颜色和尺寸我们基本可以看出三种样本的区域轮廓。</p>
<h2 id="准备数据：归一化数值"><a href="#准备数据：归一化数值" class="headerlink" title="准备数据：归一化数值"></a>准备数据：归一化数值</h2><p>我们发现飞行里程的数值远远大于另两个数值，这样我们计算的结果会被飞行里程严重影响，我们认为三个特征是同等重要的，所以作为三个等权重的特征，飞行里程不应该严重影响结果。</p>
<p>在处理这种不同取值范围的特征时，我们需要先归一化数值，将数值范围处理到0到1或-1到1之间，通常使用的公式是</p>
<p>$$newValue = (oldValue-min) / max-min)$$</p>
<p>程序如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">autoNorm</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    minVals = dataSet.min(<span class="number">0</span>)</span><br><span class="line">    maxVals = dataSet.max(<span class="number">0</span>)</span><br><span class="line">    ranges = maxVals - minVals</span><br><span class="line">    normDataSet = zeros(shape(dataSet))</span><br><span class="line">    m = dataSet.shape[<span class="number">0</span>]</span><br><span class="line">    normDataSet = dataSet - tile(minVals, (m, <span class="number">1</span>))</span><br><span class="line">    normDataSet = normDataSet/tile(ranges, (m, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> normDataSet, ranges, minVals</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">normMat, ranges, minVals = autoNorm(datingDataMat)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">normMat</span><br></pre></td></tr></table></figure>
<pre><code>array([[0.44832535, 0.39805139, 0.56233353],
       [0.15873259, 0.34195467, 0.98724416],
       [0.28542943, 0.06892523, 0.47449629],
       ...,
       [0.29115949, 0.50910294, 0.51079493],
       [0.52711097, 0.43665451, 0.4290048 ],
       [0.47940793, 0.3768091 , 0.78571804]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ranges</span><br></pre></td></tr></table></figure>
<pre><code>array([9.1273000e+04, 2.0919349e+01, 1.6943610e+00])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">minVals</span><br></pre></td></tr></table></figure>
<pre><code>array([0.      , 0.      , 0.001156])
</code></pre><h2 id="测试算法：构建完整程序验证分类器"><a href="#测试算法：构建完整程序验证分类器" class="headerlink" title="测试算法：构建完整程序验证分类器"></a>测试算法：构建完整程序验证分类器</h2><p>选择90%的数据作为训练集，10%作为测试集，记录每一次错误的分类，最后用预测错误的总数除以测试集数据的总数就计算出了错误率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">datingClassTest</span><span class="params">()</span>:</span></span><br><span class="line">    hoRatio = <span class="number">0.10</span></span><br><span class="line">    datingDataMat, datingLabels = file2matrix(<span class="string">'./MLiA_SourceCode/machinelearninginaction/Ch02/datingTestSet2.txt'</span>)</span><br><span class="line">    normMat, ranges, minVals = autoNorm(datingDataMat)</span><br><span class="line">    m = normMat.shape[<span class="number">0</span>]</span><br><span class="line">    numTestVecs = int(m*hoRatio)</span><br><span class="line">    errorCount = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTestVecs):</span><br><span class="line">        classifierResult = classify0(normMat[i, :], normMat[numTestVecs:m, :],\</span><br><span class="line">                                    datingLabels[numTestVecs:m], <span class="number">3</span>)</span><br><span class="line">        <span class="comment">#print("the classifier came back with: %d, the real answer is: %d"% (classifierResult, datingLabels[i]))</span></span><br><span class="line">        <span class="keyword">if</span> classifierResult != datingLabels[i]:</span><br><span class="line">            errorCount += <span class="number">1.0</span></span><br><span class="line">    print(<span class="string">"the total error rate is: %f"</span>%(errorCount/float(numTestVecs)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datingClassTest()</span><br></pre></td></tr></table></figure>
<pre><code>the total error rate is: 0.050000
</code></pre><p>这里计算出的错误率为5%，书中写的为2.4%，担心算错用source code跑了一遍错误率为6.6%</p>
<h2 id="使用算法：构建完整的可用系统"><a href="#使用算法：构建完整的可用系统" class="headerlink" title="使用算法：构建完整的可用系统"></a>使用算法：构建完整的可用系统</h2><h1 id="手写识别系统"><a href="#手写识别系统" class="headerlink" title="手写识别系统"></a>手写识别系统</h1><p>现在开始第二个示例，识别手写数字，数据已经被图形软件处理为文本格式，先打开几个观察一下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readTrainingDigits</span><span class="params">(filename)</span>:</span></span><br><span class="line">    fr = open(filename, <span class="string">'r'</span>)</span><br><span class="line">    data = fr.read()</span><br><span class="line">    print(data)</span><br><span class="line">    fr.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">readTrainingDigits(<span class="string">'./MLiA_SourceCode/machinelearninginaction/Ch02/digits/trainingDigits/0_0.txt'</span>)</span><br><span class="line">readTrainingDigits(<span class="string">'./MLiA_SourceCode/machinelearninginaction/Ch02/digits/trainingDigits/1_0.txt'</span>)</span><br><span class="line">readTrainingDigits(<span class="string">'./MLiA_SourceCode/machinelearninginaction/Ch02/digits/trainingDigits/2_0.txt'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>00000000000001111000000000000000
00000000000011111110000000000000
00000000001111111111000000000000
00000001111111111111100000000000
00000001111111011111100000000000
00000011111110000011110000000000
00000011111110000000111000000000
00000011111110000000111100000000
00000011111110000000011100000000
00000011111110000000011100000000
00000011111100000000011110000000
00000011111100000000001110000000
00000011111100000000001110000000
00000001111110000000000111000000
00000001111110000000000111000000
00000001111110000000000111000000
00000001111110000000000111000000
00000011111110000000001111000000
00000011110110000000001111000000
00000011110000000000011110000000
00000001111000000000001111000000
00000001111000000000011111000000
00000001111000000000111110000000
00000001111000000001111100000000
00000000111000000111111000000000
00000000111100011111110000000000
00000000111111111111110000000000
00000000011111111111110000000000
00000000011111111111100000000000
00000000001111111110000000000000
00000000000111110000000000000000
00000000000011000000000000000000

00000000000000001111000000000000
00000000000000011111111000000000
00000000000000011111111100000000
00000000000000011111111110000000
00000000000000011111111110000000
00000000000000111111111100000000
00000000000000111111111100000000
00000000000001111111111100000000
00000000000000111111111100000000
00000000000000111111111100000000
00000000000000111111111000000000
00000000000001111111111000000000
00000000000011111111111000000000
00000000000111111111110000000000
00000000001111111111111000000000
00000001111111111111111000000000
00000011111111111111110000000000
00000111111111111111110000000000
00000111111111111111110000000000
00000001111111111111110000000000
00000001111111011111110000000000
00000000111100011111110000000000
00000000000000011111110000000000
00000000000000011111100000000000
00000000000000111111110000000000
00000000000000011111110000000000
00000000000000011111110000000000
00000000000000011111111000000000
00000000000000011111111000000000
00000000000000011111111000000000
00000000000000000111111110000000
00000000000000000111111100000000

00000000001111111000000000000000
00000000011111111100000000000000
00000000011111111110000000000000
00000000011111111111100000000000
00000000111111111111100000000000
00000001111111111111110000000000
00000011111110001111110000000000
00000001111110000111111000000000
00000001111110000111111000000000
00000001111110000111111000000000
00000001111100000111111000000000
00000001111110000011111100000000
00000001111111000011111100000000
00000000111111000011111000000000
00000000111110000111111000000000
00000000001110000011111100000000
00000000000000000011111000000000
00000000000000000111110000000000
00000000000000000111111000000000
00000000000000001111110000000000
00000000000000011111110000000000
00000000000000111111100000000000
00000000000000011111110000000000
00000000000000111111100000000000
00000000000001111111000000000000
00000000000011111110000000000000
00000000001111111111111111111000
00000000011111111111111111111100
00000000111111111111111111111100
00000000011111111111111111111100
00000000001111111111111111111100
00000000000111111111111111110000
</code></pre><p>这是一个32×32的数字矩阵并且可以很明显的看出数字的轮廓。</p>
<h2 id="准备数据：将图像转换为测试向量"><a href="#准备数据：将图像转换为测试向量" class="headerlink" title="准备数据：将图像转换为测试向量"></a>准备数据：将图像转换为测试向量</h2><p>如上所示，每一个文件都是一个32×32构成的数字矩阵，为了使用前面写的分类器，我们必须将图像格式化处理为一个1×1024的向量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span><span class="params">(filename)</span>:</span></span><br><span class="line">    returnVect = zeros((<span class="number">1</span>,<span class="number">1024</span>))</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">        lineStr = fr.readline()</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">            returnVect[<span class="number">0</span>,<span class="number">32</span>*i+j] = int(lineStr[j])</span><br><span class="line">    <span class="keyword">return</span> returnVect</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">testVector = img2vector(<span class="string">'./MLiA_SourceCode/machinelearninginaction/Ch02/digits/testDigits/0_13.txt'</span>)</span><br><span class="line">testVector[<span class="number">0</span>, <span class="number">0</span>:<span class="number">31</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">testVector[<span class="number">0</span>, <span class="number">32</span>:<span class="number">63</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,
       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
</code></pre><h2 id="测试算法：使用k-近邻算法识别手写数字"><a href="#测试算法：使用k-近邻算法识别手写数字" class="headerlink" title="测试算法：使用k-近邻算法识别手写数字"></a>测试算法：使用k-近邻算法识别手写数字</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> trange</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handwritingClassTest</span><span class="params">()</span>:</span></span><br><span class="line">    hwLabels = []</span><br><span class="line">    trainingFileList = os.listdir(<span class="string">'./MLiA_SourceCode/machinelearninginaction/Ch02/digits/trainingDigits'</span>)           <span class="comment">#load the training set</span></span><br><span class="line">    m = len(trainingFileList)</span><br><span class="line">    trainingMat = zeros((m,<span class="number">1024</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        fileNameStr = trainingFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]     <span class="comment">#take off .txt</span></span><br><span class="line">        classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</span><br><span class="line">        hwLabels.append(classNumStr)</span><br><span class="line">        trainingMat[i,:] = img2vector(<span class="string">'./MLiA_SourceCode/machinelearninginaction/Ch02/digits/trainingDigits/%s'</span> % fileNameStr)</span><br><span class="line">    testFileList = os.listdir(<span class="string">'./MLiA_SourceCode/machinelearninginaction/Ch02/digits/testDigits'</span>)        <span class="comment">#iterate through the test set</span></span><br><span class="line">    errorCount = <span class="number">0.0</span></span><br><span class="line">    mTest = len(testFileList)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> trange(mTest):</span><br><span class="line">        fileNameStr = testFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]     <span class="comment">#take off .txt</span></span><br><span class="line">        classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</span><br><span class="line">        vectorUnderTest = img2vector(<span class="string">'./MLiA_SourceCode/machinelearninginaction/Ch02/digits/testDigits/%s'</span> % fileNameStr)</span><br><span class="line">        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, <span class="number">3</span>)</span><br><span class="line">        <span class="comment">#print ("the classifier came back with: %d, the real answer is: %d" % (classifierResult, classNumStr))</span></span><br><span class="line">        <span class="keyword">if</span> (classifierResult != classNumStr): errorCount += <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"\nthe total number of errors is: %d"</span> % errorCount)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"\nthe total error rate is: %f"</span> % (errorCount/float(mTest)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">handwritingClassTest()</span><br></pre></td></tr></table></figure>
<pre><code>100%|██████████| 946/946 [00:30&lt;00:00, 31.30it/s]


the total number of errors is: 10

the total error rate is: 0.010571
</code></pre><p>错误率只有1%，</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>k-近邻算法是分类数据最简单有效的算法，使用算法时我们必须有接近实际数据的训练样本，必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间，此外，由于必须对数据集中的每个数据计算距离值，实际使用会非常耗时,它的另一个缺陷是无法给出任何数据的基本结构信息，因此我们无法知晓平均实例样本和典型实例样本具体有什么特征。</p>
]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>k-邻近算法</tag>
        <tag>KNN</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实战（五）</title>
    <url>/2020/04/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%EF%BC%89/</url>
    <content><![CDATA[<p>Logistic回归是一个最优化算法，比如如何在最短时间从A点到达B点？</p>
<p>回归：假设我们有一些数据点，我们用一条直线对这些点进行拟合（该线称为最佳拟合直线），这个拟合过程就叫做回归。</p>
<p>根据现有的数据对分类边界线建立回归公式，依次进行分类。这里的“回归”一次源于最佳拟合，表示要找到最佳拟合参数集。</p>
<p>Logistic回归的一般过程：</p>
<ol>
<li>收集数据</li>
<li>准备数据：由于需要进行距离计算，因此数据类型必须为数值型，另外结构化数据格式最佳</li>
<li>分析数据</li>
<li>训练算法：大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数</li>
<li>测试算法</li>
<li>使用算法：首先，需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作</li>
</ol>
<h1 id="基于Logistic回归和Sigmoid函数的分类"><a href="#基于Logistic回归和Sigmoid函数的分类" class="headerlink" title="基于Logistic回归和Sigmoid函数的分类"></a>基于Logistic回归和Sigmoid函数的分类</h1><pre><code>logistic回归
优点：计算代价不高，易于理解和实现
缺点：容易欠拟合，分类精度可能不高
适用数据类型：数值型和标称型数据
</code></pre><p>首先我们需要一个阶跃函数（step function），Sigmoid函数，它的值域在0-1之间：</p>
<p>$$\sigma(z)=\frac{1}{1+e^{-z}}$$</p>
<p>下图给出了sigmoid函数在不同坐标尺度下的两条曲线图。</p>
<p>为了实现logistic回归分类器，我们可以在每个特征上都乘以一个回归系数，然后把所有的结果值相加，将这个总和带入sigmoid函数中，进而得到一个范围在0~1之间的数值，任何大于0.5的数据被分入1类，小于0.5的即被归为0类，所以logistic回归也可以被看成一种概率估计。</p>
<p><img src="05_1.jpg" alt=""></p>
<p>两种坐标尺度下的sigmoid函数图，上图的横坐标为-5~5，这时曲线变化较为平滑.</p>
<h1 id="基于最优化方法的最佳回归系数确定"><a href="#基于最优化方法的最佳回归系数确定" class="headerlink" title="基于最优化方法的最佳回归系数确定"></a>基于最优化方法的最佳回归系数确定</h1><p>sigmoid函数的输入记为z，由下面的公式得出：</p>
<p>$$z=w_0x_0+w_1x_1+w_2x_2+…+w_nx_n$$</p>
<p>如果采用向量写法，上述公式可以写成$Z=W^TX$，它表示将这两个数值向量对应元素相乘然后全部加起来得到z的值。其中的向量x是分类器输入的数据，向量w是我们需要求得的最佳系数（weight）。</p>
<h2 id="梯度上升"><a href="#梯度上升" class="headerlink" title="梯度上升"></a>梯度上升</h2><p>梯度上升算法的思想是：要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。如果梯度记为$\nabla$，则函数$f(x,y)$的梯度由下式表示</p>
<p><img src="05_2.jpg" alt=""></p>
<p>这个梯度意味着要沿x的方向移动<img src="05_5.jpg" alt=""> ，沿y的方向移动<img src="05_6.jpg" alt=""> ，其中$f(x,y)$必须要在待计算的点上由定义并且可微。</p>
<p><img src="05_3.jpg" alt=""></p>
<p>梯度上升算法到达每个点后都会重新估计移动方向。从P0开始，计算完该点的梯度，函数就根据梯度移动到下一点P1，在P1点，梯度再次被重新计算，并且沿新的梯度方向移动到P2。如此循环迭代，直到满足停止条件。迭代的过程中，梯度算子总能保证我们选取到最佳的移动方向。</p>
<p>梯度上升算法沿梯度方向移动了一步，可以看到，梯度算子总是指向函数增长最快的方向，这里所说的是移动方向，而未提到移动量的大小，该量值称为步长，记作$\alpha$，用向量表示的话，梯度上升算法的迭代公式如下：</p>
<p>$$w := w+\alpha\nabla_wf(W)$$</p>
<p>该公式将一直被迭代执行，直到达到某个停止条件为止。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">梯度下降算法</span><br><span class="line">经常听到的应该是梯度下降算法，他与这里的梯度上升算法是一样的，只是公式中的加法需要变成减法，因此对应公式可以写成</span><br></pre></td></tr></table></figure>
<p>$$w := w-\alpha\nabla_wf(W)$$<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">梯度上升算法用来求函数的最大值，而梯度下降用来求函数的最小值</span><br></pre></td></tr></table></figure></p>
<p>接下来我们将对下面的数据集使用梯度上升的算法来进行分类，求出最佳回归系数。</p>
<p><img src="05_4.jpg" alt=""></p>
<h2 id="训练算法：使用梯度上升找到最佳参数"><a href="#训练算法：使用梯度上升找到最佳参数" class="headerlink" title="训练算法：使用梯度上升找到最佳参数"></a>训练算法：使用梯度上升找到最佳参数</h2><p>上图有100个样本点，每个点包含两个数值型特征X1和X2。在此数据集上，我们将通过使用梯度上升法找到最佳回归系数，也就是拟合出logistic回归模型的最佳参数。</p>
<p>伪代码如下：</p>
<pre><code>每个回归系数初始化为1
重复R次：
    计算整个数据集的梯度
    使用alpha × gradient更新回归系数的向量
返回回归系数
</code></pre><p>logistic回归梯度上升优化算法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    fr = open(<span class="string">'MLiA_SourceCode/machinelearninginaction/Ch05/testSet.txt'</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr = line.strip().split()</span><br><span class="line">        dataMat.append([<span class="number">1.0</span>, float(lineArr[<span class="number">0</span>]), float(lineArr[<span class="number">1</span>])])</span><br><span class="line">        labelMat.append(int(lineArr[<span class="number">2</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat, labelMat</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataArr, labelMat = loadDataSet()</span><br></pre></td></tr></table></figure>
<p>loadDataSet()函数的作用是打开testSet.txt文件，逐行读取，每行的前两个值是X1和X2，第三个值是对应的类别标签，为了方便计算将X0设置为1.0</p>
<p>下面是sigmoid函数:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(inX)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1</span>+exp(-inX))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> trange</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradAscent</span><span class="params">(dataMat, classLabels)</span>:</span></span><br><span class="line">    <span class="comment"># 转换为numpy矩阵的数据类型</span></span><br><span class="line">    dataMatrix = mat(dataMat)</span><br><span class="line">    labelMat = mat(classLabels).transpose() <span class="comment"># 转置</span></span><br><span class="line">    m, n = shape(dataMatrix)</span><br><span class="line">    alpha = <span class="number">0.001</span> <span class="comment"># 步长</span></span><br><span class="line">    maxCycles = <span class="number">500</span> <span class="comment"># 迭代次数</span></span><br><span class="line">    weights = ones((n, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> trange(maxCycles):</span><br><span class="line">        h = sigmoid(dataMatrix * weights)</span><br><span class="line">        error = (labelMat-h)</span><br><span class="line">        weights = weights + alpha * dataMatrix.transpose() * error</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
<p>gradAscent()函数用来完成梯度上升算法的实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weights = gradAscent(dataArr, labelMat)</span><br><span class="line">weights</span><br></pre></td></tr></table></figure>
<pre><code>100%|██████████| 500/500 [00:00&lt;00:00, 15757.40it/s]

matrix([[ 4.12414349],
        [ 0.48007329],
        [-0.6168482 ]])
</code></pre><h2 id="分析数据：画出决策边界"><a href="#分析数据：画出决策边界" class="headerlink" title="分析数据：画出决策边界"></a>分析数据：画出决策边界</h2><p>上面已经解出一组回归系数，接下来画出分割线。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>]=<span class="keyword">False</span> <span class="comment">#用来正常显示负号</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotBestFit</span><span class="params">(weights)</span>:</span></span><br><span class="line">    dataMat, labelMat = loadDataSet()</span><br><span class="line">    dataArr = array(dataMat)</span><br><span class="line">    n = shape(dataArr)[<span class="number">0</span>]</span><br><span class="line">    xcord1 = []</span><br><span class="line">    ycord1 = []</span><br><span class="line">    xcord2 = []</span><br><span class="line">    ycord2 = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">if</span> int(labelMat[i]) == <span class="number">1</span>:</span><br><span class="line">            xcord1.append(dataArr[i, <span class="number">1</span>])</span><br><span class="line">            ycord1.append(dataArr[i, <span class="number">2</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            xcord2.append(dataArr[i, <span class="number">1</span>])</span><br><span class="line">            ycord2.append(dataArr[i, <span class="number">2</span>])</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    ax.scatter(xcord1, ycord1, s=<span class="number">30</span>, c=<span class="string">'red'</span>, marker=<span class="string">'s'</span>)</span><br><span class="line">    ax.scatter(xcord2, ycord2, s=<span class="number">30</span>, c=<span class="string">'green'</span>)</span><br><span class="line">    x = arange(<span class="number">-3.0</span>, <span class="number">3.0</span>, <span class="number">0.1</span>)</span><br><span class="line">    y = (-weights[<span class="number">0</span>]-weights[<span class="number">1</span>]*x)/weights[<span class="number">2</span>]</span><br><span class="line">    ax.plot(x, y.T) <span class="comment"># y转置是为了xy矩阵结构相同x.shape(60,) y.T.shape(60, 1)</span></span><br><span class="line">    plt.xlabel(<span class="string">'X1'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'X2'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>我们设定$0=w_0x_0+w_1x_1+w_2x_2$，然后用解出X1和X2的关系式（其中X0=1），画出线段</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plotBestFit(weights)</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%EF%BC%89_17_0.png" alt="png"></p>
<p>这个分类结果相当不错，看图可知只分错了两个点。</p>
<h2 id="训练算法：随机梯度上升"><a href="#训练算法：随机梯度上升" class="headerlink" title="训练算法：随机梯度上升"></a>训练算法：随机梯度上升</h2><p>梯度上升算法每次迭代都需要遍历整个数据集，如果有十亿样本和上千万特征，那么改方法的计算复杂度就太高了，一种改进方法是一次仅用一个样本来更新回归系数，该方法称为随机梯度上升算法。由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习算法。与在线学习相对应，一次处理所有数据被称作是“批处理”。</p>
<p>随机梯度上升算法可以写成如下的伪代码：</p>
<pre><code>所有回归系数初始化为1
对数据集中每个样本
    计算该样本的梯度
    使用alpha×gradient更新回归系数
然会回归系数
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent0</span><span class="params">(dataMatrix, classLabels)</span>:</span></span><br><span class="line">    m, n = shape(dataMatrix)</span><br><span class="line">    alpha = <span class="number">0.01</span></span><br><span class="line">    weights = ones(n)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        h = sigmoid(sum(dataMatrix[i]*weights))</span><br><span class="line">        error = classLabels[i] - h</span><br><span class="line">        weights = weights + alpha * error * dataMatrix[i]</span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
<p>随机梯度上升算法和梯度上升算法代码很相似，有两点区别，第一，后者的变量h和error都是向量，前者全是数值；第二，前者没有矩阵转置过程，所有变量的数据类型都是numpy数组。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weights = stocGradAscent0(array(dataArr), labelMat)</span><br><span class="line">plotBestFit(weights)</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%EF%BC%89_22_0.png" alt="png"></p>
<p>观察上图发现有些欠拟合，梯度上升的算法迭代了500次，而这个结果只迭代了200次，所以还算不错了。</p>
<p><del>下图展示了随机梯度上升算法在二百次迭代过程中回归系数的变换情况。</del></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%run MLiA_SourceCode/machinelearninginaction/Ch05/plotSDerror.py</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%EF%BC%89_24_0.png" alt="png"></p>
<p><del>和书上画的不一样，略过。</del></p>
<h3 id="改进的随机梯度上升算法"><a href="#改进的随机梯度上升算法" class="headerlink" title="改进的随机梯度上升算法"></a>改进的随机梯度上升算法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stocGradAscent1</span><span class="params">(dataMatrix, classLabels, numIter=<span class="number">150</span>)</span>:</span></span><br><span class="line">    m, n = shape(dataMatrix)</span><br><span class="line">    weights = ones(n)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(numIter):</span><br><span class="line">        dataIndex = list(range(m))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment"># 1.alpha每次迭代需要调整</span></span><br><span class="line">            alpha = <span class="number">4</span> / (<span class="number">1.0</span> + j + i) + <span class="number">0.0001</span></span><br><span class="line">            <span class="comment"># 2.随机选取更新</span></span><br><span class="line">            randIndex = int(random.uniform(<span class="number">0</span>, len(dataIndex)))</span><br><span class="line">            h = sigmoid(sum(dataMatrix[randIndex]*weights))</span><br><span class="line">            error = classLabels[randIndex] - h</span><br><span class="line">            weights = weights + alpha * error * dataMatrix[randIndex]</span><br><span class="line">            <span class="keyword">del</span>(dataIndex[randIndex])</span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure>
<p>上面的程序改进有两处，1处改进会缓解数据波动或高频波动，虽然alpha会随迭代次数不断减小，但永远不会减小到0，这样做的原因是保证多次迭代后，新数据任然具有一定的影响。</p>
<p>2处改进通过随机选取样本来更新回归系数，这种方法将减小周期波动。</p>
<p>改进算法还增加了迭代次数作为第三个参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%run MLiA_SourceCode/machinelearninginaction/Ch05/plotSDerror.py</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%EF%BC%89_29_0.png" alt="png"></p>
<p>使用随机样本选择和alpha动态减少机制的随机梯度上升算法stocGradAscent1()所生成的系数收敛示意图。该方法比采用固定alpha的方法收敛速度更快。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weights = stocGradAscent1(array(dataArr), labelMat)</span><br><span class="line">plotBestFit(weights)</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%BA%94%EF%BC%89_31_0.png" alt="png"></p>
<p>程序执行结果与gradAscent()差不多，但是计算量更少。</p>
<h1 id="实例：从疝气病症预测马的死亡率"><a href="#实例：从疝气病症预测马的死亡率" class="headerlink" title="实例：从疝气病症预测马的死亡率"></a>实例：从疝气病症预测马的死亡率</h1><p>使用logistic回归预测患有疝病的马的存活问题。提供的数据有368个样本28个特征。</p>
<ol>
<li>收集数据</li>
<li>准备数据：用python解析文本并填充缺失值</li>
<li>分析数据：可视化观察数据</li>
<li>训练算法：使用优化算法，找到最佳系数</li>
<li>测试算法：为了量化回归效果，需要观察错误率。根据错误率决定是否回退到训练阶段，通过改变迭代的次数和步长等参数来得到更好的回归系数</li>
<li>使用算法：实现简单的命令程序来收集马的病症并输出预测结果</li>
</ol>
<h2 id="准备数据：处理数据中的缺失值"><a href="#准备数据：处理数据中的缺失值" class="headerlink" title="准备数据：处理数据中的缺失值"></a>准备数据：处理数据中的缺失值</h2><p>当数据缺失时，可以用以下方法来解决这个问题：</p>
<pre><code>使用可用特征的均值来填补缺失值
使用特殊值来填补缺失值，如-1
忽略有缺失值的样本
使用相似样本的均值来填补缺失值
使用另外的机器学习算预测缺失值
</code></pre><p>现在为了可以使用算法，我们要做两件事</p>
<p>第一，选择实数0来替换所有的缺失值，修改回归系数的更新公式</p>
<p>weights = weights + alpha × error × dataMatrix[randIndex]</p>
<p>这样做是为了当randIndex对应的特征值为0时，weights将不会更新</p>
<p>第二，如果在测试数据集中发现一条数据的类别标签已经缺失，简单的做法是将该条数据丢弃。</p>
<h2 id="测试算法：用logistic回归进行分类"><a href="#测试算法：用logistic回归进行分类" class="headerlink" title="测试算法：用logistic回归进行分类"></a>测试算法：用logistic回归进行分类</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyVector</span><span class="params">(inX, weights)</span>:</span></span><br><span class="line">    prob = sigmoid(sum(inX*weights))</span><br><span class="line">    <span class="keyword">if</span> prob &gt; <span class="number">0.5</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">colicTest</span><span class="params">()</span>:</span></span><br><span class="line">    frTrain = open(<span class="string">'MLiA_SourceCode/machinelearninginaction/Ch05/horseColicTraining.txt'</span>)</span><br><span class="line">    frTest = open(<span class="string">'MLiA_SourceCode/machinelearninginaction/Ch05/horseColicTest.txt'</span>)</span><br><span class="line">    trainingSet = []</span><br><span class="line">    trainingLabels = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> frTrain.readlines():</span><br><span class="line">        currLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        lineArr = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">            lineArr.append(float(currLine[i]))</span><br><span class="line">        trainingSet.append(lineArr)</span><br><span class="line">        trainingLabels.append(float(currLine[<span class="number">21</span>]))</span><br><span class="line">    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, <span class="number">500</span>)</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    numTestVec = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> frTest.readlines():</span><br><span class="line">        numTestVec += <span class="number">1.0</span></span><br><span class="line">        currLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        lineArr = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">            lineArr.append(float(currLine[i]))</span><br><span class="line">        <span class="keyword">if</span> int(classifyVector(array(lineArr), trainWeights)) != int(currLine[<span class="number">21</span>]):</span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line">    errorRate = (float(errorCount)/numTestVec)</span><br><span class="line">    print(<span class="string">"the error tate of this test is %f"</span> % errorRate)</span><br><span class="line">    <span class="keyword">return</span> errorRate</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiTest</span><span class="params">()</span>:</span></span><br><span class="line">    numTests = <span class="number">10</span></span><br><span class="line">    errorSum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(numTests):</span><br><span class="line">        errorSum += colicTest()</span><br><span class="line">    print(<span class="string">"after %d iterations the average error rate is: %f"</span> % (numTests, errorSum/float(numTests)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">multiTest()</span><br></pre></td></tr></table></figure>
<pre><code>the error tate of this test is 0.268657
the error tate of this test is 0.373134
the error tate of this test is 0.343284
the error tate of this test is 0.328358
the error tate of this test is 0.313433
the error tate of this test is 0.298507
the error tate of this test is 0.417910
the error tate of this test is 0.328358
the error tate of this test is 0.283582
the error tate of this test is 0.373134
after 10 iterations the average error rate is: 0.332836
</code></pre><p>十次迭代后的平均错误率为33%，还算不错</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Logistic回归的目的是寻找一个非线性函数sigmoid的最佳拟合参数，求解过程可以由最优化算法完成。在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法又可以简化为随机梯度上升。</p>
<p>随机梯度上升算法与梯度上升算法的效果相当，但占用更少的计算资源，并且，随机梯度上升算法是一个在线算法，它可以在新数据到来时就更新参数，而不需要重新读取整个数据集</p>
<p>如何处理缺失值，取决于实际中的需求。</p>
]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实战（八）</title>
    <url>/2020/04/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%85%AB%EF%BC%89/</url>
    <content><![CDATA[<h1 id="预测数值型数据：回归"><a href="#预测数值型数据：回归" class="headerlink" title="预测数值型数据：回归"></a>预测数值型数据：回归</h1><p>分类的目标变量是标称型数据，而回归是对连续性数据做出预测。</p>
<h1 id="用线性回归找到最佳拟合直线"><a href="#用线性回归找到最佳拟合直线" class="headerlink" title="用线性回归找到最佳拟合直线"></a>用线性回归找到最佳拟合直线</h1><p>线性回归</p>
<pre><code>优点：结果易于理解，计算上不复杂
缺点：对非线性的数据拟合不好
适用数据类型：数值型和标称型数据
</code></pre><p>回归的目的是预测数值型的目标值。最直接的办法是依据输入写出一个目标值的计算公式。</p>
<pre><code>Z = 0.1*X + 0.2*Y
</code></pre><p>这就是所谓的回归方程（regression equation）其中0.1和0.2称作回归系数（regression weights），求回归系数的过程就是回归。</p>
<p>回归的一般方法：</p>
<ol>
<li>收集数据</li>
<li>准备数据：回归需要数值型数据，标称型数据将被转换成二进制数据</li>
<li>分析数据：绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘制在图上作为对比</li>
<li>训练算法：找到回归系数</li>
<li>测试算法：适用$R^2$或者预测值和数据的拟合度，来分析模型的效果</li>
<li>使用算法：使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续性数据而不仅仅是离散的类别标签</li>
</ol>
<p>假如输入数据为矩阵$X$，回归系数为向量$w$，对于给定的数据$X_1$预测结果将会通过$Y_1=X^T_1w$得到。</p>
<p>找到$w$的方法是找到使误差最小的$w$，误差指预测值和真实值之间的差值，使用该差值简单的累加将使正差值和负差值相互抵消，所以采用平方误差。</p>
<p>平方误差可以写作：</p>
<p>$$\sum_{i=1}^m(y_i-x_i^Tw)^2$$</p>
<p>用矩阵表示可以写作:</p>
<p>$$(y-Xw)^T(y-Xw)$$</p>
<p>对$w$求导，得到$X^T(Y-Xw)$，令其等于0，解出</p>
<p>$$w=(X^TX)^{-1}X^Ty$$</p>
<p>$(X^TX)^{-1}$表示矩阵的逆，矩阵的逆可能并不存在，因此要在代码中作出判断。</p>
<p>上述方法也称作OLS“普通最小二乘法”（ordinary least squares）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span>      <span class="comment">#general function to parse tab -delimited floats</span></span><br><span class="line">    numFeat = len(open(fileName).readline().split(<span class="string">'\t'</span>)) - <span class="number">1</span> <span class="comment">#get number of fields </span></span><br><span class="line">    dataMat = []; labelMat = []</span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr =[]</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeat):</span><br><span class="line">            lineArr.append(float(curLine[i]))</span><br><span class="line">        dataMat.append(lineArr)</span><br><span class="line">        labelMat.append(float(curLine[<span class="number">-1</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat,labelMat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standRegres</span><span class="params">(xArr, yArr)</span>:</span></span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat = mat(yArr).T</span><br><span class="line">    xTx = xMat.T*xMat</span><br><span class="line">    <span class="keyword">if</span> linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">        print(<span class="string">"This matrix is singular, cannot do inverse"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    ws = xTx.I * (xMat.T*yMat)</span><br><span class="line">    <span class="keyword">return</span> ws</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">xArr, yArr = loadDataSet(<span class="string">'MLiA_SourceCode/Ch08/ex0.txt'</span>)</span><br><span class="line">xArr[<span class="number">0</span>:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[[1.0, 0.067732], [1.0, 0.42781]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ws = standRegres(xArr, yArr)</span><br><span class="line">ws</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[3.00774324],
        [1.69532264]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">xMat = mat(xArr)</span><br><span class="line">yMat = mat(yArr)</span><br><span class="line">yHat = xMat*ws</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.scatter(xMat[:,<span class="number">1</span>].flatten().A[<span class="number">0</span>], yMat.T[:,<span class="number">0</span>].flatten().A[<span class="number">0</span>], <span class="number">10</span>)</span><br><span class="line">xCopy = xMat.copy()</span><br><span class="line">xCopy.sort(<span class="number">0</span>)</span><br><span class="line">yHat=xCopy*ws</span><br><span class="line">ax.plot(xCopy[:,<span class="number">1</span>], yHat)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%85%AB%EF%BC%89_7_0.png" alt="png"></p>
<p>几乎任意数据都可以用上述法放建立模型，有一种方法可以计算预测值yHat序列和真实值y序列的相关系数来鉴别拟合效果。</p>
<p>在python中可以用numpy库的corrcoef(yEstimate, yActual)来计算相关性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">yHat = xMat*ws</span><br><span class="line">corrcoef(yHat.T, yMat)</span><br></pre></td></tr></table></figure>
<pre><code>array([[1.        , 0.98647356],
       [0.98647356, 1.        ]])
</code></pre><p>上面的矩阵包含所有两两组合的相关系数，对角线为1.0是因为yMat和自己匹配是完美的，yHat和yMat的相关系数为0.98。</p>
<h1 id="局部加权线性回归"><a href="#局部加权线性回归" class="headerlink" title="局部加权线性回归"></a>局部加权线性回归</h1><p>线性回归有可能出现欠拟合，因为求的具有最小均方误差的无偏估计。可以引入一些偏差，从而降低预测的均方误差。其中一个方法是局部加权线性回归（locally weighted linear regression， LWLR），在该算法中，我们给待预测点附近的每个点赋予一定的权重；然后在这个子集上基于最小均方差来进行普通的回归。与KNN一样，这种算法每次预测需要事先选取出对应的数据子集，该算法解出的回归系数w的形式如下</p>
<p>$$w=(X^TWX)^{-1}X^TW_y$$</p>
<p>其中w是一个矩阵，用来给每个数据点赋予权重。</p>
<p>LWLR使用“核”（与支持向量机类似）来对附近的点赋予更高的权重。最常用的是高斯核，因为高斯核符合正态分布，高斯核对应的权重如下：</p>
<p>$$w(i,i)=exp(\frac{|x^{i}-x|}{-2k^2})$$</p>
<p>这样就构建了一个只含对角元素的权重矩阵w，并且点x与x(i)越近，w(i,i)将会越大。<br>上述公式包含了一个需要用户指定的参数k，它决定了对附近的点赋予多大的权重，这是使用LWLR时唯一要考虑的参数。<br>下图可以看到参数k与权重的关系</p>
<p><img src="08_fig04_alt.jpg" alt=""></p>
<p>每个点的权重图（假定我们正预测的点是=0.5），最上面的图是原始数据集，第二个图显示了当k=0.5时，大部分的数据都用于训练回归模型，最下面的图显示的是当k=0.01时，仅有很少的局部点被用于训练回归模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lwlr</span><span class="params">(testPoint, xArr, yArr, k=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat = mat(yArr).T</span><br><span class="line">    m = shape(xMat)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 创建对角矩阵</span></span><br><span class="line">    weights = mat(eye((m)))</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">        diffMat = testPoint - xMat[j, :]</span><br><span class="line">        <span class="comment"># 权重大小以指数级衰减</span></span><br><span class="line">        weights[j, j] = exp(diffMat*diffMat.T/(<span class="number">-2.0</span>*k**<span class="number">2</span>))</span><br><span class="line">    xTx = xMat.T * (weights * xMat)</span><br><span class="line">    <span class="keyword">if</span> linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">        print(<span class="string">'This matrix is singular, cannot do inverse'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    ws = xTx.I * (xMat.T * (weights * yMat))</span><br><span class="line">    <span class="keyword">return</span> testPoint * ws</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lwlrTest</span><span class="params">(testArr, xArr, yArr, k=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    m = shape(testArr)[<span class="number">0</span>]</span><br><span class="line">    yHat = zeros(m)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        yHat[i] = lwlr(testArr[i], xArr, yArr, k)</span><br><span class="line">    <span class="keyword">return</span> yHat</span><br></pre></td></tr></table></figure>
<p>lwlr()的作用是，给定x空间中的任意一点，计算出对应的预测值yHat。权重矩阵是一个方阵，阶数等于样本点个数。也就是说，该矩阵为每个样本初始化了一个权重。接着遍历数据集，计算每个样本点对应的权重，随着样本点与待预测点距离的递增，权重将以指数级衰减。输入参数K控制衰减速度。</p>
<p>测试算法，对单点进行估计：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">xArr[:<span class="number">2</span>], yArr[:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<pre><code>([[1.0, 0.067732], [1.0, 0.42781]], [3.176513, 3.816464])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lwlr(xArr[<span class="number">0</span>], xArr, yArr, <span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[3.12204471]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lwlr(xArr[<span class="number">0</span>], xArr, yArr, <span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[3.20175729]])
</code></pre><p>对所有数据点估计，并绘图：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">yHat1 = lwlrTest(xArr, xArr, yArr, k=<span class="number">1.0</span>)</span><br><span class="line">yHat2 = lwlrTest(xArr, xArr, yArr, k=<span class="number">0.01</span>)</span><br><span class="line">yHat3 = lwlrTest(xArr, xArr, yArr, k=<span class="number">0.003</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotLwlr</span><span class="params">(xArr, yHat)</span>:</span></span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    srtInd = xMat[:, <span class="number">1</span>].argsort(<span class="number">0</span>)</span><br><span class="line">    xSort = xMat[srtInd][:, <span class="number">0</span>, :]</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    ax.plot(xSort[:, <span class="number">1</span>], yHat[srtInd])</span><br><span class="line">    ax.scatter(xMat[:, <span class="number">1</span>].flatten().A[<span class="number">0</span>], mat(yArr).T.flatten().A[<span class="number">0</span>], <span class="number">2</span>, c=<span class="string">'red'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plotLwlr(xArr, yHat1)</span><br><span class="line">plotLwlr(xArr, yHat2)</span><br><span class="line">plotLwlr(xArr, yHat3)</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%85%AB%EF%BC%89_20_0.png" alt="png"></p>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%85%AB%EF%BC%89_20_1.png" alt="png"></p>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%85%AB%EF%BC%89_20_2.png" alt="png"></p>
<p>当k=1.0时，权重很大，如同将所有数据视为等权重，得到的最佳拟合直线和标准回归一致</p>
<p>当k=0.01时，得到了非常好的效果，抓住了数据的潜在模式</p>
<p>当k=0.003时，拟合的直线与数据点过于贴近，过拟合</p>
<p>局部加权线性回归也存在一个问题，即增加了计算量，因为它对每个点做预测时都必须使用整个数据集。</p>
<h1 id="实例：预测鲍鱼的年龄"><a href="#实例：预测鲍鱼的年龄" class="headerlink" title="实例：预测鲍鱼的年龄"></a>实例：预测鲍鱼的年龄</h1><p>鲍鱼年龄可以从鲍鱼壳的层数来推算得到。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rssError</span><span class="params">(yArr, yHatArr)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> ((yArr-yHatArr)**<span class="number">2</span>).sum()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">abX, abY = loadDataSet(<span class="string">'MLiA_SourceCode/Ch08/abalone.txt'</span>)</span><br><span class="line">yHat01 = lwlrTest(abX[: <span class="number">99</span>], abX[: <span class="number">99</span>], abY[: <span class="number">99</span>], <span class="number">0.1</span>)</span><br><span class="line">yHat1 = lwlrTest(abX[: <span class="number">99</span>], abX[: <span class="number">99</span>], abY[: <span class="number">99</span>], <span class="number">1</span>)</span><br><span class="line">yHat10 = lwlrTest(abX[: <span class="number">99</span>], abX[: <span class="number">99</span>], abY[: <span class="number">99</span>], <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>分析误差大小</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rssError(abY[: <span class="number">99</span>], yHat01.T)</span><br></pre></td></tr></table></figure>
<pre><code>56.78868743048742
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rssError(abY[: <span class="number">99</span>], yHat1.T)</span><br></pre></td></tr></table></figure>
<pre><code>429.8905618704059
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rssError(abY[: <span class="number">99</span>], yHat10.T)</span><br></pre></td></tr></table></figure>
<pre><code>549.1181708828803
</code></pre><p>可以看出，使用较小的核，可以得到较低的误差，但对新的数据不一定能达到很好的预测效果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">yHat01 = lwlrTest(abX[<span class="number">100</span>: <span class="number">199</span>], abX[: <span class="number">99</span>], abY[: <span class="number">99</span>], <span class="number">0.1</span>)</span><br><span class="line">yHat1 = lwlrTest(abX[<span class="number">100</span>: <span class="number">199</span>], abX[: <span class="number">99</span>], abY[: <span class="number">99</span>], <span class="number">1</span>)</span><br><span class="line">yHat10 = lwlrTest(abX[<span class="number">100</span>: <span class="number">199</span>], abX[: <span class="number">99</span>], abY[: <span class="number">99</span>], <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rssError(abY[<span class="number">100</span>: <span class="number">199</span>], yHat01.T)</span><br></pre></td></tr></table></figure>
<pre><code>57913.51550155909
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rssError(abY[<span class="number">100</span>: <span class="number">199</span>], yHat1.T)</span><br></pre></td></tr></table></figure>
<pre><code>573.5261441894984
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rssError(abY[<span class="number">100</span>: <span class="number">199</span>], yHat10.T)</span><br></pre></td></tr></table></figure>
<pre><code>517.5711905381573
</code></pre><p>从上面的结果可以看出，核的大小为10的时候测试误差最小，但在训练集上误差却最大，接下来和简单的线性回归比较：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ws = standRegres(abX[<span class="number">0</span>: <span class="number">99</span>], abY[<span class="number">0</span>: <span class="number">99</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">yHat = mat(abX[<span class="number">100</span>: <span class="number">199</span>])*ws</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rssError(abY[<span class="number">100</span>: <span class="number">199</span>], yHat.T.A)</span><br></pre></td></tr></table></figure>
<pre><code>518.6363153245542
</code></pre><p>简单的线性回归和局部加权线性回归的结果类似。</p>
<h1 id="缩减系数来“理解”数据"><a href="#缩减系数来“理解”数据" class="headerlink" title="缩减系数来“理解”数据"></a>缩减系数来“理解”数据</h1><p>如果数据的特征比样本点还多（m&gt;n），说明输入矩阵X不是满秩矩阵，求逆时会出错，也就是在计算$(X^TX)^{-1}$的时候会出错。为了解决这个问题，统计学引入了岭回归（ridge regression）。</p>
<h2 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h2><p>简单来说，岭回归就是在矩阵$(X^TX)$上加一个$\lambda I$从而使矩阵非奇异，进而对$(X^TX)+\lambda I$求逆。其中$I$是一个mxm的单位矩阵，对角线上的元素全为1，其它元素全为0，岭回归公式为：</p>
<p>$$w = (X^TX + \lambda I)^{-1}X^Ty$$</p>
<p>岭回归最先用于处理特征数多于样本数的情况，现在也用于在估计中加入偏差。这里通过引入λ来限制所有w之和，通过引入该惩罚项，能够减少不重要的参数，这个技术在统计学中也叫作缩减（shrinkage）。</p>
<p>岭回归中的岭是什么？</p>
<pre><code>岭回归使用了单位矩阵乘以向量λ，我们观察其中的单位矩阵I，可以看到I贯穿整个对角线，其余元素全是0。形象地，在0构成的平面上有一条1组成的“岭”，这就是岭回归中岭的由来。
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ridgeRegres</span><span class="params">(xMat, yMat, lam=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">    xTx = xMat.T*xMat</span><br><span class="line">    denom = xTx + eye(shape(xMat)[<span class="number">1</span>]) * lam</span><br><span class="line">    <span class="keyword">if</span> linalg.det(denom) == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"This matrix is singular, cannot do invers"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    ws = denom.I * (xMat.T * yMat)</span><br><span class="line">    <span class="keyword">return</span> ws</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ridgeTest</span><span class="params">(xArr, yArr)</span>:</span></span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat = mat(yArr).T</span><br><span class="line">    <span class="comment"># 数据归一化</span></span><br><span class="line">    yMean = mean(yMat, <span class="number">0</span>)</span><br><span class="line">    yMat = yMat - yMean</span><br><span class="line">    xMeans = mean(xMat, <span class="number">0</span>)</span><br><span class="line">    xVar = var(xMat, <span class="number">0</span>)</span><br><span class="line">    xMat = (xMat - xMeans)/xVar</span><br><span class="line">    </span><br><span class="line">    numTestPts = <span class="number">30</span></span><br><span class="line">    wMat = zeros((numTestPts, shape(xMat)[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTestPts):</span><br><span class="line">        ws = ridgeRegres(xMat, yMat, exp(i<span class="number">-10</span>))</span><br><span class="line">        wMat[i, :] = ws.T</span><br><span class="line">    <span class="keyword">return</span> wMat</span><br></pre></td></tr></table></figure>
<p>ridgeRegres()用于计算回归系数，而ridgeTest()用于在一组λ上测试结果。为了使用岭回归和缩减技术，首先需要对特征做标准化处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ridgeWeights = ridgeTest(abX, abY)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>]=<span class="keyword">False</span> <span class="comment"># 用来正常显示负号</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.plot(ridgeWeights)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%85%AB%EF%BC%89_46_0.png" alt="png"></p>
<p>上图绘制了回归系数log(λ)的关系。在最左边，即λ最小时，可以得到所有系数的原始值；而在右边，系数全部缩减成0；在中间的某值将可以取得最好的预测效果。为了定量的找到最佳参数值，还需要进行交叉验证。</p>
<p>还有一些其它的缩减方法，例如lasso，LAR，PCA回归以及子集选择等，与岭回归一样，这些方法不仅可以提高预测精确率，而且可以解释回归系数。</p>
<h2 id="向前逐步回归"><a href="#向前逐步回归" class="headerlink" title="向前逐步回归"></a>向前逐步回归</h2><p>向前逐步算法它属于一种贪心算法，即每一步都尽可能减小误差，一开始，权重都设为1，然后每一步所做的决策是对某个权重增加或减少一个很小的值。</p>
<p>伪代码如下：</p>
<pre><code>数据标准化，使其分布满足0均值和单位方差
在每轮迭代过程中：
    设置当前最小误差lowestError为正无穷
    对每个特征：
        增大或减小：
            改变一个系数得到一个新的W
            计算新W下的误差
            如果误差Error小于当前最小误差lowestError：设置Wbest等于当前的W
         将W设置为最新的Wbest
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularize</span><span class="params">(xMat)</span>:</span><span class="comment">#regularize by columns</span></span><br><span class="line">    inMat = xMat.copy()</span><br><span class="line">    inMeans = mean(inMat,<span class="number">0</span>)   <span class="comment">#calc mean then subtract it off</span></span><br><span class="line">    inVar = var(inMat,<span class="number">0</span>)      <span class="comment">#calc variance of Xi then divide by it</span></span><br><span class="line">    inMat = (inMat - inMeans)/inVar</span><br><span class="line">    <span class="keyword">return</span> inMat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stageWise</span><span class="params">(xArr, yArr, eps=<span class="number">0.01</span>, numIt=<span class="number">100</span>)</span>:</span></span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat = mat(yArr).T</span><br><span class="line">    yMean = mean(yMat, <span class="number">0</span>)</span><br><span class="line">    yMat = yMat - yMean</span><br><span class="line">    xMat = regularize(xMat)</span><br><span class="line">    m, n = shape(xMat)</span><br><span class="line">    returnMat = zeros((numIt, n))</span><br><span class="line">    ws = zeros((n, <span class="number">1</span>))</span><br><span class="line">    wsTest = ws.copy()</span><br><span class="line">    wsMax = ws.copy()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numIt):</span><br><span class="line">        <span class="comment">#print(ws.T)</span></span><br><span class="line">        lowestError = inf</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">for</span> sign <span class="keyword">in</span> [<span class="number">-1</span>, <span class="number">1</span>]:</span><br><span class="line">                wsTest = ws.copy()</span><br><span class="line">                wsTest[j] += eps*sign</span><br><span class="line">                yTest = xMat*wsTest</span><br><span class="line">                rssE = rssError(yMat.A, yTest.A)</span><br><span class="line">                <span class="keyword">if</span> rssE &lt; lowestError:</span><br><span class="line">                    lowestError = rssE</span><br><span class="line">                    wsMax = wsTest</span><br><span class="line">        ws = wsMax.copy()</span><br><span class="line">        returnMat[i, :] = ws.T</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> returnMat</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">stageWise(abX, abY, <span class="number">0.01</span>, <span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<pre><code>array([[ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.  ],
       [ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.  ],
       ...,
       [ 0.05,  0.  ,  0.09, ..., -0.64,  0.  ,  0.36],
       [ 0.04,  0.  ,  0.09, ..., -0.64,  0.  ,  0.36],
       [ 0.05,  0.  ,  0.09, ..., -0.64,  0.  ,  0.36]])
</code></pre><p>上述结果中w1和w6都是0，这表示他们不对目标值造成任何影响，也就是说这些特征可能是不需要的，另外，在参数eps设置为0.01的情况下，一段时间后，系数就已经饱和，并在特定值之间震荡，这是因为步长太大的缘故，这里看到第一个权重在0.04和0.05之间震荡。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">stageWise(abX, abY, <span class="number">0.001</span>, <span class="number">5000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>array([[ 0.   ,  0.   ,  0.   , ...,  0.   ,  0.   ,  0.   ],
       [ 0.   ,  0.   ,  0.   , ...,  0.   ,  0.   ,  0.   ],
       [ 0.   ,  0.   ,  0.   , ...,  0.   ,  0.   ,  0.   ],
       ...,
       [ 0.043, -0.011,  0.12 , ..., -0.963, -0.105,  0.187],
       [ 0.044, -0.011,  0.12 , ..., -0.963, -0.105,  0.187],
       [ 0.043, -0.011,  0.12 , ..., -0.963, -0.105,  0.187]])
</code></pre><p>接着把这些结果与最小二乘法比较。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">xMat = mat(abX)</span><br><span class="line">yMat = mat(abY).T</span><br><span class="line">xMat = regularize(xMat)</span><br><span class="line">yM = mean(yMat, <span class="number">0</span>)</span><br><span class="line">yMat = yMat - yM</span><br><span class="line">weights = standRegres(xMat, yMat.T)</span><br><span class="line">weights.T</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[ 0.0430442 , -0.02274163,  0.13214087,  0.02075182,  2.22403814,
         -0.99895312, -0.11725427,  0.16622915]])
</code></pre><p>可以看到在5000次迭代以后，逐步线性回归算法与常规的最小二乘法效果类似。<br>使用0.005的epsilon值经过1000次迭代后的结果如图。和使用0.001迭代5000图像类似。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.plot(stageWise(abX, abY, <span class="number">0.005</span>, <span class="number">1000</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%85%AB%EF%BC%89_56_0.png" alt="png"></p>
<p>逐步线性回归算法的主要优点在于它可以帮助人们理解现有的模型并做出改进。当构建了一个模型后，可以运行该算法找出重要特征，这样就有可能停止对那些不重要的特征收集。最后，如果用于测试，该算法每100次迭代后就可以构建出一个模型，可以使用类似于10折交叉验证的方法比较这些模型，最终选择误差最小的模型。</p>
<p>当应用缩减方法时，模型也就增加了偏差（bias），于此同时减小模型的方差。</p>
<h1 id="权衡方差于偏差"><a href="#权衡方差于偏差" class="headerlink" title="权衡方差于偏差"></a>权衡方差于偏差</h1><p><img src="08_fig08_alt.jpg" alt=""></p>
<p>偏差方差折中与测试误差及训练误差的关系，上面的曲线就是测试误差，在中间部分最低。为了做出最好的预测，我们应该调整模型复杂度来达到测试误差的最小值。</p>
<h1 id="实例预测乐高玩具套装的价格"><a href="#实例预测乐高玩具套装的价格" class="headerlink" title="实例预测乐高玩具套装的价格"></a>实例预测乐高玩具套装的价格</h1><p>用回归法预测乐高套装的价格</p>
<ol>
<li>收集数据：用Google Shopping的API收集数据</li>
<li>准备数据：从返回的json数据中抽取价格</li>
<li>分析数据：可视化观察数据</li>
<li>训练算法：构建不同的模型，采用逐步线性回归和直接的线性回归模型</li>
<li>测试算法：使用交叉验证来测试不同的模型，分析哪个效果最好</li>
<li>使用算法：这次练习的目标就是生成数据模型</li>
</ol>
<h2 id="收集数据"><a href="#收集数据" class="headerlink" title="收集数据"></a>收集数据</h2><p>因为Google api已经无法访问，所以使用以下code来收集数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scrapePage</span><span class="params">(inFile, outFile, yr, numPce, origPrc)</span>:</span></span><br><span class="line">    <span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup </span><br><span class="line">    fr = open(inFile)</span><br><span class="line">    fw=open(outFile,<span class="string">'a'</span>) <span class="comment"># a is append mode writing</span></span><br><span class="line">    soup = BeautifulSoup(fr.read())</span><br><span class="line">    i=<span class="number">1</span></span><br><span class="line">    currentRow = soup.findAll(<span class="string">'table'</span>, r=<span class="string">"%d"</span> % i)</span><br><span class="line">    <span class="keyword">while</span>(len(currentRow)!=<span class="number">0</span>):</span><br><span class="line">        title = currentRow[<span class="number">0</span>].findAll(<span class="string">'a'</span>)[<span class="number">1</span>].text</span><br><span class="line">        lwrTitle = title.lower()</span><br><span class="line">        <span class="keyword">if</span> (lwrTitle.find(<span class="string">'new'</span>) &gt; <span class="number">-1</span>) <span class="keyword">or</span> (lwrTitle.find(<span class="string">'nisb'</span>) &gt; <span class="number">-1</span>):</span><br><span class="line">            newFlag = <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            newFlag = <span class="number">0.0</span></span><br><span class="line">        soldUnicde = currentRow[<span class="number">0</span>].findAll(<span class="string">'td'</span>)[<span class="number">3</span>].findAll(<span class="string">'span'</span>)</span><br><span class="line">        <span class="keyword">if</span> len(soldUnicde)==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"item #%d did not sell"</span> % i)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            soldPrice = currentRow[<span class="number">0</span>].findAll(<span class="string">'td'</span>)[<span class="number">4</span>]</span><br><span class="line">            priceStr = soldPrice.text</span><br><span class="line">            priceStr = priceStr.replace(<span class="string">'$'</span>,<span class="string">''</span>) <span class="comment">#strips out $</span></span><br><span class="line">            priceStr = priceStr.replace(<span class="string">','</span>,<span class="string">''</span>) <span class="comment">#strips out ,</span></span><br><span class="line">            <span class="keyword">if</span> len(soldPrice)&gt;<span class="number">1</span>:</span><br><span class="line">                priceStr = priceStr.replace(<span class="string">'Free shipping'</span>, <span class="string">''</span>) <span class="comment">#strips out Free Shipping</span></span><br><span class="line">            print(<span class="string">"%s\t%d\t%s"</span> % (priceStr,newFlag,title))</span><br><span class="line">            fw.write(<span class="string">"%d\t%d\t%d\t%f\t%s\n"</span> % (yr,numPce,newFlag,origPrc,priceStr))</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        currentRow = soup.findAll(<span class="string">'table'</span>, r=<span class="string">"%d"</span> % i)</span><br><span class="line">    fw.close()</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setDataCollect</span><span class="params">(outFile)</span>:</span></span><br><span class="line">    scrapePage(<span class="string">'MLiA_SourceCode/Ch08/setHtml/lego8288.html'</span>, outFile, <span class="number">2006</span>, <span class="number">800</span>, <span class="number">49.99</span>)</span><br><span class="line">    scrapePage(<span class="string">'MLiA_SourceCode/Ch08/setHtml/lego10030.html'</span>, outFile, <span class="number">2002</span>, <span class="number">3096</span>, <span class="number">269.99</span>)</span><br><span class="line">    scrapePage(<span class="string">'MLiA_SourceCode/Ch08/setHtml/lego10179.html'</span>, outFile, <span class="number">2007</span>, <span class="number">5195</span>, <span class="number">499.99</span>)</span><br><span class="line">    scrapePage(<span class="string">'MLiA_SourceCode/Ch08/setHtml/lego10181.html'</span>, outFile, <span class="number">2007</span>, <span class="number">3428</span>, <span class="number">199.99</span>)</span><br><span class="line">    scrapePage(<span class="string">'MLiA_SourceCode/Ch08/setHtml/lego10189.html'</span>, outFile, <span class="number">2008</span>, <span class="number">5922</span>, <span class="number">299.99</span>)</span><br><span class="line">    scrapePage(<span class="string">'MLiA_SourceCode/Ch08/setHtml/lego10196.html'</span>, outFile, <span class="number">2009</span>, <span class="number">3263</span>, <span class="number">249.99</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">setDataCollect(<span class="string">'result.txt'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>85.00    0    Lego Technic 8288 Crawler crane
102.50    0    Lego Technic 8288 Crawler Crane USED SET 
77.00    0    Lego Technic 8288 Crawler Crane
item #4 did not sell
162.50    0    RARE Lego Technic 8288 Crawler Crane
699.99    0    Lego Star Wars Imperial Star Destroyer (10030)  Sealed!
602.00    0    Lego Star Wars UCS Imperial Star Destroyer #10030
515.00    0    Lego 10030 Imperial Star Destroyer 
510.00    0    Lego Star Wars 10030 Ultimate Imperial Star Destroyer
375.00    0    Lego Star Wars Imperial Star Destroyer (10030)
1050.00    1    LEGO STAR DESTROYER NEW IN SEALED BOX 10030 STAR WARS
740.00    0    IMPERIAL STAR DESTROYER #10030 Lego Star Wars   SEALED 
759.00    1    LEGO STAR WARS 10030 UCS IMPERIAL DESTROYER NISB NEW
730.00    0    Lego 10030 Star Destroyer, MISB, Old Gray, Ships Free!
750.00    1    NEW STAR WARS LEGO SET 10030 IMPERIAL STAR DESTROYER
item #11 did not sell
910.00    0    LEGO star wars Millenium Falcon #10179 MISB
1199.99    1    Lego Star Wars - 10179 Ultimate Millennium Falcon - NEW
811.88    0    Lego Star Wars - 10179 Ultimate Millennium Falcon-USED
item #4 did not sell
1324.79    0    Lego Star Wars Millennium Falcon 10179
850.00    1    NEW LEGO 10179 STAR WARS UC MILLENNIUM FALCON - NISB
800.00    1    NEW LEGO 10179 STAR WARS UC MILLENNIUM FALCON - NISB
810.00    0    lego star warsUltimateCol​lectors millennium falcon10179
1075.00    1    Lego Star Wars Ultimate Millenium Falcon 10179 NEW MISB
1050.00    0    LEGO STAR WARS 10179 UCS MILLENIUM FALCON! MINT IN BOX
1199.99    1    LEGO 10179 STAR WARS MILLENNIUM FALCON UCS NEW/SEALED
1342.31    0    Lego Star Wars 10179 Collectors Millennium Falcon
1000.00    1    Star Wars - UCS Millennium Falcon - 10179 - New In Box
1780.00    0    LEGO STAR WARS 10188 10179 DEATH STAR MILLENIUM FALCON
750.00    0    STAR WARS Lego 10179 Ultimate CS MILLENNIUM FALCON! 
item #16 did not sell
2204.99    0     HUGE LOT OF LEGOS 10179 FALCON &amp; MORE STARWARS &amp; MORE
item #18 did not sell
925.00    1    Lego #10179 BRAND NEW Star Wars UCS Millenium Falcon
860.00    0    LEGO STAR WARS UCS MILLENNIUM FALCON #10179 WITH BOX
item #21 did not sell
item #22 did not sell
1199.99    1    Lego Star Wars 10179 UCS Millenium Falcon - NEW!
1099.99    1    Lego Star Wars 10179 UCS Millennium Falcon  NiSB  HUGE!
1149.99    1    NEW LEGO 10179 STAR WARS MILLENNIUM FALCON NEW/SEALED
800.00    1    NEW LEGO 10179 STAR WARS UC MILLENNIUM FALCON - NISB
850.00    1    NEW LEGO 10179 STAR WARS UC MILLENNIUM FALCON - NISB
469.95    0    Lego Star Wars Death Star II 10143 MNIB SOLD OUT A++
479.00    0    NIB Box Collectors Starwars Death Star II - 10143
299.99    0    Lego Star Wars Death Star II 10143 -Excellent Condition
369.00    0    Lego Star Wars Death Star ll # 10143
424.95    1    LEGO Star Wars  Death Star II 10143 *Damaged Box* NEW
380.00    1    NEW Lego Star Wars Death Star II #10143
305.00    0    LEGO Star Wars Death Star II 10143 
530.00    1    LEGO Taj Mahal NEW IN BOX MINT CONDITION! LAST ONE!
item #2 did not sell
599.95    1    LEGO 10189 TAJ MAHAL - BRAND NEW - RARE &amp; SOLD OUT!
510.00    0    Lego~Taj Mahal~#10189~pu​t together once~EUC
423.00    0    Lego Taj Mahal 10189- Put together ONCE - perfect shape
item #6 did not sell
item #7 did not sell
599.99    1    Lego - Taj Mahal 10189  - NEW Sealed
item #9 did not sell
589.99    1    LEGO 10189 TAJ MAHAL NEW SEALED IN BOX FAST SHIPPING
569.99    1    LEGO 10189 TAJ MAHAL NEW SEALED MINT FREE SHIPPING 
529.99    1    Lego 10189 Taj Mahal ***New &amp; Sealed***
500.00    0    LEGO TAJ MAHAL 
549.95    1    LEGO 10189 TAJ MAHAL - BRAND NEW - RARE &amp; SOLD OUT!
300.00    0    Lego TAJ MAHAL 10189 100% Complete, No Box, Inst. Incl.
item #16 did not sell
380.00    1    Lego - Grand Carousel 10196 - NEW Sealed
399.00    1    Lego Grand Carousel 10196 - NIB Sealed, Brand New
427.99    1    Lego 10196 Grand Carousel ***New &amp; Sealed***
360.00    0    Grand Carousel Lego 10196 Rare Used Extra Minifigs
item #5 did not sell
item #6 did not sell
399.00    1    Lego City 10196 Grand Carousel New In BOX! 
399.95    1    LEGO CREATOR CAROUSEL 10196 Box New *MISB*
499.99    1    Lego - Grand Carousel 10196  - NEW Sealed
item #10 did not sell
399.95    0    LEGO Grand Carousel 10196 NIB
item #12 did not sell
331.51    1    Lego Carousel 10196, New Unopened Bags
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lgX, lgY = loadDataSet(<span class="string">'result.txt'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="训练算法：建立模型"><a href="#训练算法：建立模型" class="headerlink" title="训练算法：建立模型"></a>训练算法：建立模型</h2><p>首先需要添加对应常数项的特征X0(X0=1)，为此创建一个全为1的矩阵。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">shape(lgX)</span><br></pre></td></tr></table></figure>
<pre><code>(126, 4)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lgX1 = mat(ones((<span class="number">126</span>, <span class="number">5</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lgX1[:, <span class="number">1</span>:<span class="number">5</span>] = mat(lgX)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lgX[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[2006.0, 800.0, 0.0, 49.99]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lgX1[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[1.000e+00, 2.006e+03, 8.000e+02, 0.000e+00, 4.999e+01]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ws = standRegres(lgX1, lgY)</span><br><span class="line">ws</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[ 5.53199701e+04],
        [-2.75928219e+01],
        [-2.68392234e-02],
        [-1.12208481e+01],
        [ 2.57604055e+00]])
</code></pre><p>检查结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lgX1[<span class="number">0</span>]*ws</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[76.07418859]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lgX1[<span class="number">-1</span>]*ws</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[431.17797678]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lgX1[<span class="number">43</span>]*ws</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[516.20733111]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lgY[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>85.0
</code></pre><p>交叉验证测试岭回归</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crossValidation</span><span class="params">(xArr, yArr, numVal=<span class="number">10</span>)</span>:</span></span><br><span class="line">    m = len(yArr)</span><br><span class="line">    indexList = list(range(m))</span><br><span class="line">    errorMat = zeros((numVal, <span class="number">30</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numVal):</span><br><span class="line">        trainX = []</span><br><span class="line">        trainY = []</span><br><span class="line">        testX = []</span><br><span class="line">        testY = []</span><br><span class="line">        random.shuffle(indexList)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="keyword">if</span> j &lt; m*<span class="number">0.9</span>:</span><br><span class="line">                trainX.append(xArr[indexList[j]])</span><br><span class="line">                trainY.append(yArr[indexList[j]])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                testX.append(xArr[indexList[j]])</span><br><span class="line">                testY.append(yArr[indexList[j]])</span><br><span class="line">        wMat = ridgeTest(trainX, trainY)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">30</span>):</span><br><span class="line">            matTestX = mat(testX)</span><br><span class="line">            matTrainX = mat(trainX)</span><br><span class="line">            meanTrain = mean(matTrainX, <span class="number">0</span>)</span><br><span class="line">            varTrain = var(matTrainX, <span class="number">0</span>)</span><br><span class="line">            matTestX = (matTestX-meanTrain)/varTrain</span><br><span class="line">            yEst = matTestX * mat(wMat[k, :]).T + mean(trainY)</span><br><span class="line">            errorMat[i, k] = rssError(yEst.T.A, array(testY))</span><br><span class="line">    meanErrors = mean(errorMat,<span class="number">0</span>)<span class="comment">#calc avg performance of the different ridge weight vectors</span></span><br><span class="line">    minMean = float(min(meanErrors))</span><br><span class="line">    bestWeights = wMat[nonzero(meanErrors==minMean)]</span><br><span class="line">    <span class="comment"># 可以非正则化得到模型</span></span><br><span class="line">    <span class="comment"># 正则化后，我们写了Xreg = (x- meanx)/var(x)</span></span><br><span class="line">    <span class="comment"># 我们现在可以用x而不是Xreg来表示:x*w/var(x) - meanX/var(x) +mean</span></span><br><span class="line">    xMat = mat(xArr)</span><br><span class="line">    yMat = mat(yArr).T</span><br><span class="line">    meanX = mean(xMat,<span class="number">0</span>)</span><br><span class="line">    varX = var(xMat,<span class="number">0</span>)</span><br><span class="line">    unReg = bestWeights/varX</span><br><span class="line">    print(<span class="string">"the best model from ridge regression is:\n"</span>, unReg)</span><br><span class="line">    print(<span class="string">"with constant term: "</span>, <span class="number">-1</span>*sum(multiply(meanX, unReg))+mean(yMat))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">crossValidation(lgX, lgY, numVal=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<pre><code>the best model from ridge regression is:
 [[-3.13000380e+01 -5.79216518e-04 -1.46976042e+01  2.33709492e+00]]
with constant term:  62728.39604629546
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ridgeTest(lgX, lgY)</span><br></pre></td></tr></table></figure>
<pre><code>array([[-1.42567890e+02, -1.59065167e+04, -3.32568485e+00,
         4.50485291e+04],
       [-1.46048534e+02, -5.88035105e+03, -3.20592314e+00,
         4.38513111e+04],
       [-1.46392961e+02, -7.53268167e+02, -2.49755326e+00,
         4.21602572e+04],
       [-1.42882929e+02,  1.24713671e+03, -5.78336771e-01,
         3.87616551e+04],
       [-1.34058297e+02,  1.65495690e+03,  3.46766980e+00,
         3.19757189e+04],
       [-1.20185790e+02,  1.28302796e+03,  9.65389591e+00,
         2.16984122e+04],
       [-1.06405098e+02,  7.16651262e+02,  1.57519175e+01,
         1.15841453e+04],
       [-9.74799755e+01,  3.21265653e+02,  1.96506716e+01,
         5.10995216e+03],
       [-9.29742820e+01,  1.28256118e+02,  2.14874762e+01,
         2.02836160e+03],
       [-9.04274807e+01,  4.86739495e+01,  2.21853397e+01,
         7.68474711e+02],
       [-8.75861154e+01,  1.80935644e+01,  2.23113328e+01,
         2.85792138e+02],
       [-8.19045992e+01,  6.66134743e+00,  2.20039861e+01,
         1.05512059e+02],
       [-6.99384526e+01,  2.43521001e+00,  2.11068607e+01,
         3.88294842e+01],
       [-5.00210980e+01,  8.84809679e-01,  1.94076129e+01,
         1.42672332e+01],
       [-2.79820303e+01,  3.21315572e-01,  1.69641166e+01,
         5.24285549e+00],
       [-1.24628930e+01,  1.17567465e-01,  1.37731297e+01,
         1.93051917e+00],
       [-4.77896841e+00,  4.34264276e-02,  9.63509521e+00,
         7.12700367e-01],
       [-1.71197823e+00,  1.61100639e-02,  5.40201378e+00,
         2.63366078e-01],
       [-6.08727885e-01,  5.96613258e-03,  2.47122138e+00,
         9.72056597e-02],
       [-2.19489801e-01,  2.20244723e-03,  9.99173868e-01,
         3.58199566e-02],
       [-8.00295532e-02,  8.11429705e-04,  3.81511828e-01,
         1.31867450e-02],
       [-2.93376909e-02,  2.98679351e-04,  1.42337367e-01,
         4.85246297e-03],
       [-1.07783727e-02,  1.09901635e-04,  5.26372259e-02,
         1.78530510e-03],
       [-3.96318049e-03,  4.04337769e-05,  1.94015377e-02,
         6.56802092e-04],
       [-1.45770631e-03,  1.48751929e-05,  7.14249987e-03,
         2.41627386e-04],
       [-5.36224096e-04,  5.47233696e-06,  2.62826610e-03,
         8.88902084e-05],
       [-1.97260935e-04,  2.01316829e-06,  9.66978104e-04,
         3.27009426e-05],
       [-7.25675812e-05,  7.40604313e-07,  3.55743958e-04,
         1.20300129e-05],
       [-2.66960317e-05,  2.72453248e-07,  1.30872593e-04,
         4.42559557e-06],
       [-9.82090911e-06,  1.00229968e-07,  4.81455670e-05,
         1.62808578e-06]])
</code></pre><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>与分类一样，回归也是预测目标值的过程。回归与分类的不同点在于，前者预测连续型变量而后者预测离散型变量。在回归方程里，求得特征对应的最佳回归系数的方法是最小化误差的平方和。</p>
<p>当数据的样本比特征数还少的时候，矩阵$X^TX$的逆不能直接计算，这时可以考虑使用缩减法。</p>
<p>缩减法还可以看做是对一个模型增加偏差的同时减小方差。</p>
]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>线性回归</tag>
        <tag>岭回归</tag>
        <tag>最小二乘法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实战（六）</title>
    <url>/2020/04/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%85%AD%EF%BC%89/</url>
    <content><![CDATA[<p>这一章的内容非常多，在神经网络大火前，SVM是最优秀的机器学习算法，尽管现在已经很少用了，但作为一本七年前的书还是很详细的讲解了，所以这里简单的记录下。</p>
<h1 id="基于最大间隔分隔数据"><a href="#基于最大间隔分隔数据" class="headerlink" title="基于最大间隔分隔数据"></a>基于最大间隔分隔数据</h1><p>支持向量机</p>
<pre><code>优点：泛化错误率低，计算开销不大，结果易理解
缺点：对参数调节和核函数选择敏感，原始分类器不加修改仅适用于处理二分类问题
适用数据类型：数值型和标称型数据
</code></pre><p><img src="06_fig01.jpg" alt="线性不可分的数据集"></p>
<p>观察上图图发现我们不能画出一条线或者圆把圆形和方形的数据分割开，而下图可以画出一条直线将两组数据分开。所以下图的数据称为线性可分（linearly separable）。</p>
<p><img src="06_fig02.jpg" alt="线性可分的数据集"></p>
<p>将数据集分开的直线称为分割超平面（separating hyperplane）。上面给出的例子数据都在二维平面上，所以分割超平面是一条直线，如果所给的数据是三维的，那么分割数据的就是一个平面。如果数据集是一个1024维的，那就需要一个1023维的对象对数据分割。如果一个数据集是N维的，需要一个N-1维的超平面分割。</p>
<p>支持向量（support vector）就是离分割超平面最近的那些点。</p>
<h1 id="寻找最大间隔"><a href="#寻找最大间隔" class="headerlink" title="寻找最大间隔"></a>寻找最大间隔</h1><p>How can we measure the line that best separates the data? To start with, look at figure 6.3. Our separating hyperplane has the form wTx+b. If we want to find the distance from A to the separating plane, we must measure normal or perpendicular to the line. This is given by |wTA+b|/||w||. The constant b is just an offset like w0 in logistic regression. All this w and b stuff describes the separating line, or hyperplane, for our data. Now, let’s talk about the classifier.</p>
<p><img src="06_fig03.jpg" alt="线性可分的数据集"></p>
<h2 id="分类器求解的优化问题"><a href="#分类器求解的优化问题" class="headerlink" title="分类器求解的优化问题"></a>分类器求解的优化问题</h2><h2 id="SVM应用的一般框架"><a href="#SVM应用的一般框架" class="headerlink" title="SVM应用的一般框架"></a>SVM应用的一般框架</h2><p>SVM的一般流程</p>
<ol>
<li>收集数据</li>
<li>准备数据：需要数值型数据</li>
<li>分析数据：有助于可视化分割超平面</li>
<li>训练算法：SVM大大部分时间都源自训练，该过程主要实现两个参数的调优</li>
<li>测试算法：十分简单的计算过程就可以实现</li>
<li>适用算法：几乎所有分类问题都可以适用SVM，值得一提的是，SVM本身是一个二分类分类器，对多分类问题SVM需要对代码修改</li>
</ol>
<h2 id="SMO高效优化算法"><a href="#SMO高效优化算法" class="headerlink" title="SMO高效优化算法"></a>SMO高效优化算法</h2><p>SMO表示序列最小优化（Sequential Minimal Optimization）</p>
<p>SMO算法的目标是求出一系列alpha和b，一旦求出了这些alpha，就很容易计算处权重向量w并得到分割超平面。</p>
<p>SMO算法的工作原理是：每次循环中选择两个alpha进行优化处理。一旦找到一<br>对合适的alpha，那么就增大其中一个同时减小另一个。这里所谓的合适们就是指两个alpha必须要符合两个条件，一，两个alpha必须要在间隔边界之外，二，两个alpha还没有进行过区间化处理或者不在边界上。</p>
<h2 id="应用简化版SMO算法处理小规模数据集"><a href="#应用简化版SMO算法处理小规模数据集" class="headerlink" title="应用简化版SMO算法处理小规模数据集"></a>应用简化版SMO算法处理小规模数据集</h2><p>$$\Sigma\alpha*label^{i} = 0$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        dataMat.append([float(lineArr[<span class="number">0</span>]), float(lineArr[<span class="number">1</span>])])</span><br><span class="line">        labelMat.append(float(lineArr[<span class="number">2</span>]))</span><br><span class="line">    fr.close()</span><br><span class="line">    <span class="keyword">return</span> dataMat, labelMat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selectJrand</span><span class="params">(i, m)</span>:</span></span><br><span class="line">    j = i</span><br><span class="line">    <span class="keyword">while</span> (j == i):</span><br><span class="line">        j = int(random.uniform(<span class="number">0</span>, m))</span><br><span class="line">    <span class="keyword">return</span> j</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clipAlpha</span><span class="params">(aj, H, L)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> aj &gt; H:</span><br><span class="line">        aj = H</span><br><span class="line">    <span class="keyword">if</span> L &gt; aj:</span><br><span class="line">        aj = L</span><br><span class="line">    <span class="keyword">return</span> aj</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataArr, labelArr = loadDataSet(<span class="string">'MLiA_SourceCode/machinelearninginaction/Ch06/testSet.txt'</span>)</span><br><span class="line">labelArr[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[-1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0]
</code></pre><p>selectJrand()有两个参数，i是第一个alpha的下表，m是所有alpha的数目，只要函数值不等于输入值i，函数就会随机选择</p>
<p>clipAlpha()的作用是调整alpha的值在H和L之间。</p>
<p>SMO伪代码大致如下：</p>
<pre><code>创建一个alpha向量并将其初始化为0向量
当迭代次数小于最大迭代次数时（外循环）
    对数据集中的每个数据向量（内循环）：
        如果给数据向量可以被优化：
            随机选择另外一个数据向量
            如果优化这两个向量
            如果两个向量都不能被优化，退出内循环
    如果所有向量都没被优化，增加迭代数目，继续下一次循环
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smoSimple</span><span class="params">(dataMatIn, classLabels, C, toler, maxIter)</span>:</span></span><br><span class="line">    dataMatrix = mat(dataMatIn)</span><br><span class="line">    labelMat = mat(classLabels).transpose()</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    alphas = mat(zeros((m,<span class="number">1</span>)))</span><br><span class="line">    iter = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (iter &lt; maxIter):</span><br><span class="line">        alphaPairsChanged = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            fXi = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b</span><br><span class="line">            Ei = fXi - float(labelMat[i]) <span class="comment">#if checks if an example violates KKT conditions</span></span><br><span class="line">            <span class="keyword">if</span> ((labelMat[i]*Ei &lt; -toler) <span class="keyword">and</span> (alphas[i] &lt; C)) <span class="keyword">or</span> ((labelMat[i]*Ei &gt; toler) <span class="keyword">and</span> (alphas[i] &gt; <span class="number">0</span>)):</span><br><span class="line">                j = selectJrand(i,m)</span><br><span class="line">                fXj = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b</span><br><span class="line">                Ej = fXj - float(labelMat[j])</span><br><span class="line">                alphaIold = alphas[i].copy(); alphaJold = alphas[j].copy();</span><br><span class="line">                <span class="keyword">if</span> (labelMat[i] != labelMat[j]):</span><br><span class="line">                    L = max(<span class="number">0</span>, alphas[j] - alphas[i])</span><br><span class="line">                    H = min(C, C + alphas[j] - alphas[i])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    L = max(<span class="number">0</span>, alphas[j] + alphas[i] - C)</span><br><span class="line">                    H = min(C, alphas[j] + alphas[i])</span><br><span class="line">                    </span><br><span class="line">                <span class="keyword">if</span> L==H: </span><br><span class="line">                    print(<span class="string">"L==H"</span>)</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                eta = <span class="number">2.0</span> * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> eta &gt;= <span class="number">0</span>:</span><br><span class="line">                    print(<span class="string">"eta&gt;=0"</span>)</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                alphas[j] -= labelMat[j]*(Ei - Ej)/eta</span><br><span class="line">                alphas[j] = clipAlpha(alphas[j],H,L)</span><br><span class="line">                <span class="keyword">if</span> (abs(alphas[j] - alphaJold) &lt; <span class="number">0.00001</span>):</span><br><span class="line">                    print(<span class="string">"j not moving enough"</span>)</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j]) <span class="comment"># update i by the same amount as j</span></span><br><span class="line">                                                                             <span class="comment"># the update is in the oppostie direction</span></span><br><span class="line">                b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T</span><br><span class="line">                b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T</span><br><span class="line">                <span class="keyword">if</span> (<span class="number">0</span> &lt; alphas[i]) <span class="keyword">and</span> (C &gt; alphas[i]): </span><br><span class="line">                    b = b1</span><br><span class="line">                <span class="keyword">elif</span> (<span class="number">0</span> &lt; alphas[j]) <span class="keyword">and</span> (C &gt; alphas[j]): </span><br><span class="line">                    b = b2</span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    b = (b1 + b2)/<span class="number">2.0</span></span><br><span class="line">                alphaPairsChanged += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                print(<span class="string">"iter: %d i:%d, pairs changed %d"</span> % (iter,i,alphaPairsChanged))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (alphaPairsChanged == <span class="number">0</span>):</span><br><span class="line">            iter += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            iter = <span class="number">0</span></span><br><span class="line">        print(<span class="string">"iteration number: %d"</span> % iter)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> b,alphas</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b, alphas = smoSimple(dataArr, labelArr, <span class="number">0.6</span>, <span class="number">0.001</span>, <span class="number">40</span>)</span><br></pre></td></tr></table></figure>
<pre><code>L==H
L==H
iter: 0 i:2, pairs changed 1
iter: 0 i:3, pairs changed 2
L==H
...
...
iteration number: 39
j not moving enough
j not moving enough
j not moving enough
iteration number: 40
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[-3.79661253]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">alphas[alphas&gt;<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[0.12629181, 0.24169497, 0.36797683]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">shape(alphas[alphas&gt;<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>(1, 3)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">supportVectors = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">if</span> alphas[i] &gt; <span class="number">0.0</span>:</span><br><span class="line">        supportVectors.append(dataArr[i])</span><br><span class="line">        print(dataArr[i], labelArr[i])</span><br></pre></td></tr></table></figure>
<pre><code>[4.658191, 3.507396] -1.0
[3.457096, -0.082216] -1.0
[6.080573, 0.418886] 1.0
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.patches <span class="keyword">import</span> Circle</span><br><span class="line">plt.rcParams[<span class="string">'axes.unicode_minus'</span>]=<span class="keyword">False</span> <span class="comment"># 用来正常显示负号</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotSupportVectors</span><span class="params">(supportVectors)</span>:</span></span><br><span class="line">    xcord0 = []</span><br><span class="line">    ycord0 = []</span><br><span class="line">    xcord1 = []</span><br><span class="line">    ycord1 = []</span><br><span class="line">    markers =[]</span><br><span class="line">    colors =[]</span><br><span class="line">    fr = open(<span class="string">'MLiA_SourceCode/machinelearninginaction/Ch06/testSet.txt'</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineSplit = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        xPt = float(lineSplit[<span class="number">0</span>])</span><br><span class="line">        yPt = float(lineSplit[<span class="number">1</span>])</span><br><span class="line">        label = int(lineSplit[<span class="number">2</span>])</span><br><span class="line">        <span class="keyword">if</span> (label == <span class="number">-1</span>):</span><br><span class="line">            xcord0.append(xPt)</span><br><span class="line">            ycord0.append(yPt)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            xcord1.append(xPt)</span><br><span class="line">            ycord1.append(yPt)</span><br><span class="line"></span><br><span class="line">    fr.close()</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    ax.scatter(xcord0,ycord0, marker=<span class="string">'s'</span>, s=<span class="number">90</span>)</span><br><span class="line">    ax.scatter(xcord1,ycord1, marker=<span class="string">'o'</span>, s=<span class="number">50</span>, c=<span class="string">'red'</span>)</span><br><span class="line">    plt.title(<span class="string">'Support Vectors Circled'</span>)</span><br><span class="line">    <span class="keyword">for</span> vector <span class="keyword">in</span> supportVectors:</span><br><span class="line">        circle = Circle(vector, <span class="number">0.5</span>, facecolor=<span class="string">'none'</span>, edgecolor=(<span class="number">0</span>,<span class="number">0.8</span>,<span class="number">0.8</span>), linewidth=<span class="number">2</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">        ax.add_patch(circle)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#plt.plot([2.3,8.5], [-6,6]) #seperating hyperplane</span></span><br><span class="line">    b = <span class="number">-3.75567</span>; w0=<span class="number">0.8065</span>; w1=<span class="number">-0.2761</span></span><br><span class="line">    x = arange(<span class="number">-2.0</span>, <span class="number">12.0</span>, <span class="number">0.1</span>)</span><br><span class="line">    y = (-w0*x - b)/w1</span><br><span class="line">    ax.plot(x,y)</span><br><span class="line">    ax.axis([<span class="number">-2</span>,<span class="number">12</span>,<span class="number">-8</span>,<span class="number">6</span>])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>圈出支持向量</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plotSupportVectors(supportVectors)</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%85%AD%EF%BC%89_18_0.png" alt="png"></p>
<h1 id="利用完整的SMO算法加速优化"><a href="#利用完整的SMO算法加速优化" class="headerlink" title="利用完整的SMO算法加速优化"></a>利用完整的SMO算法加速优化</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">optStruct</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,dataMatIn, classLabels, C, toler, kTup)</span>:</span>  <span class="comment"># Initialize the structure with the parameters </span></span><br><span class="line">        self.X = dataMatIn</span><br><span class="line">        self.labelMat = classLabels</span><br><span class="line">        self.C = C</span><br><span class="line">        self.tol = toler</span><br><span class="line">        self.m = shape(dataMatIn)[<span class="number">0</span>]</span><br><span class="line">        self.alphas = mat(zeros((self.m,<span class="number">1</span>)))</span><br><span class="line">        self.b = <span class="number">0</span></span><br><span class="line">        self.eCache = mat(zeros((self.m,<span class="number">2</span>))) <span class="comment">#first column is valid flag</span></span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcEk</span><span class="params">(oS, k)</span>:</span></span><br><span class="line">    fXk = float(multiply(oS.alphas, oS.labelMat).T*(oS.X*oS.X[k, :].T)) + oS.b</span><br><span class="line">    Ek = fXk - float(oS.labelMat[k])</span><br><span class="line">    <span class="keyword">return</span> Ek</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selectJ</span><span class="params">(i, oS, Ei)</span>:</span>         <span class="comment">#this is the second choice -heurstic, and calcs Ej</span></span><br><span class="line">    maxK = <span class="number">-1</span>; maxDeltaE = <span class="number">0</span>; Ej = <span class="number">0</span></span><br><span class="line">    oS.eCache[i] = [<span class="number">1</span>,Ei]  <span class="comment">#set valid #choose the alpha that gives the maximum delta E</span></span><br><span class="line">    validEcacheList = nonzero(oS.eCache[:,<span class="number">0</span>].A)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> (len(validEcacheList)) &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> validEcacheList:   <span class="comment">#loop through valid Ecache values and find the one that maximizes delta E</span></span><br><span class="line">            <span class="keyword">if</span> k == i: <span class="keyword">continue</span> <span class="comment">#don't calc for i, waste of time</span></span><br><span class="line">            Ek = calcEk(oS, k)</span><br><span class="line">            deltaE = abs(Ei - Ek)</span><br><span class="line">            <span class="keyword">if</span> (deltaE &gt; maxDeltaE):</span><br><span class="line">                maxK = k; maxDeltaE = deltaE; Ej = Ek</span><br><span class="line">        <span class="keyword">return</span> maxK, Ej</span><br><span class="line">    <span class="keyword">else</span>:   <span class="comment">#in this case (first time around) we don't have any valid eCache values</span></span><br><span class="line">        j = selectJrand(i, oS.m)</span><br><span class="line">        Ej = calcEk(oS, j)</span><br><span class="line">    <span class="keyword">return</span> j, Ej</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateEk</span><span class="params">(oS, k)</span>:</span><span class="comment">#after any alpha has changed update the new value in the cache</span></span><br><span class="line">    Ek = calcEk(oS, k)</span><br><span class="line">    oS.eCache[k] = [<span class="number">1</span>,Ek]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">innerL</span><span class="params">(i, oS, istraces=True)</span>:</span></span><br><span class="line">    Ei = calcEk(oS, i)</span><br><span class="line">    <span class="keyword">if</span> ((oS.labelMat[i]*Ei &lt; -oS.tol) <span class="keyword">and</span> (oS.alphas[i] &lt; oS.C)) <span class="keyword">or</span> ((oS.labelMat[i]*Ei &gt; oS.tol) <span class="keyword">and</span> (oS.alphas[i] &gt; <span class="number">0</span>)):</span><br><span class="line">        j,Ej = selectJ(i, oS, Ei) <span class="comment">#this has been changed from selectJrand</span></span><br><span class="line">        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy();</span><br><span class="line">        <span class="keyword">if</span> (oS.labelMat[i] != oS.labelMat[j]):</span><br><span class="line">            L = max(<span class="number">0</span>, oS.alphas[j] - oS.alphas[i])</span><br><span class="line">            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            L = max(<span class="number">0</span>, oS.alphas[j] + oS.alphas[i] - oS.C)</span><br><span class="line">            H = min(oS.C, oS.alphas[j] + oS.alphas[i])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> L==H:</span><br><span class="line">            <span class="keyword">if</span> istraces: </span><br><span class="line">                print(<span class="string">"L==H"</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        eta = <span class="number">2.0</span> * oS.X[i,:]*oS.X[j,:].T - oS.X[i,:]*oS.X[i,:].T - oS.X[j,:]*oS.X[j,:].T</span><br><span class="line">        <span class="keyword">if</span> eta &gt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> istraces: </span><br><span class="line">                print(<span class="string">"eta&gt;=0"</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta</span><br><span class="line">        oS.alphas[j] = clipAlpha(oS.alphas[j],H,L)</span><br><span class="line">        updateEk(oS, j) <span class="comment"># added this for the Ecache</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (abs(oS.alphas[j] - alphaJold) &lt; <span class="number">0.00001</span>):</span><br><span class="line">            <span class="keyword">if</span> istraces: </span><br><span class="line">                print(<span class="string">"j not moving enough"</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])<span class="comment">#update i by the same amount as j</span></span><br><span class="line">        updateEk(oS, i) <span class="comment">#added this for the Ecache                    #the update is in the oppostie direction</span></span><br><span class="line">        b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i]-alphaIold)*oS.X[i,:]*oS.X[i,:].T - oS.labelMat[j]*(oS.alphas[j] - alphaJold)*oS.X[i,:]*oS.X[j,:].T</span><br><span class="line">        b2 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i]-alphaIold)*oS.X[i,:]*oS.X[i,:].T - oS.labelMat[j]*(oS.alphas[j] - alphaJold)*oS.X[j,:]*oS.X[j,:].T</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (<span class="number">0</span> &lt; oS.alphas[i]) <span class="keyword">and</span> (oS.C &gt; oS.alphas[i]):</span><br><span class="line">            oS.b = b1</span><br><span class="line">        <span class="keyword">elif</span> (<span class="number">0</span> &lt; oS.alphas[j]) <span class="keyword">and</span> (oS.C &gt; oS.alphas[j]):</span><br><span class="line">            oS.b = b2</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            oS.b = (b1 + b2)/<span class="number">2.0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smoP</span><span class="params">(dataMatIn, classLabels, C, toler, maxIter, kTup=<span class="params">(<span class="string">'lin'</span>, <span class="number">0</span>)</span>, istraces=True)</span>:</span>    <span class="comment">#full Platt SMO</span></span><br><span class="line">    oS = optStruct(mat(dataMatIn),mat(classLabels).transpose(),C,toler, kTup)</span><br><span class="line">    iter = <span class="number">0</span></span><br><span class="line">    entireSet = <span class="keyword">True</span>; alphaPairsChanged = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> (iter &lt; maxIter) <span class="keyword">and</span> ((alphaPairsChanged &gt; <span class="number">0</span>) <span class="keyword">or</span> (entireSet)):</span><br><span class="line">        alphaPairsChanged = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> entireSet:   <span class="comment"># go over all</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(oS.m):        </span><br><span class="line">                alphaPairsChanged += innerL(i, oS)</span><br><span class="line">                <span class="keyword">if</span> istraces:</span><br><span class="line">                    print(<span class="string">"fullSet, iter: %d i:%d, pairs changed %d"</span> % (iter,i,alphaPairsChanged))</span><br><span class="line">            iter += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># go over non-bound (railed) alphas</span></span><br><span class="line">            nonBoundIs = nonzero((oS.alphas.A &gt; <span class="number">0</span>) * (oS.alphas.A &lt; C))[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> nonBoundIs:</span><br><span class="line">                alphaPairsChanged += innerL(i, oS, istraces)</span><br><span class="line">                <span class="keyword">if</span> istraces:</span><br><span class="line">                    print(<span class="string">"non-bound, iter: %d i:%d, pairs changed %d"</span> % (iter,i,alphaPairsChanged))</span><br><span class="line">            iter += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> entireSet:</span><br><span class="line">            entireSet = <span class="keyword">False</span> <span class="comment">#toggle entire set loop</span></span><br><span class="line">        <span class="keyword">elif</span> (alphaPairsChanged == <span class="number">0</span>):</span><br><span class="line">            entireSet = <span class="keyword">True</span>  </span><br><span class="line">        print(<span class="string">"iteration number: %d"</span> % iter)</span><br><span class="line">    <span class="keyword">return</span> oS.b, oS.alphas</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataArr, labelArr = loadDataSet(<span class="string">'MLiA_SourceCode/machinelearninginaction/Ch06/testSet.txt'</span>)</span><br><span class="line">b, alphas = smoP(dataArr, labelArr, <span class="number">0.6</span>, <span class="number">0.001</span>, <span class="number">40</span>)</span><br></pre></td></tr></table></figure>
<pre><code>fullSet, iter: 0 i:0, pairs changed 1
fullSet, iter: 0 i:1, pairs changed 1
fullSet, iter: 0 i:2, pairs changed 2
j not moving enough
...
...
fullSet, iter: 2 i:97, pairs changed 0
fullSet, iter: 2 i:98, pairs changed 0
fullSet, iter: 2 i:99, pairs changed 0
iteration number: 3
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">supportVectors = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">if</span> alphas[i] &gt; <span class="number">0.0</span>:</span><br><span class="line">        supportVectors.append(dataArr[i])</span><br><span class="line">        print(dataArr[i], labelArr[i])</span><br></pre></td></tr></table></figure>
<pre><code>[3.542485, 1.977398] -1.0
[7.55151, -1.58003] 1.0
[8.127113, 1.274372] 1.0
[7.108772, -0.986906] 1.0
[6.080573, 0.418886] 1.0
[3.107511, 0.758367] -1.0
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plotSupportVectors(supportVectors)</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%85%AD%EF%BC%89_25_0.png" alt="png"></p>
<p>如何用上面得到的alpha值来进行分类？首先必须基于alpha值得到超平面，计算w。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcWs</span><span class="params">(alphas, dataArr, classLabels)</span>:</span></span><br><span class="line">    X = mat(dataArr)</span><br><span class="line">    labelMat = mat(classLabels).transpose()</span><br><span class="line">    m, n = shape(X)</span><br><span class="line">    w = zeros((n ,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        w += multiply(alphas[i]*labelMat[i], X[i,:].T)</span><br><span class="line">    <span class="keyword">return</span> w</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ws = calcWs(alphas, dataArr, labelArr)</span><br><span class="line">ws</span><br></pre></td></tr></table></figure>
<pre><code>array([[ 0.65139219],
       [-0.18666913]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datMat = mat(dataArr)</span><br><span class="line">datMat[<span class="number">0</span>]*mat(ws)+b</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[-0.94421679]])
</code></pre><p>如果该值大于0那么其属于1类，小于0则属于-1类。对于dataMat[0]点应该时类别-1，验证检查：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">labelArr[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>-1.0
</code></pre><p>写个函数全部检查一遍看看</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkResult</span><span class="params">(alphas, b, dataArr, labelArr)</span>:</span></span><br><span class="line">    ws = calcWs(alphas, dataArr, labelArr)</span><br><span class="line">    datMat = mat(dataArr)</span><br><span class="line">    n = len(datMat)</span><br><span class="line">    errorCount = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        result = datMat[i]*mat(ws)+b</span><br><span class="line">        result = <span class="number">1.0</span> <span class="keyword">if</span> float(result) &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">-1.0</span></span><br><span class="line">        <span class="keyword">if</span> result != labelArr[i]:</span><br><span class="line">            errorCount += <span class="number">1.0</span></span><br><span class="line">    print(<span class="string">"the error tate of this test is %f"</span> % float(errorCount/n))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">checkResult(alphas, b, dataArr, labelArr)</span><br></pre></td></tr></table></figure>
<pre><code>the error tate of this test is 0.000000
</code></pre><p>测试结果全部都分类正确</p>
<h1 id="在复杂的数据上应用核函数"><a href="#在复杂的数据上应用核函数" class="headerlink" title="在复杂的数据上应用核函数"></a>在复杂的数据上应用核函数</h1><h2 id="利用核函数将数据映射到高维空间"><a href="#利用核函数将数据映射到高维空间" class="headerlink" title="利用核函数将数据映射到高维空间"></a>利用核函数将数据映射到高维空间</h2><h2 id="径向基核函数"><a href="#径向基核函数" class="headerlink" title="径向基核函数"></a>径向基核函数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%run MLiA_SourceCode/machinelearninginaction/Ch06/plotRBF.py</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%85%AD%EF%BC%89_37_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernelTrans</span><span class="params">(X, A, kTup)</span>:</span> <span class="comment">#calc the kernel or transform data to a higher dimensional space</span></span><br><span class="line">    m,n = shape(X)</span><br><span class="line">    K = mat(zeros((m,<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">if</span> kTup[<span class="number">0</span>]==<span class="string">'lin'</span>:</span><br><span class="line">        K = X * A.T   <span class="comment">#linear kernel</span></span><br><span class="line">    <span class="keyword">elif</span> kTup[<span class="number">0</span>]==<span class="string">'rbf'</span>:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            deltaRow = X[j,:] - A</span><br><span class="line">            K[j] = deltaRow*deltaRow.T</span><br><span class="line">        K = exp(K/(<span class="number">-1</span>*kTup[<span class="number">1</span>]**<span class="number">2</span>)) <span class="comment">#divide in NumPy is element-wise not matrix like Matlab</span></span><br><span class="line">    <span class="keyword">else</span>: <span class="keyword">raise</span> NameError(<span class="string">'Houston We Have a Problem -- \</span></span><br><span class="line"><span class="string">    That Kernel is not recognized'</span>)</span><br><span class="line">    <span class="keyword">return</span> K</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">optStruct</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,dataMatIn, classLabels, C, toler, kTup)</span>:</span>  <span class="comment"># Initialize the structure with the parameters </span></span><br><span class="line">        self.X = dataMatIn</span><br><span class="line">        self.labelMat = classLabels</span><br><span class="line">        self.C = C</span><br><span class="line">        self.tol = toler</span><br><span class="line">        self.m = shape(dataMatIn)[<span class="number">0</span>]</span><br><span class="line">        self.alphas = mat(zeros((self.m,<span class="number">1</span>)))</span><br><span class="line">        self.b = <span class="number">0</span></span><br><span class="line">        self.eCache = mat(zeros((self.m,<span class="number">2</span>))) <span class="comment">#first column is valid flag</span></span><br><span class="line">        self.K = mat(zeros((self.m,self.m)))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.m):</span><br><span class="line">            self.K[:,i] = kernelTrans(self.X, self.X[i,:], kTup)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">innerL</span><span class="params">(i, oS, istraces=False)</span>:</span></span><br><span class="line">    Ei = calcEk(oS, i)</span><br><span class="line">    <span class="keyword">if</span> ((oS.labelMat[i]*Ei &lt; -oS.tol) <span class="keyword">and</span> (oS.alphas[i] &lt; oS.C)) <span class="keyword">or</span> ((oS.labelMat[i]*Ei &gt; oS.tol) <span class="keyword">and</span> (oS.alphas[i] &gt; <span class="number">0</span>)):</span><br><span class="line">        j,Ej = selectJ(i, oS, Ei) <span class="comment">#this has been changed from selectJrand</span></span><br><span class="line">        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy();</span><br><span class="line">        <span class="keyword">if</span> (oS.labelMat[i] != oS.labelMat[j]):</span><br><span class="line">            L = max(<span class="number">0</span>, oS.alphas[j] - oS.alphas[i])</span><br><span class="line">            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            L = max(<span class="number">0</span>, oS.alphas[j] + oS.alphas[i] - oS.C)</span><br><span class="line">            H = min(oS.C, oS.alphas[j] + oS.alphas[i])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> L==H:</span><br><span class="line">            <span class="keyword">if</span> istraces: </span><br><span class="line">                print(<span class="string">"L==H"</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        eta = <span class="number">2.0</span> * oS.K[i,j] - oS.K[i,i] - oS.K[j,j] <span class="comment">#changed for kernel</span></span><br><span class="line">        <span class="keyword">if</span> eta &gt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> istraces: </span><br><span class="line">                print(<span class="string">"eta&gt;=0"</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta</span><br><span class="line">        oS.alphas[j] = clipAlpha(oS.alphas[j],H,L)</span><br><span class="line">        updateEk(oS, j) <span class="comment"># added this for the Ecache</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (abs(oS.alphas[j] - alphaJold) &lt; <span class="number">0.00001</span>):</span><br><span class="line">            <span class="keyword">if</span> istraces: </span><br><span class="line">                print(<span class="string">"j not moving enough"</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])<span class="comment">#update i by the same amount as j</span></span><br><span class="line">        updateEk(oS, i) <span class="comment">#added this for the Ecache                    #the update is in the oppostie direction</span></span><br><span class="line">        b1 = oS.b - Ei- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,i] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[i,j]</span><br><span class="line">        b2 = oS.b - Ej- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,j]- oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[j,j]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (<span class="number">0</span> &lt; oS.alphas[i]) <span class="keyword">and</span> (oS.C &gt; oS.alphas[i]):</span><br><span class="line">            oS.b = b1</span><br><span class="line">        <span class="keyword">elif</span> (<span class="number">0</span> &lt; oS.alphas[j]) <span class="keyword">and</span> (oS.C &gt; oS.alphas[j]):</span><br><span class="line">            oS.b = b2</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            oS.b = (b1 + b2)/<span class="number">2.0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcEk</span><span class="params">(oS, k)</span>:</span></span><br><span class="line">    fXk = float(multiply(oS.alphas,oS.labelMat).T*oS.K[:,k] + oS.b)</span><br><span class="line">    Ek = fXk - float(oS.labelMat[k])</span><br><span class="line">    <span class="keyword">return</span> Ek</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testRbf</span><span class="params">(k1=<span class="number">1.3</span>)</span>:</span></span><br><span class="line">    dataArr,labelArr = loadDataSet(<span class="string">'MLiA_SourceCode/machinelearninginaction/Ch06/testSetRBF.txt'</span>)</span><br><span class="line">    b,alphas = smoP(dataArr, labelArr, <span class="number">200</span>, <span class="number">0.0001</span>, <span class="number">10000</span>, (<span class="string">'rbf'</span>, k1)) <span class="comment">#C=200 important</span></span><br><span class="line">    datMat=mat(dataArr); labelMat = mat(labelArr).transpose()</span><br><span class="line">    svInd=nonzero(alphas.A&gt;<span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">    sVs=datMat[svInd] <span class="comment">#get matrix of only support vectors</span></span><br><span class="line">    labelSV = labelMat[svInd];</span><br><span class="line">    print(<span class="string">"there are %d Support Vectors"</span> % shape(sVs)[<span class="number">0</span>])</span><br><span class="line">    m,n = shape(datMat)</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        kernelEval = kernelTrans(sVs,datMat[i,:],(<span class="string">'rbf'</span>, k1))</span><br><span class="line">        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b</span><br><span class="line">        <span class="keyword">if</span> sign(predict)!=sign(labelArr[i]): errorCount += <span class="number">1</span></span><br><span class="line">    print(<span class="string">"the training error rate is: %f"</span> % (float(errorCount)/m))</span><br><span class="line">    dataArr,labelArr = loadDataSet(<span class="string">'MLiA_SourceCode/machinelearninginaction/Ch06/testSetRBF2.txt'</span>)</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    datMat=mat(dataArr); labelMat = mat(labelArr).transpose()</span><br><span class="line">    m,n = shape(datMat)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        kernelEval = kernelTrans(sVs,datMat[i,:],(<span class="string">'rbf'</span>, k1))</span><br><span class="line">        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b</span><br><span class="line">        <span class="keyword">if</span> sign(predict)!=sign(labelArr[i]): errorCount += <span class="number">1</span>    </span><br><span class="line">    print(<span class="string">"the test error rate is: %f"</span> % (float(errorCount)/m))</span><br></pre></td></tr></table></figure>
<h2 id="在测试中使用核函数"><a href="#在测试中使用核函数" class="headerlink" title="在测试中使用核函数"></a>在测试中使用核函数</h2><p>用加入了核函数的算法再次训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">testRbf()</span><br></pre></td></tr></table></figure>
<pre><code>fullSet, iter: 0 i:0, pairs changed 1
fullSet, iter: 0 i:1, pairs changed 1
...
...
fullSet, iter: 6 i:99, pairs changed 0
iteration number: 7
there are 29 Support Vectors
the training error rate is: 0.070000
the test error rate is: 0.050000
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">testRbf(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>fullSet, iter: 0 i:0, pairs changed 1
fullSet, iter: 0 i:1, pairs changed 2
...
...
iteration number: 7
there are 89 Support Vectors
the training error rate is: 0.000000
the test error rate is: 0.070000
</code></pre><p>当k1=0.1时候，支持向量为89个，k1=1.3的时候时29个，当减小σ，训练错误率就会降低，但测试错误率就会上升。</p>
<p>支持向量的数目存在一个最优值。SVM的优点在于它能对数据进行高效分类。如果支持向量太少，就可能会得到一个很差的决策边界；如果支持向量太多，也就相当于每次都利用整个数据集进行分类，这种分类法发称为K近邻。</p>
<h1 id="实例：手写识别问题回顾"><a href="#实例：手写识别问题回顾" class="headerlink" title="实例：手写识别问题回顾"></a>实例：手写识别问题回顾</h1><p>基于SVM的数字识别</p>
<ol>
<li>收集数据</li>
<li>准备数据：基于二值图像构造向量</li>
<li>分析数据：对图像向量进行目测</li>
<li>训练算法：采用两种不同的核函数，并对径向（radial direction）基核函数采用不同的设置来运行SMO算法</li>
<li>测试算法：编写一个函数来测试不同的核函数并计算错误率</li>
<li>使用算法</li>
</ol>
<p>首先把第二章的img2vector()函数复制过来。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span><span class="params">(filename)</span>:</span></span><br><span class="line">    returnVect = zeros((<span class="number">1</span>,<span class="number">1024</span>))</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">        lineStr = fr.readline()</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">            returnVect[<span class="number">0</span>,<span class="number">32</span>*i+j] = int(lineStr[j])</span><br><span class="line">    <span class="keyword">return</span> returnVect</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadImages</span><span class="params">(dirName)</span>:</span></span><br><span class="line">    <span class="keyword">from</span> os <span class="keyword">import</span> listdir</span><br><span class="line">    hwLabels = []</span><br><span class="line">    trainingFileList = listdir(dirName)         <span class="comment"># load the training set</span></span><br><span class="line">    m = len(trainingFileList)</span><br><span class="line">    trainingMat = zeros((m,<span class="number">1024</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        fileNameStr = trainingFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]     <span class="comment"># take off .txt</span></span><br><span class="line">        classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> classNumStr == <span class="number">9</span>:</span><br><span class="line">            hwLabels.append(<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            hwLabels.append(<span class="number">1</span>)</span><br><span class="line">        trainingMat[i,:] = img2vector(<span class="string">'%s/%s'</span> % (dirName, fileNameStr))</span><br><span class="line">    <span class="keyword">return</span> trainingMat, hwLabels</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testDigits</span><span class="params">(kTup=<span class="params">(<span class="string">'rbf'</span>, <span class="number">10</span>)</span>, istrances=False)</span>:</span></span><br><span class="line">    dataArr,labelArr = loadImages(<span class="string">'MLiA_SourceCode/machinelearninginaction/Ch06/digits/trainingDigits'</span>)</span><br><span class="line">    b,alphas = smoP(dataArr, labelArr, <span class="number">200</span>, <span class="number">0.0001</span>, <span class="number">10000</span>, kTup, istrances)</span><br><span class="line">    datMat=mat(dataArr)</span><br><span class="line">    labelMat = mat(labelArr).transpose()</span><br><span class="line">    svInd=nonzero(alphas.A&gt;<span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">    sVs=datMat[svInd] </span><br><span class="line">    labelSV = labelMat[svInd];</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"there are %d Support Vectors"</span> % shape(sVs)[<span class="number">0</span>])</span><br><span class="line">    supportVectors = shape(sVs)[<span class="number">0</span>]</span><br><span class="line">    m,n = shape(datMat)</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        kernelEval = kernelTrans(sVs,datMat[i,:],kTup)</span><br><span class="line">        predict = kernelEval.T * multiply(labelSV,alphas[svInd]) + b</span><br><span class="line">        <span class="keyword">if</span> sign(predict) != sign(labelArr[i]):</span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line">    trainErrorRate = float(errorCount)/m        </span><br><span class="line">    print(<span class="string">"the training error rate is: %f"</span> % (float(errorCount)/m))</span><br><span class="line">    dataArr,labelArr = loadImages(<span class="string">'MLiA_SourceCode/machinelearninginaction/Ch06/digits/testDigits'</span>)</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    datMat=mat(dataArr) </span><br><span class="line">    labelMat = mat(labelArr).transpose()</span><br><span class="line">    m,n = shape(datMat)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        kernelEval = kernelTrans(sVs,datMat[i,:],kTup)</span><br><span class="line">        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b</span><br><span class="line">        <span class="keyword">if</span> sign(predict) != sign(labelArr[i]):</span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"the test error rate is: %f"</span> % (float(errorCount)/m))</span><br><span class="line">    testErrorTate = float(errorCount)/m</span><br><span class="line">    <span class="keyword">return</span> trainErrorRate, testErrorTate, supportVectors</span><br></pre></td></tr></table></figure>
<p>尝试不同的参数和线性核函数来学习：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">parameters = [[<span class="string">'rbf'</span>, <span class="number">0.1</span>],</span><br><span class="line">             [<span class="string">'rbf'</span>, <span class="number">5</span>],</span><br><span class="line">             [<span class="string">'rbf'</span>, <span class="number">10</span>],</span><br><span class="line">             [<span class="string">'rbf'</span>, <span class="number">50</span>],</span><br><span class="line">             [<span class="string">'rbf'</span>, <span class="number">100</span>],</span><br><span class="line">             [<span class="string">'lin'</span>, <span class="number">0</span>]]</span><br><span class="line">trainErrorRate = []</span><br><span class="line">testErrorTate = []</span><br><span class="line">supportVectors = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, j <span class="keyword">in</span> parameters:</span><br><span class="line">    result = testDigits(kTup=(i, j))</span><br><span class="line">    trainErrorRate.append(result[<span class="number">0</span>])</span><br><span class="line">    testErrorTate.append(result[<span class="number">1</span>])</span><br><span class="line">    supportVectors.append(result[<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<pre><code>iteration number: 1
iteration number: 2
iteration number: 3
...
...
iteration number: 9
there are 39 Support Vectors
the training error rate is: 0.000000
the test error rate is: 0.021505
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"内核,设置\t训练错误率\t测试错误率\t支持向量数"</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(parameters)):</span><br><span class="line">    print(<span class="string">"%s,%.1f \t%.4f \t\t%.4f \t\t%d"</span> %(parameters[i][<span class="number">0</span>], parameters[i][<span class="number">1</span>], trainErrorRate[i], testErrorTate[i], supportVectors[i]))</span><br></pre></td></tr></table></figure>
<pre><code>内核,设置    训练错误率    测试错误率    支持向量数
rbf,0.1     0.0000         0.5215         402
rbf,5.0     0.0000         0.0323         402
rbf,10.0     0.0000         0.0054         132
rbf,50.0     0.0149         0.0269         31
rbf,100.0     0.0050         0.0108         34
lin,0.0     0.0000         0.0215         39
</code></pre><p>观察发现，最小的训练错误率并不对应最小的支持向量数，线性核函数的效果并不是特别糟糕。可以牺牲线性核函数的错误率来换取分类速度的提高。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>支持向量机时一种分类器，之所以称为“机”时因为它会产生一个二值决策结果，即它是一种决策“机”，支持向量机的泛化错误率较低，也就是说它具有良好的学习能力，并且学到的结果具有很好的推广性。</p>
<p>核函数从一个低纬空间映射到一个高纬空间，可以将一个低维空间中的非线性问题转换为高纬度空间下的线性问题来求解。</p>
<p>支持向量机是一个二分类器。当解决多分类问题时，则需要额外的方法对其进行扩展，SVM的效果也对优化参数和所用核函数中的参数敏感。</p>
]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>SMO</tag>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实战（十一）</title>
    <url>/2020/05/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<h1 id="使用Apriori算法进行关联分析"><a href="#使用Apriori算法进行关联分析" class="headerlink" title="使用Apriori算法进行关联分析"></a>使用Apriori算法进行关联分析</h1><p>在大规模数据集中寻找物品的隐含关系被称作关联分析(association analysis)或者关联规则学习(association rule learning)，例如商品的定向推荐。</p>
<h1 id="关联分析"><a href="#关联分析" class="headerlink" title="关联分析"></a>关联分析</h1><p>Aprior算法</p>
<pre><code>优点：易编码事先
缺点：在大数据集上可能较慢
适用数据类型：数值型或标称型数据
</code></pre><p>关联分析是一种大规模数据集中寻找有趣关系的任务。这些关系可以有两种形式：频繁项集或者关联规则。频繁项集（frequent item sets）是经常出现在一块的物品的集合，关联规则（association rules）暗示两种物品之间可能存在很强的关系。</p>
<p>频繁项集是指那些经常出现在一起的物品集合（啤酒和尿布）</p>
<p>一个项集的支持度（support）被定义为数据集中包含该项集的记录所占比例。支持度是针对项集来说的，因此可以定义一个最小支持度，而只保留满足最小支持度的项集。</p>
<p>可信度或置信度（confidence）是针对一条关联规则来定义的。</p>
<p>支持度和可信度是用来量化关联分析是否成功的方法。</p>
<h1 id="Apriori原理"><a href="#Apriori原理" class="headerlink" title="Apriori原理"></a>Apriori原理</h1><p>Apriori算法的一般过程</p>
<ol>
<li>收集数据</li>
<li>准备数据</li>
<li>分析数据</li>
<li>训练数据：使用apriori算法来找到频繁项集</li>
<li>测试算法：不需要测试过程</li>
<li>使用算法：用来发现频繁项集以及物品之间的关联规则</li>
</ol>
<p>Apriori原理，如果某个项集是频繁的，那么他的所有子集也是频繁的。对于下图的例子意味着，{0, 1}是频繁的，那么{0}，{1}也是频繁的，这个原理反过来看就是，如果说一个项集是频繁的，那么它的所有超集也是频繁的。</p>
<p><img src="11fig02.jpg" alt=""></p>
<h1 id="使用Apriori算法来发现频繁集"><a href="#使用Apriori算法来发现频繁集" class="headerlink" title="使用Apriori算法来发现频繁集"></a>使用Apriori算法来发现频繁集</h1><p>Apriori算法的两个输入参数分别是最小支持度和数据集。该算法首先会生成所有单个物品的项集列表。接着扫描交易记录来查看哪些项集满足最小支持度要求，那些不满足最小支持度的集合会被去掉。然后对剩下的集合进行组合生成包含两个元素的项集。接下重新扫描交易记录，去掉不满足最小支持度的项集。重复该过程直到所有项集都被去掉。</p>
<h2 id="生成候选集"><a href="#生成候选集" class="headerlink" title="生成候选集"></a>生成候选集</h2><p>创建一个用于构建初始集合的函数，和一个通过扫描数据集以寻找交易记录子集的函数。</p>
<p>数据集扫描的伪代码如下：</p>
<pre><code>对数据集中的每条交易记录tran
对每个候选项集can:
    检查一下can是否是tran的子集:
    如果是，则增加can的计数值
对每个候选集：
如果其支持度不低于最小值，则保留该项集
返回所有频繁项集列表
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [[<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">2</span>, <span class="number">5</span>]]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createC1</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    C1 = []</span><br><span class="line">    <span class="keyword">for</span> transaction <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> transaction:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> [item] <span class="keyword">in</span> C1:</span><br><span class="line">                C1.append([item])</span><br><span class="line">    C1.sort()</span><br><span class="line">    ret = map(frozenset, C1) <span class="comment"># 返回一个冻结的集合</span></span><br><span class="line">    <span class="keyword">return</span> list(ret)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scanD</span><span class="params">(D, Ck, minSupport)</span>:</span></span><br><span class="line">    ssCnt = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> tid <span class="keyword">in</span> D:</span><br><span class="line">        <span class="keyword">for</span> can <span class="keyword">in</span> Ck:</span><br><span class="line">            <span class="keyword">if</span> can.issubset(tid):</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> can <span class="keyword">in</span> ssCnt:</span><br><span class="line">                    ssCnt[can] = <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    ssCnt[can] += <span class="number">1</span></span><br><span class="line">    numItems = float(len(D))</span><br><span class="line">    retList = []</span><br><span class="line">    supportData = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> ssCnt:</span><br><span class="line">        support = ssCnt[key]/numItems</span><br><span class="line">        <span class="keyword">if</span> support &gt;= minSupport:</span><br><span class="line">            retList.insert(<span class="number">0</span>, key)</span><br><span class="line">        supportData[key] = support</span><br><span class="line">    <span class="keyword">return</span> retList, supportData</span><br></pre></td></tr></table></figure>
<p>C1是大小为1的所有候选项集的集合。</p>
<p>L1是满足最低要求的项集构成的集合L1。</p>
<p>createC1()函数将构建第一个候选项集的列表C1，scanD()有三个参数，分别是数据集，候选项集列表Ck，以及感兴趣项集的最小支持度minSupport。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataSet = loadDataSet()</span><br><span class="line">dataSet</span><br></pre></td></tr></table></figure>
<pre><code>[[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">C1 = createC1(dataSet)</span><br><span class="line">C1</span><br></pre></td></tr></table></figure>
<pre><code>[frozenset({1}),
 frozenset({2}),
 frozenset({3}),
 frozenset({4}),
 frozenset({5})]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">D = list(map(set, dataSet))</span><br><span class="line">D</span><br></pre></td></tr></table></figure>
<pre><code>[{1, 3, 4}, {2, 3, 5}, {1, 2, 3, 5}, {2, 5}]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">L1, suppData0 = scanD(D, C1, <span class="number">0.5</span>)</span><br><span class="line">suppData0, L1</span><br></pre></td></tr></table></figure>
<pre><code>({frozenset({1}): 0.5,
  frozenset({3}): 0.75,
  frozenset({4}): 0.25,
  frozenset({2}): 0.75,
  frozenset({5}): 0.75},
 [frozenset({5}), frozenset({2}), frozenset({3}), frozenset({1})])
</code></pre><p>上述4个项集构成了L1列表，该列表中的每个单物品项集至少出现在50%以上的记录中。由于物品4并没有达到最小支持度，所以不在L1中。</p>
<h2 id="组织完整的Apriori算法"><a href="#组织完整的Apriori算法" class="headerlink" title="组织完整的Apriori算法"></a>组织完整的Apriori算法</h2><p>整个Apriori算法的伪代码如下：</p>
<pre><code>当集合中项的个数大于0时
    构建一个k个项目组成的候选项集的列表
    检查数据以确认每个项集都是频繁的
    保留频繁项集并构建k+1项组成的候选项的列表
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aprioriGen</span><span class="params">(Lk, k)</span>:</span></span><br><span class="line">    retList = []</span><br><span class="line">    lenLk = len(Lk)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(lenLk):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, lenLk):</span><br><span class="line">            L1 = list(Lk[i])[: k<span class="number">-2</span>]</span><br><span class="line">            L2 = list(Lk[j])[: k<span class="number">-2</span>]</span><br><span class="line">            L1.sort()</span><br><span class="line">            L2.sort()</span><br><span class="line">            <span class="keyword">if</span> L2 == L2:</span><br><span class="line">                retList.append(Lk[i] | Lk[j])</span><br><span class="line">    <span class="keyword">return</span> retList</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apriori</span><span class="params">(dataSet, minSupport=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    C1 = createC1(dataSet)</span><br><span class="line">    D = map(set, dataSet)</span><br><span class="line">    D = list(D)</span><br><span class="line">    L1, supportData = scanD(D, C1, minSupport)</span><br><span class="line">    L = [L1]</span><br><span class="line">    k = <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (len(L[k<span class="number">-2</span>]) &gt; <span class="number">0</span>):</span><br><span class="line">        Ck = aprioriGen(L[k<span class="number">-2</span>], k)</span><br><span class="line">        Lk, supK = scanD(D, Ck, minSupport)</span><br><span class="line">        supportData.update(supK)</span><br><span class="line">        L.append(Lk)</span><br><span class="line">        k += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> L, supportData</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">L, suppData = apriori(dataSet)</span><br><span class="line">L</span><br></pre></td></tr></table></figure>
<pre><code>[[frozenset({5}), frozenset({2}), frozenset({3}), frozenset({1})],
 [frozenset({2, 3}), frozenset({3, 5}), frozenset({2, 5}), frozenset({1, 3})],
 [frozenset({2, 3, 5})],
 []]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">L[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[frozenset({5}), frozenset({2}), frozenset({3}), frozenset({1})]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">L[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[frozenset({2, 3}), frozenset({3, 5}), frozenset({2, 5}), frozenset({1, 3})]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">L[<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[frozenset({2, 3, 5})]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">L[<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[]
</code></pre><p>每个项集都是在apriori()中调用函数aprioriGen()来生成的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">aprioriGen(L[<span class="number">0</span>], <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[frozenset({2, 5}),
 frozenset({3, 5}),
 frozenset({1, 5}),
 frozenset({2, 3}),
 frozenset({1, 2}),
 frozenset({1, 3})]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">L, suppData = apriori(dataSet, minSupport=<span class="number">0.7</span>)</span><br><span class="line">L</span><br></pre></td></tr></table></figure>
<pre><code>[[frozenset({5}), frozenset({2}), frozenset({3})], [frozenset({2, 5})], []]
</code></pre><p>变量suppData是一个字典，它包含我们项集的支持度值。</p>
<h1 id="从频繁项集中挖掘关联规则"><a href="#从频繁项集中挖掘关联规则" class="headerlink" title="从频繁项集中挖掘关联规则"></a>从频繁项集中挖掘关联规则</h1><p>要找到关联规则，我们首先从一个频繁项集开始。我们知道集合中的元素是不重复的，但我们想知道基于这些元素能否获得其他内容。某个元素或者某个元素集合可能会推导出另一个元素。</p>
<p>关联规则的量化指标是，可信度。一条规则P-&gt;H的可信度定义为support(P|H)/support(P)。</p>
<p><img src="11fig04.jpg" alt=""></p>
<p>对于频繁项集{0，1，2，3}的关联规则网络示意图。阴影区域给出的是低可信度的规则。如果发现0，1，2—&gt;3是一条低可信度规则，那么所有其以3作为后件的规则可信度也会较低。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateRules</span><span class="params">(L, supportData, minConf=<span class="number">0.7</span>)</span>:</span></span><br><span class="line">    bigRuleList = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(L)):</span><br><span class="line">        <span class="keyword">for</span> freqSet <span class="keyword">in</span> L[i]:</span><br><span class="line">            H1 = [frozenset([item]) <span class="keyword">for</span> item <span class="keyword">in</span> freqSet]</span><br><span class="line">            <span class="keyword">if</span> (i &gt; <span class="number">1</span>):</span><br><span class="line">                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                calcConf(freqSet, H1, supportData, bigRuleList, minConf)</span><br><span class="line">    <span class="keyword">return</span> bigRuleList    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcConf</span><span class="params">(freqSet, H, supportData, brl, minConf=<span class="number">0.7</span>)</span>:</span></span><br><span class="line">    prunedH = [] </span><br><span class="line">    <span class="keyword">for</span> conseq <span class="keyword">in</span> H:</span><br><span class="line">        conf = supportData[freqSet]/supportData[freqSet-conseq]</span><br><span class="line">        <span class="keyword">if</span> conf &gt;= minConf: </span><br><span class="line">            print(freqSet-conseq,<span class="string">'--&gt;'</span>,conseq,<span class="string">'conf:'</span>,conf)</span><br><span class="line">            brl.append((freqSet-conseq, conseq, conf))</span><br><span class="line">            prunedH.append(conseq)</span><br><span class="line">    <span class="keyword">return</span> prunedH</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rulesFromConseq</span><span class="params">(freqSet, H, supportData, brl, minConf=<span class="number">0.7</span>)</span>:</span></span><br><span class="line">    m = len(H[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> (len(freqSet) &gt; (m + <span class="number">1</span>)): </span><br><span class="line">        Hmp1 = aprioriGen(H, m+<span class="number">1</span>)</span><br><span class="line">        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)</span><br><span class="line">        <span class="keyword">if</span> (len(Hmp1) &gt; <span class="number">1</span>):</span><br><span class="line">            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">L, suppData=apriori(dataSet, minSupport=<span class="number">0.5</span>)</span><br><span class="line">rules = generateRules(L, suppData, minConf=<span class="number">0.7</span>)</span><br></pre></td></tr></table></figure>
<pre><code>frozenset({5}) --&gt; frozenset({2}) conf: 1.0
frozenset({2}) --&gt; frozenset({5}) conf: 1.0
frozenset({1}) --&gt; frozenset({3}) conf: 1.0
frozenset({5}) --&gt; frozenset({2, 3}) conf: 2.0
frozenset({3}) --&gt; frozenset({2, 5}) conf: 2.0
frozenset({2}) --&gt; frozenset({3, 5}) conf: 2.0
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rules = generateRules(L, suppData, minConf=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>
<pre><code>frozenset({3}) --&gt; frozenset({2}) conf: 0.6666666666666666
frozenset({2}) --&gt; frozenset({3}) conf: 0.6666666666666666
frozenset({5}) --&gt; frozenset({3}) conf: 0.6666666666666666
frozenset({3}) --&gt; frozenset({5}) conf: 0.6666666666666666
frozenset({5}) --&gt; frozenset({2}) conf: 1.0
frozenset({2}) --&gt; frozenset({5}) conf: 1.0
frozenset({3}) --&gt; frozenset({1}) conf: 0.6666666666666666
frozenset({1}) --&gt; frozenset({3}) conf: 1.0
frozenset({5}) --&gt; frozenset({2, 3}) conf: 2.0
frozenset({3}) --&gt; frozenset({2, 5}) conf: 2.0
frozenset({2}) --&gt; frozenset({3, 5}) conf: 2.0
</code></pre><p>一旦降低可信度阈值，就可以获得更多的规则。</p>
<h1 id="实例：发现国会投票中的模式"><a href="#实例：发现国会投票中的模式" class="headerlink" title="实例：发现国会投票中的模式"></a>实例：发现国会投票中的模式</h1><ol>
<li>收集数据：使用votesmart模块访问投票纪录</li>
<li>准备数据：构造一个函数来将投票转化为一串交易记录</li>
<li>分析数据：查看准备的数据以确保其正确性</li>
<li>训练算法：使用aprioi()和generateRules()函数来发现投票纪录中的有趣信息</li>
<li>测试算法：不适用，即没有测试过程</li>
<li>使用算法</li>
</ol>
<h1 id="实例：发现毒蘑菇的相似特征"><a href="#实例：发现毒蘑菇的相似特征" class="headerlink" title="实例：发现毒蘑菇的相似特征"></a>实例：发现毒蘑菇的相似特征</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mushDatSet = [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> open(<span class="string">'MLiA_SourceCode/Ch11/mushroom.dat'</span>).readlines()]</span><br></pre></td></tr></table></figure>
<p>第一个特征表示有毒或者可使用，有毒为2，可食用为1，下个特征是蘑菇伞的形状，有六种可能的值。为了找到毒蘑菇中存在的公共特征，可以运用Apriori算法来寻找特征值为2的频繁项集。</p>
<p>使用 Apriori 工具包</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> efficient_apriori <span class="keyword">import</span> apriori</span><br><span class="line">L, suppData = apriori(mushDatSet, min_support=<span class="number">0.3</span>, min_confidence=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> L[<span class="number">2</span>]:</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'2'</span> <span class="keyword">in</span> item:</span><br><span class="line">        print(item)</span><br></pre></td></tr></table></figure>
<pre><code>(&apos;2&apos;, &apos;23&apos;)
(&apos;2&apos;, &apos;34&apos;)
(&apos;2&apos;, &apos;36&apos;)
(&apos;2&apos;, &apos;39&apos;)
(&apos;2&apos;, &apos;59&apos;)
(&apos;2&apos;, &apos;63&apos;)
(&apos;2&apos;, &apos;67&apos;)
(&apos;2&apos;, &apos;76&apos;)
(&apos;2&apos;, &apos;85&apos;)
(&apos;2&apos;, &apos;86&apos;)
(&apos;2&apos;, &apos;90&apos;)
(&apos;2&apos;, &apos;93&apos;)
(&apos;2&apos;, &apos;28&apos;)
(&apos;2&apos;, &apos;53&apos;)
</code></pre><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>关联分析是用于发现大数据集中元素间有趣关系的一个工具集，可以采用两种方式来量化这些关系。第一种方式是频繁项集，它会给出经常在一起出现的元素项。第二种方式是关联规则。</p>
<p>Apriori原理是说如果一个元素项是不频繁的，那么这些包含该元素的超集也是不频繁的。Apriori算法从单元素项集开始，通过组合满足最小支持度要求的项集来形成更大的集合。支持度用来独立一个集合在原始数据中出现的频率。</p>
<p>每次增加频繁项集的大小，Apriori算法都会重新扫描整个数据集。当数据集很大时，这会显著降低频繁项集发现的速度。</p>
]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>关联分析</tag>
        <tag>Apriori算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实战（十二）</title>
    <url>/2020/06/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<h1 id="使用FP-growth算法来高效发现频繁项集"><a href="#使用FP-growth算法来高效发现频繁项集" class="headerlink" title="使用FP-growth算法来高效发现频繁项集"></a>使用FP-growth算法来高效发现频繁项集</h1><p>FP-growth算法只会对数据库进行两次扫描，而Apriori算法对于每个潜在的频繁项集都会扫描数据集判断给定模式是否频繁，因此FP-growth算法的速度要比Apriori算法快。</p>
<h1 id="FP树：用于编码数据集的有效方式"><a href="#FP树：用于编码数据集的有效方式" class="headerlink" title="FP树：用于编码数据集的有效方式"></a>FP树：用于编码数据集的有效方式</h1><p>FP-growth算法</p>
<pre><code>优点：一般要快于Apriori
缺点：实现比较困难，在某些数据集上性能会下降
适用数据类型：标称型数据类型
</code></pre><p>FP-growth算法将数据存储在一种称为FP树的紧凑数据结构中。FP代表频繁模式（Frequent Pattern）。一颗FP树看上去与计算机科学中的其他树结构类似，但是它通过链接（link）来连接相似元素，被连起来的元素项可以看成一个链表。</p>
<p><img src="12fig01.jpg" alt=""></p>
<p>同搜索树不同的是，一个元素可以在一棵FP树中出现多次。FP树会存储项集的出现频率，而每个项集会以路径的方式存储在树中。存在相似元素的集合会共享树的一部分。只有当集合之间完全不同时，树才会分叉。树节点上给出集合中的单个元素以及在序列中的出现次数，路径会给出该序列的出现次数。</p>
<p>相似项之间的链接即节点链接（node link），用于快速发现相似的位置。</p>
<table>
<thead>
<tr>
<th>事务ID</th>
<th>事务中的元素项目</th>
</tr>
</thead>
<tbody>
<tr>
<td>001</td>
<td>r, z, h, j, p</td>
</tr>
<tr>
<td>002</td>
<td>z, y, x, w, v, u, t, s</td>
</tr>
<tr>
<td>003</td>
<td>z</td>
</tr>
<tr>
<td>004</td>
<td>r, x, n, o, s</td>
</tr>
<tr>
<td>005</td>
<td>y, r, x, z, q, t, p</td>
</tr>
<tr>
<td>006</td>
<td>y, z, x, e, q, s, t, m</td>
</tr>
</tbody>
</table>
<p>在上表中，元素z出现了5次，集合{r,z}出现了1次（001和005都出现了，但书中写1次）。于是可以得出结论：z一定是自己本身或者其他符号一起出现了3次。我们再看下z的其他可能性。集合{t,s,y,x,z}出现了2次，集合{t,r,y,x,z}出现了1次。元素项z的右边标的是5，表示z出现了5次，其中刚才已经给出了4次出现，所以它一定单独出现过1次。</p>
<p><code>吐槽：这书这两章写的云山雾绕</code></p>
<p>FP-growth的一般流程</p>
<ol>
<li>收集数据</li>
<li>准备数据：由于存储的是集合，所以需要离散数据。如果要处理连续数据，需要将它们量化为离散值。</li>
<li>分析数据</li>
<li>训练算法：构建一个FP树，并对树进行挖掘</li>
<li>测试算法：没有测试过程</li>
<li>使用算法：可用于识别经常出现的元素项，从而用于制定决策，推荐元素或者进行预测等应用中。</li>
</ol>
<h1 id="构建FP树"><a href="#构建FP树" class="headerlink" title="构建FP树"></a>构建FP树</h1><h2 id="FP树的类定义"><a href="#FP树的类定义" class="headerlink" title="FP树的类定义"></a>FP树的类定义</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">treeNode</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, nameValue, numOccur, parentNode)</span>:</span></span><br><span class="line">        self.name = nameValue</span><br><span class="line">        self.count = numOccur</span><br><span class="line">        self.nodeLink = <span class="keyword">None</span></span><br><span class="line">        self.parent = parentNode</span><br><span class="line">        self.children = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inc</span><span class="params">(self, numOccur)</span>:</span></span><br><span class="line">        self.count += numOccur</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">disp</span><span class="params">(self, ind=<span class="number">1</span>)</span>:</span></span><br><span class="line">        print(<span class="string">' '</span>*ind, self.name, <span class="string">' '</span>, self.count)</span><br><span class="line">        <span class="keyword">for</span> child <span class="keyword">in</span> self.children.values():</span><br><span class="line">            child.disp(ind+<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>上面的程序给出了FP树中结点的类定义。类中包含用于存放节点名字的变量和1个计数值，nodeLinke变量用于链接相似的元素项。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rootNode = treeNode(<span class="string">'pyramid'</span>, <span class="number">9</span>, <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rootNode.children[<span class="string">'eye'</span>] = treeNode(<span class="string">'eye'</span>, <span class="number">13</span>, <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rootNode.disp()</span><br></pre></td></tr></table></figure>
<pre><code>pyramid   9
 eye   13
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rootNode.children[<span class="string">'phoenix'</span>] = treeNode(<span class="string">'phoenix'</span>, <span class="number">3</span>, <span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rootNode.disp()</span><br></pre></td></tr></table></figure>
<pre><code>pyramid   9
 eye   13
 phoenix   3
</code></pre><h2 id="构建FP树-1"><a href="#构建FP树-1" class="headerlink" title="构建FP树"></a>构建FP树</h2><p>需要一个头指针表来指向给定类型的第一个实列。利用头指针表，可以快速访问FP树中一个给定类型的所有元素。</p>
<p><img src="12fig02.jpg" alt=""></p>
<p>使用一个字典来保存头指针表。头指针表还可以用来保存FP树中每类元素的总数。</p>
<p>对不满足最小支持度的数据进行去除，然后后重排序得到下表</p>
<table>
<thead>
<tr>
<th>事务ID</th>
<th>事务中的元素项目</th>
<th>过滤及重排序后的事务</th>
</tr>
</thead>
<tbody>
<tr>
<td>001</td>
<td>r, z, h, j, p</td>
<td>z,r</td>
</tr>
<tr>
<td>002</td>
<td>z, y, x, w, v, u, t, s</td>
<td>z, x, y, s, t</td>
</tr>
<tr>
<td>003</td>
<td>z</td>
<td>z</td>
</tr>
<tr>
<td>004</td>
<td>r, x, n, o, s</td>
<td>x, s, r</td>
</tr>
<tr>
<td>005</td>
<td>y, r, x, z, q, t, p</td>
<td>z, x, y, r, t</td>
</tr>
<tr>
<td>006</td>
<td>y, z, x, e, q, s, t, m</td>
<td>z, x, y, s, t</td>
</tr>
</tbody>
</table>
<p>FP树构建函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet, minSup=<span class="number">1</span>)</span>:</span></span><br><span class="line">    headerTable = &#123;&#125;</span><br><span class="line">    <span class="comment"># 查看数据集两次</span></span><br><span class="line">    <span class="keyword">for</span> trans <span class="keyword">in</span> dataSet: <span class="comment"># 第一遍计数出现的频率</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> trans:</span><br><span class="line">            headerTable[item] = headerTable.get(item, <span class="number">0</span>) + dataSet[trans]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> list(headerTable.keys()):  <span class="comment"># 删除不满足最小支持度的元素项</span></span><br><span class="line">        <span class="keyword">if</span> headerTable[k] &lt; minSup: </span><br><span class="line">            <span class="keyword">del</span>(headerTable[k])</span><br><span class="line">            </span><br><span class="line">    freqItemSet = set(headerTable.keys())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> len(freqItemSet) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span>, <span class="keyword">None</span>  <span class="comment"># 没有满足最小支持度的项目则返回</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> headerTable:</span><br><span class="line">        headerTable[k] = [headerTable[k], <span class="keyword">None</span>]</span><br><span class="line"></span><br><span class="line">    retTree = treeNode(<span class="string">'Null Set'</span>, <span class="number">1</span>, <span class="keyword">None</span>) <span class="comment"># create tree</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> tranSet, count <span class="keyword">in</span> dataSet.items():  <span class="comment"># 第二次遍历数据集</span></span><br><span class="line">        localD = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> tranSet:   <span class="comment"># 更具全局频率对每个事务中的元素进行排序</span></span><br><span class="line">            <span class="keyword">if</span> item <span class="keyword">in</span> freqItemSet:</span><br><span class="line">                localD[item] = headerTable[item][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> len(localD) &gt; <span class="number">0</span>:</span><br><span class="line">            orderedItems = [v[<span class="number">0</span>] <span class="keyword">for</span> v <span class="keyword">in</span> sorted(localD.items(), key=<span class="keyword">lambda</span> p: p[<span class="number">1</span>], reverse=<span class="keyword">True</span>)]</span><br><span class="line">            updateTree(orderedItems, retTree, headerTable, count) <span class="comment"># 对剩下的元素迭代调用upadteTree函数</span></span><br><span class="line">    <span class="keyword">return</span> retTree, headerTable</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateTree</span><span class="params">(items, inTree, headerTable, count)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> items[<span class="number">0</span>] <span class="keyword">in</span> inTree.children:</span><br><span class="line">        inTree.children[items[<span class="number">0</span>]].inc(count)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        inTree.children[items[<span class="number">0</span>]] = treeNode(items[<span class="number">0</span>], count, inTree)</span><br><span class="line">        <span class="keyword">if</span> headerTable[items[<span class="number">0</span>]][<span class="number">1</span>] == <span class="keyword">None</span>:</span><br><span class="line">            headerTable[items[<span class="number">0</span>]][<span class="number">1</span>] = inTree.children[items[<span class="number">0</span>]]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            updateHeader(headerTable[items[<span class="number">0</span>]][<span class="number">1</span>], inTree.children[items[<span class="number">0</span>]])</span><br><span class="line">    <span class="keyword">if</span> len(items) &gt; <span class="number">1</span>:</span><br><span class="line">        updateTree(items[<span class="number">1</span>::], inTree.children[items[<span class="number">0</span>]], headerTable, count)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateHeader</span><span class="params">(nodeToTest, targetNode)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> (nodeToTest.nodeLink != <span class="keyword">None</span>):</span><br><span class="line">        nodeToTest = nodeToTest.nodeLink</span><br><span class="line">    nodeToTest.nodeLink = targetNode</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadSimpDat</span><span class="params">()</span>:</span></span><br><span class="line">    simpDat = [[<span class="string">'r'</span>, <span class="string">'z'</span>, <span class="string">'h'</span>, <span class="string">'j'</span>, <span class="string">'p'</span>],</span><br><span class="line">               [<span class="string">'z'</span>, <span class="string">'y'</span>, <span class="string">'x'</span>, <span class="string">'w'</span>, <span class="string">'v'</span>, <span class="string">'u'</span>, <span class="string">'t'</span>, <span class="string">'s'</span>],</span><br><span class="line">               [<span class="string">'z'</span>],</span><br><span class="line">               [<span class="string">'r'</span>, <span class="string">'x'</span>, <span class="string">'n'</span>, <span class="string">'o'</span>, <span class="string">'s'</span>],</span><br><span class="line">               [<span class="string">'y'</span>, <span class="string">'r'</span>, <span class="string">'x'</span>, <span class="string">'z'</span>, <span class="string">'q'</span>, <span class="string">'t'</span>, <span class="string">'p'</span>],</span><br><span class="line">               [<span class="string">'y'</span>, <span class="string">'z'</span>, <span class="string">'x'</span>, <span class="string">'e'</span>, <span class="string">'q'</span>, <span class="string">'s'</span>, <span class="string">'t'</span>, <span class="string">'m'</span>]]</span><br><span class="line">    <span class="keyword">return</span> simpDat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createInitSet</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    retDict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> trans <span class="keyword">in</span> dataSet:</span><br><span class="line">        retDict[frozenset(trans)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> retDict</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">simpDat = loadSimpDat()</span><br><span class="line">simpDat</span><br></pre></td></tr></table></figure>
<pre><code>[[&apos;r&apos;, &apos;z&apos;, &apos;h&apos;, &apos;j&apos;, &apos;p&apos;],
 [&apos;z&apos;, &apos;y&apos;, &apos;x&apos;, &apos;w&apos;, &apos;v&apos;, &apos;u&apos;, &apos;t&apos;, &apos;s&apos;],
 [&apos;z&apos;],
 [&apos;r&apos;, &apos;x&apos;, &apos;n&apos;, &apos;o&apos;, &apos;s&apos;],
 [&apos;y&apos;, &apos;r&apos;, &apos;x&apos;, &apos;z&apos;, &apos;q&apos;, &apos;t&apos;, &apos;p&apos;],
 [&apos;y&apos;, &apos;z&apos;, &apos;x&apos;, &apos;e&apos;, &apos;q&apos;, &apos;s&apos;, &apos;t&apos;, &apos;m&apos;]]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">initSet = createInitSet(simpDat)</span><br><span class="line">initSet</span><br></pre></td></tr></table></figure>
<pre><code>{frozenset({&apos;h&apos;, &apos;j&apos;, &apos;p&apos;, &apos;r&apos;, &apos;z&apos;}): 1,
 frozenset({&apos;s&apos;, &apos;t&apos;, &apos;u&apos;, &apos;v&apos;, &apos;w&apos;, &apos;x&apos;, &apos;y&apos;, &apos;z&apos;}): 1,
 frozenset({&apos;z&apos;}): 1,
 frozenset({&apos;n&apos;, &apos;o&apos;, &apos;r&apos;, &apos;s&apos;, &apos;x&apos;}): 1,
 frozenset({&apos;p&apos;, &apos;q&apos;, &apos;r&apos;, &apos;t&apos;, &apos;x&apos;, &apos;y&apos;, &apos;z&apos;}): 1,
 frozenset({&apos;e&apos;, &apos;m&apos;, &apos;q&apos;, &apos;s&apos;, &apos;t&apos;, &apos;x&apos;, &apos;y&apos;, &apos;z&apos;}): 1}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myFPtree, myHeaderTab = createTree(initSet, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myFPtree.disp()</span><br></pre></td></tr></table></figure>
<pre><code>Null Set   1
 z   5
  r   1
  x   3
   s   2
    y   2
     t   2
   r   1
    y   1
     t   1
 x   1
  r   1
   s   1
</code></pre><p>上面给出的元素项及其对应的频率计数值，其中每个缩进表示所处的树的深度。</p>
<h1 id="从一棵FP树中挖掘频繁项集"><a href="#从一棵FP树中挖掘频繁项集" class="headerlink" title="从一棵FP树中挖掘频繁项集"></a>从一棵FP树中挖掘频繁项集</h1><p>从FP树中抽取频繁项集的三个基本步骤如下：</p>
<ol>
<li>从FP树中获得条件模式基</li>
<li>利用条件模式基，构建一个条件FP树</li>
<li>迭代重复1，2步骤，直到树包含一个元素为止</li>
</ol>
<h2 id="抽取调剂模式基"><a href="#抽取调剂模式基" class="headerlink" title="抽取调剂模式基"></a>抽取调剂模式基</h2><p>发现以给定元素项结尾的所有路径的函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ascendTree</span><span class="params">(leafNode, prefixPath)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> leafNode.parent != <span class="keyword">None</span>:</span><br><span class="line">        prefixPath.append(leafNode.name)</span><br><span class="line">        ascendTree(leafNode.parent, prefixPath)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findPrefixPath</span><span class="params">(basePat, treeNode)</span>:</span></span><br><span class="line">    condPats = &#123;&#125;</span><br><span class="line">    <span class="keyword">while</span> treeNode != <span class="keyword">None</span>:</span><br><span class="line">        prefixPath = []</span><br><span class="line">        ascendTree(treeNode, prefixPath)</span><br><span class="line">        <span class="keyword">if</span> len(prefixPath) &gt; <span class="number">1</span>:</span><br><span class="line">            condPats[frozenset(prefixPath[<span class="number">1</span>:])] = treeNode.count</span><br><span class="line">        treeNode = treeNode.nodeLink</span><br><span class="line">    <span class="keyword">return</span> condPats</span><br></pre></td></tr></table></figure>
<p>上述代码用于给定元素项生成一个条件模式基，这通过访问树中所有包含给定元素项的节点来完成。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">findPrefixPath(<span class="string">'x'</span>, myHeaderTab[<span class="string">'x'</span>][<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>{frozenset({&apos;z&apos;}): 3}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">findPrefixPath(<span class="string">'z'</span>, myHeaderTab[<span class="string">'z'</span>][<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>{}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">findPrefixPath(<span class="string">'r'</span>, myHeaderTab[<span class="string">'r'</span>][<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>{frozenset({&apos;z&apos;}): 1, frozenset({&apos;x&apos;}): 1, frozenset({&apos;x&apos;, &apos;z&apos;}): 1}
</code></pre><h2 id="创建条件FP树"><a href="#创建条件FP树" class="headerlink" title="创建条件FP树"></a>创建条件FP树</h2><p>递归查找频繁项集的mineTree函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mineTree</span><span class="params">(inTree, headerTable, minSup, preFix, freqItemList)</span>:</span></span><br><span class="line">    bigL = [v[<span class="number">0</span>] <span class="keyword">for</span> v <span class="keyword">in</span> sorted(headerTable.items(), key=<span class="keyword">lambda</span> p: p[<span class="number">1</span>][<span class="number">0</span>])]</span><br><span class="line">    <span class="keyword">for</span> basePat <span class="keyword">in</span> bigL: </span><br><span class="line">        newFreqSet = preFix.copy()</span><br><span class="line">        newFreqSet.add(basePat)</span><br><span class="line">        freqItemList.append(newFreqSet)</span><br><span class="line">        condPattBases = findPrefixPath(basePat, headerTable[basePat][<span class="number">1</span>])</span><br><span class="line">        myCondTree, myHead = createTree(condPattBases, minSup)</span><br><span class="line">        <span class="keyword">if</span> myHead != <span class="keyword">None</span>:</span><br><span class="line">            print(<span class="string">'conditional tree for: '</span>,newFreqSet)</span><br><span class="line">            myCondTree.disp(<span class="number">1</span>)</span><br><span class="line">            mineTree(myCondTree, myHead, minSup, newFreqSet, freqItemList)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">freqItems = []</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mineTree(myFPtree, myHeaderTab, <span class="number">3</span>, set([]), freqItems)</span><br></pre></td></tr></table></figure>
<pre><code>conditional tree for:  {&apos;s&apos;}
  Null Set   1
   x   3
conditional tree for:  {&apos;y&apos;}
  Null Set   1
   x   3
    z   3
conditional tree for:  {&apos;y&apos;, &apos;z&apos;}
  Null Set   1
   x   3
conditional tree for:  {&apos;t&apos;}
  Null Set   1
   y   3
    x   3
     z   3
conditional tree for:  {&apos;t&apos;, &apos;x&apos;}
  Null Set   1
   y   3
conditional tree for:  {&apos;t&apos;, &apos;z&apos;}
  Null Set   1
   y   3
    x   3
conditional tree for:  {&apos;t&apos;, &apos;x&apos;, &apos;z&apos;}
  Null Set   1
   y   3
conditional tree for:  {&apos;x&apos;}
  Null Set   1
   z   3
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">freqItems</span><br></pre></td></tr></table></figure>
<pre><code>[{&apos;r&apos;},
 {&apos;s&apos;},
 {&apos;s&apos;, &apos;x&apos;},
 {&apos;y&apos;},
 {&apos;x&apos;, &apos;y&apos;},
 {&apos;y&apos;, &apos;z&apos;},
 {&apos;x&apos;, &apos;y&apos;, &apos;z&apos;},
 {&apos;t&apos;},
 {&apos;t&apos;, &apos;y&apos;},
 {&apos;t&apos;, &apos;x&apos;},
 {&apos;t&apos;, &apos;x&apos;, &apos;y&apos;},
 {&apos;t&apos;, &apos;z&apos;},
 {&apos;t&apos;, &apos;y&apos;, &apos;z&apos;},
 {&apos;t&apos;, &apos;x&apos;, &apos;z&apos;},
 {&apos;t&apos;, &apos;x&apos;, &apos;y&apos;, &apos;z&apos;},
 {&apos;x&apos;},
 {&apos;x&apos;, &apos;z&apos;},
 {&apos;z&apos;}]
</code></pre><h1 id="实例：从新闻网站点击流中挖掘"><a href="#实例：从新闻网站点击流中挖掘" class="headerlink" title="实例：从新闻网站点击流中挖掘"></a>实例：从新闻网站点击流中挖掘</h1><p>有近100万条用户浏览数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">parsedDat = [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> open(<span class="string">'MLiA_SourceCode/Ch12/kosarak.dat'</span>).readlines()]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">initSet = createInitSet(parsedDat)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myFPtree, myHeaderTab = createTree(initSet, <span class="number">100000</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myFreqList = []</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mineTree(myFPtree, myHeaderTab, <span class="number">100000</span>, set([]), myFreqList)</span><br></pre></td></tr></table></figure>
<pre><code>conditional tree for:  {&apos;1&apos;}
  Null Set   1
   6   107404
conditional tree for:  {&apos;3&apos;}
  Null Set   1
   6   186289
    11   117401
   11   9718
conditional tree for:  {&apos;3&apos;, &apos;11&apos;}
  Null Set   1
   6   117401
conditional tree for:  {&apos;11&apos;}
  Null Set   1
   6   261773
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myFreqList</span><br></pre></td></tr></table></figure>
<pre><code>[{&apos;1&apos;},
 {&apos;1&apos;, &apos;6&apos;},
 {&apos;3&apos;},
 {&apos;11&apos;, &apos;3&apos;},
 {&apos;11&apos;, &apos;3&apos;, &apos;6&apos;},
 {&apos;3&apos;, &apos;6&apos;},
 {&apos;11&apos;},
 {&apos;11&apos;, &apos;6&apos;},
 {&apos;6&apos;}]
</code></pre><h2 id="测试库函数"><a href="#测试库函数" class="headerlink" title="测试库函数"></a>测试库函数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%pip install pyfpgrowth</span><br></pre></td></tr></table></figure>
<pre><code>Collecting pyfpgrowth
  Downloading https://files.pythonhosted.org/packages/d2/4c/8b7cd90b4118ff0286d6584909b99e1ca5642bdc9072fa5a8dd361c864a0/pyfpgrowth-1.0.tar.gz (1.6MB)
Installing collected packages: pyfpgrowth
  Running setup.py install for pyfpgrowth: started
    Running setup.py install for pyfpgrowth: finished with status &apos;done&apos;
Successfully installed pyfpgrowth-1.0
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pyfpgrowth</span><br><span class="line">patterns = pyfpgrowth.find_frequent_patterns(initSet, <span class="number">100000</span>)</span><br><span class="line">rules = pyfpgrowth.generate_association_rules(patterns, <span class="number">0.7</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">patterns</span><br></pre></td></tr></table></figure>
<pre><code>{(&apos;1&apos;,): 140597,
 (&apos;1&apos;, &apos;6&apos;): 107404,
 (&apos;11&apos;, &apos;3&apos;): 127119,
 (&apos;11&apos;, &apos;3&apos;, &apos;6&apos;): 117401,
 (&apos;3&apos;, &apos;6&apos;): 186289,
 (&apos;11&apos;,): 282963,
 (&apos;11&apos;, &apos;6&apos;): 261773,
 (&apos;6&apos;,): 412762}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rules</span><br></pre></td></tr></table></figure>
<pre><code>{(&apos;1&apos;,): ((&apos;6&apos;,), 0.7639138815195203),
 (&apos;11&apos;, &apos;3&apos;): ((&apos;6&apos;,), 0.9235519473878806),
 (&apos;11&apos;,): ((&apos;6&apos;,), 0.9251138841473974)}
</code></pre><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>FP-growth算法是一种用于发现数据集中频繁模式的有效方法。FP-growth算法利用Apriori原则，执行更快。Apriori算法产生候选项集，然后扫描数据集来检查它们是否频繁发。由于只对数据集扫描两次，因此FP-growth算法执行更快。在FP-growth算法中，数据集存储在一个称为FP树的结构中。FP树构建完成后，可以通过查找元素项的条件基及构建条件FP树来发现频繁项集。该过程不断以更多元素作为条件重复进行，知道FP树只包含一个元素为止。</p>
]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>FP-growth</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实战（十四）</title>
    <url>/2020/06/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89/</url>
    <content><![CDATA[<h1 id="利用SVD简化数据"><a href="#利用SVD简化数据" class="headerlink" title="利用SVD简化数据"></a>利用SVD简化数据</h1><p>奇异值分解（singular value decomposition，SVD）</p>
<h1 id="SVD的应用"><a href="#SVD的应用" class="headerlink" title="SVD的应用"></a>SVD的应用</h1><p>奇异值分解</p>
<pre><code>优点：简化数据，去除噪声，提高算法的结果
缺点：数据的转换可能难以理解
适用数据类型：数值型数据
</code></pre><p>利用SVD实现，我们能够用小得多的数据集来表示原始数据集。这样做，实际上是去除噪声和冗余信息。</p>
<h2 id="隐性语义索引"><a href="#隐性语义索引" class="headerlink" title="隐性语义索引"></a>隐性语义索引</h2><p>SVD的应用之一就是信息检索。我们称利用SVD的方法为隐性语义索引（Latent Semantic Indexing，LSI）。</p>
<p>在LSI中，一个矩阵是由文档和词语组成的。当我们在该矩阵上应用SVD时，就会构建出多个奇异值。这些奇异值代表了文档中的概念或主题，这一特点可以用于更高效的文档搜索。在词语拼写错误时，只基于词语存在与否的简单搜索方法会遇到问题。简单搜索的另一个问题就是同义词的使用。</p>
<h2 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h2><p>SVD的另一个应用就是推荐系统。简单版本的推荐系统能够计算项或人之间的相似度。更先进的方法则先利用SVD从数据中构建一个主题空间，然后再再该空间下计算相似度。</p>
<h1 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h1><p>SVD是矩阵分解的一种类型，矩阵分解是将数据矩阵分解为多个独立部分的过程。<br>SVD将原始的数据集矩阵Data分解成三个矩阵$U,\Sigma,V^T$。如果原始矩阵Data是m行n列，那么$U,\Sigma,V^T$分别是m行m列，m行n列，n行n列。上述过程可以写成下面结果：</p>
<p>$$Data_{m×n}=U_{m×m}\Sigma_{m×n}V^T_{n×n}$$<br>上述分解中会构建一个矩阵$\Sigma$，该矩阵只有对角元素，其他元素均为0。另一个惯例就是$\Sigma$的对角元素是从大到小排列的。这些对角元素称为奇异值（Singular Value），它们对应了原始数据矩阵Data的奇异值。奇异值和特征值是有关系的，这里的奇异值就是矩阵$Data*Data^T$特征值的平方根。</p>
<h1 id="利用Python实现SVD"><a href="#利用Python实现SVD" class="headerlink" title="利用Python实现SVD"></a>利用Python实现SVD</h1><p>在numpy中提供了svd的实现<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">U, Sigma, VT = linalg.svd([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">7</span>, <span class="number">7</span>]])</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">U</span><br></pre></td></tr></table></figure>
<pre><code>array([[-0.14142136, -0.98994949],
       [-0.98994949,  0.14142136]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Sigma</span><br></pre></td></tr></table></figure>
<pre><code>array([10.,  0.])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">VT</span><br></pre></td></tr></table></figure>
<pre><code>array([[-0.70710678, -0.70710678],
       [-0.70710678,  0.70710678]])
</code></pre><p>注意到，矩阵Sigma以行向量array([10., 0.])返回，而非如下矩阵</p>
<pre><code>array([[10,  0],
       [ 0,  0]])
</code></pre><p>这是由于矩阵处理对角元素其他均为0，因此这种仅返回对角元素的方式能节省空间，我们需要知道Sigma是一个矩阵。</p>
<p>构建以下矩阵</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadExData</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span>[[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">           [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">5</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Data = loadExData()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">U, Sigma, VT = linalg.svd(Data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Sigma</span><br></pre></td></tr></table></figure>
<pre><code>array([9.64365076e+00, 5.29150262e+00, 7.40623935e-16, 4.05103551e-16,
       2.21838243e-32])
</code></pre><p>前三个数值比其他值大了很多，于是我们可以将最后两个数值去掉了。</p>
<p>接下来我们的原始数据集就可以用如下结果来近似</p>
<p>$$Data_{m×n}≈U_{m×3}\Sigma_{3×3}V^T_{3×n}$$</p>
<p><img src="14fig02.jpg" alt=""></p>
<p>SVD示意图。矩阵Data被分解。浅灰色区域是原始数据，深灰色区域是矩阵近似计算仅需的数据</p>
<p>我们试图重构原始矩阵，首先构建一个3×3的矩阵Sig3</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Sig3 = mat([[Sigma[<span class="number">0</span>], <span class="number">0</span>, <span class="number">0</span>], </span><br><span class="line">            [<span class="number">0</span>, Sigma[<span class="number">1</span>], <span class="number">0</span>], </span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, Sigma[<span class="number">2</span>]]])</span><br></pre></td></tr></table></figure>
<p>接下来重构原始矩阵的近似矩阵。由于Sig3仅为3×3的矩阵，因而我们只需要使用矩阵U的前3列和$V^T$的前3行。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = U[:, :<span class="number">3</span>]*Sig3*VT[:<span class="number">3</span>, :]</span><br><span class="line">data.astype(<span class="string">'int'</span>)</span><br></pre></td></tr></table></figure></p>
<pre><code>matrix([[0, 0, 0, 2, 2],
        [0, 0, 0, 3, 3],
        [0, 0, 0, 1, 1],
        [1, 1, 1, 0, 0],
        [2, 2, 2, 0, 0],
        [5, 5, 5, 0, 0],
        [1, 1, 1, 0, 0]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Data</span><br></pre></td></tr></table></figure>
<pre><code>[[0, 0, 0, 2, 2],
 [0, 0, 0, 3, 3],
 [0, 0, 0, 1, 1],
 [1, 1, 1, 0, 0],
 [2, 2, 2, 0, 0],
 [5, 5, 5, 0, 0],
 [1, 1, 1, 0, 0]]
</code></pre><p>通过SVD我们可以用一个小很多的矩阵来表示一个大矩阵。</p>
<h1 id="基于协同过滤的推荐引擎"><a href="#基于协同过滤的推荐引擎" class="headerlink" title="基于协同过滤的推荐引擎"></a>基于协同过滤的推荐引擎</h1><h2 id="相似度计算"><a href="#相似度计算" class="headerlink" title="相似度计算"></a>相似度计算</h2><pre><code>欧氏距离
皮尔逊相关系数（pearson correlation）
余弦相似度（cosine similarity）
</code></pre><p>相似度计算<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> linalg <span class="keyword">as</span> la</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eulidSim</span><span class="params">(inA, inB)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+la.norm(inA - inB))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pearsSim</span><span class="params">(inA, inB)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(inA) &lt; <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span>+<span class="number">0.5</span>*corrcoef(inA, inB, rowvar=<span class="number">0</span>)[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cosSim</span><span class="params">(inA, inB)</span>:</span></span><br><span class="line">    num = float(inA.T * inB)</span><br><span class="line">    denom = la.norm(inA) * la.norm(inB)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span>+<span class="number">0.5</span>*(num/denom)</span><br></pre></td></tr></table></figure></p>
<p>测试一下这三个函数<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myMat = mat(loadExData())</span><br></pre></td></tr></table></figure></p>
<p>欧氏距离：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ecludSim(myMat[:, <span class="number">0</span>], myMat[:, <span class="number">4</span>])</span><br></pre></td></tr></table></figure></p>
<pre><code>0.12973190755680383
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ecludSim(myMat[:, <span class="number">0</span>], myMat[:, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>1.0
</code></pre><p>余弦相似度<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cosSim(myMat[:, <span class="number">0</span>], myMat[:, <span class="number">4</span>])</span><br></pre></td></tr></table></figure></p>
<pre><code>0.5
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cosSim(myMat[:, <span class="number">0</span>], myMat[:, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>1.0
</code></pre><p>皮尔逊相关系数<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pearsSim(myMat[:, <span class="number">0</span>], myMat[:, <span class="number">4</span>])</span><br></pre></td></tr></table></figure></p>
<pre><code>0.20596538173840329
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pearsSim(myMat[:, <span class="number">0</span>], myMat[:, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>1.0
</code></pre><p>上面的相似度计算都是假设数据采用了列向量方式进行表示。如果利用上述函数来计算两个向量的相似度就会遇到问题（我们很容易对上述函数进行修改以计算行向量之间的相似度）。这里采用列向量的表示方法，暗示着我们将利用基于物品的相似度计算方法。</p>
<h2 id="基于物品的相似度还是基于用户的相似度"><a href="#基于物品的相似度还是基于用户的相似度" class="headerlink" title="基于物品的相似度还是基于用户的相似度"></a>基于物品的相似度还是基于用户的相似度</h2><p>我们计算了两个餐馆菜肴之间的距离，这称为基于物品（item-based）的相似度。另一种计算用户距离的方法称为基于用户（user-based）。基于物品相似度计算的时间会随着物品数量的增加而增加，基于用户的相似度计算时间则会随用户数量的增加而增加。</p>
<h2 id="推荐引擎的评价"><a href="#推荐引擎的评价" class="headerlink" title="推荐引擎的评价"></a>推荐引擎的评价</h2><p>通常用于推荐引擎的评价指标称为最小均方根误差（Root Mean Squared Error，RMSE）的指标，它首先计算均方误差的平均值再取其平方根。</p>
<h2 id="实例：餐馆菜肴推荐引擎"><a href="#实例：餐馆菜肴推荐引擎" class="headerlink" title="实例：餐馆菜肴推荐引擎"></a>实例：餐馆菜肴推荐引擎</h2><p>推荐未尝过的菜肴</p>
<ol>
<li>寻找用户没有评级的菜肴，即在用户物品矩阵中的0值</li>
<li>在用户没有评级的所有物品中，对每个物品预计一个可能的评级分数。（我们认为用户可能对物品的打分）</li>
<li>对这些物品的评分从高到低排序，返回前N个物品<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standEst</span><span class="params">(dataMat, user, simMeas, item)</span>:</span></span><br><span class="line">    n = shape(dataMat)[<span class="number">1</span>]</span><br><span class="line">    simTotal = <span class="number">0.0</span></span><br><span class="line">    ratSimTotal = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        userRating = dataMat[user, j]</span><br><span class="line">        <span class="keyword">if</span> userRating == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        overLap = nonzero(logical_and(dataMat[:,item].A&gt;<span class="number">0</span>, dataMat[:,j].A&gt;<span class="number">0</span>))[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> len(overLap) == <span class="number">0</span>:</span><br><span class="line">            similarity = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            similarity = simMeas(dataMat[overLap,item], dataMat[overLap,j])</span><br><span class="line">        print(<span class="string">'the %d and %d similarity is: %f'</span> % (item, j, similarity))</span><br><span class="line">        simTotal += similarity</span><br><span class="line">        ratSimTotal += similarity * userRating</span><br><span class="line">    <span class="keyword">if</span> simTotal == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> ratSimTotal/simTotal</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">recommend</span><span class="params">(dataMat, user, N=<span class="number">3</span>, simMeas=cosSim, estMethod=standEst)</span>:</span></span><br><span class="line">    unratedItems = nonzero(dataMat[user, :].A==<span class="number">0</span>)[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> len(unratedItems) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'you rated everything'</span></span><br><span class="line">    itemScores = []</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> unratedItems:</span><br><span class="line">        estimatedScore = estMethod(dataMat, user, simMeas, item)</span><br><span class="line">        itemScores.append((item, estimatedScore))</span><br><span class="line">    <span class="keyword">return</span> sorted(itemScores, key=<span class="keyword">lambda</span> jj: jj[<span class="number">1</span>], reverse=<span class="keyword">True</span>)[:N]</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>第一个函数standEst()用来计算给定相似度计算方法的条件下，用户对物品的评估分值。第二个函数recommend()也就是推荐引擎，他会调用standEst()函数。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myMat = mat(loadExData())</span><br><span class="line">myMat[<span class="number">0</span>, <span class="number">1</span>] = myMat[<span class="number">0</span>, <span class="number">0</span>] = myMat[<span class="number">1</span>, <span class="number">0</span>] = myMat[<span class="number">2</span>, <span class="number">0</span>] = <span class="number">4</span></span><br><span class="line">myMat[<span class="number">3</span>, <span class="number">3</span>] = <span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myMat</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[4, 4, 0, 2, 2],
        [4, 0, 0, 3, 3],
        [4, 0, 0, 1, 1],
        [1, 1, 1, 2, 0],
        [2, 2, 2, 0, 0],
        [5, 5, 5, 0, 0],
        [1, 1, 1, 0, 0]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">recommend(myMat, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>the 1 and 0 similarity is: 1.000000
the 1 and 3 similarity is: 0.928746
the 1 and 4 similarity is: 1.000000
the 2 and 0 similarity is: 1.000000
the 2 and 3 similarity is: 1.000000
the 2 and 4 similarity is: 0.000000

[(2, 2.5), (1, 2.0243290220056256)]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">recommend(myMat, <span class="number">2</span>, simMeas=ecludSim)</span><br></pre></td></tr></table></figure>
<pre><code>the 1 and 0 similarity is: 1.000000
the 1 and 3 similarity is: 0.309017
the 1 and 4 similarity is: 0.333333
the 2 and 0 similarity is: 1.000000
the 2 and 3 similarity is: 0.500000
the 2 and 4 similarity is: 0.000000

[(2, 3.0), (1, 2.8266504712098603)]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">recommend(myMat, <span class="number">2</span>, simMeas=pearsSim)</span><br></pre></td></tr></table></figure>
<pre><code>the 1 and 0 similarity is: 1.000000
the 1 and 3 similarity is: 1.000000
the 1 and 4 similarity is: 1.000000
the 2 and 0 similarity is: 1.000000
the 2 and 3 similarity is: 1.000000
the 2 and 4 similarity is: 0.000000

[(2, 2.5), (1, 2.0)]
</code></pre><p>这个例子给出了如何利用基于物品相似度和多个相似度计算方法来进行推荐的过程。</p>
<h2 id="利用SVD提高推荐效果"><a href="#利用SVD提高推荐效果" class="headerlink" title="利用SVD提高推荐效果"></a>利用SVD提高推荐效果</h2><p>下面是一个更大的矩阵<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadExData2</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span>[[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">           [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>]]</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">U, Sigma, VT = la.svd(mat(loadExData2()))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Sigma</span><br></pre></td></tr></table></figure>
<pre><code>array([15.77075346, 11.40670395, 11.03044558,  4.84639758,  3.09292055,
        2.58097379,  1.00413543,  0.72817072,  0.43800353,  0.22082113,
        0.07367823])
</code></pre><p>首先对Sigma中的值求平方<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Sig2 = Sigma**<span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sum(Sig2)</span><br></pre></td></tr></table></figure>
<pre><code>541.9999999999995
</code></pre><p>在计算总能量的90%<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sum(Sig2)*<span class="number">0.9</span></span><br></pre></td></tr></table></figure></p>
<pre><code>487.7999999999996
</code></pre><p>然后计算前两个元素包含的能量<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sum(Sig2[:<span class="number">2</span>])</span><br></pre></td></tr></table></figure></p>
<pre><code>378.8295595113579
</code></pre><p>该值低于总能量的90%，于是计算前三个元素所包含的能量<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sum(Sig2[:<span class="number">3</span>])</span><br></pre></td></tr></table></figure></p>
<pre><code>500.50028912757926
</code></pre><p>该值高于90%，这就可以了。于是我们将一个11维的矩阵转换为一个3维的矩阵，下面对转换后的三维空间构造一个相似度计算函数。我们利用SVD将所有的菜肴映射到一个低维空间中去。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svdEst</span><span class="params">(dataMat, user, simMeas, item)</span>:</span></span><br><span class="line">    n = shape(dataMat)[<span class="number">1</span>]</span><br><span class="line">    simTotal = <span class="number">0.0</span></span><br><span class="line">    ratSimTotal = <span class="number">0.0</span></span><br><span class="line">    U,Sigma,VT = la.svd(dataMat)</span><br><span class="line">    Sig4 = mat(eye(<span class="number">4</span>)*Sigma[:<span class="number">4</span>])</span><br><span class="line">    xformedItems = dataMat.T * U[:,:<span class="number">4</span>] * Sig4.I</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        userRating = dataMat[user, j]</span><br><span class="line">        <span class="keyword">if</span> userRating == <span class="number">0</span> <span class="keyword">or</span> j==item:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        similarity = simMeas(xformedItems[item,:].T, xformedItems[j,:].T)</span><br><span class="line">        print(<span class="string">'the %d and %d similarity is: %f'</span> % (item, j, similarity))</span><br><span class="line">        simTotal += similarity</span><br><span class="line">        ratSimTotal += similarity * userRating</span><br><span class="line">    <span class="keyword">if</span> simTotal == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> ratSimTotal/simTotal</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myMat = mat(loadExData2())</span><br><span class="line">recommend(myMat, <span class="number">1</span>, estMethod=svdEst)</span><br></pre></td></tr></table></figure>
<pre><code>the 0 and 3 similarity is: 0.490950
the 0 and 5 similarity is: 0.484274
the 0 and 10 similarity is: 0.512755
the 1 and 3 similarity is: 0.491294
the 1 and 5 similarity is: 0.481516
the 1 and 10 similarity is: 0.509709
the 2 and 3 similarity is: 0.491573
the 2 and 5 similarity is: 0.482346
the 2 and 10 similarity is: 0.510584
the 4 and 3 similarity is: 0.450495
the 4 and 5 similarity is: 0.506795
the 4 and 10 similarity is: 0.512896
the 6 and 3 similarity is: 0.743699
the 6 and 5 similarity is: 0.468366
the 6 and 10 similarity is: 0.439465
the 7 and 3 similarity is: 0.482175
the 7 and 5 similarity is: 0.494716
the 7 and 10 similarity is: 0.524970
the 8 and 3 similarity is: 0.491307
the 8 and 5 similarity is: 0.491228
the 8 and 10 similarity is: 0.520290
the 9 and 3 similarity is: 0.522379
the 9 and 5 similarity is: 0.496130
the 9 and 10 similarity is: 0.493617

[(4, 3.344714938469228), (7, 3.329402072452697), (9, 3.328100876390069)]
</code></pre><p>尝试另一种方法<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">recommend(myMat, <span class="number">1</span>, simMeas=pearsSim, estMethod=svdEst)</span><br></pre></td></tr></table></figure></p>
<pre><code>the 0 and 3 similarity is: 0.341942
the 0 and 5 similarity is: 0.124132
the 0 and 10 similarity is: 0.116698
the 1 and 3 similarity is: 0.345560
the 1 and 5 similarity is: 0.126456
the 1 and 10 similarity is: 0.118892
the 2 and 3 similarity is: 0.345149
the 2 and 5 similarity is: 0.126190
the 2 and 10 similarity is: 0.118640
the 4 and 3 similarity is: 0.450126
the 4 and 5 similarity is: 0.528504
the 4 and 10 similarity is: 0.544647
the 6 and 3 similarity is: 0.923822
the 6 and 5 similarity is: 0.724840
the 6 and 10 similarity is: 0.710896
the 7 and 3 similarity is: 0.319482
the 7 and 5 similarity is: 0.118324
the 7 and 10 similarity is: 0.113370
the 8 and 3 similarity is: 0.334910
the 8 and 5 similarity is: 0.119673
the 8 and 10 similarity is: 0.112497
the 9 and 3 similarity is: 0.566918
the 9 and 5 similarity is: 0.590049
the 9 and 10 similarity is: 0.602380

[(4, 3.346952186702173), (9, 3.3353796573274694), (6, 3.3071930278130366)]
</code></pre><h1 id="实例：基于SVD的图像压缩"><a href="#实例：基于SVD的图像压缩" class="headerlink" title="实例：基于SVD的图像压缩"></a>实例：基于SVD的图像压缩</h1><p>我们可以使用SVD来对数据降维，从而实现图像的压缩。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printMat</span><span class="params">(inMat, thresh=<span class="number">0.8</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">            <span class="keyword">if</span> float(inMat[i, k] &gt; thresh):</span><br><span class="line">                print(<span class="number">1</span>, end=<span class="string">''</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="number">0</span>, end=<span class="string">''</span>)</span><br><span class="line">        print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imgCompress</span><span class="params">(numSV=<span class="number">3</span>, thresh=<span class="number">0.8</span>)</span>:</span></span><br><span class="line">    myl = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> open(<span class="string">'MLiA_SourceCode/Ch14/0_5.txt'</span>).readlines():</span><br><span class="line">        newRow = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">            newRow.append(int(line[i]))</span><br><span class="line">        myl.append(newRow)</span><br><span class="line">    myMat = mat(myl)</span><br><span class="line">    print(<span class="string">'***originam matrix*****'</span>)</span><br><span class="line">    printMat(myMat, thresh)</span><br><span class="line">    U, Sigma, VT = la.svd(myMat)</span><br><span class="line">    SigRecon = mat(zeros((numSV, numSV)))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(numSV):</span><br><span class="line">        SigRecon[k, k] = Sigma[k]</span><br><span class="line">    reconMat = U[:, :numSV] * SigRecon*VT[:numSV, :]</span><br><span class="line">    print(<span class="string">"****reconstructed matrix using %d singular values******"</span> % numSV)</span><br><span class="line">    printMat(reconMat, thresh)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">imgCompress(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>***originam matrix*****
00000000000000110000000000000000
00000000000011111100000000000000
00000000000111111110000000000000
00000000001111111111000000000000
00000000111111111111100000000000
00000001111111111111110000000000
00000000111111111111111000000000
00000000111111100001111100000000
00000001111111000001111100000000
00000011111100000000111100000000
00000011111100000000111110000000
00000011111100000000011110000000
00000011111100000000011110000000
00000001111110000000001111000000
00000011111110000000001111000000
00000011111100000000001111000000
00000001111100000000001111000000
00000011111100000000001111000000
00000001111100000000001111000000
00000001111100000000011111000000
00000000111110000000001111100000
00000000111110000000001111100000
00000000111110000000001111100000
00000000111110000000011111000000
00000000111110000000111111000000
00000000111111000001111110000000
00000000011111111111111110000000
00000000001111111111111110000000
00000000001111111111111110000000
00000000000111111111111000000000
00000000000011111111110000000000
00000000000000111111000000000000
****reconstructed matrix using 2 singular values******
00000000000000000000000000000000
00000000000000000000000000000000
00000000000001111100000000000000
00000000000011111111000000000000
00000000000111111111100000000000
00000000001111111111110000000000
00000000001111111111110000000000
00000000011110000000001000000000
00000000111100000000001100000000
00000000111100000000001110000000
00000000111100000000001110000000
00000000111100000000001110000000
00000000111100000000001110000000
00000000111100000000001110000000
00000000111100000000001110000000
00000000111100000000001110000000
00000000111100000000001110000000
00000000111100000000001110000000
00000000111100000000001110000000
00000000111100000000001110000000
00000000111100000000001110000000
00000000111100000000001110000000
00000000111100000000001110000000
00000000111100000000001110000000
00000000111100000000001110000000
00000000111100000000001100000000
00000000001111111111111000000000
00000000001111111111110000000000
00000000001111111111110000000000
00000000000011111111100000000000
00000000000011111111000000000000
00000000000000000000000000000000
</code></pre><p>可以看到，只需要两个奇异值就能相当精确的对图像实现重构。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>SVD是一种强大的降维工具，我们可以利用SVD来逼近矩阵并从中提取重要特征。通过保留矩阵80%~90%的能量，就可以得到重要的特征并去掉噪声。SVD已经运用到了多个应用中，其中一个成功的案例就是推荐引擎。</p>
<p>推荐引擎将物品推荐给用户，协调过滤则是一种基于用户喜好或行为数据的推荐实现方法。协调过滤的核心是相似度计算方法，很多相似度计算方法都可以用计算物品或用户之间的相似度。通过降低空间下计算相似度，SVD提高了推荐引擎的效果。</p>
<p>在大规模数据集上，SVD的计算和推荐可能是一个很困难的工程问题。通过离线方式来进行分解和相似度计算，是一种减少冗余计算和推荐所需时间的办法。</p>
]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实战（十）</title>
    <url>/2020/05/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%8D%81%EF%BC%89/</url>
    <content><![CDATA[<h1 id="利用K-均值聚类算法对未标注数据分组"><a href="#利用K-均值聚类算法对未标注数据分组" class="headerlink" title="利用K-均值聚类算法对未标注数据分组"></a>利用K-均值聚类算法对未标注数据分组</h1><p>聚类是一种无监督的学习，它将相似的对象归到同一个簇中，它有点像全自动分类。聚类方法几乎可以应用于所有对象，簇内的对象越相似，聚类的效果越好。<br>K-means聚类算法，它可以发现k个不同的簇，且每个簇中心采用簇中所含值的均值计算而成。</p>
<p>簇识别（cluster identification）。簇识别给出聚类结果的含义。假定有一些数据，现在将相似数据归到一起，簇识别会告诉我们这些簇到底都是些什么。聚类与分类的最大不同在于，分类的目标事先已知，而聚类则不一样。因为其产生的结果与分类相同，而只是类别没有预先定义，聚类有时也被称为无监督分类（unsupervised classification）。</p>
<p>聚类分析试图将相似对象归入同一簇，将不相似对象归到不同簇。</p>
<h1 id="K-均值聚类算法"><a href="#K-均值聚类算法" class="headerlink" title="K-均值聚类算法"></a>K-均值聚类算法</h1><pre><code>优点：容易实现
缺点：可能收敛到局部最小值，在大规模数据集上收敛较慢
适用数据类型：数值型数据
</code></pre><p>K-means是发现给定数据集的k个簇的算法。簇的个数k是用户给定的，每一个簇通过其质心（centroid），即簇中所有点的中心来描述。</p>
<p>K-means算法的工作流程，首先随机确定k个初始点作为质心，然后将数据集的每个点分配到一个簇中，具体来讲，为每个点找距其最近的质心，并将其分配给该质心所对应的簇。这一步完成之后，每个簇的质心更新为该簇所有点的平均值。</p>
<p>伪代码如下：</p>
<pre><code>创建k个点作为其实质心
当任意一个点的簇分配结果发生改变时
    对数据集中的每个数据点
        对每个质心
            计算质心与数据点之间的距离
        将数据点分配到距离其最近的簇
    对每一个簇，计算簇中所有点的均值并将均值作为质心
</code></pre><p>K-means聚类的一般流程</p>
<ol>
<li>收集数据</li>
<li>准备数据：需要数值型数据来计算距离，也可以将标称型数据映射为二值型数据再用于距离计算</li>
<li>分析数据</li>
<li>训练算法：不适用无监督学习，即无监督学习没有训练过程</li>
<li>测试算法：应用聚类算法，观察结果。可以使用量化的误差指标如误差平方和来评价算法的结果</li>
<li>使用算法：可以用于所希望的任何应用。通常情况下，簇质心可以代表整个簇的数据来做出决策</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        fltLine = map(float, curLine)</span><br><span class="line">        fltLine = list(fltLine)</span><br><span class="line">        dataMat.append(fltLine)</span><br><span class="line">    <span class="keyword">return</span> dataMat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span><span class="params">(vecA, vecB)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sqrt(sum(power(vecA - vecB, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span><span class="params">(dataSet, k)</span>:</span></span><br><span class="line">    n = shape(dataSet)[<span class="number">1</span>]</span><br><span class="line">    centroids = mat(zeros((k, n)))</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        minJ = min(dataSet[:, j])</span><br><span class="line">        rangeJ = float(max(dataSet[:, j]) - minJ)</span><br><span class="line">        centroids[:, j] = minJ + rangeJ * random.rand(k, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> centroids</span><br></pre></td></tr></table></figure>
<p>distEclud()是计算两个向量的欧式距离</p>
<p>randCent()是为给定数据集构建一个包含k个随机质心的集合。随机质心必须要在整个数据集的边界之内，这可以找到数据集每一维的最小值和最大值来完成。然后随机生成0到1.0之间的随机数并通过取值范围和最小值，以便确保随机点在数据的边界之内。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datMat = mat(loadDataSet(<span class="string">'MLiA_SourceCode/Ch10/testSet.txt'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">min(datMat[:, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[-5.379713]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">min(datMat[:, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[-4.232586]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max(datMat[:, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[5.1904]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max(datMat[:, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[4.838138]])
</code></pre><p>看下randCent()生成的值是否在max和min之间。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">randCent(datMat, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>matrix([[-0.55362342, -1.69185255],
        [ 0.43137443, -4.17749883]])
</code></pre><p>测试计算距离的函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">distEclud(datMat[<span class="number">0</span>], datMat[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>5.184632816681332
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataSet, k, distMeas=distEclud, createCent=randCent)</span>:</span></span><br><span class="line">    m = shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">    clusterAssment = mat(zeros((m, <span class="number">2</span>)))</span><br><span class="line">    </span><br><span class="line">    centroids = createCent(dataSet, k)</span><br><span class="line">    clusterChanged = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">while</span> clusterChanged:</span><br><span class="line">        clusterChanged = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            minDist = inf</span><br><span class="line">            minIndex = <span class="number">-1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k): <span class="comment"># 寻找最近的质心</span></span><br><span class="line">                distJI = distMeas(centroids[j, :], dataSet[i, :])</span><br><span class="line">                <span class="keyword">if</span> distJI &lt; minDist:</span><br><span class="line">                    minDist = distJI</span><br><span class="line">                    minIndex = j</span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i, <span class="number">0</span>] != minIndex:</span><br><span class="line">                clusterChanged = <span class="keyword">True</span></span><br><span class="line">            clusterAssment[i, :] = minIndex, minDist**<span class="number">2</span></span><br><span class="line">        print(centroids)</span><br><span class="line">        <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):</span><br><span class="line">            ptsInClust = dataSet[nonzero(clusterAssment[:, <span class="number">0</span>].A == cent)[<span class="number">0</span>]] <span class="comment"># 更新质心的位置</span></span><br><span class="line">            centroids[cent, :] = mean(ptsInClust, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">centroids, clusterAssment = kMeans(datMat, <span class="number">4</span>)</span><br><span class="line">centroids</span><br></pre></td></tr></table></figure>
<pre><code>[[ 4.01323567  3.90379869]
 [-3.02008248 -3.35713241]
 [ 0.85731381  0.6868651 ]
 [ 0.45281866 -3.89960214]]
[[ 2.72275519  3.38230919]
 [-3.53973889 -2.89384326]
 [-0.92392975  2.12807596]
 [ 2.42776071 -3.19858565]]
[[ 2.6265299   3.10868015]
 [-3.53973889 -2.89384326]
 [-2.31079352  2.63181095]
 [ 2.7481024  -2.90572575]]
[[ 2.6265299   3.10868015]
 [-3.53973889 -2.89384326]
 [-2.46154315  2.78737555]
 [ 2.65077367 -2.79019029]]


matrix([[ 2.6265299 ,  3.10868015],
        [-3.53973889, -2.89384326],
        [-2.46154315,  2.78737555],
        [ 2.65077367, -2.79019029]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotScatter</span><span class="params">(data)</span>:</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    ax.scatter(data[:, <span class="number">0</span>].T.tolist()[<span class="number">0</span>], data[:, <span class="number">1</span>].T.tolist()[<span class="number">0</span>], <span class="number">10</span>, c=<span class="string">'red'</span>)</span><br><span class="line">    ax.scatter(centroids[:, <span class="number">0</span>].T.tolist()[<span class="number">0</span>], centroids[:, <span class="number">1</span>].T.tolist()[<span class="number">0</span>], <span class="number">50</span>, marker=<span class="string">'*'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plotScatter(datMat)</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%8D%81%EF%BC%89_17_0.png" alt="png"></p>
<p>上面的结果得到四个质心。</p>
<h1 id="使用后处理来提高聚类性能"><a href="#使用后处理来提高聚类性能" class="headerlink" title="使用后处理来提高聚类性能"></a>使用后处理来提高聚类性能</h1><p>利用点到质心的距离的平方值，来评价聚类质量。</p>
<p>另一种用于度量聚类效果的指标是SSE（sum of squared error，误差平方和），对应clusterAssment矩阵第一列之和。SSE值越小表示数据点越接近于他们的质心，聚类效果也越好。因为对误差取了平方，因此更加重视那些远离中心的点。</p>
<h1 id="二分K-均值算法"><a href="#二分K-均值算法" class="headerlink" title="二分K-均值算法"></a>二分K-均值算法</h1><p>为了克服K-均值算法收敛于局部最小值的问题，有人提出了一个称为二分K-均值的算法。该算法首先将所有点作为一个簇，然后将该簇一分为二。之后选择另一个簇进行划分，选择哪一个簇进行划分取决于对其划分是否可以最大程度降低SSE值。</p>
<p>伪代码如下：</p>
<pre><code>将所有点看成一个簇
当簇数目小于k时
    对每一个簇
        计算总误差
        在给定的簇上面进行K-均值聚类（k=2）
        计算将该簇一分为二后的总误差
    选择使得误差最小的那个簇进行划分操作
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">biKmeans</span><span class="params">(dataSet, k, distMeas=distEclud)</span>:</span></span><br><span class="line">    m = shape(dataSet)[<span class="number">0</span>]</span><br><span class="line">    clusterAssment = mat(zeros((m,<span class="number">2</span>)))</span><br><span class="line">    centroid0 = mean(dataSet, axis=<span class="number">0</span>).tolist()[<span class="number">0</span>]</span><br><span class="line">    centList = [centroid0] <span class="comment"># 创建一个初始簇</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算每个点到平均值的距离的平方</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">        clusterAssment[j, <span class="number">1</span>] = distMeas(mat(centroid0), dataSet[j, :])**<span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (len(centList) &lt; k):</span><br><span class="line">        lowestSSE = inf</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(centList)):</span><br><span class="line">            ptsInCurrCluster = dataSet[nonzero(clusterAssment[:, <span class="number">0</span>].A==i)[<span class="number">0</span>], :] <span class="comment"># 获取当前数据集i中的数据点</span></span><br><span class="line">            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, <span class="number">2</span>, distMeas)</span><br><span class="line">            sseSplit = sum(splitClustAss[:, <span class="number">1</span>]) <span class="comment"># 将SEE与当前的最小值进行比较</span></span><br><span class="line">            sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:, <span class="number">0</span>].A!=i)[<span class="number">0</span>], <span class="number">1</span>])</span><br><span class="line">            print(<span class="string">"sseSplit, and notSplit: "</span>, sseSplit,sseNotSplit)</span><br><span class="line">            <span class="keyword">if</span> (sseSplit + sseNotSplit) &lt; lowestSSE:</span><br><span class="line">                bestCentToSplit = i</span><br><span class="line">                bestNewCents = centroidMat</span><br><span class="line">                bestClustAss = splitClustAss.copy()</span><br><span class="line">                lowestSSE = sseSplit + sseNotSplit</span><br><span class="line">        bestClustAss[nonzero(bestClustAss[:,<span class="number">0</span>].A == <span class="number">1</span>)[<span class="number">0</span>],<span class="number">0</span>] = len(centList)</span><br><span class="line">        bestClustAss[nonzero(bestClustAss[:,<span class="number">0</span>].A == <span class="number">0</span>)[<span class="number">0</span>],<span class="number">0</span>] = bestCentToSplit</span><br><span class="line">        print(<span class="string">'the bestCentToSplit is: '</span>,bestCentToSplit)</span><br><span class="line">        print(<span class="string">'the len of bestClustAss is: '</span>, len(bestClustAss))</span><br><span class="line">        centList[bestCentToSplit] = bestNewCents[<span class="number">0</span>,:].tolist()[<span class="number">0</span>] <span class="comment"># 用两个最佳质心替换一个质心</span></span><br><span class="line">        centList.append(bestNewCents[<span class="number">1</span>,:].tolist()[<span class="number">0</span>])</span><br><span class="line">        clusterAssment[nonzero(clusterAssment[:,<span class="number">0</span>].A == bestCentToSplit)[<span class="number">0</span>],:]= bestClustAss <span class="comment"># 更新簇的分配结果</span></span><br><span class="line">    <span class="keyword">return</span> mat(centList), clusterAssment</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datMat3 = mat(loadDataSet(<span class="string">'MLiA_SourceCode/Ch10/testSet2.txt'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">centroids, clusterAssment = biKmeans(datMat3, <span class="number">3</span>)</span><br><span class="line">centroids</span><br></pre></td></tr></table></figure>
<pre><code>[[ 0.94619158 -0.94117951]
 [ 3.40877878 -0.10489155]]
[[-1.70351595  0.27408125]
 [ 2.93386365  3.12782785]]
sseSplit, and notSplit:  541.2976292649145 0.0
the bestCentToSplit is:  0
the len of bestClustAss is:  60
[[-2.2226205   2.99958388]
 [-2.07781041  4.61823253]]
[[-1.32962218 -0.58601139]
 [-3.466158    4.32880371]]
[[-0.45965615 -2.7782156 ]
 [-2.94737575  3.3263781 ]]
sseSplit, and notSplit:  67.2202000797829 39.52929868209309
[[3.31450775 4.54204866]
 [2.24406919 1.79975326]]
[[3.26127644 3.86529411]
 [2.66598045 2.52444636]]
[[3.43738162 3.905037  ]
 [2.598185   2.60968842]]
sseSplit, and notSplit:  28.094839828868793 501.7683305828214
the bestCentToSplit is:  0
the len of bestClustAss is:  40


matrix([[-0.45965615, -2.7782156 ],
        [ 2.93386365,  3.12782785],
        [-2.94737575,  3.3263781 ]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plotScatter(datMat3)</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%8D%81%EF%BC%89_24_0.png" alt="png"></p>
<h1 id="实例：对地图上的点进行聚类"><a href="#实例：对地图上的点进行聚类" class="headerlink" title="实例：对地图上的点进行聚类"></a>实例：对地图上的点进行聚类</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distSLC</span><span class="params">(vecA, vecB)</span>:</span><span class="comment">#Spherical Law of Cosines</span></span><br><span class="line">    a = sin(vecA[<span class="number">0</span>,<span class="number">1</span>]*pi/<span class="number">180</span>) * sin(vecB[<span class="number">0</span>,<span class="number">1</span>]*pi/<span class="number">180</span>)</span><br><span class="line">    b = cos(vecA[<span class="number">0</span>,<span class="number">1</span>]*pi/<span class="number">180</span>) * cos(vecB[<span class="number">0</span>,<span class="number">1</span>]*pi/<span class="number">180</span>) * \</span><br><span class="line">                      cos(pi * (vecB[<span class="number">0</span>,<span class="number">0</span>]-vecA[<span class="number">0</span>,<span class="number">0</span>]) /<span class="number">180</span>)</span><br><span class="line">    <span class="keyword">return</span> arccos(a + b)*<span class="number">6371.0</span> <span class="comment">#pi is imported with numpy</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clusterClubs</span><span class="params">(numClust=<span class="number">5</span>)</span>:</span></span><br><span class="line">    datList = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> open(<span class="string">'MLiA_SourceCode/Ch10/places.txt'</span>, <span class="string">'r'</span>).readlines():</span><br><span class="line">        lineArr = line.split(<span class="string">'\t'</span>)</span><br><span class="line">        datList.append([float(lineArr[<span class="number">4</span>]), float(lineArr[<span class="number">3</span>])])</span><br><span class="line">    datMat = mat(datList)</span><br><span class="line">    myCentroids, clustAssing = biKmeans(datMat, numClust, distMeas=distSLC)</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    rect=[<span class="number">0.1</span>,<span class="number">0.1</span>,<span class="number">0.8</span>,<span class="number">0.8</span>]</span><br><span class="line">    scatterMarkers=[<span class="string">'s'</span>, <span class="string">'o'</span>, <span class="string">'^'</span>, <span class="string">'8'</span>, <span class="string">'p'</span>, \</span><br><span class="line">                    <span class="string">'d'</span>, <span class="string">'v'</span>, <span class="string">'h'</span>, <span class="string">'&gt;'</span>, <span class="string">'&lt;'</span>]</span><br><span class="line">    axprops = dict(xticks=[], yticks=[])</span><br><span class="line">    ax0=fig.add_axes(rect, label=<span class="string">'ax0'</span>, **axprops)</span><br><span class="line">    imgP = plt.imread(<span class="string">'MLiA_SourceCode/Ch10/Portland.png'</span>)</span><br><span class="line">    ax0.imshow(imgP)</span><br><span class="line">    ax1=fig.add_axes(rect, label=<span class="string">'ax1'</span>, frameon=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numClust):</span><br><span class="line">        ptsInCurrCluster = datMat[nonzero(clustAssing[:,<span class="number">0</span>].A==i)[<span class="number">0</span>],:]</span><br><span class="line">        markerStyle = scatterMarkers[i % len(scatterMarkers)]</span><br><span class="line">        ax1.scatter(ptsInCurrCluster[:,<span class="number">0</span>].flatten().A[<span class="number">0</span>], ptsInCurrCluster[:,<span class="number">1</span>].flatten().A[<span class="number">0</span>], marker=markerStyle, s=<span class="number">90</span>)</span><br><span class="line">    ax1.scatter(myCentroids[:,<span class="number">0</span>].flatten().A[<span class="number">0</span>], myCentroids[:,<span class="number">1</span>].flatten().A[<span class="number">0</span>], marker=<span class="string">'+'</span>, s=<span class="number">300</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clusterClubs()</span><br></pre></td></tr></table></figure>
<pre><code>[[-122.56739405   45.62557076]
 [-122.63486396   45.51036263]]
[[-122.842918     45.646831  ]
 [-122.62856971   45.5103284 ]]
[[-122.76690133   45.612314  ]
 [-122.62552961   45.50776091]]
[[-122.729442     45.58514429]
 [-122.62063813   45.5040831 ]]
[[-122.74941346   45.545862  ]
 [-122.60434434   45.50451707]]
[[-122.74823556   45.52585431]
 [-122.59648847   45.50821685]]
[[-122.72797062   45.51642875]
 [-122.58031918   45.51010827]]
[[-122.7142141    45.51492203]
 [-122.56818551   45.5102949 ]]
[[-122.70981637   45.51478609]
 [-122.56409551   45.51016235]]
sseSplit, and notSplit:  3073.8303715312386 0.0
the bestCentToSplit is:  0
the len of bestClustAss is:  69
[[-122.74578835   45.53605534]
 [-122.83598851   45.6117388 ]]
[[-122.70552277   45.51052658]
 [-122.842918     45.646831  ]]
sseSplit, and notSplit:  1351.7802960650447 1388.799845546737
[[-122.51444985   45.56152247]
 [-122.6350006    45.49520857]]
[[-122.54062592   45.52653233]
 [-122.607424     45.47994085]]
[[-122.54052872   45.52505652]
 [-122.613193     45.47913283]]
sseSplit, and notSplit:  917.0774766267409 1685.0305259845018
the bestCentToSplit is:  1
the len of bestClustAss is:  37
[[-122.79462233   45.64436218]
 [-122.77702826   45.44723276]]
[[-122.72070683   45.59796783]
 [-122.70730319   45.49559031]]
sseSplit, and notSplit:  1047.9405733077342 917.0774766267409
[[-122.52922184   45.56204495]
 [-122.41440445   45.48137939]]
[[-122.55266787   45.52993361]
 [-122.4009285    45.46897   ]]
sseSplit, and notSplit:  361.2106086859341 1898.9745985610039
[[-122.63507677   45.48340811]
 [-122.60541227   45.407053  ]]
[[-122.6105264   45.4923452]
 [-122.626526    45.413071 ]]
sseSplit, and notSplit:  81.83580692942014 2388.16393003474
the bestCentToSplit is:  0
the len of bestClustAss is:  32
[[-122.82888364   45.5832033 ]
 [-122.71555836   45.59441721]]
[[-122.842918    45.646831 ]
 [-122.6962646   45.5881952]]
sseSplit, and notSplit:  24.09829508946755 1797.0816445068451
[[-122.52145964   45.55057242]
 [-122.48348974   45.49208357]]
[[-122.57237273   45.5439008 ]
 [-122.4927627    45.4967901 ]]
sseSplit, and notSplit:  307.68720928070644 1261.8846458842365
[[-122.60429789   45.49458255]
 [-122.55803361   45.45874909]]
[[-122.61647322   45.49408122]
 [-122.60335233   45.43428767]]
[[-122.6105264   45.4923452]
 [-122.626526    45.413071 ]]
sseSplit, and notSplit:  81.83580692942014 1751.0739773579726
[[-122.78221469   45.49159997]
 [-122.66948399   45.51952507]]
[[-122.7680632    45.4665528 ]
 [-122.66932819   45.51373875]]
[[-122.761804     45.46639582]
 [-122.66733593   45.5169996 ]]
sseSplit, and notSplit:  335.01842722575645 1085.0138820543707
the bestCentToSplit is:  3
the len of bestClustAss is:  26
</code></pre><p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%8D%81%EF%BC%89_27_1.png" alt="png"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>聚类是一种无监督学习方法。所谓无监督学习是指事先不知道要寻找的内容，即没有目标变量。聚类将数据点归到多个簇中，其中相似数据点处于同一簇，而不相似数据点处于不同簇中。聚类可以使用多种不同的方法来计算相似度。</p>
<p>一种广泛使用的聚类算法是K-均值算法，其中k是用户指定的要创建的簇的数目。K-均值聚类算法以k个随机质心开始。算法会计算每一个点到直线的距离。每个点会被分配到距其最近的簇质心，然后紧接着基于新分配到簇的点更新簇质心。重复以上过程数次，直到质心不再改变。</p>
<p>为获得更好的聚类效果，可以使用二分K-均值的聚类算法。二分K-均值算法首先将所有点作为一个簇，然后使用K-均值算法(k=2)对其划分。下一次迭代时，选择有最大误差的簇进行划分。该过程重复直到k个簇创建成功为止。</p>
<p>二分K-均值的聚类效果要好于K-均值算法。</p>
]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>聚类</tag>
        <tag>K-均值</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习总结</title>
    <url>/2018/09/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>Andrew Ng的机器学习入门课程已经全部看完了，笔记也写了一些，这里总结所有所学的内容，说实话，现在完全忘记了开始所学的内容了。</p>
<h1 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a>什么是机器学习</h1><p>Arthur Samuel。他定义机器学习为，在进行特定编程的情况下给予计算学习能力的领域。</p>
<p>Tom Mitchell。他定义的的机器学习是，一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E后，经过P评判，程序再处理T时的性能有所提升。</p>
<p>周志华。他再机器学习一书中的意思是，让机器从数据中学习，进而得到一个更加符合现实规律的模型，通过对模型的使用使得机器比以往表现的更好，这就是机器学习。</p>
<p>我的愚见。机器学习就是在已有的数据中发现规律再寻找符合这个规律的数据。</p>
<h1 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h1><p>回归（房价预测），分类（肿瘤预测），给出特征值与其对应的结果。</p>
<h1 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h1><p>聚类（新闻、邮件的分类），只根据特征值寻找其中的规律。</p>
<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><h2 id="模型表示"><a href="#模型表示" class="headerlink" title="模型表示"></a>模型表示</h2><p>m：训练集中实例的数量</p>
<p>x：特征值/输入变量</p>
<p>y：目标值/输出变量</p>
<p>（x，y）：训练集中的实例<br>
第i个实例：$(x^i, y^i)$
<br>h：学习算法中的解决方案或函数，也称为假设（hypothesis）</p>

$h_\theta(x)=\theta_0+\theta_1x$

<h2 id="线性回归代价函数"><a href="#线性回归代价函数" class="headerlink" title="线性回归代价函数"></a>线性回归代价函数</h2><p>预测函数$h_\theta(x)$是关于$x$的函数,而代价函数是一个关于$(\theta_0,\theta_1)$的函数</p>

$J(\theta_0,\theta_1) = \frac{1}{2m} \sum^m_{i=1}(h_\theta(x^i)-y^i)^2$

优化目标：$minimize J(\theta_0,\theta_1)$

<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2>
梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数$J(\theta_0,\theta_1)$的最小值。
梯度下降背后的思想是：开始时我们随机选择一个参数组合$(\theta_0, \theta_1, ......,\theta_n)$，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到一个局部最小值，因为我们没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否是全局最小值，选择不同的初始参数组合，可能回找到不同的局部最小值。

<p>线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即：<br>
$\frac{\partial }{\partial {{\theta }{j}}}J({{\theta }{0}},{{\theta }{1}})=\frac{\partial }{\partial {{\theta }{j}}}\frac{1}{2m}{{\sum\limits_{i=1}^{m}{\left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}}^{2}}$
<br>
$j=0$ 时：$\frac{\partial }{\partial {{\theta }{0}}}J({{\theta }{0}},{{\theta }{1}})=\frac{1}{m}{{\sum\limits{i=1}^{m}{\left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}}}$
<br>
$j=1$ 时：$\frac{\partial }{\partial {{\theta }{1}}}J({{\theta }{0}},{{\theta }{1}})=\frac{1}{m}\sum\limits{i=1}^{m}{\left( \left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)\cdot {{x}^{(i)}} \right)}$
<br>则算法写成：</p>
<p>Repeat {<br>
​ ${\theta_{0}}:={\theta_{0}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{ \left({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}$
<br>
​ ${\theta_{1}}:={\theta_{1}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{\left( \left({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)\cdot {{x}^{(i)}} \right)}$
<br>
​ }
</p>
<h2 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h2>
尝试将所有特征的尺度都尽量缩放到-1到1之间，
最简单的方法是令：${{x}{n}}=\frac{{{x}{n}}-{{\mu}{n}}}{{{s}{n}}}$，其中 ${\mu_{n}}$是平均值，${s_{n}}$是标准差。

<h2 id="学习速率"><a href="#学习速率" class="headerlink" title="学习速率"></a>学习速率</h2><p>梯度下降算法的每次迭代受到学习率的影响，如果学习率$a$过小，则达到收敛所需的迭代次数会非常高；如果学习率$a$过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。</p>
<p>通常可以考虑尝试些学习率：<br>
$\alpha=0.01，0.03，0.1，0.3，1，3，10$
</p>
<h2 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h2><p>正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\frac{\partial}{\partial{\theta_{j}}}J\left( {\theta_{j}} \right)=0$ 。 假设我们的训练集特征矩阵为 $X$（包含了 ${{x}_{0}}=1$）并且我们的训练集结果为向量 $y$，则利用正规方程解出向量 $\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}y$ 。</p>
<p>梯度下降与正规方程的比较</p>
<table>
<thead>
<tr>
<th>梯度下降</th>
<th>正规方程</th>
</tr>
</thead>
<tbody>
<tr>
<td>需要选择学习速率</td>
<td>不需要</td>
</tr>
<tr>
<td>需要多次迭代</td>
<td>需要计算${{\left( {X^T}X \right)}^{-1}}{X^{T}}$如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂读为$O(n^3)$，通常来说n小于一万时还可以接受</td>
</tr>
<tr>
<td>适用于各种类型的模型</td>
<td>只适用于线性模型，不适合逻辑回归等其他模型</td>
</tr>
</tbody>
</table>
<p>总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数$\theta $的替代方法。具体地说，只要特征变量数量小于一万，通常使用标准方程法，而不使用梯度下降法。</p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>逻辑回归(Logistic Regression)一般用在分类问题中。</p>
<h2 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h2>
$h_\theta(x) = g(\theta^TX)$

$g\left( z \right)=\frac{1}{1+{{e}^{-z}}}$

<p>X代表特征向量，g代表逻辑函数(Logistic function)，常用的逻辑函数为S形函数(Sigmoid function)</p>
<p><img src="2.jpg" alt="image"></p>
<h2 id="判定边界"><a href="#判定边界" class="headerlink" title="判定边界"></a>判定边界</h2><p>在逻辑回归中，我们预测：</p>
<p>当${h_\theta}\left( x \right)&gt;=0.5$时，预测 $y=1$。</p>
<p>当${h_\theta}\left( x \right)&lt;0.5$时，预测 $y=0$。</p>
<p>根据 S 形函数图像，我们知道当</p>
<p>$z=0$ 时 $g(z)=0.5$</p>
<p>$z&gt;0$ 时 $g(z)&gt;0.5$</p>
<p>$z&lt;0$ 时 $g(z)&lt;0.5$</p>
<p>又 $z={\theta^{T}}x$，即：</p>
<p>${\theta^{T}}x&gt;=0$ 时，预测 $y=1$.</p>
<p>${\theta^{T}}x&lt;0$ 时，预测 $y=0$</p>
<p>接下来看价函数</p>
<h2 id="逻辑回归代价函数"><a href="#逻辑回归代价函数" class="headerlink" title="逻辑回归代价函数"></a>逻辑回归代价函数</h2><p>逻辑回归的代价函数为：<br>$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{{Cost}\left( {h_\theta}\left( {x}^{\left( i \right)} \right),{y}^{\left( i \right)} \right)}$.</p>
${h_\theta}\left( x \right)$与 $Cost\left( {h_\theta}\left( x \right),y \right)$之间的关系如下图所示：.<br><br><img src="3.jpg" alt="image"><br><br>这样构建的$Cost\left( {h_\theta}\left( x \right),y \right)$函数的特点是：当实际的 $y=1$ 且${h_\theta}\left( x \right)$也为 1 时误差为 0，当 $y=1$ 但${h_\theta}\left( x \right)$不为1时误差随着${h_\theta}\left( x \right)$变小而变大；当实际的 $y=0$ 且${h_\theta}\left( x \right)$也为 0 时代价为 0，当$y=0$ 但${h_\theta}\left( x \right)$不为 0时误差随着 ${h_\theta}\left( x \right)$的变大而变大。 将构建的 $Cost\left( {h_\theta}\left( x \right),y \right)$简化如下： $Cost\left( {h_\theta}\left( x \right),y \right)=-y\times log\left( {h_\theta}\left( x \right) \right)-(1-y)\times log\left( 1-{h_\theta}\left( x \right) \right)$ 带入代价函数得到：<br><br>$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)]}$.<br><br>即：<br><br>$J\left( \theta \right)=-\frac{1}{m}\sum\limits_{i=1}^{m}{[{{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)+\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)]}$.<br><br>在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为：<br><br>Repeat { $\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j} J(\theta)$ (simultaneously update all ) }
<p>求导后得到：</p>
Repeat { $\theta_j := \theta_j - \alpha \frac{1}{m}\sum\limits_{i=1}^{m}{{\left( {h_\theta}\left( \mathop{x}^{\left( i \right)} \right)-\mathop{y}^{\left( i \right)} \right)}}\mathop{x}_{j}^{(i)}$ (simultaneously update all ) }
<h2 id="高级优化"><a href="#高级优化" class="headerlink" title="高级优化"></a>高级优化</h2><p>共轭梯度法 BFGS (变尺度法) </p>
<p>L-BFGS (限制变尺度法)</p>
<p>线性搜索(line search)</p>
<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>正则化可以改善或者减少过拟合问题。</p>
$...+\frac{\lambda}{2m}\sum\limits_{j=1}^{n}\theta_j^2$
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><p>当特征他多时，需要神经网络。</p>
<h2 id="标记方法"><a href="#标记方法" class="headerlink" title="标记方法"></a>标记方法</h2><p>训练样本数：$m$</p>
<p>输入信号：$x$</p>
<p>输出信号：$y$</p>
<p>神经网络层数：$L$</p>
<p>每层的neuron个数：$S_1$ - $S_L$</p>
<h2 id="神经网络的分类"><a href="#神经网络的分类" class="headerlink" title="神经网络的分类"></a>神经网络的分类</h2><p>二类分类：$S_L = 0, y = 0 or 1$</p>
<p>K类分类：$S_L = k, y_i = 1 (k &gt; 2)$</p>
<p><img src="4.jpg" alt="image"></p>
<h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2>$\newcommand{\subk}[1]{ #1_k }$ $$h_\theta\left(x\right)\in \mathbb{R}^{K}$$ $${\left({h_\theta}\left(x\right)\right)}_{i}={i}^{th} \text{output}$$
$J(\Theta) = -\frac{1}{m} \left[ \sum\limits_{i=1}^{m} \sum\limits_{k=1}^{k} {y_k}^{(i)} \log \subk{(h_\Theta(x^{(i)}))} + \left( 1 - y_k^{(i)} \right) \log \left( 1- \subk{\left( h_\Theta \left( x^{(i)} \right) \right)} \right) \right] + \frac{\lambda}{2m} \sum\limits_{l=1}^{L-1} \sum\limits_{i=1}^{s_l} \sum\limits_{j=1}^{s_l+1} \left( \Theta_{ji}^{(l)} \right)^2$
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>向前传播的算法是:</p>
<p><img src="4.jpg" alt="image"></p>
<p><img src="5.jpg" alt="image"></p>
<p>反向传播的算法就是先正向传播计算出每一层的激活单元，然后利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播计算出直至第二层的所有误差。</p>
在求出了$\Delta_{ij}^{(l)}$之后，我们便可以计算代价函数的偏导数了，计算方法如下：
$ D_{ij}^{(l)} :=\frac{1}{m}\Delta_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}$ ${if}; j \neq 0$
$ D_{ij}^{(l)} :=\frac{1}{m}\Delta_{ij}^{(l)}$ ${if}; j = 0$
<h2 id="神经网络的总结"><a href="#神经网络的总结" class="headerlink" title="神经网络的总结"></a>神经网络的总结</h2><p>网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。</p>
<p>第一层的单元数即我们训练集的特征数量。</p>
<p>最后一层的单元数是我们训练集的结果的类的数量。</p>
<p>如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。</p>
<p>我们真正要决定的是隐藏层的层数和每个中间层的单元数。</p>
<p>训练神经网络：</p>
<p>参数的随机初始化</p>
<p>利用正向传播方法计算所有的$h_{\theta}(x)$</p>
<p>编写计算代价函数 $J$ 的代码</p>
<p>利用反向传播方法计算所有偏导数</p>
<p>利用数值检验方法检验这些偏导数</p>
<p>使用优化算法来最小化代价函数</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习（一）</title>
    <url>/2018/04/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<p>这篇博文是对Andrew Ng的 机器学习入门的学习笔记，<br>关于机器学习已经看了一段时间了，现在开始正式总结一下这段时间所学的东西，一边学习一边记录，希望能够更完这个博文。</p>
<p>首先复习数学，哎，学校学的全还给老师了。。。</p>
<h1 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h1><blockquote>
<p>线性代数是数学的一个分支，它的研究对象是向量，向量空间（或称线性空间），线性变换和有限维的线性方程组。向量空间是现代数学的一个重要课题；因而，线性代数被广泛地应用于抽象代数和泛函分析中；通过解析几何，线性代数得以被具体表示。线性代数的理论已被泛化为算子理论。由于科学研究中的非线性模型通常可以被近似为线性模型，使得线性代数被广泛地应用于自然科学和社会科学中。</p>
</blockquote>
<h2 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>由m$\times$n个数排列成m行n列的矩阵，简称m$\times$记作：</p>
<p><img src="1.png" alt=""></p>
<h3 id="矩阵加法"><a href="#矩阵加法" class="headerlink" title="矩阵加法"></a>矩阵加法</h3><p><img src="2.png" alt="">矩阵的加法满足下列运算律(A，B，C都是同型矩阵)：</p>
<blockquote>
<p>A + B = B + A</p>
</blockquote>
<blockquote>
<p>(A + B) + C = A + (B + C)</p>
</blockquote>
<p>只有行列相同的的矩阵才可以进行加法</p>
<h3 id="矩阵减法"><a href="#矩阵减法" class="headerlink" title="矩阵减法"></a>矩阵减法</h3><p><img src="3.png" alt=""></p>
<h3 id="数乘"><a href="#数乘" class="headerlink" title="数乘"></a>数乘</h3><p><img src="4.png" alt=""><br>矩阵的加减法和矩阵的数乘合称矩阵的线性运算</p>
<h3 id="转置"><a href="#转置" class="headerlink" title="转置"></a>转置</h3><p>把矩阵A的行和列互换产生的新矩阵称之为矩阵A的转置<br><img src="5.png" alt="">矩阵的转置满足一下定律：</p>
<p>$$(A^T)^T = A$$</p>
<p>$(\lambda A^T) = \lambda A^T$</p>
<p>$(AB)^T = B^TA^T$</p>
<h2 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h2><p>两个矩阵能够相乘，当且仅当第一个矩阵A的列数等于第二个矩阵B的行数时才能定义，如果A是$m\times n$的矩阵B是$n\times p$的矩阵，他们的乘积C将是一个$m\times p$的矩阵$C=(C_{ij})$,它的每个元素是：</p>
<p>$c_{i,j} = a_{i,1}b_{1,j} + a_{i,2}b_{2,j} + … + a_{i,n}b{n,j} = \sum_{r=1}^n a_{i,r}b_{r,j}$</p>
<p>记作：C = AB</p>
<p>例如：<br><img src="6.png" alt="">矩阵的乘法满足以下运算律：<br>结合律，分配律，矩阵乘法不满足交换律。</p>
<p>转置:$(AB)^T=B^TA^T$</p>
<ul>
<li>当矩阵A的列数等于矩阵B的行数时，A与B可以相乘。</li>
<li>矩阵C的行数等于矩阵A的行数，C的列数等于B的列数。</li>
<li>乘积C的第m行第n列的元素等于矩阵A的第m行的元素与矩阵B的第n列对应元素乘积之和。</li>
</ul>
<h1 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h1><p>对于机器学习不需要理解的太深入，深入的自己也没学懂&gt;_&lt;,大概就是知道且会求偏导，知道斜率的意义就够了，其他部分太复杂就不记录了。。。好后悔当初没有好好学习微积分</p>
<h1 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h1><p>先验概率和后验概率：</p>
<p>后验概率是指在得到“结果”的信息后重新修正的概率，是“执果寻因”问题中的”果”。先验概率与后验概率有不可分割的联系，后验概率的计算要以先验概率为基础。<br>事情还没有发生，要求这件事情发生的可能性的大小，是先验概率。事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小，是后验概率。<br>先验概率不是根据有关自然状态的全部资料测定的，而只是利用现有的材料(主要是历史资料)计算的；后验概率使用了有关自然状态更加全面的资料，既有先验概率资料，也有补充资料；<br>先验概率的计算比较简单，没有使用贝叶斯公式；而后验概率的计算，要使用贝叶斯公式，而且在利用样本资料计算逻辑概率时，还要使用理论概率分布，需要更多的数理统计知识。</p>
<p>目前先知道这点就够了，具体以后再补充</p>
<p>数学知识复习完毕 正式开始机器学习</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实战（四）</title>
    <url>/2020/03/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
    <content><![CDATA[<p>前两章的分类器只能给出分类结果，而不能给出概率，这一章将学习一个最简单的概率分类器，朴素贝叶斯分类器。之所以称为朴素，是因为整个形式化过程只做最原始，最简单的假设。</p>
<h1 id="基于贝叶斯决策理论的分类方法"><a href="#基于贝叶斯决策理论的分类方法" class="headerlink" title="基于贝叶斯决策理论的分类方法"></a>基于贝叶斯决策理论的分类方法</h1><pre><code>朴素贝叶斯
优点：在数据较少的情况下仍然有效，可以处理多分类问题
缺点：对输入数据的准备方式较为敏感
适用数据类型：标称型数据
</code></pre><p>朴素贝叶斯是贝叶斯理论的一部分，假设我们有一个数据集，它由两类组成</p>
<p><img src="04fig01.jpg" alt=""></p>
<p>我们现在用p1(x,y)，表示数据点(x,y)属于类别1（图中圆点表示的类别）的概率，用p2(x,y)表示数据点(x,y)属于类别2（图中用三角形表示的类别）的概率，那么对于一个新数据点(x,y)数据点，可以用下面的规则来判断它的类别：</p>
<pre><code>如果p1(x,y) &gt; p2(x,y)，那么类别为1
如果p2(x,y) &gt; p1(x,y)，那么类别为2
</code></pre><p>也就是说我们会选择高概率对应的类别，这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。</p>
<h1 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h1><p>$$p(c|x)=\frac{p(x|c)p(c)}{p(x)}$$</p>
<p>读作c在x发生的条件下发生的概率</p>
<h1 id="使用条件概率"><a href="#使用条件概率" class="headerlink" title="使用条件概率"></a>使用条件概率</h1><p>根据上面所说我们可以知道</p>
<p>$$p(c_i|x)=\frac{p(x|c_i)p(c_i)}{p(x)}$$</p>
<pre><code>如果P(c1|x,y) &gt; P(c2|x,y)，那么类别为C1
如果P(c1|x,y) &lt; P(c2|x,y)，那么类别为C2
</code></pre><p>使用贝叶斯准则，我们可以通过已知的三个概率值来计算未知的概率值。</p>
<p>注释：P(c1|x,y)读作：c1在x发生的条件下发生的概率与y的联合概率。联合概率表示两个事件共同发生的概率。A与B的联合概率表示为 P(AB) 或者P(A,B),或者P(A∩B)</p>
<h1 id="使用朴素贝叶斯进行文档分类"><a href="#使用朴素贝叶斯进行文档分类" class="headerlink" title="使用朴素贝叶斯进行文档分类"></a>使用朴素贝叶斯进行文档分类</h1><p>朴素贝叶斯是适用于文档分类的常用算法，我们可以观察文档中出现的词，并把每个词出现或者不出现作为一个特征，这样得到的特征数目就会跟词汇表中的词目一样多。</p>
<p>朴素贝叶斯的一般过程</p>
<ol>
<li>收集数据：可以使用任何方法，本章使用的是RSS源</li>
<li>准备数据：需要数值型或者布尔型数据</li>
<li>分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好</li>
<li>训练算法：计算不同的独立特征的条件概率</li>
<li>测试算法：计算错误率</li>
<li>使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意分类场景中使用。</li>
</ol>
<p>假设词汇表有1000个单词，想要得到好的概率分布，就需要足够的样本，假定样本数为N。由统计学知，如果每个特征需要N个样本，那么对于10个特征将需要$N^{10}$个样本，对于包含1000个特征的词汇表将需要$N^{1000}$个样本。所需要的样本数会随着特征数目增大而迅速增长。</p>
<p>如果特征之间相互独立，那么样本数可以从$N^{1000}$减少到1000×N个。所谓的独立（independence）指的是统计意义上的独立，即一个特征或单词出现的可能性与它和其他单词相邻没有关系。另一个要求是是说每个特征的重要程度是相同的。当然这在现实中是不可能的。</p>
<h1 id="使用Python进行文本分类"><a href="#使用Python进行文本分类" class="headerlink" title="使用Python进行文本分类"></a>使用Python进行文本分类</h1><p>如何从文本中获取特征，我们要构建一个文本词条（token），它是一些单词的组合，然后将一个文本段表示为一个向量词条，其中值为1表示单词出现在文本中，0表示单词未出现在文本中。</p>
<h2 id="准备数据：从文本中构建词向量"><a href="#准备数据：从文本中构建词向量" class="headerlink" title="准备数据：从文本中构建词向量"></a>准备数据：从文本中构建词向量</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    postingList=[[<span class="string">'my'</span>, <span class="string">'dog'</span>, <span class="string">'has'</span>, <span class="string">'flea'</span>, <span class="string">'problems'</span>, <span class="string">'help'</span>, <span class="string">'please'</span>],</span><br><span class="line">                 [<span class="string">'maybe'</span>, <span class="string">'not'</span>, <span class="string">'take'</span>, <span class="string">'him'</span>, <span class="string">'to'</span>, <span class="string">'dog'</span>, <span class="string">'park'</span>, <span class="string">'stupid'</span>],</span><br><span class="line">                 [<span class="string">'my'</span>, <span class="string">'dalmation'</span>, <span class="string">'is'</span>, <span class="string">'so'</span>, <span class="string">'cute'</span>, <span class="string">'I'</span>, <span class="string">'love'</span>, <span class="string">'him'</span>],</span><br><span class="line">                 [<span class="string">'stop'</span>, <span class="string">'posting'</span>, <span class="string">'stupid'</span>, <span class="string">'worthless'</span>, <span class="string">'garbage'</span>],</span><br><span class="line">                 [<span class="string">'mr'</span>, <span class="string">'licks'</span>, <span class="string">'ate'</span>, <span class="string">'my'</span>, <span class="string">'steak'</span>, <span class="string">'how'</span>, <span class="string">'to'</span>, <span class="string">'stop'</span>, <span class="string">'him'</span>],</span><br><span class="line">                 [<span class="string">'quit'</span>, <span class="string">'buying'</span>, <span class="string">'worthless'</span>, <span class="string">'dog'</span>, <span class="string">'food'</span>, <span class="string">'stupid'</span>]]</span><br><span class="line">    classVec = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]    <span class="comment">#1 is abusive, 0 not</span></span><br><span class="line">    <span class="keyword">return</span> postingList,classVec</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="comment"># 创建一个空集</span></span><br><span class="line">    vocabSet = set([])</span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="comment"># 创建两个集合的并集</span></span><br><span class="line">        vocabSet = vocabSet | set(document)</span><br><span class="line">    <span class="keyword">return</span> list(vocabSet)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setOfWords2Vec</span><span class="params">(vocabList, inputSet)</span>:</span></span><br><span class="line">    <span class="comment"># 创建一个所有元素都为0的向量</span></span><br><span class="line">    returnVec = [<span class="number">0</span>]*len(vocabList)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">            returnVec[vocabList.index(word)] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"the word: %s is not in my Vocabulary!"</span> % word)</span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br></pre></td></tr></table></figure>
<p>第一个函数createVocabList()创建了一个实验样本。该函数返回几个切分好的文本词条，已经去除标点符号，第二个返回值是一个类别标签的集合，有两类，侮辱性和非侮辱性。</p>
<p>createVocabList()函数创建了一个包含文档所有单词的列表，列表中没有重复值。</p>
<p>setOfWords2Vec()输入参数是词汇表，和某个文档，输出是这个文档的向量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">listOPosts, listClasses = loadDataSet()</span><br><span class="line">myVocabList = createVocabList(listOPosts)</span><br><span class="line">myVocabList</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;posting&apos;,
 &apos;to&apos;,
 &apos;please&apos;,
 &apos;help&apos;,
 &apos;him&apos;,
 &apos;worthless&apos;,
 &apos;mr&apos;,
 &apos;love&apos;,
 &apos;is&apos;,
 &apos;stop&apos;,
 &apos;has&apos;,
 &apos;stupid&apos;,
 &apos;flea&apos;,
 &apos;I&apos;,
 &apos;quit&apos;,
 &apos;problems&apos;,
 &apos;steak&apos;,
 &apos;cute&apos;,
 &apos;garbage&apos;,
 &apos;food&apos;,
 &apos;park&apos;,
 &apos;dog&apos;,
 &apos;dalmation&apos;,
 &apos;licks&apos;,
 &apos;buying&apos;,
 &apos;ate&apos;,
 &apos;not&apos;,
 &apos;maybe&apos;,
 &apos;take&apos;,
 &apos;so&apos;,
 &apos;how&apos;,
 &apos;my&apos;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">setOfWords2Vec(myVocabList, listOPosts[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[0,
 0,
 1,
 1,
 0,
 0,
 0,
 0,
 0,
 0,
 1,
 0,
 1,
 0,
 0,
 1,
 0,
 0,
 0,
 0,
 0,
 1,
 0,
 0,
 0,
 0,
 0,
 0,
 0,
 0,
 0,
 1]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">setOfWords2Vec(myVocabList, listOPosts[<span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[1,
 0,
 0,
 0,
 0,
 1,
 0,
 0,
 0,
 1,
 0,
 1,
 0,
 0,
 0,
 0,
 0,
 0,
 1,
 0,
 0,
 0,
 0,
 0,
 0,
 0,
 0,
 0,
 0,
 0,
 0,
 0]
</code></pre><h2 id="训练算法：从词向量计算概率"><a href="#训练算法：从词向量计算概率" class="headerlink" title="训练算法：从词向量计算概率"></a>训练算法：从词向量计算概率</h2><p>我们使用前面的贝叶斯公式，将x,y替换位w,w表示一个向量，它由多个数值组成：</p>
<p>$$p(c_i|w)=\frac{p(w|c_i)p(c_i)}{p(w)}$$</p>
<p>计算方法：</p>
<p>$p(c_i)=类别i中的单词数\div总的单词数$</p>
<p>$p(w|c_i)=p(w_0,w_1,w_2…w_N|c_i)=p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)…p(w_N|c_i)$</p>
<p>伪代码：</p>
<pre><code>计算每个类别中的单词数
对每篇训练文档：
    对每个类别：
    如果词条出现在文档中-&gt;增加该词条的计数值
    增加所有词条的计数值
对每个类别：
    对每个词条：
    将该词条的数目除以总词条数目得到条件概率
返回每个类别的条件概率
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix, trainCategory)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化概率</span></span><br><span class="line">    numTrainDocs = len(trainMatrix)</span><br><span class="line">    numWords = len(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    pAbusive = sum(trainCategory)/float(numTrainDocs)</span><br><span class="line">    p0Num = zeros(numWords)</span><br><span class="line">    p1Num = zeros(numWords)</span><br><span class="line">    p0Denom = <span class="number">0.0</span></span><br><span class="line">    p1Denom = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 向量相加</span></span><br><span class="line">            p1Num += trainMatrix[i]</span><br><span class="line">            p1Denom += sum(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += sum(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 对每个元素做除法</span></span><br><span class="line">    p1vect = p1Num/p1Denom</span><br><span class="line">    p0vect = p0Num/p0Denom</span><br><span class="line">    <span class="keyword">return</span> p0vect, p1vect, pAbusive</span><br></pre></td></tr></table></figure>
<p>代码中的输入为文档矩阵trainMatrix，和每篇文档类别标签所构成的向量trainCategory。首先计算侮辱性文档（class=1）的概率，即P(1).因为这是个二分类问题，所有可以通过计算p(0)=1-p(1)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trainMat = []</span><br><span class="line"><span class="keyword">for</span> postinDoc <span class="keyword">in</span> listOPosts:</span><br><span class="line">    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">p0V, p1V, pAb = trainNB0(trainMat, listClasses)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">p0V</span><br></pre></td></tr></table></figure>
<pre><code>array([0.        , 0.04166667, 0.04166667, 0.04166667, 0.08333333,
       0.        , 0.04166667, 0.04166667, 0.04166667, 0.04166667,
       0.04166667, 0.        , 0.04166667, 0.04166667, 0.        ,
       0.04166667, 0.04166667, 0.04166667, 0.        , 0.        ,
       0.        , 0.04166667, 0.04166667, 0.04166667, 0.        ,
       0.04166667, 0.        , 0.        , 0.        , 0.04166667,
       0.04166667, 0.125     ])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">p1V</span><br></pre></td></tr></table></figure>
<pre><code>array([0.05263158, 0.05263158, 0.        , 0.        , 0.05263158,
       0.10526316, 0.        , 0.        , 0.        , 0.05263158,
       0.        , 0.15789474, 0.        , 0.        , 0.05263158,
       0.        , 0.        , 0.        , 0.05263158, 0.05263158,
       0.05263158, 0.10526316, 0.        , 0.        , 0.05263158,
       0.        , 0.05263158, 0.05263158, 0.05263158, 0.        ,
       0.        , 0.        ])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pAb</span><br></pre></td></tr></table></figure>
<pre><code>0.5
</code></pre><h2 id="测试算法：根据实际情况修改分类器"><a href="#测试算法：根据实际情况修改分类器" class="headerlink" title="测试算法：根据实际情况修改分类器"></a>测试算法：根据实际情况修改分类器</h2><p>在计算多个概率的乘积一获得分档属于某个类别的概率，即计算$p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)$时候，如果其中一个概率的值为0，那么最后的乘积也为0，为了降低这种影响，我们将所有词出现的次数初始化为1，将分母初始化为2。</p>
<p>另一个遇到的问题是下溢，是由于太多的很小的数相乘造成的，可以求对数避免下溢。</p>
<p><img src="04fig04.jpg" alt=""></p>
<p>观察上图发现，f(x)和ln(f(x))的曲线趋势是相同的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix, trainCategory)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化概率</span></span><br><span class="line">    numTrainDocs = len(trainMatrix)</span><br><span class="line">    numWords = len(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    pAbusive = sum(trainCategory)/float(numTrainDocs)</span><br><span class="line">    <span class="comment"># 初始化为 1</span></span><br><span class="line">    p0Num = ones(numWords)</span><br><span class="line">    p1Num = ones(numWords)</span><br><span class="line">    <span class="comment"># 分母改为 2</span></span><br><span class="line">    p0Denom = <span class="number">2.0</span></span><br><span class="line">    p1Denom = <span class="number">2.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 向量相加</span></span><br><span class="line">            p1Num += trainMatrix[i]</span><br><span class="line">            p1Denom += sum(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += sum(trainMatrix[i])</span><br><span class="line">    <span class="comment"># 对每个元素做除法，并求对数</span></span><br><span class="line">    p1vect = log(p1Num/p1Denom)</span><br><span class="line">    p0vect = log(p0Num/p0Denom)</span><br><span class="line">    <span class="keyword">return</span> p0vect, p1vect, pAbusive</span><br></pre></td></tr></table></figure>
<p>朴素贝叶斯分类函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifyNB</span><span class="params">(vec2Classify, p0Vec, p1Vec, pClass1)</span>:</span></span><br><span class="line">    p1 = sum(vec2Classify * p1Vec) + log(pClass1)</span><br><span class="line">    p0 = sum(vec2Classify * p0Vec) + log(<span class="number">1.0</span>-pClass1)</span><br><span class="line">    <span class="keyword">if</span> p1 &gt; p0:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testingNB</span><span class="params">()</span>:</span></span><br><span class="line">    listOposts, listClasses = loadDataSet()</span><br><span class="line">    myVocabList = createVocabList(listOPosts)</span><br><span class="line">    trainMat = []</span><br><span class="line">    <span class="keyword">for</span> postinDoc <span class="keyword">in</span> listOposts:</span><br><span class="line">        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))</span><br><span class="line">    p0V, p1V, pAb = trainNB0(array(trainMat), array(listClasses))</span><br><span class="line">    testEntry = [<span class="string">'love'</span>, <span class="string">'my'</span>, <span class="string">'dalmation'</span>]</span><br><span class="line">    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    print(testEntry, <span class="string">'classif as '</span>, classifyNB(thisDoc, p0V, p1V, pAb))</span><br><span class="line">    testEntry = [<span class="string">'stupid'</span>, <span class="string">'garbage'</span>]</span><br><span class="line">    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    print(testEntry, <span class="string">'classif as '</span>, classifyNB(thisDoc, p0V, p1V, pAb))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">testingNB()</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;love&apos;, &apos;my&apos;, &apos;dalmation&apos;] classif as  0
[&apos;stupid&apos;, &apos;garbage&apos;] classif as  1
</code></pre><p>测试结果，第一句话是非侮辱性的，第二句是侮辱性的，分类正确</p>
<h2 id="准备数据：文档词袋模型"><a href="#准备数据：文档词袋模型" class="headerlink" title="准备数据：文档词袋模型"></a>准备数据：文档词袋模型</h2><p>每个词的出现次数作为一个特征，这个可以被描述为词集模型（set of word model），如果一个词在文档中的出现不止一次，这种方法被称为词袋模型（bag of words model），修改setOfWords2Vec()函数为bagOfWords2Vec()</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bagOfWords2Vec</span><span class="params">(vocabList, inputSet)</span>:</span></span><br><span class="line">    returnVec = [<span class="number">0</span>]*len(vocabList)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">            returnVec[vocabList.index(word)] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br></pre></td></tr></table></figure>
<p>现在分类器已经构建好了，下面利用该分类器过滤垃圾邮件。</p>
<h1 id="实例：使用朴素贝叶斯过滤垃圾邮件"><a href="#实例：使用朴素贝叶斯过滤垃圾邮件" class="headerlink" title="实例：使用朴素贝叶斯过滤垃圾邮件"></a>实例：使用朴素贝叶斯过滤垃圾邮件</h1><ol>
<li>收集数据：提供的文本文件</li>
<li>准备数：将文本文件解析成词条向量</li>
<li>分析数据：检查词条确保解析的正确性</li>
<li>训练算法：使用我们之前建立的trainBN()函数</li>
<li>测试算法：使用classifyNB()，并且构建一个新的测试函数来计算文档集的错误</li>
<li>使用算法：构建一个完整的程序过程对一组文档进行分类，将错分的文档输出到屏幕上</li>
</ol>
<h2 id="准备数据：切分文本"><a href="#准备数据：切分文本" class="headerlink" title="准备数据：切分文本"></a>准备数据：切分文本</h2><p>使用python的string.split()方法切分</p>
<p>使用re.compile(‘\W*’)去除标点和数字。</p>
<p>去除空字符串</p>
<p>使用.lower()转换为小写</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">regEx = re.compile(<span class="string">'\W'</span>)</span><br><span class="line">emailText = open(<span class="string">'MLiA_SourceCode/machinelearninginaction/Ch04/email/ham/6.txt'</span>).read()</span><br><span class="line">listOfTokens = regEx.split(emailText)</span><br><span class="line">listOfTokens = [tok.lower() <span class="keyword">for</span> tok <span class="keyword">in</span> listOfTokens <span class="keyword">if</span> len(tok) &gt; <span class="number">0</span> <span class="keyword">and</span> re.search(<span class="string">'[^0-9]'</span>, tok)]</span><br></pre></td></tr></table></figure>
<h2 id="测试算法：使用朴素贝叶斯进行交叉验证"><a href="#测试算法：使用朴素贝叶斯进行交叉验证" class="headerlink" title="测试算法：使用朴素贝叶斯进行交叉验证"></a>测试算法：使用朴素贝叶斯进行交叉验证</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textParse</span><span class="params">(bigString)</span>:</span></span><br><span class="line">    listOfTokens = re.split(<span class="string">r'\W'</span>, bigString)</span><br><span class="line">    <span class="keyword">return</span> [tok.lower() <span class="keyword">for</span> tok <span class="keyword">in</span> listOfTokens <span class="keyword">if</span> len(tok) &gt; <span class="number">2</span> <span class="keyword">and</span> re.search(<span class="string">'[^0-9]'</span>, tok)]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spamTest</span><span class="params">()</span>:</span></span><br><span class="line">    docList = []</span><br><span class="line">    classList = []</span><br><span class="line">    fullText = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">26</span>):</span><br><span class="line">        wordList = textParse(open(<span class="string">'MLiA_SourceCode/machinelearninginaction/Ch04/email/spam/%d.txt'</span> % i).read())</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">1</span>)</span><br><span class="line">        wordList = textParse(open(<span class="string">'MLiA_SourceCode/machinelearninginaction/Ch04/email/ham/%d.txt'</span> % i).read())</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">0</span>)</span><br><span class="line">    vocabList = createVocabList(docList)</span><br><span class="line">    trainingSet = list(range(<span class="number">50</span>))</span><br><span class="line">    testSet = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        randIndex = int(random.uniform(<span class="number">0</span>, len(trainingSet)))</span><br><span class="line">        testSet.append(trainingSet[randIndex])</span><br><span class="line">        <span class="keyword">del</span>(trainingSet[randIndex])</span><br><span class="line">    trainMat = []</span><br><span class="line">    trainClasses = []</span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> trainingSet:</span><br><span class="line">        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))</span><br><span class="line">        trainClasses.append(classList[docIndex])</span><br><span class="line">    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> testSet:</span><br><span class="line">        wordVector = setOfWords2Vec(vocabList, docList[docIndex])</span><br><span class="line">        <span class="keyword">if</span> classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:</span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line">            print(<span class="string">"classification error"</span>,docList[docIndex])</span><br><span class="line">    print(<span class="string">'the error rate is '</span>, float(errorCount)/len(testSet))</span><br><span class="line">    <span class="keyword">return</span> float(errorCount)/len(testSet)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spamTest()</span><br></pre></td></tr></table></figure>
<pre><code>the error rate is  0.0

0.0
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spamTest()</span><br></pre></td></tr></table></figure>
<pre><code>classification error [&apos;this&apos;, &apos;mail&apos;, &apos;was&apos;, &apos;sent&apos;, &apos;from&apos;, &apos;notification&apos;, &apos;only&apos;, &apos;address&apos;, &apos;that&apos;, &apos;cannot&apos;, &apos;accept&apos;, &apos;incoming&apos;, &apos;mail&apos;, &apos;please&apos;, &apos;not&apos;, &apos;reply&apos;, &apos;this&apos;, &apos;message&apos;, &apos;thank&apos;, &apos;you&apos;, &apos;for&apos;, &apos;your&apos;, &apos;online&apos;, &apos;reservation&apos;, &apos;the&apos;, &apos;store&apos;, &apos;you&apos;, &apos;selected&apos;, &apos;has&apos;, &apos;located&apos;, &apos;the&apos;, &apos;item&apos;, &apos;you&apos;, &apos;requested&apos;, &apos;and&apos;, &apos;has&apos;, &apos;placed&apos;, &apos;hold&apos;, &apos;your&apos;, &apos;name&apos;, &apos;please&apos;, &apos;note&apos;, &apos;that&apos;, &apos;all&apos;, &apos;items&apos;, &apos;are&apos;, &apos;held&apos;, &apos;for&apos;, &apos;day&apos;, &apos;please&apos;, &apos;note&apos;, &apos;store&apos;, &apos;prices&apos;, &apos;may&apos;, &apos;differ&apos;, &apos;from&apos;, &apos;those&apos;, &apos;online&apos;, &apos;you&apos;, &apos;have&apos;, &apos;questions&apos;, &apos;need&apos;, &apos;assistance&apos;, &apos;with&apos;, &apos;your&apos;, &apos;reservation&apos;, &apos;please&apos;, &apos;contact&apos;, &apos;the&apos;, &apos;store&apos;, &apos;the&apos;, &apos;phone&apos;, &apos;number&apos;, &apos;listed&apos;, &apos;below&apos;, &apos;you&apos;, &apos;can&apos;, &apos;also&apos;, &apos;access&apos;, &apos;store&apos;, &apos;information&apos;, &apos;such&apos;, &apos;store&apos;, &apos;hours&apos;, &apos;and&apos;, &apos;location&apos;, &apos;the&apos;, &apos;web&apos;, &apos;http&apos;, &apos;www&apos;, &apos;borders&apos;, &apos;com&apos;, &apos;online&apos;, &apos;store&apos;, &apos;storedetailview_98&apos;]
the error rate is  0.1

0.1
</code></pre><p>每一次得到的错误率都不同，要想更好的评估错误率，可以重复多次，十次计算求平均错误率为6%</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">errorRate = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    errorRate += spamTest()</span><br><span class="line">errorRate/<span class="number">10</span></span><br></pre></td></tr></table></figure>
<pre><code>the error rate is  0.2
the error rate is  0.1
the error rate is  0.0
the error rate is  0.0
the error rate is  0.1
the error rate is  0.1
the error rate is  0.1
the error rate is  0.0
the error rate is  0.1
the error rate is  0.0

0.06999999999999999
</code></pre><h1 id="实例：使用朴素贝叶斯分类器从个人广告中获取区域倾向"><a href="#实例：使用朴素贝叶斯分类器从个人广告中获取区域倾向" class="headerlink" title="实例：使用朴素贝叶斯分类器从个人广告中获取区域倾向"></a>实例：使用朴素贝叶斯分类器从个人广告中获取区域倾向</h1><p>下面将使用来自不同城市的广告训练一个分类器，然后观察分类的效果，我们的目的不是使用该分类器进行分类，而是通过观察单词和条件概率值来发现与特定城市相关的内容，</p>
<h2 id="收集数据：导入RSS源"><a href="#收集数据：导入RSS源" class="headerlink" title="收集数据：导入RSS源"></a>收集数据：导入RSS源</h2><p>利用python下载RSS的文本。</p>
<p>首先需要安装feedparser,<a href="https://github.com/kurtmckee/feedparser" target="_blank" rel="noopener">https://github.com/kurtmckee/feedparser</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%pip install feedparser</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> feedparser</span><br></pre></td></tr></table></figure>
<p>接下来作者使用了RSS源<a href="http://newyork.craigslist.org/stp/index.rss" target="_blank" rel="noopener">http://newyork.craigslist.org/stp/index.rss</a> 已经不能访问了<br>书中作者的意思是以来自源 <a href="http://newyork.craigslist.org/stp/index.rss" target="_blank" rel="noopener">http://newyork.craigslist.org/stp/index.rss</a> 中的文章作为分类为1的文章，以来自源 <a href="http://sfbay.craigslist.org/stp/index.rss" target="_blank" rel="noopener">http://sfbay.craigslist.org/stp/index.rss</a> 中的文章作为分类为0的文章</p>
<p>为了能够跑通示例代码，可以找两可用的RSS源作为替代。</p>
<p>我用的是这两个源：</p>
<p>NASA Image of the Day：<a href="http://www.nasa.gov/rss/dyn/image_of_the_day.rss" target="_blank" rel="noopener">http://www.nasa.gov/rss/dyn/image_of_the_day.rss</a></p>
<p>Yahoo Sports - NBA - Houston Rockets News：<a href="http://sports.yahoo.com/nba/teams/hou/rss.xml" target="_blank" rel="noopener">http://sports.yahoo.com/nba/teams/hou/rss.xml</a></p>
<p>也就是说，如果算法运行正确的话，所有来自于 nasa 的文章将会被分类为1，所有来自于yahoo sports的休斯顿火箭队新闻将会分类为0</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ny=feedparser.parse(<span class="string">'https://www.nasa.gov/rss/dyn/image_of_the_day.rss'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">len(ny[<span class="string">'entries'</span>])</span><br></pre></td></tr></table></figure>
<pre><code>60
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcMostFreq</span><span class="params">(vocabList,fullText)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> operator</span><br><span class="line">    freqDict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> vocabList:</span><br><span class="line">        freqDict[token]=fullText.count(token)</span><br><span class="line">    sortedFreq = sorted(freqDict.items(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedFreq[:<span class="number">30</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">localWords</span><span class="params">(feed1,feed0)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> feedparser</span><br><span class="line">    docList=[]</span><br><span class="line">    classList = []</span><br><span class="line">    fullText =[]</span><br><span class="line">    minLen = min(len(feed1[<span class="string">'entries'</span>]),len(feed0[<span class="string">'entries'</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(minLen):</span><br><span class="line">        wordList = textParse(feed1[<span class="string">'entries'</span>][i][<span class="string">'summary'</span>])</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">1</span>) <span class="comment">#NY is class 1</span></span><br><span class="line">        wordList = textParse(feed0[<span class="string">'entries'</span>][i][<span class="string">'summary'</span>])</span><br><span class="line">        docList.append(wordList)</span><br><span class="line">        fullText.extend(wordList)</span><br><span class="line">        classList.append(<span class="number">0</span>)</span><br><span class="line">    vocabList = createVocabList(docList)<span class="comment">#create vocabulary</span></span><br><span class="line">    top30Words = calcMostFreq(vocabList,fullText)   <span class="comment">#remove top 30 words</span></span><br><span class="line">    <span class="keyword">for</span> pairW <span class="keyword">in</span> top30Words:</span><br><span class="line">        <span class="keyword">if</span> pairW[<span class="number">0</span>] <span class="keyword">in</span> vocabList: vocabList.remove(pairW[<span class="number">0</span>])</span><br><span class="line">    trainingSet = list(range(<span class="number">2</span>*minLen))</span><br><span class="line">    testSet=[]           <span class="comment">#create test set</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        randIndex = int(random.uniform(<span class="number">0</span>,len(trainingSet)))</span><br><span class="line">        testSet.append(trainingSet[randIndex])</span><br><span class="line">        <span class="keyword">del</span>(trainingSet[randIndex])  </span><br><span class="line">    trainMat=[]; trainClasses = []</span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> trainingSet:<span class="comment">#train the classifier (get probs) trainNB0</span></span><br><span class="line">        trainMat.append(bagOfWords2Vec(vocabList, docList[docIndex]))</span><br><span class="line">        trainClasses.append(classList[docIndex])</span><br><span class="line">    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))</span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> docIndex <span class="keyword">in</span> testSet:        <span class="comment">#classify the remaining items</span></span><br><span class="line">        wordVector = bagOfWords2Vec(vocabList, docList[docIndex])</span><br><span class="line">        <span class="keyword">if</span> classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:</span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line">    print(<span class="string">'the error rate is: '</span>,float(errorCount)/len(testSet))</span><br><span class="line">    <span class="keyword">return</span> vocabList,p0V,p1V</span><br></pre></td></tr></table></figure>
<p>calcMostFreq()函数的功能是遍历词汇表中的每个词并统计它在文本中出现的次数，然后根据出现次数从高到低对词典进行排序，返回排序最高的30个单词。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ny=feedparser.parse(<span class="string">'http://www.nasa.gov/rss/dyn/image_of_the_day.rss'</span>)</span><br><span class="line">sf=feedparser.parse(<span class="string">'http://sports.yahoo.com/nba/teams/hou/rss.xml'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vocabList, pSF, pNY=localWords(ny, sf)</span><br></pre></td></tr></table></figure>
<pre><code>the error rate is:  0.5
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vocabList, pSF, pNY=localWords(ny, sf)</span><br></pre></td></tr></table></figure>
<pre><code>the error rate is:  0.35
</code></pre><h2 id="分析数据：显示地域相关的用词"><a href="#分析数据：显示地域相关的用词" class="headerlink" title="分析数据：显示地域相关的用词"></a>分析数据：显示地域相关的用词</h2><p>先对pSF和pNY进行排序，然后按照顺序将词打印出来。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getTopWords</span><span class="params">(ny,sf)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> operator</span><br><span class="line">    vocabList,p0V,p1V=localWords(ny,sf)</span><br><span class="line">    topNY=[]; topSF=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(p0V)):</span><br><span class="line">        <span class="keyword">if</span> p0V[i] &gt; <span class="number">-6.0</span> : topSF.append((vocabList[i],p0V[i]))</span><br><span class="line">        <span class="keyword">if</span> p1V[i] &gt; <span class="number">-6.0</span> : topNY.append((vocabList[i],p1V[i]))</span><br><span class="line">    sortedSF = sorted(topSF, key=<span class="keyword">lambda</span> pair: pair[<span class="number">1</span>], reverse=<span class="keyword">True</span>)</span><br><span class="line">    print(<span class="string">"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**"</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> sortedSF:</span><br><span class="line">        print(item[<span class="number">0</span>])</span><br><span class="line">    sortedNY = sorted(topNY, key=<span class="keyword">lambda</span> pair: pair[<span class="number">1</span>], reverse=<span class="keyword">True</span>)</span><br><span class="line">    print(<span class="string">"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**"</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> sortedNY:</span><br><span class="line">        print(item[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">getTopWords(ny,sf)</span><br></pre></td></tr></table></figure>
<pre><code>the error rate is:  0.2
SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**
westbrook
but
fund
michael
los
also
amid
NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**
arms
station
program
agency
spacecraft
milky
</code></pre><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>对于分类而言，使用概率有时要比使用硬规则更为有效。贝叶斯概率即贝叶斯准则提供了一种利用一直值来估计未知概率的有效方法。</p>
<p>可以通过特征之间的条件独立性假设，降低对数据量的需求。独立性假设是指一个词出现的概率不依赖与文档中的其他词。</p>
<p>编程贝叶斯时需要考虑很多实际因素。下溢出就是其中之一，可以通过对概率取对数来解决。还有其他方法改进，比如移除停用词。</p>
]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习策略</title>
    <url>/2019/04/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5/</url>
    <content><![CDATA[<p>当我们的系统达到了90%的准确率时，觉得还是不够好，我们有很多想法去改善我们的系统，比如，去收集更多的训练数据，收集更多不同姿势图片丰富样本的多样性，或者更多的反例集。或者再用梯度下降训练算法，训练久一点。或者尝试用一个完全不同的优化算法，比如Adam优化算法。或者尝试使用规模更大或者更小的神经网络。或者试试dropout或者L2正则化。或者想要修改网络的架构，比如修改激活函数，改变隐藏单元的数目之类的方法。</p>
<p>如何选择更好的方法而不是浪费时间，这里记录下一些策略。</p>
<h1 id="单一评估指标"><a href="#单一评估指标" class="headerlink" title="单一评估指标"></a>单一评估指标</h1><p>percision：查准率</p>
<pre><code>被你的分类器中标记为真的例子中，有多少真的为真
</code></pre><p>recall：查全率</p>
<pre><code>对于所有为真的例子，有多少被识别出来
</code></pre><p>查准率和查全率之间往往需要折衷，两个指标都要顾及到，用F1分数可以更好的衡量系统的优劣</p>

$F_1 = \frac{2}{\frac{1}{P} + \frac{1}{R}}$

<p>在数学中，这个函数叫做查准率P和查全率R的调和平均数</p>
<h1 id="满足和优化指标"><a href="#满足和优化指标" class="headerlink" title="满足和优化指标"></a>满足和优化指标</h1><p>除了F1分数或者其它衡量准确度的指标外，我们还要考虑运行时间，就是需要多长时间来分类一张图。A分类器需要80毫秒，B需要95毫秒，C需要1500毫秒，就是说需要1.5秒来分类图像。</p>
<p><img src="0.png" alt="image"></p>
<p>如何选取上图中的分类器，可以这样判断一下代价</p>
<pre><code>cost = accuracy - 0.5 × running time
</code></pre><p>这个方法可能太过刻意，当然在具体情况具体考虑。</p>
<h1 id="训练-开发-测试集合"><a href="#训练-开发-测试集合" class="headerlink" title="训练/开发/测试集合"></a>训练/开发/测试集合</h1><p>设立训练集，开发集和测试集的方式大大影响了建立机器学习应用方面取得进展的速度。所以，我们希望最终应用的目标数据是和训练集合来自同一处。比如，我们想要做手机摄像头识别猫，训练集最好来自手机拍摄的图片而不是在网上爬取的很清晰的图片或者卡通猫之类。</p>
<h2 id="开发集和测试集的大小"><a href="#开发集和测试集的大小" class="headerlink" title="开发集和测试集的大小"></a>开发集和测试集的大小</h2><p>如果你总共有100个样本，这样70/30或者60/20/20分的经验法则是相当合理的。如果你有几千个样本或者有一万个样本，这些做法也还是合理的。</p>
<p>但在现代机器学习中，我们更习惯操作规模大得多的数据集，比如说你有1百万个训练样本，这样分可能更合理，98%作为训练集，1%开发集，1%测试集。</p>
<h1 id="错误率指标"><a href="#错误率指标" class="headerlink" title="错误率指标"></a>错误率指标</h1>
$Error = \frac{1}{\sum_{}^{}w^{(i)}}\sum_{i = 1}^{m_{{dev}}}{w^{(i)}I\{ y_{{pred}}^{(i)} \neq y^{(i)}\}}$

<h1 id="可避免偏差"><a href="#可避免偏差" class="headerlink" title="可避免偏差"></a>可避免偏差</h1><p>我们使用猫分类器来做例子，比如人类具有近乎完美的准确度，所以人类水平的错误是1%。在这种情况下，如果我们的学习算法达到8%的训练错误率和10%的开发错误率，你的算法在训练集上的表现和人类水平的表现有很大差距的话，说明你的算法对训练集的拟合并不好。所以从减少偏差和方差的工具这个角度看，在这种情况下，我会把重点放在减少偏差上。需要做的是，比如说训练更大的神经网络，或者跑久一点梯度下降，就试试能不能在训练集上做得更好。</p>
<h1 id="进行误差分析"><a href="#进行误差分析" class="headerlink" title="进行误差分析"></a>进行误差分析</h1><p>进行错误分析，应该找一组错误样本，可能在你的开发集里或者测试集里，观察错误标记的样本，看看假阳性（false positives）和假阴性（false negatives），统计属于不同错误类型的错误数量。在这个过程中，可能会得到启发，归纳出新的错误类型。如果过了一遍错误样本，然后说，天，有这么多Instagram滤镜或Snapchat滤镜，这些滤镜干扰了我的分类器，你就可以在途中新建一个错误类型。总之，通过统计不同错误标记类型占总数的百分比，可以帮发现哪些问题需要优先解决，或者构思新优化方向的灵感。</p>
<p>检查是否有错误的标记，当我们的数据量较少时，应该检查下是否有错误的标记。</p>
<h1 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h1><p>迁移学习最有用的场合是，如果你尝试优化任务B的性能，通常这个任务数据相对较少，例如，在放射科中你知道很难收集很多X射线扫描图来搭建一个性能良好的放射科诊断系统，所以在这种情况下，你可能会找一个相关但不同的任务，如图像识别，其中你可能用1百万张图片训练过了，并从中学到很多低层次特征，所以那也许能帮助网络在任务B在放射科任务上做得更好，尽管任务B没有这么多数据。迁移学习什么时候是有意义的？它确实可以显著提高你的学习任务的性能，但任务A实际上数据量比任务B要少，这种情况下增益可能不多。</p>
<h1 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h1><p>在多任务学习中，是同时开始学习的，试图让单个神经网络同时做几件事情，然后希望这里每个任务都能帮到其他所有任务。</p>
<p>多任务学习能让你训练一个神经网络来执行许多任务，这可以给你更高的性能，比单独完成各个任务更高的性能。但要注意，实际上迁移学习比多任务学习使用频率更高。我看到很多任务都是，如果你想解决一个机器学习问题，但你的数据集相对较小，那么迁移学习真的能帮到你，就是如果你找到一个相关问题，其中数据量要大得多，你就能以它为基础训练你的神经网络，然后迁移到这个数据量很少的任务上来。</p>
<h1 id="端到端的深度学习"><a href="#端到端的深度学习" class="headerlink" title="端到端的深度学习"></a>端到端的深度学习</h1><p>简而言之，以前有一些数据处理系统或者学习系统，它们需要多个阶段的处理。那么端到端深度学习就是忽略所有这些不同的阶段，用单个神经网络代替它。</p>
<p>我们来看一些例子，以语音识别为例，你的目标是输入，比如说一段音频，然后把它映射到一个输出，就是这段音频的听写文本。所以传统上，语音识别需要很多阶段的处理。首先你会提取一些特征，一些手工设计的音频特征，也许你听过MFCC，这种算法是用来从音频中提取一组特定的人工设计的特征。在提取出一些低层次特征之后，你可以应用机器学习算法在音频片段中找到音位，所以音位是声音的基本单位，比如说“Cat”这个词是三个音节构成的，Cu-、Ah-和Tu-，算法就把这三个音位提取出来，然后你将音位串在一起构成独立的词，然后你将词串起来构成音频片段的听写文本。</p>
<p>所以和这种有很多阶段的流水线相比，端到端深度学习做的是，你训练一个巨大的神经网络，输入就是一段音频，输出直接是听写文本。AI的其中一个有趣的社会学效应是，随着端到端深度学习系统表现开始更好，有一些花了大量时间或者整个事业生涯设计出流水线各个步骤的研究员，还有其他领域的研究员，不只是语言识别领域的，也许是计算机视觉，还有其他领域，他们花了大量的时间，写了很多论文，有些甚至整个职业生涯的一大部分都投入到开发这个流水线的功能或者其他构件上去了。而端到端深度学习就只需要把训练集拿过来，直接学到了和之间的函数映射，直接绕过了其中很多步骤。对一些学科里的人来说，这点相当难以接受，他们无法接受这样构建AI系统，因为有些情况，端到端方法完全取代了旧系统，某些投入了多年研究的中间组件也许已经过时了。</p>
<p>事实证明，端到端深度学习的挑战之一是，你可能需要大量数据才能让系统表现良好，比如，你只有3000小时数据去训练你的语音识别系统，那么传统的流水线效果真的很好。但当你拥有非常大的数据集时，比如10,000小时数据或者100,000小时数据，这样端到端方法突然开始很厉害了。所以当你的数据集较小的时候，传统流水线方法其实效果也不错，通常做得更好。你需要大数据集才能让端到端方法真正发出耀眼光芒。如果你的数据量适中，那么也可以用中间件方法，你可能输入还是音频，然后绕过特征提取，直接尝试从神经网络输出音位，然后也可以在其他阶段用，所以这是往端到端学习迈出的一小步，但还没有到那里。</p>
<p>另一个例子，比如说你希望观察一个孩子手部的X光照片，并估计一个孩子的年龄。</p>
<p>处理这个例子的一个非端到端方法，就是照一张图，然后分割出每一块骨头，所以就是分辨出那段骨头应该在哪里，那段骨头在哪里，那段骨头在哪里，等等。然后，知道不同骨骼的长度，你可以去查表，查到儿童手中骨头的平均长度，然后用它来估计孩子的年龄，所以这种方法实际上很好。</p>
<p>相比之下，如果你直接从图像去判断孩子的年龄，那么你需要大量的数据去直接训练。据我所知，这种做法今天还是不行的，因为没有足够的数据来用端到端的方式来训练这个任务。</p>
<p>你可以想象一下如何将这个问题分解成两个步骤，第一步是一个比较简单的问题，也许你不需要那么多数据，也许你不需要许多X射线图像来切分骨骼。而任务二，收集儿童手部的骨头长度的统计数据，你不需要太多数据也能做出相当准确的估计，所以这个多步方法看起来很有希望，也许比端对端方法更有希望，至少直到你能获得更多端到端学习的数据之前。</p>
<p>所以端到端深度学习系统是可行的，它表现可以很好，也可以简化系统架构，让你不需要搭建那么多手工设计的单独组件。</p>
<h2 id="是否要使用端到端的深度学习"><a href="#是否要使用端到端的深度学习" class="headerlink" title="是否要使用端到端的深度学习"></a>是否要使用端到端的深度学习</h2><p>应用端到端学习的一些好处，首先端到端学习真的只是让数据说话。所以如果你有足够多的数据，那么不管从x到y最适合的函数映射是什么，如果你训练一个足够大的神经网络，希望这个神经网络能自己搞清楚，而使用纯机器学习方法，直接从到输入去训练的神经网络，可能更能够捕获数据中的任何统计信息，而不是被迫引入人类的成见。</p>
<p>例如，在语音识别领域，早期的识别系统有这个音位概念，就是基本的声音单元，如cat单词的“cat”的Cu-、Ah-和Tu-，我觉得这个音位是人类语言学家生造出来的，我实际上认为音位其实是语音学家的幻想，用音位描述语言也还算合理。但是不要强迫你的学习算法以音位为单位思考，这点有时没那么明显。如果你让你的学习算法学习它想学习的任意表示方式，而不是强迫你的学习算法使用音位作为表示方式，那么其整体表现可能会更好。</p>
<p>端到端深度学习的第二个好处就是这样，所需手工设计的组件更少，所以这也许能够简化你的设计工作流程，你不需要花太多时间去手工设计功能，手工设计这些中间表示方式。</p>
<p>这里有一些缺点，首先，它可能需要大量的数据。要直接学到这个到的映射。</p>
<p>另一个缺点是，它排除了可能有用的手工设计组件。如果你没有很多数据，你的学习算法就没办法从很小的训练集数据中获得洞察力。当你有大量数据时，手工设计的东西就不太重要了，但是当你没有太多的数据时，构造一个精心设计的系统，实际上可以将人类对这个问题的很多认识直接注入到问题里，进入算法里应该挺有帮助的。</p>
<p>所以端到端深度学习的弊端之一是它把可能有用的人工设计的组件排除在外了，精心设计的人工组件可能非常有用，但它们也有可能真的伤害到你的算法表现。例如，强制你的算法以音位为单位思考，也许让算法自己找到更好的表示方法更好。所以这是一把双刃剑，可能有坏处，可能有好处，但往往好处更多，手工设计的组件往往在训练集更小的时候帮助更大。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习（三）</title>
    <url>/2018/04/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[<h1 id="特征缩放-（feature-scaling）"><a href="#特征缩放-（feature-scaling）" class="headerlink" title="特征缩放 （feature scaling）"></a>特征缩放 （feature scaling）</h1><p>确保不同的特征值在同一个范围内，这样能保证梯度下降能够更快的收敛。</p>
<p>例如：<br>x1是房屋大小，非常的大（0-2000）<br>x2是房间卧室数（1-5）<br>参数适当的缩放，使收敛的更快<br><img src="1.png" alt="">建议把特征缩放到-1到1的范围，可以偏差，但不能偏差太大，特征值不需要太精确，只是希望梯度下降收敛更快。</p>
<h1 id="学习速率的选取"><a href="#学习速率的选取" class="headerlink" title="学习速率的选取"></a>学习速率的选取</h1><p>如何选择梯度下降学习速率<code>$\alpha$</code>，可以画出代价函数随迭代步数<code>$J(\theta)$</code>增加的函数曲线，观察曲线来判断梯度下降算法是否收敛，下面这个曲线中，当迭代达到三百时基本已经停止下降，所以这个曲线中三百是最佳的迭代次数<br><img src="2.png" alt="">一个典型例子来的判断是否收敛，比如代价函数<code>$J(\theta)$</code>已经小于一个ε，比如设置ε为0.001，选择一个阈值来告诉算法已经收敛。</p>
<p><code>$\alpha$</code>过小会收敛太慢，过大会导致震荡无法收敛，通常会尝试多个α来找到最佳的学习速率，比如从0.001到1，每扩大三倍选取一个alpha来确定学习速率。</p>
<h1 id="多特征值的线性回归问题"><a href="#多特征值的线性回归问题" class="headerlink" title="多特征值的线性回归问题"></a>多特征值的线性回归问题</h1><p><img src="3.png" alt=""><img src="4.png" alt=""><br>和前一章一样，对每个参数θ求J的偏导数，然后把它们全部置零，然后求出θ1到θn的值，这样就能求出最小的代价函数的所有θ的值。这是一个非常复杂的微分方程，用线性代数的方法可以快速解决。</p>
<h2 id="Normal-Equation"><a href="#Normal-Equation" class="headerlink" title="Normal Equation"></a>Normal Equation</h2><p>构建两个矩阵，矩阵X由x0（全部为1），x1，x2…xn构成，y是结果矩阵，X和y矩阵是以列排的<br><img src="5.png" alt="">简单的说就是: </p>
<blockquote>
<p>通过计算X的转置乘以X的逆乘以X的转置乘以Y来得到θ<br><img src="6.png" alt=""><br>就是这个公式，这里懒得写为什么了，这个也是最小二乘法的公式。<br>优点是不需要选取学习速率，不需要迭代，但是当特征值大于百万级别求矩阵的逆会非常慢，这时则应该选择梯度下降而不是标准方程。</p>
</blockquote>
<p>当特征值存在线性关系时，会导致矩阵不可逆，但是可以通过求伪逆来获取结果刚，对结果影响不大。</p>
<h1 id="Python代码实现"><a href="#Python代码实现" class="headerlink" title="Python代码实现"></a>Python代码实现</h1><blockquote>
<p><a href="https://xvjie.wang/2018/04/12/Python%E5%AE%9E%E7%8E%B0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">Python实现梯度下降</a></p>
</blockquote>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习（七）</title>
    <url>/2018/05/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89/</url>
    <content><![CDATA[<p>[toc]</p>
<p>这里开始学习神经网络，前一个视屏的结尾吴恩达教授说会线性回归和逻辑回归就超过硅谷的大部分程序员了，而且那些程序员混的还不错，听到这里当然是很开心啊，但是想视频是2011年的，七年前懂这些的确是不易了，现在是2018年了，我才开始学，是不是太迟了？</p>
<h1 id="非线性假设（Non-linear-Hypotheses）"><a href="#非线性假设（Non-linear-Hypotheses）" class="headerlink" title="非线性假设（Non linear Hypotheses）"></a>非线性假设（Non linear Hypotheses）</h1><p>神经网络实际上是一个相对古老的算法，是20世纪80年代时期出现的，但是没有成为发展的热点，随着现代计算机计算能力的提升，近年来，神经网络又成为机器学习算法中的一个热点。</p>
<p>之前已经学习过线性回归和逻辑回归算法，为什么还要研究神经网络呢？为了阐述研究神经网络算法目的，我们首先来看几个机器学习的问题作为例子，这几个例子都依赖于复杂的非线性分类器</p>
<p>考虑这个监督学习的分类问题，我们已经有了对应的训练集，如果利用逻辑回归算法来解决这个问题，首先需要构造一个包含很多非线性项的逻辑回归函数，这里g仍是s型函数（即f(x)=1/(1+e^-x)）。我们能让函数包含许多像这样的多项式，当多项式的项数足够的的时候你能够得到一个分开正样本和负样本的判定边界。</p>
<p>例如当只有两项时比如x1和x2，这种方法确实能够得到不错的结果，因为你可以把x1和x2的所有组合都包含到多项式中，但是对于许多复杂的机器学习问题而言，设计的多项式往往多于两项。</p>
<p><img src="1.png" alt=""><br>例如我们之前讨论过的房价预测问题，假设现在处理的是关于房屋的分类问题而不是一个回归问题。假设你对一栋房子的多方面特点都有所了解，你想预测房屋在未来半年内能够被卖出去的概率，这是一个分类问题。</p>
<p>我们可以想到许多特征，对于不同的房子有可能有上百个特征，对于这类问题如果要包含所有的二次项，即使只包含二项式或多项式的计算，最终的多项式也可能有很多项，比如x1^2 ,x1x2 ,x1x3 ,x1x4直到x1x100,接着还有x2^2, x2x3等等很多项。因此即使只考虑二阶项，也就是说两个项的乘积x1乘以x1等等类似于此的项，那么，在n=100的情况下最终也有5050个二次项。</p>
<p>而且随着特征个数n的增加，二次项的个数大约以 n^2 的量级增长，其中n是原始项的个数，即我们之前说过的x1到x100这些项。事实上二次项的个数大约是（n^2）/2个，因此要包含所有的二次项是很困难的，所以这可能不是一个好的做法。</p>
<p>而且由于项数的过多，最后的结果可能是过拟合的，此外，在处理这么多项时也存在运算量过大的问题。当然，我们也可以试试只包含上边这些二次项的子集，例如，我们只考虑x1^2， x2^2， x3^3直到 x100^2这些项，这样就可以将二次项的数量大幅度减少，减少到只有100个二次项。但是由于忽略了太多项，在处理类似左上角的数据时，不太可能得到理想的结果。</p>
<p>实际上如果只考虑x1的平方到x100的平方这一百个二次项，那么你可能会拟合出一些特别的假设，比如可能拟合出一个椭圆状的曲线，但是肯定不会拟合出左上角这个数据集的分界线，所以5000个二次项看起来已经很多了。</p>
<p>而现在的假设还包括三次项， 例如x1x2x3, x1^2x2, x10x11x17等等，类似的三次项有很多很多，事实上，三次项的个数是n^3的量级增加。当n=100时，可以计算出来最后能得到大概17000个三次项。</p>
<p>所以，当初始特征个数n增大时，这些高阶多项式将以几何级数递增，特征空间也随之急剧膨胀。当特征值个数n很大时，如果找出附加项来建立一些分类器，这并不是一个好做法。对于许多实际的机器学习问题，特征个数n是很大的。</p>
<p>我们看看下边这个例子，这是关于计算机视觉中的一个问题。假设你想要使用机器学习算法来训练一个分类器，使他检测一个图像是否为一辆汽车。很多人可能会好奇，觉得这对计算器视觉来说有什么难的？</p>
<p><img src="2.png" alt=""><br>当我们自己看这幅图像时里面有什么事一目了然的事情，你肯定会奇怪，为什么学算法会不知道图像是什么。</p>
<p>为了解答这个问题，我们取出这幅图像的一部分，将其放大，比如这幅图中，汽车的门把手，红框中的部分，人肉眼看到一辆车时，计算机看到的是一个这样的数据矩阵。</p>
<p>它们表示了像素强度值，告诉我们图像中每个像素的亮度值。因此，对于计算机视觉来说问题就变成了，根据这个像素点亮度矩阵来告诉我们这些数值是否代表一个汽车门把手。</p>
<p><img src="3.png" alt=""><br>具体而言，当机器学习算法构造一个汽车识别器时，我们想出一个带标签的样本集，其中一些样本是各类汽车，而另一部分样本是其他任何东西。将这个样本输入给学习算法以训练出一个分类器，当训练完毕后，我们输入一副新的图片，让分类器判别“这是什么东西？”理想情况下，分类器能识别出这是一辆汽车。</p>
<p><img src="4.png" alt=""><br>为了理解引入非线性分类器的必要性，我们从学习算法的训练样本中挑选出一些汽车的图片和非汽车的图片。让我们从其中每幅图片中挑出一组像素电，例如上图像素点1的位置和像素2的位置。</p>
<p>在坐标系中标出这幅汽车的位置，其他坐标系中的位置取决于像素点1和像素点2的亮度。让我们用同样的方法在坐标系中标出其他图片中汽车的位置。接着我们在坐标系中继续画上两个非汽车样本。</p>
<p>然后我们继续在坐标系中画上更多新样本，用“+”表示汽车图片，用“-”表示非汽车图片，我们将发现汽车样本和非汽车样本分布在坐标系中的不同区域，因此我们现在需要一个非线性分类器，来尽量分开这两类样本。</p>
<p>这个分类问题中特征空间的维度是多少？</p>
<p>显然在真实情况下，我们不可能只取两个像素点来做特征。假设我们用50*50像素的图片，注意，我们的图片已经足够小了，长宽只各有50个像素，但这依然是25000个像素点，因此，我们的特征向量的元素数量 n=2500。特征向量X包含了所有像素点的亮度值。</p>
<p>对于典型的计算机图片表示方法，如果储存的每个像素点灰度值（色彩的强烈程度），那么每个元素的值应该在0 到255之间。因此，这个问题中n=2500</p>
<p>但是这只是使用灰度图片的情况，如果我们用的是RGB彩色图像，每个像素点包含红，绿，蓝三个子像素，那么n=7500。</p>
<p>因此，如果我们非要通过包含所有的二次项来解决这个非线性问题，那么仅仅二次项 xi * xj总共有大约300万个（2500^2/2），这个数字大的有点离谱了。对于每个样本来说，要发现并表示所有这300万个项，这个计算成本太高。因此，只是简单的增加二次项或者三次项之类的逻辑回归算法并不是一个解决复杂线性问题的好办法。因为n很大时，将会产生非常多的特征项。</p>
<p>接下来，我们会讨论神经网络，他在解决复杂的非线性分类问题上，被证明是一种好的多的算法，及时你输入的特征空间或输入的特征维度n很大，也能轻松搞定。</p>
<h1 id="神经元和大脑（Neurons-and-the-brain）"><a href="#神经元和大脑（Neurons-and-the-brain）" class="headerlink" title="神经元和大脑（Neurons and the brain）"></a>神经元和大脑（Neurons and the brain）</h1><p>神经网络是一种很古老的算法，他最初产生的目的是制造模拟大脑的机器。我们将会讨论神经网络，因为他能很好的解决不同的机器学习问题，而不是只因为他们在逻辑上行的通。</p>
<p>神经网络产生的原因是人们想尝试设计出模拟大脑的计算。从某种意义上说，如果我们想要建立学习系统那为什么不去模拟我们所认识的最神奇的学习机器–人类的大脑的？</p>
<p>神经网络逐渐兴起于二十世纪八九十年代，应用的非常广泛。但由于各种原因在90年代的后期应用减少，其中一个原因是神经网络是一种计算量有些偏大的算法，但是最近神经网络又东山再起了，大概 由于近年来计算机的运行速度变快，才足以真正运行起大规模的神经网络。</p>
<p>正式由于这个原因和其他一些我们后面会讨论的技术因素，如今的神经网络对于许多应用来说是最先进的技术。</p>
<p>当你模拟大脑时，是指想制造出于人类大脑效果相同的机器。大脑可以学会去看而不是听的方式处理图像，学会处理我们的触觉。我们能学习数学，学习计算微积分，而且大脑能处理各种不同的令人惊奇的事情。似乎如果你想要模仿它，你需要写许多不同的软件来模拟所有大脑告诉我们这些五花八门的奇妙的事情。</p>
<p>如果假设大脑处理所有这些不同事情不需要上千个程序去实现他，相反，大脑只需要一个简单的学习算法就可以了呢？</p>
<p>尽管这只是一个假设，不过让我和你分享一些这方面的证据。</p>
<p><img src="5.png" alt=""><br>如图大脑这个部分，这一小片红色区域是你的听觉皮层，如果你通过我说的话来理解我表达的内容，那么是靠耳朵接收到声音信号并把声音信号传递给你的听觉表皮层，正因如此，你才能明白我的话。</p>
<p>神经系统科学家做了一个有趣的实验，把耳朵到听觉表皮的神经切断。在这种情况下，将其重新接到一个动物的大脑上，这样从眼睛看到的视觉神经的信号最终将传到听觉表皮层，结果表明，听觉表皮层将会学会“看”。</p>
<p>所以，如果你对动物这样做那么动物就可以完成视觉辨别任务，他们可以看图像，并根据图像做出适当的决定。它们正是通过脑组织中的这个部分完成的。</p>
<p><img src="6.png" alt=""><br>下面在举另一个例子，这块红色的脑组织是你的躯体感觉皮层，这是你用来处理触觉的。如果你做一个和刚才类似的重接实验，那么躯体感觉皮层也能会“看”，这个实验和其他一些类似的实验被称为神经重接实验。从这个意义上说，如果人体有同一块脑组织可以处理光、声或触觉信号，那么也许存在一种学习方法可以同时处理视觉，听觉和触觉，而不是需要成千个不同的程序或者算法来做这些。</p>
<p>大脑能够完成的成千上万的事，我们需要做的就是找出一些近似的或实际的大脑学习算法，然后实现它，大脑通过自学掌握如何处理这些不同类型的数据，在很大程度上可以猜想，如果我们把任何一种传感器接到大脑的任何一个部位，大脑就会学会处理它。</p>
<p><img src="7.png" alt=""><br>再看上图的几个例子，左上角这张图是用舌头学会“看”的一个例子。这实际上是一个名为BrainPort的系统，他现在正在FDA（美国食品药物管理局）的临床试验阶段，他帮助失明人士看见事物。他的原理是，在你前额戴上一个灰度摄像头，他能够获取你面前的事物的低分辨率的灰度图像。你连接一根线到舌头上安装的电极阵列上，那么每个像素都被映射到你舌头上的某个位置。可能电压值高的点对应一个暗像素，电压值的点对应亮像素。</p>
<p>即使依靠它现在的功能，使用这种系统就能够让人在几十分钟里学会用我们的舌头看东西。</p>
<p>下面是第二个例子，关于人体回声定位或者说人体声纳。你有两种方法可以实现，你可以弹响指或者咂舌头。现在有失明人士确实在学校里接受这样的培训，并学会解读从环境反弹回来的声波模式—这就是声纳。</p>
<p>如果你搜索 YouTube 之后，就会发现有些视频讲述了一个令人称奇的孩子，他因为癌症眼球惨遭移除，虽然失去了眼球，但是通过打响指他可以四处走动而不撞到任何东西。他能滑滑板，他可以将篮球投入篮框中。</p>
<p>第三个例子是触觉皮带，如果你把它戴在腰上，蜂鸣器会响，而且总是朝向北时发出嗡嗡声。它可以使人拥有方向感，用类似于鸟类感知方向的方式。</p>
<p>还有一些离奇的例子，如果你在青蛙身上插入第三只眼，青蛙也能学会使用那只眼睛。</p>
<p>因此，这将会非常令人惊奇。如果你能把几乎任何传感器接入到大脑中，大脑的学习算法就能找出学习数据的方法并处理这些数据。从某种意义上来说如果我们能找出大脑的学习算法，然后在计算机上执行大脑学习算法或与之相似的算法，也许这将是我们向人工智能迈进做出的最好的尝试。</p>
<h1 id="模型表示（Model-Representation）"><a href="#模型表示（Model-Representation）" class="headerlink" title="模型表示（Model Representation）"></a>模型表示（Model Representation）</h1><p>神经网络是在模仿大脑中的神经元或者神经网络时发明的。因此，要解释如何表示模型假设，我们不妨先来看单个神经元在大脑中是什么样的。</p>
<p><img src="8.png" alt=""><br>我们的大脑中充满了如上图所示的这样的的神经元，神经元是大脑中俄细胞，其中有两点值得我们注意，一是神经元有像这样的细胞主题（Nucleus），二是神经元有一定数量的输入神经和输出神经。这些输入神经叫做树突（dendrite），可以把他们想象成输入电线，他们接受来自其他神经元的信息。神经元的输出神经焦作轴突（Axon），这些输出神经是用来给其他神经元传递信号或者传递信息的。</p>
<p>简而言之，神经元是一个计算单元，他从输入神经接受一定数目的信息，并做一些计算，然后将结果通过他的轴突传送到其他节点或者大脑中的其他神经元。</p>
<p>下面是神经元的示意图：</p>
<p><img src="9.png" alt=""><br>神经元利用微弱的电流进行沟通。这些弱电流也称作动作电位，其实就是微弱的电流。所以如果神经元想要传递一个消息，他就会通过它的轴突发送一段微弱电流给其他神经元。</p>
<p><img src="10.png" alt=""><br>上图中，黄色的圆圈就代表了一个神经元，X为输入向量，θ代表神经元的权重（就是我们之前所说的模型参数），hθ(x)代表激励函数（在神经网络术语中，激励函数只对类似非线性函数（g(z)的另一个术语称呼，g(z)等于1除以1加e的-z次方）。</p>
<p>实际上你可以这样理解，神经元就是权重θ。</p>
<p>当将输入送进神经元后，经计算（就是X^Tθ）会有一个输出，这个输出再送进激励函数中，便得到了神经元的真实输出。</p>
<p>注意：当我们绘制一个神经网络时，通常我会只绘制节点x1,x2,x3等等，但有时可以增加一个额外的节点x0，这个x0节点有时被称作偏置神经元。但因为x0总是等于1，去哦们会画出它，有时我们不会画出，这要看画出他是否对例子有利。</p>
<p><img src="11.png" alt=""><br>神经网络就是不同的神经元组合在一起，第一层为输入层，最后一层为输出层，而且中间的所有层均为隐藏层。</p>
<p>注意：输入单元x1，x2，x3，再说一次，有时也可以画上额外的节点x0.同时，这里有三个神经元，我在里面写了a<sub>1</sub><sup>(2)</sup> 、 a<sub>2</sub><sup>(2)</sup> 和a<sub>3</sub><sup>(2)</sup> ,然后再次说明这里我可以添加一个a<sub>0</sub><sup>(2)</sup> ，这和x0一样，代表一个额外的偏度单位，它的值永远是1，注意：a<sub>1</sub><sup>(2)</sup> 、 a<sub>2</sub><sup>(2)</sup> 和a<sub>3</sub><sup>(2)</sup>中计算的是g（X^Tθ）的值，而a<sub>0</sub><sup>(2)</sup>中存放的就是偏置1。</p>
<p><img src="12.png" alt=""><br>如果一个网络在第j层有sj个单元，在j+1层有sj+1个单元，那么矩阵θ<sup>(j)</sup>即控制第j层到第j+1层的映射。</p>
<p>矩阵θ<sup>(j)</sup>的维度是s<sub>(j+1)</sub>*(s<sub>j</sub>+1),s<sub>(j+1)</sub>行，(s<sub>j</sub>+1)列</p>
<p>总之，上面的图展示了是怎样定义一个人工网络的。这个神经网络定义了函数h:从输入x到输出y的映射。我将这些假设的参数记为大写的θ，这样一来不同的θ对应不同的假设，所以我们有不同的函数，比如说从x到y的映射。</p>
<p>以上就是我们怎么从数学上定义神经网络的假设</p>
<p>下面将讲解如何高效的进行计算，并展示一个向量化的实现方法，更重要的是让你明白这样表示神经网络是一个好方法，并且明白它们怎样帮助我们学习复杂的非线性假设</p>
<p><img src="13.png" alt=""><br>以前我们说过计算出假设输出的步骤，通过左边的这些方程计算出三个隐藏的单元的激励值，然后利用这些值来计算假设函数h(x)的最终输出，接下来我要定义一些额外的项，因此，上图中蓝色线的项把他定义为z上标（2）下标1，这样一来就有了a<sup>(2)</sup><sub>1</sub> 这个项，等于g(z<sup>(2)</sup><sub>1</sub>)(上标2的意思是与第二层相关，即神经网络的隐藏层有关)接下来画红线的项同样定义为z<sup>(2)</sup><sub>2</sub>，最后一项定义为z<sup>(2)</sup><sub>3</sub>，这样我们就有a<sup>(2)</sup><sub>3</sub>=g(z<sup>(2)</sup><sub>3</sub>)，所以这些Z的值是线性组合，是输入值x1,x2,x3的加权线性组合，他将进入一个特定的神经元，类似于矩阵向量的乘法。</p>
<p>现在看一线灰色框里的一维数组，你可能会注意到这一块对应了矩阵向量的运算x1乘以向量x，观察到这一点我就能够将神经网络的运算向量化，具体而言我们定义特征向量x为x0,x1,x2,x3组成的向量，其中x0=1，并定义z^2为<br>这些值组成的向量，注意：这里的Z(2)是一个三维向量。</p>
<p>下面我们可以这样向量化<sub>1</sub><sup>(2)</sup> 、 a<sub>2</sub><sup>(2)</sup> 和a<sub>3</sub><sup>(2)</sup>的计算我们只用两个步骤z(2)等于θ(1)乘以x，然后a(2)等于g(z(2))，需要明白的是这里的z(2)是一个三维向量，并且a(2)也是一个三维向量因此这里的激励将s函数逐元素作用于z(2)中的每个元素z(2)就等于θ(1)乘以a(1)。当然x也有偏置单元x0，</p>
<p>顺便说一下，为了让我们的符号和接下来的工作一致，在输入层，虽然我们有输入x但是我们还可以把这些想成是第一层的激励，所以我们可以定义第一层的激励a(1)=x,因此a(1)就是一个向量了，我们可以把这里的x替换成a(1)</p>
<p>现在我们得到了a1，a2，a3的值，但是我们同样需要一个值a0，他对应隐藏层得到这个输出的偏置单元，这时a(2)就是一个四维的特征向量。</p>
<p>为了计算假设的实际输出值h，我们只需要计算z(3),z(3)等于绿色框框中的项目，最后假设函数h(x)输出他等于a(3),a(3)是输出层唯一的单元，他是一个实数。</p>
<p>这个h(x)的计算过程也成为向前传播(forward propagation),这样的命名是因为我们是从输入层的激励开始，然后进行向前传播给隐藏层，并计算隐藏层，然后我们继续向前传播，计算输出层的激励，这个从输入层到隐藏层再到输出层一次计算激励的过程叫向前传播。</p>
<p>我们刚刚得到了这一过程的向量化实现方法，如果用右边的公式计算，会得到一个有效的计算h(x)的方法</p>
<p>这种向前传播的角度，可以帮助我们了解神经网络的原理，帮助我们学习非线性假设</p>
<p><img src="14.png" alt=""><br>看上面这幅图，我们先盖住图片左边的部分，如果只看右边，这看起来很像逻辑回归，在逻辑回归中我们用最后一个节点，也就是最后一个逻辑回归单元来预测h(x)的值，具体来说，假设输出的h(x)等于s型激励函数g(Θa1+Θa2…)。其中a由那三个单元一样，为了和我们之前的定义保持一致，需要添加红色的上标和下标1，因为我们只有一个输出单元，但如果你只观察蓝色的部分，这看起来非常像标准的逻辑回归模型，不同之处在于，我现在用的是大写的Θ，而不是小写的Θ，这样做完我们只得到了逻辑回归，但是逻辑回归输入特征值是通过隐藏层计算的。</p>
<p>再说一遍，神经网络所做的就像逻辑回归，但是它不是使用x1，x2，x3作为输入特征，而是用a1，a2，a3作为新的输入特征，同样的我们需要把上标加上来和之前的记号保持一致，有趣的是特征值a1，a2，a2是当做输入函数来学习的，具体来说，就是从第一层映射到第二层的函数，这个函数由其他一组参数θ(1)决定，而在神经网络上，他没有用输入特征x1，x2，x3，来训练逻辑回归而是自己训练逻辑回归的输入a1，a2，a3，可以想象，如果在θ1中选择不同的参数，可以学习一些很有趣和复杂的特征，就可以得到一个更好的假设，比使用原始输入x1，x2或x3时得到的假设更好。</p>
<p>你也可以x1，x2，x3等作为输入项，但是这个算法可以灵活的快速学习任意的特征项，把这些a1，a2，a3,输入这个最后的单元，实际上他是逻辑回归。</p>
<p><img src="15.png" alt=""><br>还可以用其他类型图表示神经网络，神经网络中神经元相连接的方式，称为神经网络的架构，所以架构是指，不同的神经元是如何相互连接的，这里有个一不同的神经网络架构的例子，你可以意识到这个第二层是如何工作的，我们有三个隐藏单元，它们根据输入层计算一个复杂的函数，然后到第三层，我们可以将第二层训练出的特征项作为输入，并在第三层计算一些更复杂的函数，这样你在第四次，即输出层时，就可以利用第三层训练出的更复杂的特征项作为输入，以此得到非常有趣的非线性假设。顺便说下，在这个网络中，第一层被称为输入层，第四层仍然是我们的输出层，这个网络有两个隐藏层，所以都被称为隐藏层任何一个不是输入层或者输出层的。</p>
<h1 id="示例和直觉（Examples-and-Intuitions）"><a href="#示例和直觉（Examples-and-Intuitions）" class="headerlink" title="示例和直觉（Examples and Intuitions）"></a>示例和直觉（Examples and Intuitions）</h1><p>接下来讲解两个例子来说明神经网络是如何计算的。</p>
<p>关于输入的复杂的非线性函数，希望这个例子可以让你了解，神经网络可以用来学习复杂的非线性假设</p>
<p><img src="16.png" alt=""><br>我们有x1，x2要么取0要么取1，所以x1和x2只能有两种取值，在这个例子中，我只画出了，两个正样本和两个负样本，你可以认为这是一个复杂样本的简单版本，咋这个复杂问题中，我们可能在右上角有一堆正样本，和左上角一堆用圈圈表示的负样本，我们想要学习一种非线性的决策边界来区分正负样本。</p>
<p>我们用左边的例子来说明，具体来讲我们需要计算的是目标函数y等于x1异或x2，或者y也可以等于x1异或非x2，其中异或非表示x1异或x2后取反，x1异或x2为真当且仅当这两个值，x1或者x2中有且仅有一个为1，如果我用xNOR的例子比用NOT作为例子结果会好一些，但这两个其实是相同的，这就意味着在x1异或x2后再取反，即他们同时为真或者同时为假的时候，我们将会获得y等于1，y为0的结果是，如果他们中仅有一个为真，则y为0。</p>
<p>我们能否找到一个神经网络模型来拟合这种训练集，为了建立能够拟合XNOR运算，我们先拟合一个简单的神经网络，它拟合了“且运算”。</p>
<p><img src="17.png" alt=""><br>假设我们有输入x1，x2都是二进制，即要么是0要么是1，我们的目标函数y等于x1且x2，一个逻辑与运算，那么我们怎样得到一个具有单个神经元的神经网络来计算这个逻辑与呢？</p>
<p>我们给这个网络分配一些权重或参数，-30，+20，+20，即我们给x的前面系数赋值，所以我们的h(x)=g(-30+20x1+20x2),右上角的图就是我们的s型函数，然后我们看四种输入的可能性，就是与运算的结果。</p>
<p><img src="18.png" alt=""><br>同样我们用神经网络实现或运算然后讲解更为赋值的神经网络。</p>
<p><img src="19.png" alt=""><br>我们只要在x1前面放入一个很大的负数，就可以实现非的功能。</p>
<p><img src="20.png" alt=""><br>我们现在把这三个功能放在一起，就可以实现x1 XNOR x2的功能。</p>
<p>当层数很多的时候，你有一个相对简单的输入量的函数作为第二层，而第三层可以建立在此基础上建立一些更加复杂的函数，然后在下一层又在计算一个稍微复杂的函数，我们可以运用更深层的函数计算更加复杂的函数。</p>
<p>神经网络还可以用于识别手写数字。</p>
<p>它使用的输入是不同的图像或者说就是一些原始的像素点。第一层计算出一些特征，然后下一层再计算出一些稍复杂的特征，然后是更复杂的特征，然后这些特征实际上被最终传递给最后一层逻辑回归分类器上，使其准确地预测出神经网络“看”到的数字。</p>
<h1 id="多类分类（Multiclass-Classification）"><a href="#多类分类（Multiclass-Classification）" class="headerlink" title="多类分类（Multiclass Classification）"></a>多类分类（Multiclass Classification）</h1><p>在多分类问题中我们如何处理？</p>
<p><img src="21.png" alt=""><br><img src="22.png" alt=""><br>和处理逻辑回归的多分类问题一样。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习（五）</title>
    <url>/2018/04/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%94%EF%BC%89/</url>
    <content><![CDATA[<p>最近事情比较多，又懒惰了，继续学习。</p>
<h1 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h1><p>什么是分类问题，例如：<br>垃圾邮件分类，恶性肿瘤预测。<br><img src="1.png" alt="">在分类问题中一般结果是0和1，1称为正样本或正类，0称为负样本或负类。</p>
<p>首先讲解的是简单的两变量分类问题<br>使用线性回归的方式解决分类问题如何？<br><img src="2.png" alt=""><br>如果是上图这样的例子来看，使用线性回归的方式貌似可以解决分类问题，但是如果存在一个严重偏差的特征时，使用线性回归拟合分类问题就会出现严重的偏差，在分类问题中最终的结果只有0和1，但是在线性回归中会出现小于1和大于0的结果。<br><img src="3.png" alt=""><br>所以使用线性回归的方式不能很好的处理分类问题，于是引出了另一种模型，逻辑回归（逻辑回归的叫法是历史原因，和回归并没有什么关系）</p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>什么样的数学模型适合回归问题呢？一个只会在0与1中间震荡的函数模型：</p>
<p><img src="4.png" alt=""><br>其中：</p>
<p>x,表示的是特征向量</p>
<p>g，代表逻辑函数(Logistic function)是一个常用的曲线函数(Sigmoid function),表达式为：</p>
<p><img src="5.png" alt=""><br>函数的图像就如上图所示。</p>
<p>h,表示的就是逻辑回归，带入到函数g中，最终得到的表达式就是</p>
<p><img src="6.png" alt=""><br>函数h表示的就是当输入特征X时，根据输入的特征计算输出变量Y=1的可能性。假设h(x)=0.7,表示的就是患有恶性肿瘤的概率为0.7</p>
<h1 id="判定边界-Decision-Boundary"><a href="#判定边界-Decision-Boundary" class="headerlink" title="判定边界(Decision Boundary)"></a>判定边界(Decision Boundary)</h1><p>判定边界能够让我们更好的理解逻辑回归和假设函数在计算什么</p>
<p><img src="7.png" alt=""><br>上图就是逻辑回归的函数和图像，看一下数学意义：<br><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">当h &gt;= <span class="number">0.5</span>时,预测结果 y = <span class="number">1</span>，</span><br><span class="line">当h &lt; <span class="number">0.5</span>时，预测结果 y = <span class="number">0</span>,</span><br><span class="line">所以：</span><br><span class="line">当 y = <span class="number">1</span> 时，h(x) = g(z) &gt;= <span class="number">0.5</span> </span><br><span class="line">那么 z &gt;= <span class="number">0</span>,也就是θtX&gt;=<span class="number">0</span>;</span><br><span class="line">当y=<span class="number">0</span>时，最后得到θtX&lt;<span class="number">0</span>。</span><br></pre></td></tr></table></figure></p>
<p>具体看下面这个例子</p>
<p><img src="8.png" alt=""><br>其中的theta的参数分别为-3,1,1<br>存在如上图所示的数据以及表示函数,如果要预测y=1的概率，最后得到的表达式为：</p>
<p><img src="9.png" alt=""><br>最后得到的结果很明显是一个过（0,3）（3,0）的直线：</p>
<p><img src="10.png" alt=""><br>其中的方程就是一个判定边界，通过这条线就可以分辨出正样本和负样本了。</p>
<p>除了这种线性的判定边界之外，还有一些其他形状的判定边界，如圆形。</p>
<p><img src="11.png" alt=""></p>
<h1 id="逻辑回归中的代价函数"><a href="#逻辑回归中的代价函数" class="headerlink" title="逻辑回归中的代价函数"></a>逻辑回归中的代价函数</h1><p><img src="12.png" alt=""><br>上面就是之前讲过的线性回归中的代价函数，这个代价函数在线性回归中能够很好地使用，但是在逻辑回归中却会出现问题，因为将逻辑回归的表达式带入到h函数中得到的是一个非凸函数的图像，那么就会存在多个局部最优解，无法像凸函数一样得到全局最优解。示例如下。</p>
<p><img src="13.png" alt=""><br>所以在逻辑回归中需要重新定义代价函数：</p>
<p><img src="14.png" alt=""><br>最后得到的函数h和Cost函数之前的关系如下：</p>
<p><img src="15.png" alt=""><br>构建一个这样的函数的好处是在于，当y=1时，h=1，如果h不为1时误差随着h的变小而增大；同样，当y=0时，h=0，如果h不为0时误差随着h的变大而增大。</p>
<h1 id="代价函数中的梯度下降"><a href="#代价函数中的梯度下降" class="headerlink" title="代价函数中的梯度下降"></a>代价函数中的梯度下降</h1><p>在上一节中的逻辑回归中的代价函数中给出了代价函数的定义</p>
<p><img src="16.png" alt=""><br>最后可以简化为:</p>
<p><img src="17.png" alt=""><br>最终的求解问题就是要求回归函数的值最小，那么同样可以使用在线性回归中所用到的梯度函数。</p>
<p><img src="18.png" alt=""><br>上图就是逻辑回归的梯度求解过程，虽然看起来和线性回归相似，但实则是完全不同的。在线性回归中，h函数为theta的转置与X的乘积，但是在逻辑回归中则不是。这样就导致了两者在运算方面和优化方面是完全不同的。但是在运行梯度下降算法之前，进行特征缩放依旧是非常重要的。</p>
<h1 id="高级优化"><a href="#高级优化" class="headerlink" title="高级优化"></a>高级优化</h1><p>优化算法除了讲到的梯度下降算法之外，还有一些叫做共轭梯度下降算法(BFGS,L-BFGS)。使用这些共轭梯度下降算法的好处在于，不需要手动地选择学习率a，这些算法会自行尝试选择a;比梯度下降算法运算更快。<br>一般情况下，在常见的机器学习算法库中都带有这些算法，不需要程序员手动实现这些算法。</p>
<h1 id="多类别分类问题"><a href="#多类别分类问题" class="headerlink" title="多类别分类问题"></a>多类别分类问题</h1><p>现实世界中除了二元的分类问题还有多元的分类问题，例如邮件的类型有工作，朋友，家人，爱好等多种，分类到不同的文件夹下，如对天气的分类，是晴天、多云、小雨等等天气。</p>
<p>多元分类问题与二元分类问题的区别如下:<br><img src="19.png" alt=""><br>多元分类的思路与二元分类问题的解决思路是类似的。可以将多元问题变为两元问题，具体如下：</p>
<p><img src="20.png" alt=""><br>这样n元的分类问题，就会进行n次的机器学习的分类算法。对每一次的分类结果即为h(x)。那么经过n此分类之后，最后得到的结果为:</p>
<p><img src="21.png" alt=""><br>那么当输入新的训练集或者是变量X，只需要按照上面的思路进行分类，其中的h(x)的最大值就是对应的最后的分类结果。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本章写完用了一周时间，但其实视频一天就看完了，博客内容基本是照抄别人的，关于具体的代码实现查看另一篇博客。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习（二）</title>
    <url>/2018/04/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<p>记得当年背英语单词的书永远都翻在第一页，abandon背了一遍又一遍，还是没有记住，现如今周志华老师的书买了好久没有翻，吴恩达的视频看了一遍又一遍，只能希望这是最后一次看了。</p>
<p><a href="https://www.bilibili.com/video/av9912938" target="_blank" rel="noopener">Machine Learning 学习视屏地址</a></p>
<h1 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a>什么是机器学习</h1><blockquote>
<p>不借助特定的程序使电脑学习的科学</p>
</blockquote>
<h1 id="监督学习（supervised-learning）"><a href="#监督学习（supervised-learning）" class="headerlink" title="监督学习（supervised learning）"></a>监督学习（supervised learning）</h1><p>例如：房价预测（回归问题），肿瘤预测（分类问题）<br>监督学习就是给出一组特征值，同时也给出这组特征所对应的结果。比如通过某一地区房子的面积和卧室数来预测房子的价格。</p>
<h1 id="无监督学习（unsupervised-learning）"><a href="#无监督学习（unsupervised-learning）" class="headerlink" title="无监督学习（unsupervised learning）"></a>无监督学习（unsupervised learning）</h1><p>无监督学习则是只给出一些特征值，但是并没有这些特征所对应的结果，通过这些特征值来寻找他们之间的关系，例如聚类问题，把同一事件的新闻划分为一类。</p>
<h1 id="模型表示（Model-Representation）"><a href="#模型表示（Model-Representation）" class="headerlink" title="模型表示（Model Representation）"></a>模型表示（Model Representation）</h1><h2 id="符号定义"><a href="#符号定义" class="headerlink" title="符号定义"></a>符号定义</h2><p>m：样本数量（training examples）<br>x：输入值，又叫特征（input variables/features）<br>y：输出值，又叫目标值（output variables/target variables）<br>（x，y）：训练样本<br>第i个训练样本：$(x^i,y^i)$<br><img src="1.png" alt=""></p>
<h2 id="监督学习的工作方式"><a href="#监督学习的工作方式" class="headerlink" title="监督学习的工作方式"></a>监督学习的工作方式</h2><p><img src="2.png" alt=""></p>
<h2 id="预测函数的表示"><a href="#预测函数的表示" class="headerlink" title="预测函数的表示"></a>预测函数的表示</h2><p>$h_\theta(x)=\theta_0+\theta_1x$</p>
<p>关于$x$单变量的线性回归方程</p>
<h1 id="代价函数（cost-function）"><a href="#代价函数（cost-function）" class="headerlink" title="代价函数（cost function）"></a>代价函数（cost function）</h1><p>预测函数$h_\theta(x)$的线性意义：<br><img src="3.png" alt="">预测函数$h_\theta(x)$是关于$x$的函数,而代价函数是一个关于$(\theta_0,\theta_1)$的函数</p>
<p>$J(\theta_0,\theta_1) = \frac{1}{2m} \sum^m_{i=1}(h_\theta(x^i)-y^i)^2$</p>
<p>优化目标：$minimize J(\theta_0,\theta_1)$<br><img src="4.png" alt=""><br>教授讲的很详细，这里记一下自己的见解吧：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">预测函数是根据已知特征向量和结果所描述的一个线性方程，根据改变线段的斜率来观察匹配到的特征吻合度达，当预测函数可以匹配到最多特征时则这个预测函数是最优解，如何获取最优解引入了代价函数，所谓代价函数就是预测函数的积分，</span><br></pre></td></tr></table></figure></p>
<h1 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h1><p>在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度</p>
<p>梯度下降是迭代法的一种,可以用于求解最小二乘问题(线性和非线性都可以)。在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。</p>
<p>通过梯度下降的方法来寻找代价函数的最优解<br><img src="5.png" alt="">符号解释</p>
<blockquote>
<p>：= 赋值符，把右边的值赋值给左边</p>
</blockquote>
<blockquote>
<p>$\alpha$ 学习速度，步长，过小收敛时间过长，过大超过最小值无法收敛</p>
</blockquote>
<p><img src="6.png" alt=""><br>$\theta_0$和$\theta_1$是同时更新的</p>
<p>梯度下降的缺点：<br>靠近极小值时收敛速度减慢。<br>直线搜索时可能会产生一些问题。<br>可能会“之字形”地下降。<br>会产生局部最优解而非全局。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这里基本简单的记录了视频前三章的内容，梳理一下知识点。<br>在机器学习中首先需要有样本，也叫训练集，然后是一个机器学习算法，把训练集扔进这个算法中，通过迭代之类的方法计算机会发现其中的规律而给出统一的模型从而做到预测分析。</p>
<p>当训练集既有输入内容又有输出结果，就是监督学习（比如回归问题），当样本里没有结果时是无监督学习（比如聚类问题）</p>
<p>梯度下降就是寻找最佳的预测模型的方式，当我们要建立一个准确的预测模型需要不断改变参数（$\theta_0,\theta_1$）,于是建立一个关于$\theta_0,\theta_1$的方程，这个方程叫代价方程（cost function），其实这个方程就是度量预测函数的结果和实际结果的方差，当方差最小就是最佳$\theta_0,\theta_1$，方法就是计算所有预测函数的结果减实际结果和的平方，方差最小就代表拟合度最佳。可视化观察就向是在一个弯曲的山谷中寻找最低点。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习（九）</title>
    <url>/2018/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89/</url>
    <content><![CDATA[<p>十二、支持向量机(Support Vector Machines)</p>
<h3 id="12-1-优化目标"><a href="#12-1-优化目标" class="headerlink" title="12.1 优化目标"></a>12.1 优化目标</h3><p>参考视频: 12 - 1 - Optimization Objective (15 min).mkv</p>
<p>到目前为止,你已经见过一系列不同的学习算法。在监督学习中，许多学习算法的性能都非常类似，因此，重要的不是你该选择使用学习算法<strong>A</strong>还是学习算法<strong>B</strong>，而更重要的是，应用这些算法时，所创建的大量数据在应用这些算法时，表现情况通常依赖于你的水平。比如：你为学习算法所设计的特征量的选择，以及如何选择正则化参数，诸如此类的事。还有一个更加强大的算法广泛的应用于工业界和学术界，它被称为支持向量机(<strong>Support Vector Machine</strong>)。与逻辑回归和神经网络相比，支持向量机，或者简称<strong>SVM</strong>，在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。因此，在接下来的视频中，我会探讨这一算法。在稍后的课程中，我也会对监督学习算法进行简要的总结。当然，仅仅是作简要描述。但对于支持向量机，鉴于该算法的强大和受欢迎度，在本课中，我会花许多时间来讲解它。它也是我们所介绍的最后一个监督学习算法。</p>
<p>正如我们之前开发的学习算法，我们从优化目标开始。那么，我们开始学习这个算法。为了描述支持向量机，事实上，我将会从逻辑回归开始展示我们如何一点一点修改来得到本质上的支持向量机。</p>
<p><img src="3d12b07f13a976e916d0c707fd03153c.png" alt=""></p>
<p>那么，在逻辑回归中我们已经熟悉了这里的假设函数形式，和右边的S型激励函数。然而，为了解释一些数学知识.我将用$z$ 表示$\theta^Tx$。</p>
<p>现在考虑下我们想要逻辑回归做什么：如果有一个 $y=1$的样本，我的意思是不管是在训练集中或是在测试集中，又或者在交叉验证集中，总之是 $y=1$，现在我们希望${{h}_{\theta }}\left( x \right)$ 趋近1。因为我们想要正确地将此样本分类，这就意味着当 ${{h}_{\theta }}\left( x \right)$趋近于1时，$\theta^Tx$ 应当远大于0，这里的$>>$意思是远远大于0。这是因为由于 $z$ 表示 $\theta^Tx$，当 $z$远大于0时，即到了该图的右边，你不难发现此时逻辑回归的输出将趋近于1。相反地，如果我们有另一个样本，即$y=0$。我们希望假设函数的输出值将趋近于0，这对应于$\theta^Tx$，或者就是 $z$ 会远小于0，因为对应的假设函数的输出值趋近0。</p>
<p><img src="66facb7fa8eddc3a860e420588c981d5.png" alt=""></p>
<p>如果你进一步观察逻辑回归的代价函数，你会发现每个样本 $(x,y)$都会为总代价函数，增加这里的一项，因此，对于总代价函数通常会有对所有的训练样本求和，并且这里还有一个$1/m$项，但是，在逻辑回归中，这里的这一项就是表示一个训练样本所对应的表达式。现在，如果我将完整定义的假设函数代入这里。那么，我们就会得到每一个训练样本都影响这一项。</p>
<p>现在，先忽略 $1/m$ 这一项，但是这一项是影响整个总代价函数中的这一项的。</p>
<p>现在，一起来考虑两种情况：</p>
<p>一种是$y$等于1的情况；另一种是 $y$ 等于0的情况。</p>
<p>在第一种情况中，假设 $y=1$ ，此时在目标函数中只需有第一项起作用，因为$y=1$时，$(1-y)$项将等于0。因此，当在 $y=1$ 的样本中时，即在 $(x, y) $中 ，我们得到 $y=1$ $-\log(1-\frac{1}{1+e^{-z}})$这样一项，这里同上一张幻灯片一致。</p>
<p>我用 $z$ 表示$\theta^Tx$，即： $z= \theta^Tx$。当然，在代价函数中，$y$ 前面有负号。我们只是这样表示，如果 $y=1$ 代价函数中，这一项也等于1。这样做是为了简化此处的表达式。如果画出关于$z$ 的函数，你会看到左下角的这条曲线，我们同样可以看到，当$z$ 增大时，也就是相当于$\theta^Tx$增大时，$z$ 对应的值会变的非常小。对整个代价函数而言，影响也非常小。这也就解释了，为什么逻辑回归在观察到正样本$y=1$时，试图将$\theta^Tx$设置得非常大。因为，在代价函数中的这一项会变的非常小。</p>
<p>现在开始建立支持向量机，我们从这里开始：</p>
<p>我们会从这个代价函数开始，也就是$-\log(1-\frac{1}{1+e^{-z}})$一点一点修改，让我取这里的$z=1$ 点，我先画出将要用的代价函数。</p>
<p><img src="b4b43ee98bff9f5e73d841af1fa316bf.png" alt=""></p>
<p>新的代价函数将会水平的从这里到右边(图外)，然后我再画一条同逻辑回归非常相似的直线，但是，在这里是一条直线，也就是我用紫红色画的曲线，就是这条紫红色的曲线。那么，到了这里已经非常接近逻辑回归中使用的代价函数了。只是这里是由两条线段组成，即位于右边的水平部分和位于左边的直线部分，先别过多的考虑左边直线部分的斜率，这并不是很重要。但是，这里我们将使用的新的代价函数，是在$y=1$的前提下的。你也许能想到，这应该能做同逻辑回归中类似的事情，但事实上，在之后的优化问题中，这会变得更坚定，并且为支持向量机，带来计算上的优势。例如，更容易计算股票交易的问题等等。</p>
<p>目前，我们只是讨论了$y=1$的情况，另外一种情况是当$y=0$时，此时如果你仔细观察代价函数只留下了第二项，因为第一项被消除了。如果当$y=0$时，那么这一项也就是0了。所以上述表达式只留下了第二项。因此，这个样本的代价或是代价函数的贡献。将会由这一项表示。并且，如果你将这一项作为$z$的函数，那么，这里就会得到横轴$z$。现在，你完成了支持向量机中的部分内容，同样地，我们要替代这一条蓝色的线，用相似的方法。</p>
<p><img src="ab372c9161375a4f7b6f0bd4a69560e9.png" alt=""></p>
<p>如果我们用一个新的代价函数来代替，即这条从0点开始的水平直线，然后是一条斜线，像上图。那么，现在让我给这两个方程命名，左边的函数，我称之为${\cos}t_1{(z)}$，同时，右边函数我称它为${\cos}t_0{(z)}$。这里的下标是指在代价函数中，对应的 $y=1$ 和 $y=0$ 的情况，拥有了这些定义后，现在，我们就开始构建支持向量机。</p>
<p><img src="59541ab1fda4f92d6f1b508c8e29ab1c.png" alt=""></p>
<p>这是我们在逻辑回归中使用代价函数$J(\theta)$。也许这个方程看起来不是非常熟悉。这是因为之前有个负号在方程外面，但是，这里我所做的是，将负号移到了表达式的里面，这样做使得方程看起来有些不同。对于支持向量机而言，实质上我们要将这替换为${\cos}t_1{(z)}$，也就是${\cos}t_1{(\theta^Tx)}$，同样地，我也将这一项替换为${\cos}t_0{(z)}$，也就是代价${\cos}t_0{(\theta^Tx)}$。这里的代价函数${\cos}t_1$，就是之前所提到的那条线。此外，代价函数${\cos}t_0$，也是上面所介绍过的那条线。因此，对于支持向量机，我们得到了这里的最小化问题，即:</p>
<p><img src="4ac1ca54cb0f2c465ab81339baaf9186.png" alt=""></p>
<p>然后，再加上正则化参数。现在，按照支持向量机的惯例，事实上，我们的书写会稍微有些不同，代价函数的参数表示也会稍微有些不同。</p>
<p>首先，我们要除去$1/m$这一项，当然，这仅仅是由于人们使用支持向量机时，对比于逻辑回归而言，不同的习惯所致，但这里我所说的意思是：你知道，我将要做的是仅仅除去$1/m$这一项，但是，这也会得出同样的 $\theta$ 最优值，好的，因为$1/m$ 仅是个常量，因此，你知道在这个最小化问题中，无论前面是否有$1/m$ 这一项，最终我所得到的最优值$\theta$都是一样的。这里我的意思是，先给你举一个实例，假定有一最小化问题：即要求当$(u-5)^2+1$取得最小值时的$u$值，这时最小值为：当$u=5$时取得最小值。</p>
<p>现在，如果我们想要将这个目标函数乘上常数10，这里我的最小化问题就变成了：求使得$10×(u-5)^2+10$最小的值$u$，然而，使得这里最小的$u$值仍为5。因此将一些常数乘以你的最小化项，这并不会改变最小化该方程时得到$u$值。因此，这里我所做的是删去常量$m$。也相同的，我将目标函数乘上一个常量$m$，并不会改变取得最小值时的$\theta$值。</p>
<p>第二点概念上的变化，我们只是指在使用支持向量机时，一些如下的标准惯例，而不是逻辑回归。因此，对于逻辑回归，在目标函数中，我们有两项：第一个是训练样本的代价，第二个是我们的正则化项，我们不得不去用这一项来平衡。这就相当于我们想要最小化$A$加上正则化参数$\lambda$，然后乘以其他项$B$对吧？这里的$A$表示这里的第一项，同时我用<strong>B</strong>表示第二项，但不包括$\lambda$，我们不是优化这里的$A+\lambda\times B$。我们所做的是通过设置不同正则参数$\lambda$达到优化目的。这样，我们就能够权衡对应的项，是使得训练样本拟合的更好。即最小化$A$。还是保证正则参数足够小，也即是对于<strong>B</strong>项而言，但对于支持向量机，按照惯例，我们将使用一个不同的参数替换这里使用的$\lambda$来权衡这两项。你知道，就是第一项和第二项我们依照惯例使用一个不同的参数称为$C$，同时改为优化目标，$C×A+B$因此，在逻辑回归中，如果给定$\lambda$，一个非常大的值，意味着给予B更大的权重。而这里，就对应于将$C$ 设定为非常小的值，那么，相应的将会给$B$比给$A$更大的权重。因此，这只是一种不同的方式来控制这种权衡或者一种不同的方法，即用参数来决定是更关心第一项的优化，还是更关心第二项的优化。当然你也可以把这里的参数$C$ 考虑成$1/\lambda$，同 $1/\lambda$所扮演的角色相同，并且这两个方程或这两个表达式并不相同，因为$C=1/\lambda$，但是也并不全是这样，如果当$C=1/\lambda$时，这两个优化目标应当得到相同的值，相同的最优值 $\theta$。因此，就用它们来代替。那么，我现在删掉这里的$\lambda$，并且用常数$C$来代替。因此，这就得到了在支持向量机中我们的整个优化目标函数。然后最小化这个目标函数，得到<strong>SVM</strong> 学习到的参数$C$。</p>
<p><img src="5a63e35db410fdb57c76de97ea888278.png" alt=""></p>
<p>最后有别于逻辑回归输出的概率。在这里，我们的代价函数，当最小化代价函数，获得参数$\theta$时，支持向量机所做的是它来直接预测$y$的值等于1，还是等于0。因此，这个假设函数会预测1。当$\theta^Tx$大于或者等于0时，或者等于0时，所以学习参数$\theta$就是支持向量机假设函数的形式。那么，这就是支持向量机数学上的定义。</p>
<p>在接下来的视频中，让我们再回去从直观的角度看看优化目标，实际上是在做什么，以及SVM的假设函数将会学习什么，同时也会谈谈如何做些许修改，学习更加复杂、非线性的函数。</p>
<h3 id="12-2-大边界的直观理解"><a href="#12-2-大边界的直观理解" class="headerlink" title="12.2 大边界的直观理解"></a>12.2 大边界的直观理解</h3><p>参考视频: 12 - 2 - Large Margin Intuition (11 min).mkv</p>
<p>人们有时将支持向量机看作是大间距分类器。在这一部分，我将介绍其中的含义，这有助于我们直观理解<strong>SVM</strong>模型的假设是什么样的。</p>
<p><img src="cc66af7cbd88183efc07c8ddf09cbc73.png" alt=""></p>
<p>这是我的支持向量机模型的代价函数，在左边这里我画出了关于z的代价函数${\cos}t_1{(z)}$，此函数用于正样本，而在右边这里我画出了关于$z$的代价函数${\cos}t_0{(z)}$，横轴表示$z$，现在让我们考虑一下，最小化这些代价函数的必要条件是什么。如果你有一个正样本，$y=1$，则只有在$z>=1$时，代价函数${\cos}t_1{(z)}$才等于0。</p>
<p>换句话说，如果你有一个正样本，我们会希望$\theta^Tx$>=1，反之，如果$y=0$，我们观察一下，函数${\cos}t_0{(z)}$，它只有在$z<=1$的区间里函数值为0。这是支持向量机的一个有趣性质。事实上，如果你有一个正样本$y=1$，则其实我们仅仅要求$\theta^tx$大于等于0，就能将该样本恰当分出，这是因为如果$\theta^tx$\>0大的话，我们的模型代价函数值为0，类似地，如果你有一个负样本，则仅需要$\theta^Tx$\<=0就会将负例正确分离，但是，支持向量机的要求更高，不仅仅要能正确分开输入的样本，即不仅仅要求$\theta^tx$\>0，我们需要的是比0值大很多，比如大于等于1，我也想这个比0小很多，比如我希望它小于等于-1，这就相当于在支持向量机中嵌入了一个额外的安全因子，或者说安全的间距因子。</=0就会将负例正确分离，但是，支持向量机的要求更高，不仅仅要能正确分开输入的样本，即不仅仅要求$\theta^tx$\></=1$的区间里函数值为0。这是支持向量机的一个有趣性质。事实上，如果你有一个正样本$y=1$，则其实我们仅仅要求$\theta^tx$大于等于0，就能将该样本恰当分出，这是因为如果$\theta^tx$\></p>
<p>当然，逻辑回归做了类似的事情。但是让我们看一下，在支持向量机中，这个因子会导致什么结果。具体而言，我接下来会考虑一个特例。我们将这个常数$C$设置成一个非常大的值。比如我们假设$C$的值为100000或者其它非常大的数，然后来观察支持向量机会给出什么结果？</p>
<p><img src="12ebd5973230e8fdf279ae09e187f437.png" alt=""></p>
<p>如果 $C$非常大，则最小化代价函数的时候，我们将会很希望找到一个使第一项为0的最优解。因此，让我们尝试在代价项的第一项为0的情形下理解该优化问题。比如我们可以把$C$设置成了非常大的常数，这将给我们一些关于支持向量机模型的直观感受。</p>
<p>​                $$\min_\limits{\theta}C\sum_\limits{i=1}^{m}\left[y^{(i)}{\cos}t_{1}\left(\theta^{T}x^{(i)}\right)+\left(1-y^{(i)}\right){\cos}t\left(\theta^{T}x^{(i)}\right)\right]+\frac{1}{2}\sum_\limits{i=1}^{n}\theta^{2}_{j}$$</p>
<p>我们已经看到输入一个训练样本标签为$y=1$，你想令第一项为0，你需要做的是找到一个$\theta$，使得$\theta^Tx>=1$，类似地，对于一个训练样本，标签为$y=0$，为了使${\cos}t_0{(z)}$ 函数的值为0，我们需要$\theta^Tx<=-1$。因此，现在考虑我们的优化问题。选择参数，使得第一项等于0，就会导致下面的优化问题，因为我们将选择参数使第一项为0，因此这个函数的第一项为0，因此是$c$乘以0加上二分之一乘以第二项。这里第一项是$c$乘以0，因此可以将其删去，因为我知道它是0。< p="">
<p>这将遵从以下的约束：$\theta^Tx^{(i)}>=1$，如果 $y^{(i)}$是等于1 的，$\theta^Tx^{(i)}<=-1$，如果样本$i$是一个负样本，这样当你求解这个优化问题的时候，当你最小化这个关于变量$\theta$的函数的时候，你会得到一个非常有趣的决策边界。< p="">
<p><img src="b1f670fddd9529727aa16a559d49d151.png" alt=""></p>
<p>具体而言，如果你考察这样一个数据集，其中有正样本，也有负样本，可以看到这个数据集是线性可分的。我的意思是，存在一条直线把正负样本分开。当然有多条不同的直线，可以把正样本和负样本完全分开。</p>
<p><img src="01105c3afd1315acf0577f8493137dcc.png" alt=""></p>
<p>比如，这就是一个决策边界可以把正样本和负样本分开。但是多多少少这个看起来并不是非常自然是么?</p>
<p>或者我们可以画一条更差的决策界，这是另一条决策边界，可以将正样本和负样本分开，但仅仅是勉强分开，这些决策边界看起来都不是特别好的选择，支持向量机将会选择这个黑色的决策边界，相较于之前我用粉色或者绿色画的决策界。这条黑色的看起来好得多，黑线看起来是更稳健的决策界。在分离正样本和负样本上它显得的更好。数学上来讲，这是什么意思呢？这条黑线有更大的距离，这个距离叫做间距(<strong>margin</strong>)。</p>
<p><img src="e68e6ca3275f433330a7981971eb4f16.png" alt=""></p>
<p>当画出这两条额外的蓝线，我们看到黑色的决策界和训练样本之间有更大的最短距离。然而粉线和蓝线离训练样本就非常近，在分离样本的时候就会比黑线表现差。因此，这个距离叫做支持向量机的间距，而这是支持向量机具有鲁棒性的原因，因为它努力用一个最大间距来分离样本。因此支持向量机有时被称为<strong>大间距分类器</strong>，而这其实是求解上一页幻灯片上优化问题的结果。</p>
<p>我知道你也许想知道求解上一页幻灯片中的优化问题为什么会产生这个结果？它是如何产生这个大间距分类器的呢？我知道我还没有解释这一点。</p>
<p>我将会从直观上略述为什么这个优化问题会产生大间距分类器。总之这个图示有助于你理解支持向量机模型的做法，即努力将正样本和负样本用最大的间距分开。</p>
<p><img src="dd6239efad3d3ee7a89a28574d7795b3.png" alt=""></p>
<p>在本节课中关于大间距分类器，我想讲最后一点：我们将这个大间距分类器中的正则化因子常数$C$设置的非常大，我记得我将其设置为了100000，因此对这样的一个数据集，也许我们将选择这样的决策界，从而最大间距地分离开正样本和负样本。那么在让代价函数最小化的过程中，我们希望找出在$y=1$和$y=0$两种情况下都使得代价函数中左边的这一项尽量为零的参数。如果我们找到了这样的参数，则我们的最小化问题便转变成：</p>
<p><img src="f4b6dee99cfb4352b3cac5287002e8de.png" alt=""></p>
<p>事实上，支持向量机现在要比这个大间距分类器所体现得更成熟，尤其是当你使用大间距分类器的时候，你的学习算法会受异常点(outlier) 的影响。比如我们加入一个额外的正样本。</p>
<p><img src="b8fbe2f6ac48897cf40497a2d034c691.png" alt=""></p>
<p>在这里，如果你加了这个样本，为了将样本用最大间距分开，也许我最终会得到一条类似这样的决策界，对么？就是这条粉色的线，仅仅基于一个异常值，仅仅基于一个样本，就将我的决策界从这条黑线变到这条粉线，这实在是不明智的。而如果正则化参数$C$，设置的非常大，这事实上正是支持向量机将会做的。它将决策界，从黑线变到了粉线，但是如果$C$ 设置的小一点，<strong>如果你将C设置的不要太大，则你最终会得到这条黑线，</strong>当然数据如果不是线性可分的，如果你在这里有一些正样本或者你在这里有一些负样本，则支持向量机也会将它们恰当分开。因此，大间距分类器的描述，仅仅是从直观上给出了正则化参数$C$非常大的情形，同时，要提醒你$C$的作用类似于$1/\lambda$，$\lambda$是我们之前使用过的正则化参数。这只是$C$非常大的情形，或者等价地 $\lambda$ 非常小的情形。你最终会得到类似粉线这样的决策界，但是实际上应用支持向量机的时候，<strong>当$C$不是非常非常大的时候，它可以忽略掉一些异常点的影响，得到更好的决策界。</strong>甚至当你的数据不是线性可分的时候，支持向量机也可以给出好的结果。</p>
<p>回顾 $C=1/\lambda$，因此：</p>
<p>$C$ 较大时，相当于 $\lambda$ 较小，可能会导致过拟合，高方差。</p>
<p>$C$ 较小时，相当于$\lambda$较大，可能会导致低拟合，高偏差。</p>
<p>我们稍后会介绍支持向量机的偏差和方差，希望在那时候关于如何处理参数的这种平衡会变得更加清晰。我希望，这节课给出了一些关于为什么支持向量机被看做大间距分类器的直观理解。它用最大间距将样本区分开，尽管从技术上讲，这只有当参数$C$是非常大的时候是真的，但是它对于理解支持向量机是有益的。</p>
<p>本节课中我们略去了一步，那就是我们在幻灯片中给出的优化问题。为什么会是这样的？它是如何得出大间距分类器的？我在本节中没有讲解，在下一节课中，我将略述这些问题背后的数学原理，来解释这个优化问题是如何得到一个大间距分类器的。</p>
<h3 id="12-3-数学背后的大边界分类（选修）"><a href="#12-3-数学背后的大边界分类（选修）" class="headerlink" title="12.3 数学背后的大边界分类（选修）"></a>12.3 数学背后的大边界分类（选修）</h3><p>参考视频: 12 - 3 - Mathematics Behind Large Margin Classification (Optional) (20 min).mkv</p>
<p>在本节课中，我将介绍一些大间隔分类背后的数学原理。本节为选学部分，你完全可以跳过它，但是听听这节课可能让你对支持向量机中的优化问题，以及如何得到大间距分类器，产生更好的直观理解。</p>
<p><img src="55e05845c636c8c99c03e6e29337d8c4.png" alt=""></p>
<p>首先，让我来给大家复习一下关于向量内积的知识。假设我有两个向量，$u$和$v$，我将它们写在这里。两个都是二维向量，我们看一下，$u^T v$的结果。$u^T v$也叫做向量$u$和$v$之间的内积。由于是二维向量，我可以将它们画在这个图上。我们说，这就是向量$u$即在横轴上，取值为某个${{u}_{1}}$，而在纵轴上，高度是某个${{u}_{2}}$作为$u$的第二个分量。现在，很容易计算的一个量就是向量$u$的范数。$\left\| u \right\|$表示$u$的范数，即$u$的长度，即向量$u$的欧几里得长度。根据毕达哥拉斯定理，$\left\| u \right\|=\sqrt{u_{1}^{2}+u_{2}^{2}}$，这是向量$u$的长度，它是一个实数。现在你知道了这个的长度是多少了。我刚刚画的这个向量的长度就知道了。</p>
现在让我们回头来看向量$v$ ，因为我们想计算内积。$v$是另一个向量，它的两个分量${{v}_{1}}$和${{v}_{2}}$是已知的。向量$v$可以画在这里，现在让我们来看看如何计算$u$和$v$之间的内积。这就是具体做法，我们将向量$v$投影到向量$u$上，我们做一个直角投影，或者说一个90度投影将其投影到$u$上，接下来我度量这条红线的长度。我称这条红线的长度为$p$，因此$p$就是长度，或者说是向量$v$投影到向量$u$上的量，我将它写下来，$p$是$v$投影到向量$u$上的长度，因此可以将${{u}^{T}}v=p\centerdot \left\| u \right\|$，或者说$u$的长度。这是计算内积的一种方法。如果你从几何上画出p的值，同时画出$u$的范数，你也会同样地计算出内积，答案是一样的。另一个计算公式是：$u^T v$就是$\left[ {{u}_{1}}\text{ }{{u}_{2}} \right]$ 这个一行两列的矩阵乘以$v$。因此可以得到${{u}_{1}}\times {{v}_{1}}+{{u}_{2}}\times {{v}_{2}}$。根据线性代数的知识，这两个公式会给出同样的结果。顺便说一句，$u^Tv=v^Tu$。因此如果你将$u$和$v$交换位置，将$u$投影到$v$上，而不是将$v$投影到$u$上，然后做同样地计算，只是把$u$和$v$的位置交换一下，你事实上可以得到同样的结果。申明一点，在这个等式中$u$的范数是一个实数，$p$也是一个实数，因此$u^T v$就是两个实数正常相乘。
<p><img src="44ad37bce4b7e03835095dccbd2a7b7a.png" alt=""></p>
<p>最后一点，需要注意的就是$p$值，$p$事实上是有符号的，即它可能是正值，也可能是负值。我的意思是说，如果$u$是一个类似这样的向量，$v$是一个类似这样的向量，$u$和$v$之间的夹角大于90度，则如果将$v$投影到$u$上，会得到这样的一个投影，这是$p$的长度，在这个情形下我们仍然有${{u}^{T}}v$是等于$p$乘以$u$的范数。唯一一点不同的是$p$在这里是负的。在内积计算中，如果$u$和$v$之间的夹角小于90度，那么那条红线的长度$p$是正值。然而如果这个夹角大于90度，则$p$将会是负的。就是这个小线段的长度是负的。如果它们之间的夹角大于90度，两个向量之间的内积也是负的。这就是关于向量内积的知识。我们接下来将会使用这些关于向量内积的性质试图来理解支持向量机中的目标函数。</p>
<p><img src="03bd4b3ff69e327f7949c3d2a73eed8a.png" alt=""></p>
<p>这就是我们先前给出的支持向量机模型中的目标函数。为了讲解方便，我做一点简化，仅仅是为了让目标函数更容易被分析。</p>
<p><img src="3cc61c6e5fe85c2a7bf8170f5bbfd8c3.png" alt=""></p>
我接下来忽略掉截距，令${{\theta }_{0}}=0$，这样更容易画示意图。我将特征数$n$置为2，因此我们仅有两个特征${{x}_{1}},{{x}_{2}}$，现在我们来看一下目标函数，支持向量机的优化目标函数。当我们仅有两个特征，即$n=2$时，这个式子可以写作：$\frac{1}{2}\left({\theta_1^2+\theta_2^2}\right)=\frac{1}{2}\left(\sqrt{\theta_1^2+\theta_2^2}\right)^2$，我们只有两个参数${{\theta }_{1}},{{\theta }_{2}}$。你可能注意到括号里面的这一项是向量$\theta$的范数，或者说是向量$\theta$的长度。我的意思是如果我们将向量$\theta$写出来，那么我刚刚画红线的这一项就是向量$\theta$的长度或范数。这里我们用的是之前学过的向量范数的定义，事实上这就等于向量$\theta$的长度。
当然你可以将其写作${{\theta }_{0}}\text{,}{{\theta }_{1}},{{\theta }_{2}}$，如果${{\theta }_{0}}=0$，那就是${{\theta }_{1}},{{\theta }_{2}}$的长度。在这里我将忽略${{\theta }_{0}}$，这样来写$\theta$的范数，它仅仅和${{\theta }_{1}},{{\theta }_{2}}$有关。但是，数学上不管你是否包含，其实并没有差别，因此在我们接下来的推导中去掉${{\theta }_{0}}$不会有影响这意味着我们的目标函数是等于$\frac{1}{2}\left\| \theta \right\|^2$。因此支持向量机做的全部事情，就是<strong>极小化参数向量</strong>$\theta$<strong>范数的平方，或者说长度的平方</strong>。<br><br>现在我将要看看这些项：$\theta^{T}x$更深入地理解它们的含义。给定参数向量$\theta $给定一个样本$x$，这等于什么呢?在前一页幻灯片上，我们画出了在不同情形下，$u^Tv$的示意图，我们将会使用这些概念，$\theta $和$x^{(i)}$就类似于$u$和$v$ 。<br><br><img src="4510b8fbc90ba2b233bb6996529f2df1.png" alt=""><br><br>让我们看一下示意图：们考察一个单一的训练样本，我有一个正样本在这里，用一个叉来表示这个样本$x^{(i)}​$，意思是在水平轴上取值为$x_1^{(i)}​$，在竖直轴上取值为$x_2^{(i)}​$。这就是我画出的训练样本。尽管我没有将其真的看做向量。它事实上就是一个始于原点，终点位置在这个训练样本点的向量。现在，我们有一个参数向量我会将它也画成向量。我将$θ_1​$画在横轴这里，将$θ_2​$ 画在纵轴这里，那么内积$θ^T x^{(i)}$ 将会是什么呢？<br><br>使用我们之前的方法，我们计算的方式就是我将训练样本投影到参数向量$\theta$，然后我来看一看这个线段的长度，我将它画成红色。我将它称为$p^{(i)}$用来表示这是第 $i$个训练样本在参数向量$\theta$上的投影。根据我们之前幻灯片的内容，我们知道的是$θ^Tx^{(i)}$将会等于$p$ 乘以向量 $θ$ 的长度或范数。这就等于$\theta_1\cdot{x_1^{(i)}}+\theta_2\cdot{x_2^{(i)}}$。这两种方式是等价的，都可以用来计算$θ$和$x^{(i)}$之间的内积。
这告诉了我们什么呢？这里表达的意思是：这个$θ^Tx^{(i)}>=1$  或者$θ^Tx^{(i)}<-1$的,约束是可以被$p^{(i)}\cdot{x}>=1$这个约束所代替的。因为$θ^Tx^{(i)}=p^{(i)}\cdot{\left\| \theta \right\|}$ ，将其写入我们的优化目标。我们将会得到没有了约束，$θ^Tx^{(i)}$而变成了$p^{(i)}\cdot{\left\| \theta \right\|}$。
<p><img src="912cb43058cee46ddf51598b7538968c.png" alt=""></p>
<p>需要提醒一点，我们之前曾讲过这个优化目标函数可以被写成等于$\frac{1}{2}\left\| \theta \right\|^2$。</p>
<p>现在让我们考虑下面这里的训练样本。现在，继续使用之前的简化，即${{\theta }_{0}}=0$，我们来看一下支持向量机会选择什么样的决策界。这是一种选择，我们假设支持向量机会选择这个决策边界。这不是一个非常好的选择，因为它的间距很小。这个决策界离训练样本的距离很近。我们来看一下为什么支持向量机不会选择它。</p>
<p>对于这样选择的参数$\theta$，可以看到参数向量$\theta$事实上是和决策界是90度正交的，因此这个绿色的决策界对应着一个参数向量$\theta$这个方向,顺便提一句${{\theta }_{0}}=0$的简化仅仅意味着决策界必须通过原点$(0,0)$。现在让我们看一下这对于优化目标函数意味着什么。</p>
<p><img src="20725ba601c1c90d1024e50fc24c579b.png" alt=""></p>
<p>比如这个样本，我们假设它是我的第一个样本$x^{(1)}$，如果我考察这个样本到参数$\theta$的投影，投影是这个短的红线段，就等于$p^{(1)}$，它非常短。类似地，这个样本如果它恰好是$x^{(2)}$，我的第二个训练样本，则它到$\theta$的投影在这里。我将它画成粉色，这个短的粉色线段是$p^{(2)}$，即第二个样本到我的参数向量$\theta$的投影。因此，这个投影非常短。$p^{(2)}$事实上是一个负值，$p^{(2)}$是在相反的方向，这个向量和参数向量$\theta$的夹角大于90度，$p^{(2)}$的值小于0。</p>
我们会发现这些$p^{(i)}$将会是非常小的数，因此当我们考察优化目标函数的时候，对于正样本而言，我们需要$p^{(i)}\cdot{\left\| \theta \right\|}>=1$,但是如果 $p^{(i)}$在这里非常小,那就意味着我们需要$\theta$的范数非常大.因为如果 $p^{(1)}$ 很小,而我们希望$p^{(1)}\cdot{\left\| \theta \right\|}>=1$,令其实现的唯一的办法就是这两个数较大。如果 $p^{(1)}$ 小，我们就希望$\theta$的范数大。类似地，对于负样本而言我们需要$p^{(2)}\cdot{\left\|\theta \right\|}<=-1$。我们已经在这个样本中看到$p^{(2)}$会是一个非常小的数，因此唯一的办法就是$\theta$的范数变大。但是我们的目标函数是希望找到一个参数$\theta$，它的范数是小的。因此，这看起来不像是一个好的参数向量$\theta$的选择。<br><br><img src="5eab58ad9cb54b3b6fda8f6c96efff24.png" alt=""><br><br>相反的，来看一个不同的决策边界。比如说，支持向量机选择了这个决策界，现在状况会有很大不同。如果这是决策界，这就是相对应的参数$\theta$的方向，因此，在这个决策界之下，垂直线是决策界。使用线性代数的知识，可以说明，这个绿色的决策界有一个垂直于它的向量$\theta$。现在如果你考察你的数据在横轴$x$上的投影，比如这个我之前提到的样本，我的样本$x^{(1)}$，当我将它投影到横轴$x$上，或说投影到$\theta$上，就会得到这样$p^{(1)}$。它的长度是$p^{(1)}$，另一个样本，那个样本是$x^{(2)}$。我做同样的投影，我会发现，$p^{(2)}$的长度是负值。你会注意到现在$p^{(1)}$ 和$p^{(2)}$这些投影长度是长多了。如果我们仍然要满足这些约束，$P^{(i)}\cdot{\left\| \theta \right\|}$\>1，则因为$p^{(1)}$变大了，$\theta$的范数就可以变小了。因此这意味着通过选择右边的决策界，而不是左边的那个，支持向量机可以使参数$\theta$的范数变小很多。因此，如果我们想令$\theta$的范数变小，从而令$\theta$范数的平方变小，就能让支持向量机选择右边的决策界。这就是支持向量机如何能有效地产生大间距分类的原因。<br><br>看这条绿线，这个绿色的决策界。我们希望正样本和负样本投影到$\theta$的值大。要做到这一点的唯一方式就是选择这条绿线做决策界。这是大间距决策界来区分开正样本和负样本这个间距的值。这个间距的值就是$p^{(1)},p^{(2)},p^{(3)}$等等的值。通过让间距变大，即通过这些$p^{(1)},p^{(2)},p^{(3)}$等等的值，支持向量机最终可以找到一个较小的$\theta$范数。这正是支持向量机中最小化目标函数的目的。<br><br>以上就是为什么支持向量机最终会找到大间距分类器的原因。因为它试图极大化这些$p^{(i)}$的范数，它们是训练样本到决策边界的距离。最后一点，我们的推导自始至终使用了这个简化假设，就是参数$θ_0=0$。<br><br><img src="65198a1748fdbe16da34afab9f33d801.png" alt=""><br><br>就像我之前提到的。这个的作用是：$θ_0=0$的意思是我们让决策界通过原点。如果你令$θ_0$不是0的话，含义就是你希望决策界不通过原点。我将不会做全部的推导。实际上，支持向量机产生大间距分类器的结论，会被证明同样成立，证明方式是非常类似的，是我们刚刚做的证明的推广。<br><br>之前视频中说过，即便$θ_0$不等于0，支持向量机要做的事情都是优化这个目标函数对应着$C$值非常大的情况，但是可以说明的是，即便$θ_0$不等于0，支持向量机仍然会找到正样本和负样本之间的大间距分隔。<br><br>总之，我们解释了为什么支持向量机是一个大间距分类器。在下一节我们，将开始讨论如何利用支持向量机的原理，应用它们建立一个复杂的非线性分类器。<br><br>### 12.4 核函数1<br><br>参考视频: 12 - 4 - Kernels I (16 min).mkv<br><br>回顾我们之前讨论过可以使用高级数的多项式模型来解决无法用直线进行分隔的分类问题：<br><br><img src="529b6dbc07c9f39f5266bd0b3f628545.png" alt=""><br><br>为了获得上图所示的判定边界，我们的模型可能是${{\theta }_{0}}+{{\theta }_{1}}{{x}_{1}}+{{\theta }_{2}}{{x}_{2}}+{{\theta }_{3}}{{x}_{1}}{{x}_{2}}+{{\theta }_{4}}x_{1}^{2}+{{\theta }_{5}}x_{2}^{2}+\cdots $的形式。<br><br>我们可以用一系列的新的特征f来替换模型中的每一项。例如令：<br>${{f}_{1}}={{x}_{1}},{{f}_{2}}={{x}_{2}},{{f}_{3}}={{x}_{1}}{{x}_{2}},{{f}_{4}}=x_{1}^{2},{{f}_{5}}=x_{2}^{2}$
<p>…得到$h_θ(x)=f_1+f_2+…+f_n$。然而，除了对原有的特征进行组合以外，有没有更好的方法来构造$f_1,f_2,f_3$？我们可以利用核函数来计算出新的特征。</p>
<p>给定一个训练实例$x$，我们利用$x$的各个特征与我们预先选定的<strong>地标</strong>(<strong>landmarks</strong>)$l^{(1)},l^{(2)},l^{(3)}$的近似程度来选取新的特征$f_1,f_2,f_3$。</p>
<p><img src="2516821097bda5dfaf0b94e55de851e0.png" alt=""></p>
<p>例如：<br>${{f}_{1}}=similarity(x,{{l}^{(1)}})=e(-\frac{{{\left\| x-{{l}^{(1)}} \right\|}^{2}}}{2{{\sigma }^{2}}})$</p>
<p>其中：${{\left\| x-{{l}^{(1)}} \right\|}^{2}}=\sum{_{j=1}^{n}}{{({{x}_{j}}-l_{j}^{(1)})}^{2}}$，为实例$x$中所有特征与地标$l^{(1)}$之间的距离的和。上例中的$similarity(x,{{l}^{(1)}})$就是核函数，具体而言，这里是一个<strong>高斯核函数</strong>(<strong>Gaussian Kernel</strong>)。 <strong>注：这个函数与正态分布没什么实际上的关系，只是看上去像而已。</strong></p>
<p>这些地标的作用是什么？<br>如果一个训练实例$x$与地标$L$之间的距离近似于0，则新特征 $f$近似于$e^{-0}=1$，如果训练实例$x$与地标$L$之间距离较远，则$f$近似于$e^{-(一个较大的数)}=0$。</p>
<p>假设我们的训练实例含有两个特征[$x_{1}$ $x{_2}$]，给定地标$l^{(1)}$与不同的$\sigma$值，见下图：</p>
<p><img src="b9acfc507a54f5ca13a3d50379972535.jpg" alt=""></p>
<p>图中水平面的坐标为 $x_{1}$，$x_{2}$而垂直坐标轴代表$f$。可以看出，只有当$x$与$l^{(1)}$重合时$f$才具有最大值。随着$x$的改变$f$值改变的速率受到$\sigma^2$的控制。</p>
<p>在下图中，当实例处于洋红色的点位置处，因为其离$l^{(1)}$更近，但是离$l^{(2)}$和$l^{(3)}$较远，因此$f_1$接近1，而$f_2$,$f_3$接近0。因此$h_θ(x)=θ_0+θ_1f_1+θ_2f_2+θ_1f_3>0$，因此预测$y=1$。同理可以求出，对于离$l^{(2)}$较近的绿色点，也预测$y=1$，但是对于蓝绿色的点，因为其离三个地标都较远，预测$y=0$。</p>
<p><img src="3d8959d0d12fe9914dc827d5a074b564.jpg" alt=""></p>
<p>这样，图中红色的封闭曲线所表示的范围，便是我们依据一个单一的训练实例和我们选取的地标所得出的判定边界，在预测时，我们采用的特征不是训练实例本身的特征，而是通过核函数计算出的新特征$f_1,f_2,f_3$。</p>
<h3 id="12-5-核函数2"><a href="#12-5-核函数2" class="headerlink" title="12.5 核函数2"></a>12.5 核函数2</h3><p>参考视频: 12 - 5 - Kernels II (16 min).mkv</p>
<p>在上一节视频里，我们讨论了核函数这个想法，以及怎样利用它去实现支持向量机的一些新特性。在这一节视频中，我将补充一些缺失的细节，并简单的介绍一下怎么在实际中使用应用这些想法。</p>
<p>如何选择地标？</p>
<p>我们通常是根据训练集的数量选择地标的数量，即如果训练集中有$m$个实例，则我们选取$m$个地标，并且令:$l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},.....,l^{(m)}=x^{(m)}$。这样做的好处在于：现在我们得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的，即：</p>
<p><img src="eca2571849cc36748c26c68708a7a5bd.png" alt=""></p>
<p><img src="ea31af620b0a0132fe494ebb4a362465.png" alt=""></p>
<p>下面我们将核函数运用到支持向量机中，修改我们的支持向量机假设为：</p>
<p>• 给定$x$，计算新特征$f$，当$θ^Tf&gt;=0$ 时，预测 $y=1$，否则反之。 </p>
<p>相应地修改代价函数为：$\sum{_{j=1}^{n=m}}\theta _{j}^{2}={{\theta}^{T}}\theta $，</p>
<p>$min C\sum\limits_{i=1}^{m}{[{{y}^{(i)}}cos {{t}_{1}}}( {{\theta }^{T}}{{f}^{(i)}})+(1-{{y}^{(i)}})cos {{t}_{0}}( {{\theta }^{T}}{{f}^{(i)}})]+\frac{1}{2}\sum\limits_{j=1}^{n=m}{\theta _{j}^{2}}$.</p>
<p>在具体实施过程中，我们还需要对最后的正则化项进行些微调整，在计算$\sum{_{j=1}^{n=m}}\theta _{j}^{2}={{\theta}^{T}}\theta $时，我们用$θ^TMθ$代替$θ^Tθ$，其中$M$是根据我们选择的核函数而不同的一个矩阵。这样做的原因是为了简化计算。</p>
<p>理论上讲，我们也可以在逻辑回归中使用核函数，但是上面使用 $M$来简化计算的方法不适用与逻辑回归，因此计算将非常耗费时间。</p>
<p>在此，我们不介绍最小化支持向量机的代价函数的方法，你可以使用现有的软件包（如<strong>liblinear</strong>,<strong>libsvm</strong>等）。在使用这些软件包最小化我们的代价函数之前，我们通常需要编写核函数，并且如果我们使用高斯核函数，那么在使用之前进行特征缩放是非常必要的。</p>
<p>另外，支持向量机也可以不使用核函数，不使用核函数又称为<strong>线性核函数</strong>(<strong>linear kernel</strong>)，当我们不采用非常复杂的函数，或者我们的训练集特征非常多而实例非常少的时候，可以采用这种不带核函数的支持向量机。</p>
<p>下面是支持向量机的两个参数$C$和$\sigma$的影响：</p>
<p>$C=1/\lambda$</p>
<p>$C$ 较大时，相当于$\lambda$较小，可能会导致过拟合，高方差；</p>
<p>$C$ 较小时，相当于$λ$较大，可能会导致低拟合，高偏差；</p>
<p>$\sigma$较大时，可能会导致低方差，高偏差；</p>
<p>$\sigma$较小时，可能会导致低偏差，高方差。</p>
<p>如果你看了本周的编程作业，你就能亲自实现这些想法，并亲眼看到这些效果。这就是利用核函数的支持向量机算法，希望这些关于偏差和方差的讨论，能给你一些对于算法结果预期的直观印象。</p>
<h3 id="12-6-使用支持向量机"><a href="#12-6-使用支持向量机" class="headerlink" title="12.6 使用支持向量机"></a>12.6 使用支持向量机</h3><p>参考视频: 12 - 6 - Using An SVM (21 min).mkv</p>
<p>目前为止，我们已经讨论了<strong>SVM</strong>比较抽象的层面，在这个视频中我将要讨论到为了运行或者运用<strong>SVM</strong>。你实际上所需要的一些东西：支持向量机算法，提出了一个特别优化的问题。但是就如在之前的视频中我简单提到的，我真的不建议你自己写软件来求解参数$\theta$，因此由于今天我们中的很少人，或者其实没有人考虑过自己写代码来转换矩阵，或求一个数的平方根等我们只是知道如何去调用库函数来实现这些功能。同样的，用以解决<strong>SVM</strong>最优化问题的软件很复杂，且已经有研究者做了很多年数值优化了。因此你提出好的软件库和好的软件包来做这样一些事儿。然后强烈建议使用高优化软件库中的一个，而不是尝试自己落实一些数据。有许多好的软件库，我正好用得最多的两个是<strong>liblinear</strong>和<strong>libsvm</strong>，但是真的有很多软件库可以用来做这件事儿。你可以连接许多你可能会用来编写学习算法的主要编程语言。</p>
<p>在高斯核函数之外我们还有其他一些选择，如：</p>
<p>多项式核函数（<strong>Polynomial Kerne</strong>l）</p>
<p>字符串核函数（<strong>String kernel</strong>）</p>
<p>卡方核函数（ <strong>chi-square kernel</strong>）</p>
<p>直方图交集核函数（<strong>histogram intersection kernel</strong>）</p>
<p>等等…</p>
<p>这些核函数的目标也都是根据训练集和地标之间的距离来构建新特征，这些核函数需要满足Mercer’s定理，才能被支持向量机的优化软件正确处理。</p>
<p>多类分类问题</p>
<p>假设我们利用之前介绍的一对多方法来解决一个多类分类问题。如果一共有$k$个类，则我们需要$k$个模型，以及$k$个参数向量$\theta $。我们同样也可以训练k个支持向量机来解决多类分类问题。但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。</p>
<p>尽管你不去写你自己的<strong>SVM</strong>的优化软件，但是你也需要做几件事：</p>
<p>1、是提出参数$C$的选择。我们在之前的视频中讨论过误差/方差在这方面的性质。</p>
<p>2、你也需要选择内核参数或你想要使用的相似函数，其中一个选择是：我们选择不需要任何内核参数，没有内核参数的理念，也叫线性核函数。因此，如果有人说他使用了线性核的<strong>SVM</strong>（支持向量机），这就意味这他使用了不带有核函数的<strong>SVM</strong>（支持向量机）。</p>
<p>从逻辑回归模型，我们得到了支持向量机模型，在两者之间，我们应该如何选择呢？</p>
<p><strong>下面是一些普遍使用的准则：</strong></p>
<p>$n$为特征数，$m$为训练样本数。</p>
<p>(1)如果相较于$m$而言，$n$要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。</p>
<p>(2)如果$n$较小，而且$m$大小中等，例如$n$在 1-1000 之间，而$m$在10-10000之间，使用高斯核函数的支持向量机。</p>
<p>(3)如果$n$较小，而$m$较大，例如$n$在1-1000之间，而$m$大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。</p>
<p>值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。</p>
<p>今天的<strong>SVM</strong>包会工作得很好，但是它们仍然会有一些慢。当你有非常非常大的训练集，且用高斯核函数是在这种情况下，我经常会做的是尝试手动地创建，拥有更多的特征变量，然后用逻辑回归或者不带核函数的支持向量机。如果你看到这个幻灯片，看到了逻辑回归，或者不带核函数的支持向量机。在这个两个地方，我把它们放在一起是有原因的。原因是：逻辑回归和不带核函数的支持向量机它们都是非常相似的算法，不管是逻辑回归还是不带核函数的<strong>SVM</strong>，通常都会做相似的事情，并给出相似的结果。但是根据你实现的情况，其中一个可能会比另一个更加有效。但是在其中一个算法应用的地方，逻辑回归或不带核函数的<strong>SVM</strong>另一个也很有可能很有效。但是随着<strong>SVM</strong>的复杂度增加，当你使用不同的内核函数来学习复杂的非线性函数时，这个体系，你知道的，当你有多达1万（10,000）的样本时，也可能是5万（50,000），你的特征变量的数量这是相当大的。那是一个非常常见的体系，也许在这个体系里，不带核函数的支持向量机就会表现得相当突出。你可以做比这困难得多需要逻辑回归的事情。</p>
<p>最后，神经网络使用于什么时候呢？ 对于所有的这些问题，对于所有的这些不同体系一个设计得很好的神经网络也很有可能会非常有效。有一个缺点是，或者说是有时可能不会使用神经网络的原因是：对于许多这样的问题，神经网络训练起来可能会特别慢，但是如果你有一个非常好的<strong>SVM</strong>实现包，它可能会运行得比较快比神经网络快很多，尽管我们在此之前没有展示，但是事实证明，<strong>SVM</strong>具有的优化问题，是一种凸优化问题。因此，好的<strong>SVM</strong>优化软件包总是会找到全局最小值，或者接近它的值。对于<strong>SVM</strong>你不需要担心局部最优。在实际应用中，局部最优不是神经网络所需要解决的一个重大问题，所以这是你在使用<strong>SVM</strong>的时候不需要太去担心的一个问题。根据你的问题，神经网络可能会比<strong>SVM</strong>慢，尤其是在这样一个体系中，至于这里给出的参考，看上去有些模糊，如果你在考虑一些问题，这些参考会有一些模糊，但是我仍然不能完全确定，我是该用这个算法还是改用那个算法，这个没有太大关系，当我遇到机器学习问题的时候，有时它确实不清楚这是否是最好的算法，但是就如在之前的视频中看到的算法确实很重要。但是通常更加重要的是：你有多少数据，你有多熟练是否擅长做误差分析和排除学习算法，指出如何设定新的特征变量和找出其他能决定你学习算法的变量等方面，通常这些方面会比你使用逻辑回归还是<strong>SVM</strong>这方面更加重要。但是，已经说过了，<strong>SVM</strong>仍然被广泛认为是一种最强大的学习算法，这是一个体系，包含了什么时候一个有效的方法去学习复杂的非线性函数。因此，实际上与逻辑回归、神经网络、<strong>SVM</strong>一起使用这些方法来提高学习算法，我认为你会很好地建立很有技术的状态。（编者注：当时<strong>GPU</strong>计算比较慢，神经网络还不流行。）</p>
<p>机器学习系统对于一个宽泛的应用领域来说，这是另一个在你军械库里非常强大的工具，你可以把它应用到很多地方，如硅谷、在工业、学术等领域建立许多高性能的机器学习系统。</p>
</=-1$。我们已经在这个样本中看到$p^{(2)}$会是一个非常小的数，因此唯一的办法就是$\theta$的范数变大。但是我们的目标函数是希望找到一个参数$\theta$，它的范数是小的。因此，这看起来不像是一个好的参数向量$\theta$的选择。<br></-1$的,约束是可以被$p^{(i)}\cdot{x}></=-1$，如果样本$i$是一个负样本，这样当你求解这个优化问题的时候，当你最小化这个关于变量$\theta$的函数的时候，你会得到一个非常有趣的决策边界。<></p></=-1$。因此，现在考虑我们的优化问题。选择参数，使得第一项等于0，就会导致下面的优化问题，因为我们将选择参数使第一项为0，因此这个函数的第一项为0，因此是$c$乘以0加上二分之一乘以第二项。这里第一项是$c$乘以0，因此可以将其删去，因为我知道它是0。<></p>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习（十一）</title>
    <url>/2018/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<p>十五、异常检测(Anomaly Detection)</p>
<h3 id="15-1-问题的动机"><a href="#15-1-问题的动机" class="headerlink" title="15.1 问题的动机"></a>15.1 问题的动机</h3><p>参考文档: 15 - 1 - Problem Motivation (8 min).mkv</p>
<p>在接下来的一系列视频中，我将向大家介绍异常检测(<strong>Anomaly detection</strong>)问题。这是机器学习算法的一个常见应用。这种算法的一个有趣之处在于：它虽然主要用于非监督学习问题，但从某些角度看，它又类似于一些监督学习问题。</p>
<p>什么是异常检测呢？为了解释这个概念，让我举一个例子吧：</p>
<p>假想你是一个飞机引擎制造商，当你生产的飞机引擎从生产线上流出时，你需要进行<strong>QA</strong>(质量控制测试)，而作为这个测试的一部分，你测量了飞机引擎的一些特征变量，比如引擎运转时产生的热量，或者引擎的振动等等。</p>
<p><img src="93d6dfe7e5cb8a46923c178171889747.png" alt=""></p>
<p>这样一来，你就有了一个数据集，从$x^{(1)}$到$x^{(m)}$，如果你生产了$m$个引擎的话，你将这些数据绘制成图表，看起来就是这个样子：</p>
<p><img src="fe4472adbf6ddd9d9b51d698cc750b68.png" alt=""></p>
<p>这里的每个点、每个叉，都是你的无标签数据。这样，异常检测问题可以定义如下：我们假设后来有一天，你有一个新的飞机引擎从生产线上流出，而你的新飞机引擎有特征变量$x_{test}$。所谓的异常检测问题就是：我们希望知道这个新的飞机引擎是否有某种异常，或者说，我们希望判断这个引擎是否需要进一步测试。因为，如果它看起来像一个正常的引擎，那么我们可以直接将它运送到客户那里，而不需要进一步的测试。</p>
<p>给定数据集 $x^{(1)},x^{(2)},..,x^{(m)}$，我们假使数据集是正常的，我们希望知道新的数据 $x_{test}$ 是不是异常的，即这个测试数据不属于该组数据的几率如何。我们所构建的模型应该能根据该测试数据的位置告诉我们其属于一组数据的可能性 $p(x)$。</p>
<p><img src="65afdea865d50cba12d4f7674d599de5.png" alt=""></p>
<p>上图中，在蓝色圈内的数据属于该组数据的可能性较高，而越是偏远的数据，其属于该组数据的可能性就越低。</p>
<p>这种方法称为密度估计，表达如下：</p>

$$
if \quad p(x)
\begin{cases}
< \varepsilon & anomaly \\
> =\varepsilon & normal
\end{cases}
$$

<p>欺诈检测：</p>
<p>$x^{(i)} = {用户的第i个活动特征}$</p>
<p>模型$p(x)$ 为我们其属于一组数据的可能性，通过$p(x) &lt; \varepsilon$检测非正常用户。</p>
<p>异常检测主要用来识别欺骗。例如在线采集而来的有关用户的数据，一个特征向量中可能会包含如：用户多久登录一次，访问过的页面，在论坛发布的帖子数量，甚至是打字速度等。尝试根据这些特征构建一个模型，可以用这个模型来识别那些不符合该模式的用户。</p>
<p>再一个例子是检测一个数据中心，特征可能包含：内存使用情况，被访问的磁盘数量，<strong>CPU</strong>的负载，网络的通信量等。根据这些特征可以构建一个模型，用来判断某些计算机是不是有可能出错了。</p>
<h3 id="15-2-高斯分布"><a href="#15-2-高斯分布" class="headerlink" title="15.2 高斯分布"></a>15.2 高斯分布</h3><p>参考视频: 15 - 2 - Gaussian Distribution (10 min).mkv</p>
<p>在这个视频中，我将介绍高斯分布，也称为正态分布。回顾高斯分布的基本知识。</p>
<p>通常如果我们认为变量 $x$ 符合高斯分布 $x \sim N(\mu, \sigma^2)$则其概率密度函数为：<br>$p(x,\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$<br>我们可以利用已有的数据来预测总体中的$μ$和$σ^2$的计算方法如下：<br>$\mu=\frac{1}{m}\sum\limits_{i=1}^{m}x^{(i)}$</p>
<p>$\sigma^2=\frac{1}{m}\sum\limits_{i=1}^{m}(x^{(i)}-\mu)^2$</p>
<p>高斯分布样例：</p>
<p><img src="fcb35433507a56631dde2b4e543743ee.png" alt=""></p>
<p>注：机器学习中对于方差我们通常只除以$m$而非统计学中的$(m-1)$。这里顺便提一下，在实际使用中，到底是选择使用$1/m$还是$1/(m-1)$其实区别很小，只要你有一个还算大的训练集，在机器学习领域大部分人更习惯使用$1/m$这个版本的公式。这两个版本的公式在理论特性和数学特性上稍有不同，但是在实际使用中，他们的区别甚小，几乎可以忽略不计。</p>
<h3 id="15-3-算法"><a href="#15-3-算法" class="headerlink" title="15.3 算法"></a>15.3 算法</h3><p>参考视频: 15 - 3 - Algorithm (12 min).mkv</p>
<p>在本节视频中，我将应用高斯分布开发异常检测算法。</p>
<p>异常检测算法：</p>
<p>对于给定的数据集 $x^{(1)},x^{(2)},…,x^{(m)}$，我们要针对每一个特征计算 $\mu$ 和 $\sigma^2$ 的估计值。</p>
<p>$\mu_j=\frac{1}{m}\sum\limits_{i=1}^{m}x_j^{(i)}$</p>
<p>$\sigma_j^2=\frac{1}{m}\sum\limits_{i=1}^m(x_j^{(i)}-\mu_j)^2$</p>
<p>一旦我们获得了平均值和方差的估计值，给定新的一个训练实例，根据模型计算 $p(x)$：</p>
<p>$p(x)=\prod\limits_{j=1}^np(x_j;\mu_j,\sigma_j^2)=\prod\limits_{j=1}^1\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})$</p>
<p>当$p(x) &lt; \varepsilon$时，为异常。</p>
<p>下图是一个由两个特征的训练集，以及特征的分布情况：</p>
<p><img src="ba47767a11ba39a23898b9f1a5a57cc5.png" alt=""></p>
<p>下面的三维图表表示的是密度估计函数，$z$轴为根据两个特征的值所估计$p(x)$值：</p>
<p><img src="82b90f56570c05966da116c3afe6fc91.jpg" alt=""></p>
<p>我们选择一个$\varepsilon$，将$p(x) = \varepsilon$作为我们的判定边界，当$p(x) &gt; \varepsilon$时预测数据为正常数据，否则为异常。</p>
<p>在这段视频中，我们介绍了如何拟合$p(x)$，也就是 $x$的概率值，以开发出一种异常检测算法。同时，在这节课中，我们也给出了通过给出的数据集拟合参数，进行参数估计，得到参数 $\mu$ 和 $\sigma$，然后检测新的样本，确定新样本是否是异常。</p>
<p>在接下来的课程中，我们将深入研究这一算法，同时更深入地介绍，怎样让算法工作地更加有效。</p>
<h3 id="15-4-开发和评价一个异常检测系统"><a href="#15-4-开发和评价一个异常检测系统" class="headerlink" title="15.4 开发和评价一个异常检测系统"></a>15.4 开发和评价一个异常检测系统</h3><p>参考视频: 15 - 4 - Developing and Evaluating an Anomaly Detection System (13 min). mkv</p>
<p>异常检测算法是一个非监督学习算法，意味着我们无法根据结果变量 $ y$ 的值来告诉我们数据是否真的是异常的。我们需要另一种方法来帮助检验算法是否有效。当我们开发一个异常检测系统时，我们从带标记（异常或正常）的数据着手，我们从其中选择一部分正常数据用于构建训练集，然后用剩下的正常数据和异常数据混合的数据构成交叉检验集和测试集。</p>
<p>例如：我们有10000台正常引擎的数据，有20台异常引擎的数据。 我们这样分配数据：</p>
<p>6000台正常引擎的数据作为训练集</p>
<p>2000台正常引擎和10台异常引擎的数据作为交叉检验集</p>
<p>2000台正常引擎和10台异常引擎的数据作为测试集</p>
<p>具体的评价方法如下：</p>
<ol>
<li><p>根据测试集数据，我们估计特征的平均值和方差并构建$p(x)$函数</p>
</li>
<li><p>对交叉检验集，我们尝试使用不同的$\varepsilon$值作为阀值，并预测数据是否异常，根据F1值或者查准率与查全率的比例来选择 $\varepsilon$</p>
</li>
<li><p>选出 $\varepsilon$ 后，针对测试集进行预测，计算异常检验系统的$F1$值，或者查准率与查全率之比</p>
</li>
</ol>
<h3 id="15-5-异常检测与监督学习对比"><a href="#15-5-异常检测与监督学习对比" class="headerlink" title="15.5 异常检测与监督学习对比"></a>15.5 异常检测与监督学习对比</h3><p>参考视频: 15 - 5 - Anomaly Detection vs. Supervised Learning (8 min).mkv</p>
<p>之前我们构建的异常检测系统也使用了带标记的数据，与监督学习有些相似，下面的对比有助于选择采用监督学习还是异常检测：</p>
<p>两者比较：</p>
<table>
<thead>
<tr>
<th>异常检测</th>
<th>监督学习</th>
</tr>
</thead>
<tbody>
<tr>
<td>非常少量的正向类（异常数据 $y=1$）, 大量的负向类（$y=0$）</td>
<td>同时有大量的正向类和负向类</td>
</tr>
<tr>
<td>许多不同种类的异常，非常难。根据非常 少量的正向类数据来训练算法。</td>
<td>有足够多的正向类实例，足够用于训练 算法，未来遇到的正向类实例可能与训练集中的非常近似。</td>
</tr>
<tr>
<td>未来遇到的异常可能与已掌握的异常、非常的不同。</td>
<td></td>
</tr>
<tr>
<td>例如： 欺诈行为检测 生产（例如飞机引擎）检测数据中心的计算机运行状况</td>
<td>例如：邮件过滤器 天气预报 肿瘤分类</td>
</tr>
</tbody>
</table>
<p>希望这节课能让你明白一个学习问题的什么样的特征，能让你把这个问题当做是一个异常检测，或者是一个监督学习的问题。另外，对于很多技术公司可能会遇到的一些问题，通常来说，正样本的数量很少，甚至有时候是0，也就是说，出现了太多没见过的不同的异常类型，那么对于这些问题，通常应该使用的算法就是异常检测算法。</p>
<h3 id="15-6-选择特征"><a href="#15-6-选择特征" class="headerlink" title="15.6 选择特征"></a>15.6 选择特征</h3><p>参考视频: 15 - 6 - Choosing What Features to Use (12 min).mkv</p>
<p>对于异常检测算法，我们使用的特征是至关重要的，下面谈谈如何选择特征：</p>
<p>异常检测假设特征符合高斯分布，如果数据的分布不是高斯分布，异常检测算法也能够工作，但是最好还是将数据转换成高斯分布，例如使用对数函数：$x= log(x+c)$，其中 $c$ 为非负常数； 或者 $x=x^c$，$c$为 0-1 之间的一个分数，等方法。(编者注：在<strong>python</strong>中，通常用<code>np.log1p()</code>函数，$log1p$就是 $log(x+1)$，可以避免出现负数结果，反向函数就是<code>np.expm1()</code>)</p>
<p><img src="0990d6b7a5ab3c0036f42083fe2718c6.jpg" alt=""></p>
<p>误差分析：</p>
<p>一个常见的问题是一些异常的数据可能也会有较高的$p(x)$值，因而被算法认为是正常的。这种情况下误差分析能够帮助我们，我们可以分析那些被算法错误预测为正常的数据，观察能否找出一些问题。我们可能能从问题中发现我们需要增加一些新的特征，增加这些新特征后获得的新算法能够帮助我们更好地进行异常检测。</p>
<p>异常检测误差分析：</p>
<p><img src="f406bc738e5e032be79e52b6facfa48e.png" alt=""></p>
<p>我们通常可以通过将一些相关的特征进行组合，来获得一些新的更好的特征（异常数据的该特征值异常地大或小），例如，在检测数据中心的计算机状况的例子中，我们可以用<strong>CPU</strong>负载与网络通信量的比例作为一个新的特征，如果该值异常地大，便有可能意味着该服务器是陷入了一些问题中。</p>
<p>在这段视频中，我们介绍了如何选择特征，以及对特征进行一些小小的转换，让数据更像正态分布，然后再把数据输入异常检测算法。同时也介绍了建立特征时，进行的误差分析方法，来捕捉各种异常的可能。希望你通过这些方法，能够了解如何选择好的特征变量，从而帮助你的异常检测算法，捕捉到各种不同的异常情况。</p>
<h3 id="15-7-多元高斯分布（选修）"><a href="#15-7-多元高斯分布（选修）" class="headerlink" title="15.7 多元高斯分布（选修）"></a>15.7 多元高斯分布（选修）</h3><p>参考视频: 15 - 7 - Multivariate Gaussian Distribution (Optional) (14 min).mkv</p>
<p>假使我们有两个相关的特征，而且这两个特征的值域范围比较宽，这种情况下，一般的高斯分布模型可能不能很好地识别异常数据。其原因在于，一般的高斯分布模型尝试的是去同时抓住两个特征的偏差，因此创造出一个比较大的判定边界。</p>
<p>下图中是两个相关特征，洋红色的线（根据ε的不同其范围可大可小）是一般的高斯分布模型获得的判定边界，很明显绿色的<strong>X</strong>所代表的数据点很可能是异常值，但是其$p(x)$值却仍然在正常范围内。多元高斯分布将创建像图中蓝色曲线所示的判定边界。</p>
<p><img src="598db991a7c930c9021cec5f6ab9beb9.png" alt=""></p>
<p>在一般的高斯分布模型中，我们计算 $p(x)$ 的方法是：<br>通过分别计算每个特征对应的几率然后将其累乘起来，在多元高斯分布模型中，我们将构建特征的协方差矩阵，用所有的特征一起来计算 $p(x)$。</p>
<p>我们首先计算所有特征的平均值，然后再计算协方差矩阵：<br>$p(x)=\prod_{j=1}^np(x_j;\mu,\sigma_j^2)=\prod_{j=1}^n\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})$</p>
<p>$\mu=\frac{1}{m}\sum_{i=1}^mx^{(i)}$</p>
<p>$\Sigma = \frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T=\frac{1}{m}(X-\mu)^T(X-\mu)$<br>
注:其中$\mu $ 是一个向量，其每一个单元都是原特征矩阵中一行数据的均值。最后我们计算多元高斯分布的$p\left( x \right)$:
$p(x)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)$
<br>其中：</p>
<p>$|\Sigma|$是定矩阵，在 <strong>Octave</strong> 中用 <code>det(sigma)</code>计算</p>
<p>$\Sigma1$ 是逆矩阵，下面我们来看看协方差矩阵是如何影响模型的：</p>
<p><img src="29df906704d254f18e92a63173dd51e7.jpg" alt=""></p>
<p>上图是5个不同的模型，从左往右依次分析：</p>
<ol>
<li><p>是一个一般的高斯分布模型</p>
</li>
<li><p>通过协方差矩阵，令特征1拥有较小的偏差，同时保持特征2的偏差</p>
</li>
<li><p>通过协方差矩阵，令特征2拥有较大的偏差，同时保持特征1的偏差</p>
</li>
<li><p>通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的正相关性</p>
</li>
<li><p>通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的负相关性</p>
</li>
</ol>
<p>多元高斯分布模型与原高斯分布模型的关系：</p>
<p>可以证明的是，原本的高斯分布模型是多元高斯分布模型的一个子集，即像上图中的第1、2、3，3个例子所示，如果协方差矩阵只在对角线的单位上有非零的值时，即为原本的高斯分布模型了。</p>
<p>原高斯分布模型和多元高斯分布模型的比较：</p>
<table>
<thead>
<tr>
<th>原高斯分布模型</th>
<th>多元高斯分布模型</th>
</tr>
</thead>
<tbody>
<tr>
<td>不能捕捉特征之间的相关性 但可以通过将特征进行组合的方法来解决</td>
<td>自动捕捉特征之间的相关性</td>
</tr>
<tr>
<td>计算代价低，能适应大规模的特征</td>
<td>计算代价较高 训练集较小时也同样适用</td>
</tr>
<tr>
<td></td>
<td>必须要有 $m&gt;n$，不然的话协方差矩阵 不可逆的，通常需要 $m&gt;10n$ 另外特征冗余也会导致协方差矩阵不可逆</td>
</tr>
</tbody>
</table>
<p>原高斯分布模型被广泛使用着，如果特征之间在某种程度上存在相互关联的情况，我们可以通过构造新新特征的方法来捕捉这些相关性。</p>
<p>如果训练集不是太大，并且没有太多的特征，我们可以使用多元高斯分布模型。</p>
<h3 id="15-8-使用多元高斯分布进行异常检测（可选）"><a href="#15-8-使用多元高斯分布进行异常检测（可选）" class="headerlink" title="15.8 使用多元高斯分布进行异常检测（可选）"></a>15.8 使用多元高斯分布进行异常检测（可选）</h3><p>参考视频: 15 - 8 - Anomaly Detection using the Multivariate Gaussian Distribution (Optional) (14 min).mkv</p>
<p>在我们谈到的最后一个视频，关于多元高斯分布，看到的一些建立的各种分布模型，当你改变参数，$\mu$ 和 $\Sigma$。在这段视频中，让我们用这些想法，并应用它们制定一个不同的异常检测算法。</p>
<p>要回顾一下多元高斯分布和多元正态分布：</p>
<p><img src="3dbee365617e9264831400e4de247adc.png" alt=""></p>
<p>分布有两个参数， $\mu$ 和 $\Sigma$。其中$\mu$这一个$n$维向量和 $\Sigma$ 的协方差矩阵，是一种$n\times n$的矩阵。而这里的公式$x$的概率，如按 $\mu$ 和参数化 $\Sigma$，和你的变量 $\mu$ 和 $\Sigma$，你可以得到一个范围的不同分布一样，你知道的，这些都是三个样本，那些我们在以前的视频看过了。</p>
<p>因此，让我们谈谈参数拟合或参数估计问题：<br>
我有一组样本${{{ x^{(1)},x^{(2)},...,x^{(m)}} }}$是一个$n$维向量，我想我的样本来自一个多元高斯分布。我如何尝试估计我的参数 $\mu$ 和 $\Sigma$ 以及标准公式？
<br>估计他们是你设置 $\mu$ 是你的训练样本的平均值。</p>
<p>$\mu=\frac{1}{m}\sum_{i=1}^{m}x^{(i)}$<br>并设置$\Sigma$：<br>$\Sigma=\frac{1}{m}\sum_{i=1}^{m}(x^{(i)}-\mu)(x^{(i)}-\mu)^T$<br>这其实只是当我们使用<strong>PCA</strong>算法时候，有 $\Sigma$ 时写出来。所以你只需插入上述两个公式，这会给你你估计的参数 $\mu$ 和你估计的参数 $\Sigma$。所以，这里给出的数据集是你如何估计 $\mu$ 和 $\Sigma$。让我们以这种方法而只需将其插入到异常检测算法。那么，我们如何把所有这一切共同开发一个异常检测算法？</p>
<p><img src="d1a228f2bec262f2206379ed844c7f4a.png" alt=""></p>
<p>首先，我们把我们的训练集，和我们的拟合模型，我们计算$p(x)$，要知道，设定$\mu$和描述的一样$\Sigma$。</p>
<p><img src="015cee3a224dde6da0181215cf91a23d.png" alt=""></p>
<p>如图，该分布在中央最多，越到外面的圈的范围越小。</p>
<p>并在该点是出路这里的概率非常低。</p>
<p>原始模型与多元高斯模型的关系如图：</p>
<p>其中：协方差矩阵$\Sigma$为：</p>
<p><img src="7104dd2548f1251e4c423e059d1d2594.png" alt=""></p>
<p>原始模型和多元高斯分布比较如图：</p>
<p><img src="f4585239738f2b5149608879fa166889.png" alt=""></p>
<h2 id="十六、推荐系统-Recommender-Systems"><a href="#十六、推荐系统-Recommender-Systems" class="headerlink" title="十六、推荐系统(Recommender Systems)"></a>十六、推荐系统(Recommender Systems)</h2><h3 id="16-1-问题形式化"><a href="#16-1-问题形式化" class="headerlink" title="16.1 问题形式化"></a>16.1 问题形式化</h3><p>参考视频: 16 - 1 - Problem Formulation (8 min).mkv</p>
<p>在接下来的视频中，我想讲一下推荐系统。我想讲推荐系统有两个原因：</p>
<p>第一、仅仅因为它是机器学习中的一个重要的应用。在过去几年，我偶尔访问硅谷不同的技术公司，我常和工作在这儿致力于机器学习应用的人们聊天，我常问他们，最重要的机器学习的应用是什么，或者，你最想改进的机器学习应用有哪些。我最常听到的答案是推荐系统。现在，在硅谷有很多团体试图建立很好的推荐系统。因此，如果你考虑网站像亚马逊，或网飞公司或易趣，或<strong>iTunes Genius</strong>，有很多的网站或系统试图推荐新产品给用户。如，亚马逊推荐新书给你，网飞公司试图推荐新电影给你，等等。这些推荐系统，根据浏览你过去买过什么书，或过去评价过什么电影来判断。这些系统会带来很大一部分收入，比如为亚马逊和像网飞这样的公司。因此，对推荐系统性能的改善，将对这些企业的有实质性和直接的影响。</p>
<p>推荐系统是个有趣的问题，在学术机器学习中因此，我们可以去参加一个学术机器学习会议，推荐系统问题实际上受到很少的关注，或者，至少在学术界它占了很小的份额。但是，如果你看正在发生的事情，许多有能力构建这些系统的科技企业，他们似乎在很多企业中占据很高的优先级。这是我为什么在这节课讨论它的原因之一。</p>
<p>我想讨论推荐系统地第二个原因是：这个班视频的最后几集我想讨论机器学习中的一些大思想，并和大家分享。这节课我们也看到了，对机器学习来说，特征是很重要的，你所选择的特征，将对你学习算法的性能有很大的影响。因此，在机器学习中有一种大思想，它针对一些问题，可能并不是所有的问题，而是一些问题，有算法可以为你自动学习一套好的特征。因此，不要试图手动设计，而手写代码这是目前为止我们常干的。有一些设置，你可以有一个算法，仅仅学习其使用的特征，推荐系统就是类型设置的一个例子。还有很多其它的，但是通过推荐系统，我们将领略一小部分特征学习的思想，至少，你将能够了解到这方面的一个例子，我认为，机器学习中的大思想也是这样。因此，让我们开始讨论推荐系统问题形式化。</p>
<p>我们从一个例子开始定义推荐系统的问题。</p>
<p>假使我们是一个电影供应商，我们有 5 部电影和 4 个用户，我们要求用户为电影打分。</p>
<p><img src="c2822f2c28b343d7e6ade5bd40f3a1fc.png" alt=""></p>
<p>前三部电影是爱情片，后两部则是动作片，我们可以看出<strong>Alice</strong>和<strong>Bob</strong>似乎更倾向与爱情片， 而 <strong>Carol</strong> 和 <strong>Dave</strong> 似乎更倾向与动作片。并且没有一个用户给所有的电影都打过分。我们希望构建一个算法来预测他们每个人可能会给他们没看过的电影打多少分，并以此作为推荐的依据。</p>
<p>下面引入一些标记：</p>
<p>$n_u$ 代表用户的数量</p>
<p>$n_m$ 代表电影的数量</p>
<p>$r(i, j)$ 如果用户j给电影 $i$ 评过分则 $r(i,j)=1$</p>
<p>$y^{(i, j)}$ 代表用户 $j$ 给电影i的评分</p>
<p>$m_j$代表用户 $j$ 评过分的电影的总数</p>
<h3 id="16-2-基于内容的推荐系统"><a href="#16-2-基于内容的推荐系统" class="headerlink" title="16.2 基于内容的推荐系统"></a>16.2 基于内容的推荐系统</h3><p>参考视频: 16 - 2 - Content Based Recommendations (15 min).mkv</p>
<p>在一个基于内容的推荐系统算法中，我们假设对于我们希望推荐的东西有一些数据，这些数据是有关这些东西的特征。</p>
<p>在我们的例子中，我们可以假设每部电影都有两个特征，如$x_1$代表电影的浪漫程度，$x_2$ 代表电影的动作程度。</p>
<p><img src="747c1fd6bff694c6034da1911aa3314b.png" alt=""></p>
<p>则每部电影都有一个特征向量，如$x^{(1)}$是第一部电影的特征向量为[0.9 0]。</p>
<p>下面我们要基于这些特征来构建一个推荐系统算法。<br>
假设我们采用线性回归模型，我们可以针对每一个用户都训练一个线性回归模型，如${{\theta }^{(1)}}$是第一个用户的模型的参数。
<br>于是，我们有：</p>
<p>$\theta^{(j)}$用户 $j$ 的参数向量</p>
<p>$x^{(i)}$电影 $i$ 的特征向量</p>
<p>对于用户 $j$ 和电影 $i$，我们预测评分为：$(\theta^{(j)})^T x^{(i)}$</p>
<p>代价函数</p>
<p>针对用户 $j$，该线性回归模型的代价为预测误差的平方和，加上正则化项：<br>$$<br>\min_{\theta (j)}\frac{1}{2}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\left(\theta_{k}^{(j)}\right)^2<br>$$</p>
<p>其中 $i:r(i,j)$表示我们只计算那些用户 $j$ 评过分的电影。在一般的线性回归模型中，误差项和正则项应该都是乘以$1/2m$，在这里我们将$m$去掉。并且我们不对方差项$\theta_0$进行正则化处理。</p>
<p>上面的代价函数只是针对一个用户的，为了学习所有用户，我们将所有用户的代价函数求和：<br>
$$
\min_{\theta^{(1)},...,\theta^{(n_u)}} \frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2
$$
<br>如果我们要用梯度下降法来求解最优解，我们计算代价函数的偏导数后得到梯度下降的更新公式为：</p>
<p>$$<br>\theta_k^{(j)}:=\theta_k^{(j)}-\alpha\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_{k}^{(i)} \quad (\text{for} \, k = 0)<br>$$</p>
<p>$$<br>\theta_k^{(j)}:=\theta_k^{(j)}-\alpha\left(\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_{k}^{(i)}+\lambda\theta_k^{(j)}\right) \quad (\text{for} \, k\neq 0)<br>$$</p>
<h3 id="16-3-协同过滤"><a href="#16-3-协同过滤" class="headerlink" title="16.3 协同过滤"></a>16.3 协同过滤</h3><p>参考视频: 16 - 3 - Collaborative Filtering (10 min).mkv</p>
<p>在之前的基于内容的推荐系统中，对于每一部电影，我们都掌握了可用的特征，使用这些特征训练出了每一个用户的参数。相反地，如果我们拥有用户的参数，我们可以学习得出电影的特征。<br>
$$
\mathop{min}\limits_{x^{(1)},...,x^{(n_m)}}\frac{1}{2}\sum_{i=1}^{n_m}\sum_{j{r(i,j)=1}}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(i)})^2
$$
<br>但是如果我们既没有用户的参数，也没有电影的特征，这两种方法都不可行了。协同过滤算法可以同时学习这两者。</p>
<p>我们的优化目标便改为同时针对$x$和$\theta$进行。<br>$$<br>J(x^{(1)},…x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})=\frac{1}{2}\sum_{(i:j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(j)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2<br>$$</p>
<p>对代价函数求偏导数的结果如下：</p>
<p>$$<br>x_k^{(i)}:=x_k^{(i)}-\alpha\left(\sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\theta_k^{j}+\lambda x_k^{(i)}\right)<br>$$</p>
<p>$$<br>\theta_k^{(i)}:=\theta_k^{(i)}-\alpha\left(\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}x_k^{(i)}+\lambda \theta_k^{(j)}\right)<br>$$</p>
<p>注：在协同过滤从算法中，我们通常不使用方差项，如果需要的话，算法会自动学得。<br>协同过滤算法使用步骤如下：</p>
<ol>
<li><p>初始 $x^{(1)},x^{(1)},…x^{(nm)},\ \theta^{(1)},\theta^{(2)},…,\theta^{(n_u)}$为一些随机小值</p>
</li>
<li><p>使用梯度下降算法最小化代价函数</p>
</li>
<li><p>在训练完算法后，我们预测$(\theta^{(j)})^Tx^{(i)}$为用户 $j$ 给电影 $i$ 的评分</p>
</li>
</ol>
<p>通过这个学习过程获得的特征矩阵包含了有关电影的重要数据，这些数据不总是人能读懂的，但是我们可以用这些数据作为给用户推荐电影的依据。<br>
例如，如果一位用户正在观看电影 $x^{(i)}$，我们可以寻找另一部电影$x^{(j)}$，依据两部电影的特征向量之间的距离$\left\| {{x}^{(i)}}-{{x}^{(j)}} \right\|$的大小。
</p>
<h3 id="16-4-协同过滤算法"><a href="#16-4-协同过滤算法" class="headerlink" title="16.4 协同过滤算法"></a>16.4 协同过滤算法</h3><p>参考视频: 16 - 4 - Collaborative Filtering Algorithm (9 min).mkv</p>
<p>协同过滤优化目标：<br>
给定$x^{(1)},...,x^{(n_m)}$，估计$\theta^{(1)},...,\theta^{(n_u)}$：
$$
\min_{\theta^{(1)},...,\theta^{(n_u)}}\frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2
$$
</p>
<p>给定$\theta^{(1)},…,\theta^{(n_u)}$，估计$x^{(1)},…,x^{(n_m)}$：</p>
<p>同时最小化$x^{(1)},…,x^{(n_m)}$和$\theta^{(1)},…,\theta^{(n_u)}$：<br>$$<br>J(x^{(1)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})=\frac{1}{2}\sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(i)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2<br>$$<br>
$$
\min_{x^{(1)},...,x^{(n_m)} \\\ \theta^{(1)},...,\theta^{(n_u)}}J(x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)})
$$
</p>
<h3 id="16-5-向量化：低秩矩阵分解"><a href="#16-5-向量化：低秩矩阵分解" class="headerlink" title="16.5 向量化：低秩矩阵分解"></a>16.5 向量化：低秩矩阵分解</h3><p>参考视频: 16 - 5 - Vectorization_ Low Rank Matrix Factorization (8 min).mkv</p>
<p>在上几节视频中，我们谈到了协同过滤算法，本节视频中我将会讲到有关该算法的向量化实现，以及说说有关该算法你可以做的其他事情。</p>
<p>举例子：</p>
<ol>
<li><p>当给出一件产品时，你能否找到与之相关的其它产品。</p>
</li>
<li><p>一位用户最近看上一件产品，有没有其它相关的产品，你可以推荐给他。</p>
</li>
</ol>
<p>我将要做的是：实现一种选择的方法，写出协同过滤算法的预测情况。</p>
<p>我们有关于五部电影的数据集，我将要做的是，将这些用户的电影评分，进行分组并存到一个矩阵中。</p>
<p>我们有五部电影，以及四位用户，那么 这个矩阵 $Y$ 就是一个5行4列的矩阵，它将这些电影的用户评分数据都存在矩阵里：</p>
<table>
<thead>
<tr>
<th><strong>Movie</strong></th>
<th><strong>Alice (1)</strong></th>
<th><strong>Bob (2)</strong></th>
<th><strong>Carol (3)</strong></th>
<th><strong>Dave (4)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Love at last</td>
<td>5</td>
<td>5</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Romance forever</td>
<td>5</td>
<td>?</td>
<td>?</td>
<td>0</td>
</tr>
<tr>
<td>Cute puppies of love</td>
<td>?</td>
<td>4</td>
<td>0</td>
<td>?</td>
</tr>
<tr>
<td>Nonstop car chases</td>
<td>0</td>
<td>0</td>
<td>5</td>
<td>4</td>
</tr>
<tr>
<td>Swords vs. karate</td>
<td>0</td>
<td>0</td>
<td>5</td>
<td>?</td>
</tr>
</tbody>
</table>
<p><img src="42a92e07b32b593bb826f8f6bc4d9eb3.png" alt=""></p>
<p>推出评分：</p>
<p><img src="c905a6f02e201a4767d869b3791e8aeb.png" alt=""></p>
<p>找到相关影片：</p>
<p><img src="0a8b49da1ab852f2996a02afcaca2322.png" alt=""></p>
<p>现在既然你已经对特征参数向量进行了学习，那么我们就会有一个很方便的方法来度量两部电影之间的相似性。例如说：电影 $i$ 有一个特征向量$x^{(i)}$，你是否能找到一部不同的电影 $j$，保证两部电影的特征向量之间的距离$x^{(i)}$和$x^{(j)}$很小，那就能很有力地表明电影$i$和电影 $j$ 在某种程度上有相似，至少在某种意义上，某些人喜欢电影 $i$，或许更有可能也对电影 $j$ 感兴趣。总结一下，当用户在看某部电影 $i$ 的时候，如果你想找5部与电影非常相似的电影，为了能给用户推荐5部新电影，你需要做的是找出电影 $j$，在这些不同的电影中与我们要找的电影 $i$ 的距离最小，这样你就能给你的用户推荐几部不同的电影了。</p>
<p>通过这个方法，希望你能知道，如何进行一个向量化的计算来对所有的用户和所有的电影进行评分计算。同时希望你也能掌握，通过学习特征参数，来找到相关电影和产品的方法。</p>
<h3 id="16-6-推行工作上的细节：均值归一化"><a href="#16-6-推行工作上的细节：均值归一化" class="headerlink" title="16.6 推行工作上的细节：均值归一化"></a>16.6 推行工作上的细节：均值归一化</h3><p>参考视频: 16 - 6 - Implementational Detail_ Mean Normalization (9 min).mkv</p>
<p>让我们来看下面的用户评分数据：</p>
<p><img src="54b1f7c3131aed24f9834d62a6835642.png" alt=""></p>
<p>如果我们新增一个用户 <strong>Eve</strong>，并且 <strong>Eve</strong> 没有为任何电影评分，那么我们以什么为依据为<strong>Eve</strong>推荐电影呢？</p>
<p>我们首先需要对结果 $Y $矩阵进行均值归一化处理，将每一个用户对某一部电影的评分减去所有用户对该电影评分的平均值：</p>
<p><img src="9ec5cb55e14bd1462183e104f8e02b80.png" alt=""></p>
<p>然后我们利用这个新的 $Y$ 矩阵来训练算法。<br>如果我们要用新训练出的算法来预测评分，则需要将平均值重新加回去，预测$(\theta^{(j)})^T x^{(i)}+\mu_i$，对于<strong>Eve</strong>，我们的新模型会认为她给每部电影的评分都是该电影的平均分。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习（八）</title>
    <url>/2018/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AB%EF%BC%89/</url>
    <content><![CDATA[<p>好吧，从这里开始博客就不是我自己手敲的了，因为一些原因，没有时间再写下去了，但又不想放弃，只能抄一份过来了，感谢Huang Haiguang老师的笔记。<a href="https://github.com/fengdu78" target="_blank" rel="noopener">https://github.com/fengdu78</a></p>
<p>十、应用机器学习的建议(Advice for Applying Machine Learning)</p>
<h3 id="10-1-决定下一步做什么"><a href="#10-1-决定下一步做什么" class="headerlink" title="10.1 决定下一步做什么"></a>10.1 决定下一步做什么</h3><p>参考视频: 10 - 1 - Deciding What to Try Next (6 min).mkv</p>
<p>​    到目前为止，我们已经介绍了许多不同的学习算法，如果你一直跟着这些视频的进度学习，你会发现自己已经不知不觉地成为一个了解许多先进机器学习技术的专家了。</p>
<p>​    然而，在懂机器学习的人当中依然存在着很大的差距，一部分人确实掌握了怎样高效有力地运用这些学习算法。而另一些人他们可能对我马上要讲的东西，就不是那么熟悉了。他们可能没有完全理解怎样运用这些算法。因此总是把时间浪费在毫无意义的尝试上。我想做的是确保你在设计机器学习的系统时，你能够明白怎样选择一条最合适、最正确的道路。因此，在这节视频和之后的几段视频中，我将向你介绍一些实用的建议和指导，帮助你明白怎样进行选择。具体来讲，我将重点关注的问题是假如你在开发一个机器学习系统，或者想试着改进一个机器学习系统的性能，你应如何决定接下来应该选择哪条道路？为了解释这一问题，我想仍然使用预测房价的学习例子，假如你已经完成了正则化线性回归，也就是最小化代价函数$J$的值，假如，在你得到你的学习参数以后，如果你要将你的假设函数放到一组新的房屋样本上进行测试，假如说你发现在预测房价时产生了巨大的误差，现在你的问题是要想改进这个算法，接下来应该怎么办？</p>
<p>​    实际上你可以想出很多种方法来改进这个算法的性能，其中一种办法是使用更多的训练样本。具体来讲，也许你能想到通过电话调查或上门调查来获取更多的不同的房屋出售数据。遗憾的是，我看到好多人花费了好多时间想收集更多的训练样本。他们总认为，要是我有两倍甚至十倍数量的训练数据，那就一定会解决问题的是吧？但有时候获得更多的训练数据实际上并没有作用。在接下来的几段视频中，我们将解释原因。</p>
<p>​    我们也将知道怎样避免把过多的时间浪费在收集更多的训练数据上，这实际上是于事无补的。另一个方法，你也许能想到的是尝试选用更少的特征集。因此如果你有一系列特征比如$x_1,x_2,x_3$等等。也许有很多特征，也许你可以花一点时间从这些特征中仔细挑选一小部分来防止过拟合。或者也许你需要用更多的特征，也许目前的特征集，对你来讲并不是很有帮助。你希望从获取更多特征的角度来收集更多的数据，同样地，你可以把这个问题扩展为一个很大的项目，比如使用电话调查来得到更多的房屋案例，或者再进行土地测量来获得更多有关，这块土地的信息等等，因此这是一个复杂的问题。同样的道理，我们非常希望在花费大量时间完成这些工作之前，我们就能知道其效果如何。我们也可以尝试增加多项式特征的方法，比如$x_1$的平方，$x_2$的平方，$x_1,x_2$的乘积，我们可以花很多时间来考虑这一方法，我们也可以考虑其他方法减小或增大正则化参数$\lambda$的值。我们列出的这个单子，上面的很多方法都可以扩展开来扩展成一个六个月或更长时间的项目。遗憾的是，大多数人用来选择这些方法的标准是凭感觉的，也就是说，大多数人的选择方法是随便从这些方法中选择一种，比如他们会说“噢，我们来多找点数据吧”，然后花上六个月的时间收集了一大堆数据，然后也许另一个人说：“好吧，让我们来从这些房子的数据中多找点特征吧”。我很遗憾不止一次地看到很多人花了至少六个月时间来完成他们随便选择的一种方法，而在六个月或者更长时间后，他们很遗憾地发现自己选择的是一条不归路。幸运的是，有一系列简单的方法能让你事半功倍，排除掉单子上的至少一半的方法，留下那些确实有前途的方法，同时也有一种很简单的方法，只要你使用，就能很轻松地排除掉很多选择，从而为你节省大量不必要花费的时间。最终达到改进机器学习系统性能的目的假设我们需要用一个线性回归模型来预测房价，当我们运用训练好了的模型来预测未知数据的时候发现有较大的误差，我们下一步可以做什么？</p>
<ol>
<li><p>获得更多的训练实例——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。</p>
</li>
<li><p>尝试减少特征的数量</p>
</li>
<li><p>尝试获得更多的特征</p>
</li>
<li><p>尝试增加多项式特征</p>
</li>
<li><p>尝试减少正则化程度$\lambda$</p>
</li>
<li><p>尝试增加正则化程度$\lambda$</p>
</li>
</ol>
<p>​        我们不应该随机选择上面的某种方法来改进我们的算法，而是运用一些机器学习诊断法来帮助我们知道上面哪些方法对我们的算法是有效的。</p>
<p>​    在接下来的两段视频中，我首先介绍怎样评估机器学习算法的性能，然后在之后的几段视频中，我将开始讨论这些方法，它们也被称为”机器学习诊断法”。“诊断法”的意思是：这是一种测试法，你通过执行这种测试，能够深入了解某种算法到底是否有用。这通常也能够告诉你，要想改进一种算法的效果，什么样的尝试，才是有意义的。在这一系列的视频中我们将介绍具体的诊断法，但我要提前说明一点的是，这些诊断法的执行和实现，是需要花些时间的，有时候确实需要花很多时间来理解和实现，但这样做的确是把时间用在了刀刃上，因为这些方法让你在开发学习算法时，节省了几个月的时间，因此，在接下来几节课中，我将先来介绍如何评价你的学习算法。在此之后，我将介绍一些诊断法，希望能让你更清楚。在接下来的尝试中，如何选择更有意义的方法。</p>
<h3 id="10-2-评估一个假设"><a href="#10-2-评估一个假设" class="headerlink" title="10.2 评估一个假设"></a>10.2 评估一个假设</h3><p>参考视频: 10 - 2 - Evaluating a Hypothesis (8 min).mkv</p>
<p>​    在本节视频中我想介绍一下怎样用你学过的算法来评估假设函数。在之后的课程中，我们将以此为基础来讨论如何避免过拟合和欠拟合的问题。</p>
<p><img src="f49730be98810b869951bbe38b6319ba.png" alt=""></p>
<p>​    当我们确定学习算法的参数的时候，我们考虑的是选择参量来使训练误差最小化，有人认为得到一个非常小的训练误差一定是一件好事，但我们已经知道，仅仅是因为这个假设具有很小的训练误差，并不能说明它就一定是一个好的假设函数。而且我们也学习了过拟合假设函数的例子，所以这推广到新的训练集上是不适用的。</p>
<p>​    那么，你该如何判断一个假设函数是过拟合的呢？对于这个简单的例子，我们可以对假设函数$h(x)$进行画图，然后观察图形趋势，但对于特征变量不止一个的这种一般情况，还有像有很多特征变量的问题，想要通过画出假设函数来进行观察，就会变得很难甚至是不可能实现。</p>
<p>​    因此，我们需要另一种方法来评估我们的假设函数过拟合检验。</p>
<p>​    为了检验算法是否过拟合，我们将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。很重要的一点是训练集和测试集均要含有各种类型的数据，通常我们要对数据进行“洗牌”，然后再分成训练集和测试集。</p>
<p><img src="9c769fd59c8a9c9f92200f538d1ab29c.png" alt=""></p>
<p>​    测试集评估在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算误差：</p>
<ol>
<li>对于线性回归模型，我们利用测试集数据计算代价函数$J$</li>
<li>对于逻辑回归模型，我们除了可以利用测试数据集来计算代价函数外：</li>
</ol>
 
$$ J_{test}{(\theta)} = -\frac{1}{{m}_{test}}\sum_\limits{i=1}^{m_{test}}\log{h_{\theta}(x^{(i)}_{test})}+(1-{y^{(i)}_{test}})\log{h_{\theta}(x^{(i)}_{test})}$$

<p>误分类的比率，对于每一个测试集实例，计算：</p>
<p><img src="751e868bebf4c0bf139db173d25e8ec4.png" alt=""></p>
<p>然后对计算结果求平均。</p>
<h3 id="10-3-模型选择和交叉验证集"><a href="#10-3-模型选择和交叉验证集" class="headerlink" title="10.3 模型选择和交叉验证集"></a>10.3 模型选择和交叉验证集</h3><p>参考视频: 10 - 3 - Model Selection and Train_Validation_Test Sets (12 min).mkv</p>
<p>​    假设我们要在10个不同次数的二项式模型之间进行选择：</p>
<p><img src="1b908480ad78ee54ba7129945015f87f.jpg" alt=""></p>
<p>​    显然越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能推广至一般情况，我们应该选择一个更能适应一般情况的模型。我们需要使用交叉验证集来帮助选择模型。</p>
<p>​    即：使用60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用20%的数据作为测试集</p>
<p><img src="7cf1cd9c123a72ca4137ca515871689d.png" alt=""></p>
<p>模型选择的方法为：</p>
<ol>
<li><p>使用训练集训练出10个模型</p>
</li>
<li><p>用10个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）</p>
</li>
<li><p>选取代价函数值最小的模型</p>
</li>
<li><p>用步骤3中选出的模型对测试集计算得出推广误差（代价函数的值）</p>
<p><strong><em>Train/validation/test error</em></strong></p>
<p><strong>Training error:</strong></p>
<p>​                $J_{train}(\theta) = \frac{1}{2m}\sum_\limits{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$</p>
<p><strong>Cross Validation error:</strong></p>
<p>​                $J_{cv}(\theta) = \frac{1}{2m_{cv}}\sum_\limits{i=1}^{m}(h_{\theta}(x^{(i)}<em>{cv})-y^{(i)}</em>{cv})^2​$</p>
<p><strong>Test error:</strong></p>
<p>​                $J_{test}(\theta)=\frac{1}{2m_{test}}\sum_\limits{i=1}^{m_{test}}(h_{\theta}(x^{(i)}<em>{cv})-y^{(i)}</em>{cv})^2$</p>
</li>
</ol>
<h3 id="10-4-诊断偏差和方差"><a href="#10-4-诊断偏差和方差" class="headerlink" title="10.4 诊断偏差和方差"></a>10.4 诊断偏差和方差</h3><p>参考视频: 10 - 4 - Diagnosing Bias vs. Variance (8 min).mkv</p>
<p>​    当你运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。那么这两种情况，哪个和偏差有关，哪个和方差有关，或者是不是和两个都有关？搞清楚这一点非常重要，因为能判断出现的情况是这两种情况中的哪一种。其实是一个很有效的指示器，指引着可以改进算法的最有效的方法和途径。在这段视频中，我想更深入地探讨一下有关偏差和方差的问题，希望你能对它们有一个更深入的理解，并且也能弄清楚怎样评价一个学习算法，能够判断一个算法是偏差还是方差有问题，因为这个问题对于弄清如何改进学习算法的效果非常重要，高偏差和高方差的问题基本上来说是欠拟合和过拟合的问题。</p>
<p><img src="20c6b0ba8375ca496b7557def6c00324.jpg" alt=""></p>
<p>​    我们通常会通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析：</p>
<p><img src="bca6906add60245bbc24d71e22f8b836.png" alt=""></p>
<p><strong>Bias/variance</strong></p>
<p><strong>Training error:</strong>                               $J_{train}(\theta) = \frac{1}{2m}\sum_\limits{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$</p>
<p><strong>Cross Validation error:</strong>                $J_{cv}(\theta) = \frac{1}{2m_{cv}}\sum_\limits{i=1}^{m}(h_{\theta}(x^{(i)}<em>{cv})-y^{(i)}</em>{cv})^2$</p>
<p><img src="64ad47693447761bd005243ae7db0cca.png" alt=""></p>
<p>​    对于训练集，当 $d$ 较小时，模型拟合程度更低，误差较大；随着 $d$ 的增长，拟合程度提高，误差减小。</p>
<p>​    对于交叉验证集，当 $d$ 较小时，模型拟合程度低，误差较大；但是随着 $d$ 的增长，误差呈现先减小后增大的趋势，转折点是我们的模型开始过拟合训练数据集的时候。</p>
<p>​    如果我们的交叉验证集误差较大，我们如何判断是方差还是偏差呢？根据上面的图表，我们知道:</p>
<p><img src="25597f0f88208a7e74a3ca028e971852.png" alt=""></p>
<p>​    训练集误差和交叉验证集误差近似时：偏差/欠拟合</p>
<p>​    交叉验证集误差远大于训练集误差时：方差/过拟合</p>
<h3 id="10-5-正则化和偏差-方差"><a href="#10-5-正则化和偏差-方差" class="headerlink" title="10.5 正则化和偏差/方差"></a>10.5 正则化和偏差/方差</h3><p>参考视频: 10 - 5 - Regularization and Bias_Variance (11 min).mkv</p>
<p>​    在我们在训练模型的过程中，一般会使用一些正则化方法来防止过拟合。但是我们可能会正则化的程度太高或太小了，即我们在选择λ的值时也需要思考与刚才选择多项式模型次数类似的问题。</p>
<p><img src="2ba317c326547f5b5313489a3f0d66ce.png" alt=""></p>
<p>​    我们选择一系列的想要测试的 $\lambda$ 值，通常是 0-10之间的呈现2倍关系的值（如：$0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10$共12个）。 我们同样把数据分为训练集、交叉验证集和测试集。</p>
<p><img src="8f557105250853e1602a78c99b2ef95b.png" alt=""></p>
<p>选择$\lambda$的方法为：</p>
<ol>
<li>使用训练集训练出12个不同程度正则化的模型</li>
<li>用12个模型分别对交叉验证集计算的出交叉验证误差</li>
<li>选择得出交叉验证误差<strong>最小</strong>的模型</li>
<li>运用步骤3中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与λ的值绘制在一张图表上：</li>
</ol>
<p><img src="38eed7de718f44f6bb23727c5a88bf5d.png" alt=""></p>
<p>​    • 当 $\lambda$ 较小时，训练集误差较小（过拟合）而交叉验证集误差较大</p>
<p>​    • 随着 $\lambda$ 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加</p>
<h3 id="10-6-学习曲线"><a href="#10-6-学习曲线" class="headerlink" title="10.6 学习曲线"></a>10.6 学习曲线</h3><p>参考视频: 10 - 6 - Learning Curves (12 min).mkv</p>
<p>​    学习曲线就是一种很好的工具，我经常使用学习曲线来判断某一个学习算法是否处于偏差、方差问题。学习曲线是学习算法的一个很好的<strong>合理检验</strong>（<strong>sanity check</strong>）。学习曲线是将训练集误差和交叉验证集误差作为训练集实例数量（$m$）的函数绘制的图表。</p>
<p>​    即，如果我们有100行数据，我们从1行数据开始，逐渐学习更多行的数据。思想是：当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据。</p>
<p><img src="969281bc9b07e92a0052b17288fb2c52.png" alt=""></p>
<p><img src="973216c7b01c910cfa1454da936391c6.png" alt=""></p>
<p>​    如何利用学习曲线识别高偏差/欠拟合：作为例子，我们尝试用一条直线来适应下面的数据，可以看出，无论训练集有多么大误差都不会有太大改观：</p>
<p><img src="4a5099b9f4b6aac5785cb0ad05289335.jpg" alt=""></p>
<p>​    也就是说在高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助。</p>
<p>​    如何利用学习曲线识别高方差/过拟合：假设我们使用一个非常高次的多项式模型，并且正则化非常小，可以看出，当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。</p>
<p><img src="2977243994d8d28d5ff300680988ec34.jpg" alt=""></p>
<p>​    也就是说在高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。</p>
<h3 id="10-7-决定下一步做什么"><a href="#10-7-决定下一步做什么" class="headerlink" title="10.7 决定下一步做什么"></a>10.7 决定下一步做什么</h3><p>参考视频: 10 - 7 - Deciding What to Do Next Revisited (7 min).mkv</p>
<p>​    我们已经介绍了怎样评价一个学习算法，我们讨论了模型选择问题，偏差和方差的问题。那么这些诊断法则怎样帮助我们判断，哪些方法可能有助于改进学习算法的效果，而哪些可能是徒劳的呢？</p>
<p>​    让我们再次回到最开始的例子，在那里寻找答案，这就是我们之前的例子。回顾 1.1 中提出的六种可选的下一步，让我们来看一看我们在什么情况下应该怎样选择：</p>
<ol>
<li><p>获得更多的训练实例——解决高方差</p>
</li>
<li><p>尝试减少特征的数量——解决高方差</p>
</li>
<li><p>尝试获得更多的特征——解决高偏差</p>
</li>
<li><p>尝试增加多项式特征——解决高偏差</p>
</li>
<li><p>尝试减少正则化程度λ——解决高偏差</p>
</li>
<li><p>尝试增加正则化程度λ——解决高方差</p>
</li>
</ol>
<p>神经网络的方差和偏差：<br><img src="c5cd6fa2eb9aea9c581b2d78f2f4ea57.png" alt=""></p>
<p>​    使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。</p>
<p>​    通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。</p>
<p>​    对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络，<br>然后选择交叉验证集代价最小的神经网络。</p>
<p>​    好的，以上就是我们介绍的偏差和方差问题，以及诊断该问题的学习曲线方法。在改进学习算法的表现时，你可以充分运用以上这些内容来判断哪些途径可能是有帮助的。而哪些方法可能是无意义的。如果你理解了以上几节视频中介绍的内容，并且懂得如何运用。那么你已经可以使用机器学习方法有效的解决实际问题了。你也能像硅谷的大部分机器学习从业者一样，他们每天的工作就是使用这些学习算法来解决众多实际问题。我希望这几节中提到的一些技巧，关于方差、偏差，以及学习曲线为代表的诊断法能够真正帮助你更有效率地应用机器学习，让它们高效地工作。</p>
<h2 id="十一、机器学习系统的设计-Machine-Learning-System-Design"><a href="#十一、机器学习系统的设计-Machine-Learning-System-Design" class="headerlink" title="十一、机器学习系统的设计(Machine Learning System Design)"></a>十一、机器学习系统的设计(Machine Learning System Design)</h2><h3 id="11-1-首先要做什么"><a href="#11-1-首先要做什么" class="headerlink" title="11.1 首先要做什么"></a>11.1 首先要做什么</h3><p>参考视频: 11 - 1 - Prioritizing What to Work On (10 min).mkv</p>
<p>​    在接下来的视频中，我将谈到机器学习系统的设计。这些视频将谈及在设计复杂的机器学习系统时，你将遇到的主要问题。同时我们会试着给出一些关于如何巧妙构建一个复杂的机器学习系统的建议。下面的课程的的数学性可能不是那么强，但是我认为我们将要讲到的这些东西是非常有用的，可能在构建大型的机器学习系统时，节省大量的时间。</p>
<p>​    本周以一个垃圾邮件分类器算法为例进行讨论。</p>
<p>​    为了解决这样一个问题，我们首先要做的决定是如何选择并表达特征向量$x$。我们可以选择一个由100个最常出现在垃圾邮件中的词所构成的列表，根据这些词是否有在邮件中出现，来获得我们的特征向量（出现为1，不出现为0），尺寸为100×1。</p>
<p>为了构建这个分类器算法，我们可以做很多事，例如：</p>
<ol>
<li><p>收集更多的数据，让我们有更多的垃圾邮件和非垃圾邮件的样本</p>
</li>
<li><p>基于邮件的路由信息开发一系列复杂的特征</p>
</li>
<li><p>基于邮件的正文信息开发一系列复杂的特征，包括考虑截词的处理</p>
</li>
<li><p>为探测刻意的拼写错误（把<strong>watch</strong> 写成<strong>w4tch</strong>）开发复杂的算法</p>
</li>
</ol>
<p>​       在上面这些选项中，非常难决定应该在哪一项上花费时间和精力，作出明智的选择，比随着感觉走要更好。当我们使用机器学习时，总是可以“头脑风暴”一下，想出一堆方法来试试。实际上，当你需要通过头脑风暴来想出不同方法来尝试去提高精度的时候，你可能已经超越了很多人了。大部分人并不尝试着列出可能的方法，他们做的只是某天早上醒来，因为某些原因有了一个突发奇想：”让我们来试试用<strong>Honey Pot</strong>项目收集大量的数据吧。”</p>
<p>​       我们将在随后的课程中讲误差分析，我会告诉你怎样用一个更加系统性的方法，从一堆不同的方法中，选取合适的那一个。因此，你更有可能选择一个真正的好方法，能让你花上几天几周，甚至是几个月去进行深入的研究。</p>
<h3 id="11-2-误差分析"><a href="#11-2-误差分析" class="headerlink" title="11.2 误差分析"></a>11.2 误差分析</h3><p>参考视频: 11 - 2 - Error Analysis (13 min).mkv</p>
<p>​    在本次课程中，我们将会讲到误差分析（<strong>Error Analysis</strong>）的概念。这会帮助你更系统地做出决定。如果你准备研究机器学习的东西，或者构造机器学习应用程序，最好的实践方法不是建立一个非常复杂的系统，拥有多么复杂的变量；而是构建一个简单的算法，这样你可以很快地实现它。</p>
<p>​    每当我研究机器学习的问题时，我最多只会花一天的时间，就是字面意义上的24小时，来试图很快的把结果搞出来，即便效果不好。坦白的说，就是根本没有用复杂的系统，但是只是很快的得到的结果。即便运行得不完美，但是也把它运行一遍，最后通过交叉验证来检验数据。一旦做完，你可以画出学习曲线，通过画出学习曲线，以及检验误差，来找出你的算法是否有高偏差和高方差的问题，或者别的问题。在这样分析之后，再来决定用更多的数据训练，或者加入更多的特征变量是否有用。这么做的原因是：这在你刚接触机器学习问题时是一个很好的方法，你并不能提前知道你是否需要复杂的特征变量，或者你是否需要更多的数据，还是别的什么。提前知道你应该做什么，是非常难的，因为你缺少证据，缺少学习曲线。因此，你很难知道你应该把时间花在什么地方来提高算法的表现。但是当你实践一个非常简单即便不完美的方法时，你可以通过画出学习曲线来做出进一步的选择。你可以用这种方式来避免一种电脑编程里的过早优化问题，这种理念是：我们必须用证据来领导我们的决策，怎样分配自己的时间来优化算法，而不是仅仅凭直觉，凭直觉得出的东西一般总是错误的。除了画出学习曲线之外，一件非常有用的事是误差分析，我的意思是说：当我们在构造垃圾邮件分类器时，我会看一看我的交叉验证数据集，然后亲自看一看哪些邮件被算法错误地分类。因此，通过这些被算法错误分类的垃圾邮件与非垃圾邮件，你可以发现某些系统性的规律：什么类型的邮件总是被错误分类。经常地这样做之后，这个过程能启发你构造新的特征变量，或者告诉你：现在这个系统的短处，然后启发你如何去提高它。</p>
<p>​    构建一个学习算法的推荐方法为：</p>
<p>​    1. 从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法</p>
<p>​    2.绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择</p>
<p>​    3.进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的实例，看看这些实例是否有某种系统化的趋势</p>
<p>​    以我们的垃圾邮件过滤器为例，误差分析要做的既是检验交叉验证集中我们的算法产生错误预测的所有邮件，看：是否能将这些邮件按照类分组。例如医药品垃圾邮件，仿冒品垃圾邮件或者密码窃取邮件等。然后看分类器对哪一组邮件的预测误差最大，并着手优化。</p>
<p>​    思考怎样能改进分类器。例如，发现是否缺少某些特征，记下这些特征出现的次数。</p>
<p>​    例如记录下错误拼写出现了多少次，异常的邮件路由情况出现了多少次等等，然后从出现次数最多的情况开始着手优化。</p>
<p>​    误差分析并不总能帮助我们判断应该采取怎样的行动。有时我们需要尝试不同的模型，然后进行比较，在模型比较时，用数值来判断哪一个模型更好更有效，通常我们是看交叉验证集的误差。</p>
<p>​    在我们的垃圾邮件分类器例子中，对于“我们是否应该将<strong>discount/discounts/discounted/discounting</strong>处理成同一个词？”如果这样做可以改善我们算法，我们会采用一些截词软件。误差分析不能帮助我们做出这类判断，我们只能尝试采用和不采用截词软件这两种不同方案，然后根据数值检验的结果来判断哪一种更好。</p>
<p>​    因此，当你在构造学习算法的时候，你总是会去尝试很多新的想法，实现出很多版本的学习算法，如果每一次你实践新想法的时候，你都要手动地检测这些例子，去看看是表现差还是表现好，那么这很难让你做出决定。到底是否使用词干提取，是否区分大小写。但是通过一个量化的数值评估，你可以看看这个数字，误差是变大还是变小了。你可以通过它更快地实践你的新想法，它基本上非常直观地告诉你：你的想法是提高了算法表现，还是让它变得更坏，这会大大提高你实践算法时的速度。所以我强烈推荐在交叉验证集上来实施误差分析，而不是在测试集上。但是，还是有一些人会在测试集上来做误差分析。即使这从数学上讲是不合适的。所以我还是推荐你在交叉验证向量上来做误差分析。</p>
<p>​    总结一下，当你在研究一个新的机器学习问题时，我总是推荐你实现一个较为简单快速、即便不是那么完美的算法。我几乎从未见过人们这样做。大家经常干的事情是：花费大量的时间在构造算法上，构造他们以为的简单的方法。因此，不要担心你的算法太简单，或者太不完美，而是尽可能快地实现你的算法。当你有了初始的实现之后，它会变成一个非常有力的工具，来帮助你决定下一步的做法。因为我们可以先看看算法造成的错误，通过误差分析，来看看他犯了什么错，然后来决定优化的方式。另一件事是：假设你有了一个快速而不完美的算法实现，又有一个数值的评估数据，这会帮助你尝试新的想法，快速地发现你尝试的这些想法是否能够提高算法的表现，从而你会更快地做出决定，在算法中放弃什么，吸收什么误差分析可以帮助我们系统化地选择该做什么。</p>
<h3 id="11-3-类偏斜的误差度量"><a href="#11-3-类偏斜的误差度量" class="headerlink" title="11.3 类偏斜的误差度量"></a>11.3 类偏斜的误差度量</h3><p>参考视频: 11 - 3 - Error Metrics for Skewed Classes (12 min).mkv</p>
<p>​    在前面的课程中，我提到了误差分析，以及设定误差度量值的重要性。那就是，设定某个实数来评估你的学习算法，并衡量它的表现，有了算法的评估和误差度量值。有一件重要的事情要注意，就是使用一个合适的误差度量值，这有时会对于你的学习算法造成非常微妙的影响，这件重要的事情就是偏斜类（skewed classes）的问题。类偏斜情况表现为我们的训练集中有非常多的同一种类的实例，只有很少或没有其他类的实例。</p>
<p>​    例如我们希望用算法来预测癌症是否是恶性的，在我们的训练集中，只有0.5%的实例是恶性肿瘤。假设我们编写一个非学习而来的算法，在所有情况下都预测肿瘤是良性的，那么误差只有0.5%。然而我们通过训练而得到的神经网络算法却有1%的误差。这时，误差的大小是不能视为评判算法效果的依据的。</p>
<p>​    <strong>查准率</strong>（<strong>Precision</strong>）和<strong>查全率</strong>（<strong>Recall</strong>） 我们将算法预测的结果分成四种情况：</p>
<p>​    1. <strong>正确肯定</strong>（<strong>True Positive,TP</strong>）：预测为真，实际为真</p>
<p>​    2.<strong>正确否定</strong>（<strong>True Negative,TN</strong>）：预测为假，实际为假</p>
<p>​    3.<strong>错误肯定</strong>（<strong>False Positive,FP</strong>）：预测为真，实际为假</p>
<p>​    4.<strong>错误否定</strong>（<strong>False Negative,FN</strong>）：预测为假，实际为真</p>
<p>​    则：查准率=<strong>TP/(TP+FP)</strong>。例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。</p>
<p>​    查全率=<strong>TP/(TP+FN)</strong>。例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。</p>
<p>​    这样，对于我们刚才那个总是预测病人肿瘤为良性的算法，其查全率是0。</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th><strong>预测值</strong></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td><strong>Positive</strong></td>
<td><strong>Negtive</strong></td>
</tr>
<tr>
<td><strong>实际值</strong></td>
<td><strong>Positive</strong></td>
<td><strong>TP</strong></td>
<td><strong>FN</strong></td>
</tr>
<tr>
<td></td>
<td><strong>Negtive</strong></td>
<td><strong>FP</strong></td>
<td><strong>TN</strong></td>
</tr>
</tbody>
</table>
<h3 id="11-4-查准率和查全率之间的权衡"><a href="#11-4-查准率和查全率之间的权衡" class="headerlink" title="11.4 查准率和查全率之间的权衡"></a>11.4 查准率和查全率之间的权衡</h3><p>参考视频: 11 - 4 - Trading Off Precision and Recall (14 min).mkv</p>
<p>​    在之前的课程中，我们谈到查准率和召回率，作为遇到偏斜类问题的评估度量值。在很多应用中，我们希望能够保证查准率和召回率的相对平衡。</p>
<p>​    在这节课中，我将告诉你应该怎么做，同时也向你展示一些查准率和召回率作为算法评估度量值的更有效的方式。继续沿用刚才预测肿瘤性质的例子。假使，我们的算法输出的结果在0-1 之间，我们使用阀值0.5 来预测真和假。</p>
<p><img src="ad00c2043ab31f32deb2a1eb456b7246.png" alt=""></p>
<p>​    查准率<strong>(Precision)=TP/(TP+FP)</strong><br>例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。</p>
<p>​    查全率<strong>(Recall)=TP/(TP+FN)</strong>例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。</p>
<p>​    如果我们希望只在非常确信的情况下预测为真（肿瘤为恶性），即我们希望更高的查准率，我们可以使用比0.5更大的阀值，如0.7，0.9。这样做我们会减少错误预测病人为恶性肿瘤的情况，同时却会增加未能成功预测肿瘤为恶性的情况。</p>
<p>​    如果我们希望提高查全率，尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断，我们可以使用比0.5更小的阀值，如0.3。</p>
<p>​    我们可以将不同阀值情况下，查全率与查准率的关系绘制成图表，曲线的形状根据数据的不同而不同：</p>
<p><img src="84067e23f2ab0423679379afc6ed6caf.png" alt=""></p>
<p>​    我们希望有一个帮助我们选择这个阀值的方法。一种方法是计算<strong>F1 值</strong>（<strong>F1 Score</strong>），其计算公式为：</p>
 
${{F}_{1}}Score:2\frac{PR}{P+R}$

<p>我们选择使得<strong>F1</strong>值最高的阀值。</p>
<h3 id="11-5-机器学习的数据"><a href="#11-5-机器学习的数据" class="headerlink" title="11.5 机器学习的数据"></a>11.5 机器学习的数据</h3><p>参考视频: 11 - 5 - Data For Machine Learning (11 min).mkv</p>
<p>​    在之前的视频中，我们讨论了评价指标。在这个视频中，我要稍微转换一下，讨论一下机器学习系统设计中另一个重要的方面，这往往涉及到用来训练的数据有多少。在之前的一些视频中，我曾告诫大家不要盲目地开始，而是花大量的时间来收集大量的数据，因为数据有时是唯一能实际起到作用的。但事实证明，在一定条件下，我会在这个视频里讲到这些条件是什么。得到大量的数据并在某种类型的学习算法中进行训练，可以是一种有效的方法来获得一个具有良好性能的学习算法。而这种情况往往出现在这些条件对于你的问题都成立。<br>并且你能够得到大量数据的情况下。这可以是一个很好的方式来获得非常高性能的学习算法。因此，在这段视频中，让我们一起讨论一下这个问题。</p>
<p>​    很多很多年前，我认识的两位研究人员<strong>Michele Banko</strong> 和<strong>Eric Brill</strong>进行了一项有趣的研究，他们尝试通过机器学习算法来区分常见的易混淆的单词，他们尝试了许多种不同的算法，并发现数据量非常大时，这些不同类型的算法效果都很好。</p>
<p><img src="1a7c575dc1b606b8e6e4de71a14dc005.png" alt=""></p>
<p>​    比如，在这样的句子中：早餐我吃了__个鸡蛋(<strong>to</strong>,<strong>two</strong>,<strong>too</strong>)，在这个例子中，“早餐我吃了2个鸡蛋”，这是一个易混淆的单词的例子。于是他们把诸如这样的机器学习问题，当做一类监督学习问题，并尝试将其分类，什么样的词，在一个英文句子特定的位置，才是合适的。他们用了几种不同的学习算法，这些算法都是在他们2001年进行研究的时候，都已经被公认是比较领先的。因此他们使用了一个方差，用于逻辑回归上的一个方差，被称作”感知器”(<strong>perceptron</strong>)。他们也采取了一些过去常用，但是现在比较少用的算法，比如 <strong>Winnow</strong>算法，很类似于回归问题，但在一些方面又有所不同，过去用得比较多，但现在用得不太多。还有一种基于内存的学习算法，现在也用得比较少了，但是我稍后会讨论一点，而且他们用了一个朴素算法。这些具体算法的细节不那么重要，我们下面希望探讨，什么时候我们会希望获得更多数据，而非修改算法。他们所做的就是改变了训练数据集的大小，并尝试将这些学习算法用于不同大小的训练数据集中，这就是他们得到的结果。</p>
<p><img src="befe860fd4b1aef2f6eebf617baf5877.jpg" alt=""></p>
<p>​    这些趋势非常明显，首先大部分算法，都具有相似的性能，其次，随着训练数据集的增大，在横轴上代表以百万为单位的训练集大小，从0.1个百万到1000百万，也就是到了10亿规模的训练集的样本，这些算法的性能也都对应地增强了。</p>
<p>​    事实上，如果你选择任意一个算法，可能是选择了一个”劣等的”算法，如果你给这个劣等算法更多的数据，那么从这些例子中看起来的话，它看上去很有可能会其他算法更好，甚至会比”优等算法”更好。由于这项原始的研究非常具有影响力，因此已经有一系列许多不同的研究显示了类似的结果。这些结果表明，许多不同的学习算法有时倾向于表现出非常相似的表现，这还取决于一些细节，但是真正能提高性能的，是你能够给一个算法大量的训练数据。像这样的结果，引起了一种在机器学习中的普遍共识：”取得成功的人不是拥有最好算法的人，而是拥有最多数据的人”。</p>
<p>​    那么这种说法在什么时候是真，什么时候是假呢？因为如果我们有一个学习算法，并且如果这种说法是真的，那么得到大量的数据通常是保证我们具有一个高性能算法的最佳方式，而不是去争辩应该用什么样的算法。</p>
<p>​    假如有这样一些假设，在这些假设下有大量我们认为有用的训练集，我们假设在我们的机器学习问题中，特征值$x$包含了足够的信息，这些信息可以帮助我们用来准确地预测$y$，例如，如果我们采用了一些容易混淆的词，如：<strong>two</strong>、<strong>to</strong>、<strong>too</strong>，假如说它能够描述$x$，捕捉到需要填写的空白处周围的词语，那么特征捕捉到之后，我们就希望有对于“早饭我吃了__鸡蛋”，那么这就有大量的信息来告诉我中间我需要填的词是“两个”(<strong>two</strong>)，而不是单词 <strong>to</strong> 或<strong>too</strong>，因此特征捕捉，哪怕是周围词语中的一个词，就能够给我足够的信息来确定出标签 $y$是什么。换句话说，从这三组易混淆的词中，我应该选什么词来填空。</p>
<p>​    那么让我们来看一看，大量的数据是有帮助的情况。假设特征值有足够的信息来预测$y$值，假设我们使用一种需要大量参数的学习算法，比如有很多特征的逻辑回归或线性回归，或者用带有许多隐藏单元的神经网络，那又是另外一种带有很多参数的学习算法，这些都是非常强大的学习算法，它们有很多参数，这些参数可以拟合非常复杂的函数，因此我要调用这些，我将把这些算法想象成低偏差算法，因为我们能够拟合非常复杂的函数，而且因为我们有非常强大的学习算法，这些学习算法能够拟合非常复杂的函数。很有可能，如果我们用这些数据运行这些算法，这种算法能很好地拟合训练集，因此，训练误差就会很低了。</p>
<p>​    现在假设我们使用了非常非常大的训练集，在这种情况下，尽管我们希望有很多参数，但是如果训练集比参数的数量还大，甚至是更多，那么这些算法就不太可能会过度拟合。也就是说训练误差有希望接近测试误差。</p>
<p>​    另一种考虑这个问题的角度是为了有一个高性能的学习算法，我们希望它不要有高的偏差和方差。</p>
<p>​    因此偏差问题，我么将通过确保有一个具有很多参数的学习算法来解决，以便我们能够得到一个较低偏差的算法，并且通过用非常大的训练集来保证。</p>
<p><img src="05a3c884505e08028d37a04472d0964a.png" alt=""></p>
<p>​    我们在此没有方差问题，我们的算法将没有方差，并且通过将这两个值放在一起，我们最终可以得到一个低误差和低方差的学习算法。这使得我们能够很好地测试测试数据集。从根本上来说，这是一个关键的假设：特征值有足够的信息量，且我们有一类很好的函数，这是为什么能保证低误差的关键所在。它有大量的训练数据集，这能保证得到更多的方差值，因此这给我们提出了一些可能的条件，如果你有大量的数据，而且你训练了一种带有很多参数的学习算法，那么这将会是一个很好的方式，来提供一个高性能的学习算法。</p>
<p>​    我觉得关键的测试：首先，一个人类专家看到了特征值 $x$，能很有信心的预测出$y$值吗？因为这可以证明 $ y$ 可以根据特征值$x$被准确地预测出来。其次，我们实际上能得到一组庞大的训练集，并且在这个训练集中训练一个有很多参数的学习算法吗？如果你不能做到这两者，那么更多时候，你会得到一个性能很好的学习算法。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习（六）</title>
    <url>/2018/05/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89/</url>
    <content><![CDATA[<p>这里记录了视频第七节的内容，主要关于正则化的问题</p>
<h1 id="欠拟合-under-fitting-和过拟合-over-fitting"><a href="#欠拟合-under-fitting-和过拟合-over-fitting" class="headerlink" title="欠拟合(under fitting)和过拟合(over fitting)"></a>欠拟合(under fitting)和过拟合(over fitting)</h1><p>现在已经学习了一些不同的机器学习算法，包括线性回归和逻辑回归，它们能够有效的解决许多问题，但是将它们应用到某些特定的机器学习中时，就会出现欠拟合或者过拟合的问题，导致效果很差，通过正则化的方法可以改善算法，下面介绍什么是过拟合与欠拟合。<br><img src="1.png" alt=""><br>继续用线性回归预测房价的例子：</p>
<ol>
<li>首先看第一幅图，使用一条直线函数来拟合数据，很显然随着房子面积的增大，房屋价格的变化越稳定或者说是越像右越趋于平滑，因此线性回归并没有很好拟合训练数据。<br>==我们把此类情况称为欠拟合(underfitting)，或者叫作叫做高偏差(bias)。==</li>
</ol>
<p>这两种说法大致相似，都表示没有很好地拟合训练数据。高偏差这个词是 machine learning 的研究初期传下来的一个专业名词，具体到这个问题，意思就是说如果用线性回归这个算法去拟合训练数据，那么该算法实际上会产生一个非常大的偏差或者说存在一个很强的偏见。</p>
<ol start="2">
<li>在第二幅图我们用了一个二次函数去拟合数据，可以拟合出一条合理的曲线，事实证明这是一个很好的拟合效果。</li>
<li>第三幅图，在这里我们有五个训练数据，所以使用了五个参数θ0到θ4的一个四次多项式去拟合数据，它似乎是一个很好的拟合，因为它成功的通过了我们的所有训练数据，但是它非常的扭曲，在上下波动，所以事实上我们并不认为它是一个预测房价的好模型。</li>
</ol>
<p>==我们把这类情况叫做过拟合(overfitting)，也叫高方差(variance)。==</p>
<p>与高偏差一样，高方差同样也是一个历史上的叫法。从第一印象上来说，如果我们拟合一个高阶多项式，那么这个函数能很好的拟合训练集（能拟合几乎所有的训练数据），但这也就面临函数可能太过庞大的问题，变量太多。<br>==同时如果我们没有足够的数据集（训练集）去约束这个变量过多的模型，那么就会发生过拟合。==<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">为什么足够多的多项式可以完美的拟合数据？</span><br><span class="line">泰勒公式展开式。</span><br></pre></td></tr></table></figure></p>
<p>概括的说，过拟合将会发生在变量（特征）过多的时候，这时候训练出的方程总能够很好的拟合训练数据，我们的代价函数无限趋于0或者就是0，但是这样千方百计拟合训练数据的曲线无法泛化到新的样本数据中，以至于无法预测新的样本价格。术语<strong>泛化</strong>指的是一个假设模型能够应用到新样本的能力，新样本指的是不在训练集中的样本数据。</p>
<p>过拟合和欠拟合的情况不仅出现在线性回归也会出现在逻辑回归的问题<br><img src="2.png" alt=""><br><strong>过多的变量（特征），同时只有非常少的训练数据，会导致出现过度拟合的问题</strong></p>
<p>如何避免过拟合呢？有以下两个方式来避免过拟合</p>
<p><img src="3.png" alt=""></p>
<ol>
<li><strong>减少选取变量的数量，保留更加重要的特征</strong></li>
</ol>
<p>具体而言，我们可以人工检查每一项变量，并以此来确定哪些变量更为重要，然后，保留那些更为重要的特征变量。至于，哪些变量应该舍弃，我们以后在讨论，这会涉及到模型选择算法，这种算法是可以自动选择采用哪些特征变量，自动舍弃不需要的变量。这类做法非常有效，但是其缺点是当你舍弃一部分特征变量时，你也舍弃了问题中的一些信息。例如，也许所有的特征变量对于预测房价都是有用的，我们实际上并不想舍弃一些信息或者说舍弃这些特征变量。</p>
<ol start="2">
<li><strong>正则化</strong></li>
</ol>
<p>正则化中我们将保留所有的特征变量，但是会减小特征变量的数量级（参数数值的大小θ(j)）。</p>
<p>这个方法非常有效，当我们有很多特征变量时，其中每一个变量都能对预测产生一点影响。正如我们在房价预测的例子中看到的那样，我们可以有很多特征变量，其中每一个变量都是有用的，因此我们不希望把它们删掉，这就导致了正则化概念的发生。</p>
<p>接下来我们会讨论怎样应用正则化和什么叫做正则化均值，然后将开始讨论怎样使用正则化来使学习算法正常工作，并避免过拟合。</p>
<h1 id="机器学习的正则化"><a href="#机器学习的正则化" class="headerlink" title="机器学习的正则化"></a>机器学习的正则化</h1><p><img src="4.png" alt=""><br>在前面介绍了用二次函数去拟合这些数据，他的拟合效果是很好的，然而我们用更高次的多项式去拟合，最终到的一个曲线，尽管他很好的拟合了训练集，但并不是一个好的结果，因为他过度拟合了数据，所以一般性不好。</p>
<p><img src="5.png" alt=""><br>然后我们考虑下面的假设，我们想要加上<strong>惩罚项</strong>从而使参数 θ3 和 θ4 足够小，上面函数是我们的优化目标，也就是说我们要尽量减少代价函数的均方差，对于这个函数我们对它添加一些项，加上 1000 乘以 θ3 的平方，再加上 1000 乘以 θ4 的平方，于是出现了下面的式子：</p>
<p><img src="6.png" alt=""><br>1000 只是我随便写的某个较大的数字而已。现在，如果我们要最小化这个函数，那么为了最小化这个新的代价函数，我们要让 θ3 和 θ4 尽可能小。因为，如果你在原有代价函数的基础上加上 1000 乘以 θ3 这一项 ，那么这个新的代价函数将变得很大，所以，当我们最小化这个新的代价函数时， 我们将使 θ3 的值接近于 0，同样 θ4 的值也接近于 0，就像我们忽略了这两个值一样。如果我们做到这一点（ θ3 和 θ4 接近 0 ），那么我们将得到一个近似的二次函数。</p>
<p><img src="7.png" alt=""><br>因此，我们最终恰当地拟合了数据，我们所使用的正是二次函数加上一些非常小，贡献很小项（因为这些项的 θ3、 θ4 非常接近于0）。显然，这是一个更好的假设。</p>
<p><img src="8.png" alt=""><br>正则化背后的思路，如果我们的参数值对应一个较小的值的（参数值较小），那么我们会道道一个形式更简单的假设。</p>
<p>在上面的例子中，我们的惩罚的只是θ3和θ4，使这两个值均接近于零，从而得到了一个更简单的假设，实际上这个假设类似一个二次函数。</p>
<p>更简单的讲，如果我们像惩罚θ3和θ4这样惩罚其他参数，那么我们往往可以得到一个相对简单的假设函数。</p>
<p>==实际上，这些参数的值越小，对应的函数曲线越平滑，也就是更加简单的函数，因此，就不易发生过拟合的问题。==</p>
<p>为什么越小的参数对应一个相对简单的假设函数，具体看下面这个例子。</p>
<p>对于房屋价格的预测我们可能又上百个特征，与刚刚所说的多项式例子不同，我们并不知道θ3和θ4是高阶多项式的项，所以，我们有一百个特征，但是我们那并不知道如何选择关联度更好的参数，如何缩小参数的数目等等。</p>
<p>因此在正则化里，我们要做的事情，就是减小我们的代价函数所有的参数值，因为我们并不知道哪一个或几个要去缩小，所以我们要修改代价函数，在后面添加一项，就像我们在方括号里的这项，当我们添加一个额外的正则化项的时候，我们缩小了每一个参数。</p>
<p><img src="9.png" alt=""><br>顺便说一下，按照惯例，我们没有去惩罚 θ0，因此 θ0的值是大的。这就是一个约定从 1 到 n 的求和，而不是从 0 到 n 的求和。但其实在实践中这只会有非常小的差异，无论你是否包括这θ0这项。但是按照惯例，通常情况下我们还是只从 θ1 到 θn 进行正则化。</p>
<p>带λ的的这项就是一个正则化项，并且λ在这里我们称做正则化参数。</p>
<p>==λ 要做的就是控制在两个不同的目标中的平衡关系。==</p>
<p><strong>第一个目标就是我们想要使假设函数更好的拟合训练数据</strong></p>
<p><strong>第二个目标是要保持我们的参数较小通过正则化</strong></p>
<p>而λ这个正则化参数需要控制的是两者之间的平衡，既平衡拟合训练的目标和保持参数值较小的目标。从而保持假设的形式相对简单，来避免过拟合。</p>
<p>对于房屋价格预测来说，我们之前所用的非常多的高阶多项式来拟合，我们将会得到一个非常弯曲和复杂的曲线函数，现在只需要通过正则化的优化，就可以得到一个更加合适的曲线，这个曲线不是一个真正的二次函数曲线，而是更加的流畅和简单的一个曲线。这样就得到了对于这个数据集更好的假设函数。</p>
<p>再一次说明下，这部分内容的确有些难以明白，为什么加上参数的影响可以具有这种效果？但如果你亲自实现了正规化，你将能够看到这种影响的最直观的感受。</p>
<p><img src="10.png" alt=""><br>在正则化线性回归中，如果正则化参数值λ被设定的非常大，那么会发生什么呢？我们非常大的惩罚参数θ1 θ2 θ3 θ4 … 也就是说，我们最终惩罚θ1 θ2 θ3 θ4 …  在一个非常大的程度，那么我们会使所有这些参数接近于零。</p>
<p><img src="11.png" alt=""><br>如果我们这么做，那么就意味着我们的假设相当于去掉了这些项，并且使我们只留下了一个简单的假设，这个假设只能表明房屋价格等于θ0的值，那就是类似与一条水平的直线，对于数据来说就是一个欠拟合。这是一个失败的假设直线，对于训练集来说这就是一条平滑的直线，它没有任何趋势，它不会去趋向大部分的训练样本的任何值。</p>
<p>另一个说法就是这种假设有过于强烈的偏见或者说使高偏差，认为预测的价格只等于θ0，对于数据来说只是一条水平线。</p>
<p>因此，为了使正则化运行良好，我们应当注意一些方面，应该去选择一个不错的正则化参数λ，当我们以后讲到多重选择时我们将讨论一种方法来自动选择正则化参数 λ，为了使用正则化，接下来我们将把这些概念应用到到线性回归和逻辑回归中去，那么我们就可以让他们避免过度拟合了。</p>
<h1 id="正则化的线性回归-（Regularized-Linear-Regression）"><a href="#正则化的线性回归-（Regularized-Linear-Regression）" class="headerlink" title="正则化的线性回归 （Regularized Linear Regression）"></a>正则化的线性回归 （Regularized Linear Regression）</h1><p>之前写过线性回归的代价函数如下：</p>
<p><img src="12.png" alt=""><br>对于线性回归的求解，我们之前运用了两种学习算法，一种基于梯度下降，一种基于正规方程。</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p><img src="13.png" alt=""></p>
<h2 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h2><p><img src="14.png" alt=""></p>
<h3 id="不可逆情况"><a href="#不可逆情况" class="headerlink" title="不可逆情况"></a>不可逆情况</h3><p>当出现样本数量M比特征数N少或等于时，矩阵XTX将出现不可逆或者奇异（singluar）矩阵，用另一个说法就是矩阵的退化（degenerate），这时我们就没办法用正规方程来求 θ。</p>
<p><img src="15.png" alt=""><br>正则化可以解决这个问题，具体的说只要正则参数是严格大于零，实际上，可以证明上图的蓝括号部分是可逆的（invertable），因此正则化可以解决任何XTX不可逆的问题。</p>
<p>所以现在可以实现线性回归避免过度拟合的问题，即使是一个相对较小的训练集合里面有很多的特征值。</p>
<h1 id="正则化的逻辑回归（Regularized-Logistic-Regression）"><a href="#正则化的逻辑回归（Regularized-Logistic-Regression）" class="headerlink" title="正则化的逻辑回归（Regularized Logistic Regression）"></a>正则化的逻辑回归（Regularized Logistic Regression）</h1><p>逻辑回归的正则化实际上和线性回归的正则化十分的相似。</p>
<p><img src="16.png" alt=""><br>同样使用梯度下降：</p>
<p><img src="17.png" alt=""><br>如果在高级优化算法中，使用正则化技术的话，那么对于这类算法我们需要自己定义costfunction</p>
<p><img src="18.png" alt=""><br>这个我们自定义的 costFunction 的输入为向量 θ ，返回值有两项，分别是代价函数 jVal 以及 梯度gradient。</p>
<p>总之我们需要的就是这个自定义函数costFunction，针对Octave而言，我们可以将这个函数作为参数传入到 fminunc 系统函数中（fminunc 用来求函数的最小值，将@costFunction作为参数代进去，注意 @costFunction 类似于C语言中的函数指针），fminunc返回的是函数 costFunction 在无约束条件下的最小值，即我们提供的代价函数 jVal 的最小值，当然也会返回向量 θ 的解。</p>
<p>上述方法显然对正则化逻辑回归是适用的。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>从这里开始感觉课程已经有些难度了，关于正则化我也是查阅了其他相关资料才得以明白，尝试写个关于正则化的程序吧。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习（四）</title>
    <url>/2018/04/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
    <content><![CDATA[<p>前四章的内容学习完毕，第五章讲了Octave这个软件的使用，类似于matlab，大学有过学习matlab经验所以这个学起来想对比较轻松，不论是在Ubuntu还是windows安装都很简单，这个的界面布局都和matlab基本一模一样。</p>
<p>虽然用python都可以实现，但Octave开源免费，比numpy更简单的实现算法，所以有必要学习一下。</p>
<p>其实关于Octave的东西并不想记录，和matlab一样，但为了这个博客的完整性还是简单的记录一下，我使用的是windows版的直接打开GUI就能使用了。</p>
<h1 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h1><figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line">%基本四则运算</span><br><span class="line"><span class="meta">&gt;&gt;</span> <span class="number">1</span>+<span class="number">2</span></span><br><span class="line">ans =  <span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;</span> <span class="number">6</span>-<span class="number">1</span></span><br><span class="line">ans =  <span class="number">5</span></span><br><span class="line"><span class="meta">&gt;&gt;</span> <span class="number">5</span>*<span class="number">8</span></span><br><span class="line">ans =  <span class="number">40</span></span><br><span class="line"><span class="meta">&gt;&gt;</span> <span class="number">1</span>/<span class="number">5</span></span><br><span class="line">ans =  <span class="number">0</span>.<span class="number">20000</span></span><br><span class="line"><span class="meta">&gt;&gt;</span> <span class="number">3</span>^<span class="number">6</span></span><br><span class="line">ans =  <span class="number">729</span></span><br><span class="line"></span><br><span class="line">%不等号是~而不是！</span><br><span class="line"><span class="meta">&gt;&gt;</span> <span class="number">1</span>==<span class="number">2</span></span><br><span class="line">ans = <span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;</span> <span class="number">1</span>~=<span class="number">2</span></span><br><span class="line">ans = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">%与 或 异或</span><br><span class="line"><span class="meta">&gt;&gt;</span> <span class="number">8</span> &gt; <span class="number">1</span> &amp;&amp; <span class="number">0</span></span><br><span class="line">ans = <span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;</span> <span class="number">9</span> &gt; <span class="number">1</span> <span class="params">||</span> <span class="number">0</span></span><br><span class="line">ans = <span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;</span> xor(<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">ans = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">%如果你想分配一个变量，但不希望在屏幕上显示结果，你可以在命令后加一个分号，可以抑制打印输出，敲入回车后，不打印任何东西。</span><br><span class="line"><span class="meta">&gt;&gt;</span> a = <span class="number">3</span></span><br><span class="line">a =  <span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;</span> a = <span class="number">3</span>;</span><br><span class="line"><span class="meta">&gt;&gt;</span> b = <span class="string">'hello word'</span>;</span><br><span class="line"><span class="meta">&gt;&gt;</span> b</span><br><span class="line">b = hello word</span><br><span class="line">%设置A等于圆周率π，如果我要打印该值，那么只需键入A像这样就打印出来了。</span><br><span class="line"><span class="meta">&gt;&gt;</span> a = pi;</span><br><span class="line"><span class="meta">&gt;&gt;</span> pi</span><br><span class="line">ans =  <span class="number">3.1416</span></span><br><span class="line"><span class="meta">&gt;&gt;</span> a</span><br><span class="line">a =  <span class="number">3.1416</span></span><br><span class="line"><span class="meta">&gt;&gt;</span> disp(sprintf(<span class="string">'2 decimals: %0.12f'</span>, a))</span><br><span class="line"><span class="number">2</span> <span class="symbol">decimals:</span> <span class="number">3.141592653590</span></span><br><span class="line">这是一种，旧风格的C语言语法，对于之前就学过C语言的同学来说，你可以使用这种基本的语法来将结果打印到屏幕。</span><br><span class="line"></span><br><span class="line">例如 sprintf命令的六个小数：<span class="number">0</span>.<span class="number">6</span>%f ,a，这应该打印π的<span class="number">6</span>位小数形式。</span><br><span class="line"></span><br><span class="line">也有一些控制输出长短格式的快捷命令：</span><br><span class="line"><span class="meta">&gt;&gt;</span> format long</span><br><span class="line"><span class="meta">&gt;&gt;</span> a</span><br><span class="line">a =  <span class="number">3.14159265358979</span></span><br><span class="line"><span class="meta">&gt;&gt;</span> format short</span><br><span class="line"><span class="meta">&gt;&gt;</span> a</span><br><span class="line">a =  <span class="number">3.1416</span></span><br></pre></td></tr></table></figure>
<p>简单的运算符就是这些，重点是关于矩阵的</p>
<h1 id="简单矩阵的创建"><a href="#简单矩阵的创建" class="headerlink" title="简单矩阵的创建"></a>简单矩阵的创建</h1><figure class="highlight tap"><table><tr><td class="code"><pre><span class="line">简单矩阵的创建</span><br><span class="line">&gt;&gt; A = [1 2;<span class="number"> 3 </span>4;<span class="number"> 5 </span>6]</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">  <span class="number"> 1 </span>  2</span><br><span class="line">  <span class="number"> 3 </span>  4</span><br><span class="line">  <span class="number"> 5 </span>  6</span><br><span class="line"></span><br><span class="line">&gt;&gt; A = [2 2;</span><br><span class="line">3 3;</span><br><span class="line">4 4]</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">  <span class="number"> 2 </span>  2</span><br><span class="line">  <span class="number"> 3 </span>  3</span><br><span class="line">  <span class="number"> 4 </span>  4</span><br><span class="line"></span><br><span class="line">&gt;&gt; B = [1<span class="number"> 2 </span>3]</span><br><span class="line">B =</span><br><span class="line"></span><br><span class="line">  <span class="number"> 1 </span> <span class="number"> 2 </span>  3</span><br><span class="line"></span><br><span class="line">&gt;&gt; B = [1; 2; 3]</span><br><span class="line">B =</span><br><span class="line"></span><br><span class="line">   1</span><br><span class="line">   2</span><br><span class="line">   3</span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br><span class="line">这个集合V是一组值，从数值1开始，增量或说是步长为0.1，直到增加到2，按照这样的方法对向量V操作，可以得到一个行向量，这是一个1行11列的矩阵，其矩阵的元素是1 1.1 1.2 1.3，依此类推，直到数值2。</span><br><span class="line"></span><br><span class="line">我也可以建立一个集合V并用命令“1:6”进行赋值，这样V就被赋值了1至6的六个整数。</span><br><span class="line">&gt;&gt; v = 1:6</span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">  <span class="number"> 1 </span> <span class="number"> 2 </span> <span class="number"> 3 </span> <span class="number"> 4 </span> <span class="number"> 5 </span>  6</span><br><span class="line"></span><br><span class="line">这里还有一些其他的方法来生成矩阵</span><br><span class="line">例如“ones(2,3)”，也可以用来生成矩阵：</span><br><span class="line">&gt;&gt; ones(2,3)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">  <span class="number"> 1 </span> <span class="number"> 1 </span>  1</span><br><span class="line">  <span class="number"> 1 </span> <span class="number"> 1 </span>  1</span><br><span class="line"></span><br><span class="line">元素都为2，两行三列的矩阵，就可以使用这个命令：</span><br><span class="line">&gt;&gt; C = 2*ones(2,3)</span><br><span class="line">C =</span><br><span class="line"></span><br><span class="line">  <span class="number"> 2 </span> <span class="number"> 2 </span>  2</span><br><span class="line">  <span class="number"> 2 </span> <span class="number"> 2 </span>  2</span><br><span class="line"></span><br><span class="line">你可以把这个方法当成一个生成矩阵的快速方法。</span><br><span class="line">w为一个一行三列的零矩阵，一行三列的A矩阵里的元素全部是零：</span><br><span class="line">&gt;&gt; W = zeros(1,3)</span><br><span class="line">W =</span><br><span class="line"></span><br><span class="line">  <span class="number"> 0 </span> <span class="number"> 0 </span>  0</span><br><span class="line"></span><br><span class="line">如果我对W进行赋值，用Rand命令建立一个一行三列的矩阵，因为使用了Rand命令，则其一行三列的元素均为随机值，如“rand(3, 3)”命令，这就生成了一个3×3的矩阵，并且其所有元素均为随机。</span><br><span class="line">&gt;&gt; rand(3,3)</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   0.60790   0.22000   0.10036</span><br><span class="line">   0.61343   0.58981   0.17660</span><br><span class="line">   0.22697   0.88276   0.42049</span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br><span class="line">你知道什么是高斯随机变量，或者，你知道什么是正态分布的随机变量，你可以设置集合W，使其等于一个一行三列的</span><br><span class="line">N矩阵，并且，来自三个值，一个平均值为0的高斯分布，方差或者等于1的标准偏差。</span><br><span class="line">&gt;&gt; w = randn(1,3)</span><br><span class="line">w =</span><br><span class="line"></span><br><span class="line">  -1.24688   1.87417  -0.70878</span><br><span class="line"></span><br><span class="line">并用hist命令绘制直方图。</span><br><span class="line">&gt;&gt; w = -9 + sqrt(10)*(randn(1, 10000));</span><br><span class="line">&gt;&gt; hist(w)</span><br><span class="line">&gt;&gt; hist(w,50)</span><br><span class="line"></span><br><span class="line">绘制单位矩阵：</span><br><span class="line">&gt;&gt; I = eye(6)</span><br><span class="line">I =</span><br><span class="line"></span><br><span class="line">Diagonal Matrix</span><br><span class="line"></span><br><span class="line">  <span class="number"> 1 </span> <span class="number"> 0 </span> <span class="number"> 0 </span> <span class="number"> 0 </span> <span class="number"> 0 </span>  0</span><br><span class="line">  <span class="number"> 0 </span> <span class="number"> 1 </span> <span class="number"> 0 </span> <span class="number"> 0 </span> <span class="number"> 0 </span>  0</span><br><span class="line">  <span class="number"> 0 </span> <span class="number"> 0 </span> <span class="number"> 1 </span> <span class="number"> 0 </span> <span class="number"> 0 </span>  0</span><br><span class="line">  <span class="number"> 0 </span> <span class="number"> 0 </span> <span class="number"> 0 </span> <span class="number"> 1 </span> <span class="number"> 0 </span>  0</span><br><span class="line">  <span class="number"> 0 </span> <span class="number"> 0 </span> <span class="number"> 0 </span> <span class="number"> 0 </span> <span class="number"> 1 </span>  0</span><br><span class="line">  <span class="number"> 0 </span> <span class="number"> 0 </span> <span class="number"> 0 </span> <span class="number"> 0 </span> <span class="number"> 0 </span>  1</span><br><span class="line"></span><br><span class="line">对命令不清楚可以通过help命令查询</span><br></pre></td></tr></table></figure>
<h1 id="size函数"><a href="#size函数" class="headerlink" title="size函数"></a>size函数</h1><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A = [<span class="number">1</span>: <span class="number">2</span>; <span class="number">3</span> <span class="number">4</span>; <span class="number">5</span> <span class="number">6</span>]</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span>   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span>   <span class="number">4</span></span><br><span class="line">   <span class="number">5</span>   <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">size</span>(A) <span class="comment">%输出[行数 列数]</span></span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">   <span class="number">3</span>   <span class="number">2</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="built_in">size</span>(A, <span class="number">1</span>) <span class="comment">%行数</span></span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">3</span></span><br><span class="line">&gt;&gt; <span class="built_in">size</span>(A, <span class="number">2</span>) <span class="comment">%列数</span></span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">2</span></span><br><span class="line">&gt;&gt; <span class="built_in">length</span>(A) <span class="comment">%行数和列数中最大值</span></span><br><span class="line"><span class="built_in">ans</span> =  <span class="number">3</span></span><br></pre></td></tr></table></figure>
<h1 id="导入与导出数据"><a href="#导入与导出数据" class="headerlink" title="导入与导出数据"></a>导入与导出数据</h1><figure class="highlight maxima"><table><tr><td class="code"><pre><span class="line"><span class="built_in">load</span> 文件名</span><br><span class="line">whos <span class="symbol">%</span>将当前的变量都显示出来</span><br><span class="line">clear A <span class="symbol">%</span>将变量A删除</span><br><span class="line"></span><br><span class="line"><span class="built_in">save</span> hello.mat A; <span class="symbol">%</span>将变量A存入hello.mat文件</span><br><span class="line"><span class="built_in">save</span> hello.txt A -<span class="built_in">ascii</span>; <span class="symbol">%</span>将A存为<span class="built_in">ascii</span></span><br></pre></td></tr></table></figure>
<h1 id="取矩阵中的值"><a href="#取矩阵中的值" class="headerlink" title="取矩阵中的值"></a>取矩阵中的值</h1><figure class="highlight xquery"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span>   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span>   <span class="number">4</span></span><br><span class="line">   <span class="number">5</span>   <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A(<span class="number">3</span>,<span class="number">2</span>) %矩阵A第三行第二列的数</span><br><span class="line">ans =  <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A(<span class="number">2</span>,:) %第二行的数</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   <span class="number">3</span>   <span class="number">4</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A<span class="comment">(:,2) %第二列的数</span></span><br><span class="line"><span class="comment">ans =</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">   2</span></span><br><span class="line"><span class="comment">   4</span></span><br><span class="line"><span class="comment">   6</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">&gt;&gt; A([1 3],:)</span> %第一行和第三行的数</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span>   <span class="number">2</span></span><br><span class="line">   <span class="number">5</span>   <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A<span class="comment">(:,2) = [10;11;12] %修改第二列的数</span></span><br><span class="line"><span class="comment">A =</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    1   10</span></span><br><span class="line"><span class="comment">    3   11</span></span><br><span class="line"><span class="comment">    5   12</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">&gt;&gt; A = [A,[100;200;300]] %增加一列数据</span></span><br><span class="line"><span class="comment">A =</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">     1    10   100</span></span><br><span class="line"><span class="comment">     3    11   200</span></span><br><span class="line"><span class="comment">     5    12   300</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">&gt;&gt; A(:)</span> %修改为一列向量</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span></span><br><span class="line">     <span class="number">3</span></span><br><span class="line">     <span class="number">5</span></span><br><span class="line">    <span class="number">10</span></span><br><span class="line">    <span class="number">11</span></span><br><span class="line">    <span class="number">12</span></span><br><span class="line">   <span class="number">100</span></span><br><span class="line">   <span class="number">200</span></span><br><span class="line">   <span class="number">300</span></span><br></pre></td></tr></table></figure>
<h1 id="拼接矩阵"><a href="#拼接矩阵" class="headerlink" title="拼接矩阵"></a>拼接矩阵</h1><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A = [<span class="number">1</span> <span class="number">2</span>; <span class="number">3</span> <span class="number">4</span>; <span class="number">5</span> <span class="number">6</span>]</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span>   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span>   <span class="number">4</span></span><br><span class="line">   <span class="number">5</span>   <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; B = [<span class="number">11</span> <span class="number">12</span>; <span class="number">13</span> <span class="number">14</span>; <span class="number">15</span> <span class="number">16</span>]</span><br><span class="line">B =</span><br><span class="line"></span><br><span class="line">   <span class="number">11</span>   <span class="number">12</span></span><br><span class="line">   <span class="number">13</span>   <span class="number">14</span></span><br><span class="line">   <span class="number">15</span>   <span class="number">16</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; C = [A B] %将矩阵A和B并列拼接</span><br><span class="line">C =</span><br><span class="line"></span><br><span class="line">    <span class="number">1</span>    <span class="number">2</span>   <span class="number">11</span>   <span class="number">12</span></span><br><span class="line">    <span class="number">3</span>    <span class="number">4</span>   <span class="number">13</span>   <span class="number">14</span></span><br><span class="line">    <span class="number">5</span>    <span class="number">6</span>   <span class="number">15</span>   <span class="number">16</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; C = [A;B] %加分号是将B矩阵拼接到A下面</span><br><span class="line">C =</span><br><span class="line"></span><br><span class="line">    <span class="number">1</span>    <span class="number">2</span></span><br><span class="line">    <span class="number">3</span>    <span class="number">4</span></span><br><span class="line">    <span class="number">5</span>    <span class="number">6</span></span><br><span class="line">   <span class="number">11</span>   <span class="number">12</span></span><br><span class="line">   <span class="number">13</span>   <span class="number">14</span></span><br><span class="line">   <span class="number">15</span>   <span class="number">16</span></span><br><span class="line">&gt;&gt;</span><br></pre></td></tr></table></figure>
<h1 id="矩阵计算"><a href="#矩阵计算" class="headerlink" title="矩阵计算"></a>矩阵计算</h1><figure class="highlight tap"><table><tr><td class="code"><pre><span class="line">&gt;&gt; a = [1 2;<span class="number"> 3 </span>4;<span class="number"> 5 </span>6]</span><br><span class="line">a =</span><br><span class="line"></span><br><span class="line">  <span class="number"> 1 </span>  2</span><br><span class="line">  <span class="number"> 3 </span>  4</span><br><span class="line">  <span class="number"> 5 </span>  6</span><br><span class="line"></span><br><span class="line">&gt;&gt; B = [11 22;<span class="number"> 33 </span>44;<span class="number"> 55 </span>66]</span><br><span class="line">B =</span><br><span class="line"></span><br><span class="line">  <span class="number"> 11 </span>  22</span><br><span class="line">  <span class="number"> 33 </span>  44</span><br><span class="line">  <span class="number"> 55 </span>  66</span><br><span class="line"></span><br><span class="line">&gt;&gt; C = [1 1;<span class="number"> 2 </span>2]</span><br><span class="line">C =</span><br><span class="line"></span><br><span class="line">  <span class="number"> 1 </span>  1</span><br><span class="line">  <span class="number"> 2 </span>  2</span><br><span class="line"></span><br><span class="line">&gt;&gt; V = [1; 2; 3]</span><br><span class="line">V =</span><br><span class="line"></span><br><span class="line">   1</span><br><span class="line">   2</span><br><span class="line">   3</span><br><span class="line"></span><br><span class="line">&gt;&gt; A*C %矩阵相乘</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   <span class="number"> 5 </span>   5</span><br><span class="line">  <span class="number"> 11 </span>  11</span><br><span class="line">  <span class="number"> 17 </span>  17</span><br><span class="line"></span><br><span class="line">&gt;&gt; A*B %相乘条件必须是A矩阵的列等于B矩阵的行，否则报错</span><br><span class="line">error: operator *: nonconformant arguments (op1 is 3x2, op2 is 3x2)</span><br><span class="line"></span><br><span class="line">&gt;&gt; A.*2 %矩阵中的每个元素都乘二</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   <span class="number"> 2 </span>   4</span><br><span class="line">   <span class="number"> 6 </span>   8</span><br><span class="line">  <span class="number"> 10 </span>  12</span><br><span class="line"></span><br><span class="line">&gt;&gt; A.^2 %每个元素的平方</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   <span class="number"> 1 </span>   4</span><br><span class="line">   <span class="number"> 9 </span>  16</span><br><span class="line">  <span class="number"> 25 </span>  36</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt; 1./V %每个元素的倒数</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   1.00000</span><br><span class="line">   0.50000</span><br><span class="line">   0.33333</span><br><span class="line">   </span><br><span class="line">&gt;&gt; V + ones(length(V), 1) %每个元素都加一</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   2</span><br><span class="line">   3</span><br><span class="line">   4</span><br><span class="line"></span><br><span class="line">&gt;&gt; A' %A的转置</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">  <span class="number"> 1 </span> <span class="number"> 3 </span>  5</span><br><span class="line">  <span class="number"> 2 </span> <span class="number"> 4 </span>  6</span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br></pre></td></tr></table></figure>
<h1 id="矩阵的索引"><a href="#矩阵的索引" class="headerlink" title="矩阵的索引"></a>矩阵的索引</h1><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">&gt;&gt; a = [<span class="number">1</span> <span class="number">15</span> <span class="number">2</span> <span class="number">0.5</span>]</span><br><span class="line">a =</span><br><span class="line"></span><br><span class="line">    <span class="number">1.00000</span>   <span class="number">15.00000</span>    <span class="number">2.00000</span>    <span class="number">0.50000</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; [val,ind] = max(a) % val 矩阵中的最大元素，ind 最大值的index</span><br><span class="line">val =  <span class="number">15</span></span><br><span class="line">ind =  <span class="number">2</span></span><br><span class="line">&gt;&gt; val = max(A) %矩阵每列的最大值</span><br><span class="line">val =</span><br><span class="line"></span><br><span class="line">   <span class="number">5</span>   <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; a &lt; <span class="number">3</span> %检查矩阵中比<span class="number">3</span>小的元素，返回布尔型</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">  <span class="number">1</span>  <span class="number">0</span>  <span class="number">1</span>  <span class="number">1</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; find(a&lt;<span class="number">3</span>) %比<span class="number">3</span>小的元素的位置</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span>   <span class="number">3</span>   <span class="number">4</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A = magic(<span class="number">3</span>) %创建一个幻方 （行，列，对角线相加想等）</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">8</span>   <span class="number">1</span>   <span class="number">6</span></span><br><span class="line">   <span class="number">3</span>   <span class="number">5</span>   <span class="number">7</span></span><br><span class="line">   <span class="number">4</span>   <span class="number">9</span>   <span class="number">2</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; [r c] = find(A&gt;=<span class="number">7</span>) % 符合A&gt;=<span class="number">7</span>元素的行列坐标</span><br><span class="line">r =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span></span><br><span class="line">   <span class="number">3</span></span><br><span class="line">   <span class="number">2</span></span><br><span class="line"></span><br><span class="line">c =</span><br><span class="line"></span><br><span class="line">   <span class="number">1</span></span><br><span class="line">   <span class="number">2</span></span><br><span class="line">   <span class="number">3</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt; sum(a) %求所有元素的和</span><br><span class="line">ans =  <span class="number">18.500</span></span><br><span class="line">&gt;&gt; prod(A) %求每列的乘积</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   <span class="number">96</span>   <span class="number">45</span>   <span class="number">84</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; sum(A) %求每列的和</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   <span class="number">15</span>   <span class="number">15</span>   <span class="number">15</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; floor(a) %返回小于元素的最小整数</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">    <span class="number">1</span>   <span class="number">15</span>    <span class="number">2</span>    <span class="number">0</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; ceil(a) %返回大于元素的最大整数</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">    <span class="number">1</span>   <span class="number">15</span>    <span class="number">2</span>    <span class="number">1</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; max(rand(<span class="number">3</span>), rand(<span class="number">3</span>)) %比较两个矩阵返回最大值</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   <span class="number">0.65329</span>   <span class="number">0.32803</span>   <span class="number">0.23948</span></span><br><span class="line">   <span class="number">0.56627</span>   <span class="number">0.37716</span>   <span class="number">0.64170</span></span><br><span class="line">   <span class="number">0.17771</span>   <span class="number">0.81867</span>   <span class="number">0.73937</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; max(A, [], <span class="number">1</span>) %返回每一列的最大值</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   <span class="number">8</span>   <span class="number">9</span>   <span class="number">7</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; max(A, [], <span class="number">2</span>) %返回每一行的最大值</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   <span class="number">8</span></span><br><span class="line">   <span class="number">7</span></span><br><span class="line">   <span class="number">9</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; A = magic(<span class="number">9</span>)</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">47</span>   <span class="number">58</span>   <span class="number">69</span>   <span class="number">80</span>    <span class="number">1</span>   <span class="number">12</span>   <span class="number">23</span>   <span class="number">34</span>   <span class="number">45</span></span><br><span class="line">   <span class="number">57</span>   <span class="number">68</span>   <span class="number">79</span>    <span class="number">9</span>   <span class="number">11</span>   <span class="number">22</span>   <span class="number">33</span>   <span class="number">44</span>   <span class="number">46</span></span><br><span class="line">   <span class="number">67</span>   <span class="number">78</span>    <span class="number">8</span>   <span class="number">10</span>   <span class="number">21</span>   <span class="number">32</span>   <span class="number">43</span>   <span class="number">54</span>   <span class="number">56</span></span><br><span class="line">   <span class="number">77</span>    <span class="number">7</span>   <span class="number">18</span>   <span class="number">20</span>   <span class="number">31</span>   <span class="number">42</span>   <span class="number">53</span>   <span class="number">55</span>   <span class="number">66</span></span><br><span class="line">    <span class="number">6</span>   <span class="number">17</span>   <span class="number">19</span>   <span class="number">30</span>   <span class="number">41</span>   <span class="number">52</span>   <span class="number">63</span>   <span class="number">65</span>   <span class="number">76</span></span><br><span class="line">   <span class="number">16</span>   <span class="number">27</span>   <span class="number">29</span>   <span class="number">40</span>   <span class="number">51</span>   <span class="number">62</span>   <span class="number">64</span>   <span class="number">75</span>    <span class="number">5</span></span><br><span class="line">   <span class="number">26</span>   <span class="number">28</span>   <span class="number">39</span>   <span class="number">50</span>   <span class="number">61</span>   <span class="number">72</span>   <span class="number">74</span>    <span class="number">4</span>   <span class="number">15</span></span><br><span class="line">   <span class="number">36</span>   <span class="number">38</span>   <span class="number">49</span>   <span class="number">60</span>   <span class="number">71</span>   <span class="number">73</span>    <span class="number">3</span>   <span class="number">14</span>   <span class="number">25</span></span><br><span class="line">   <span class="number">37</span>   <span class="number">48</span>   <span class="number">59</span>   <span class="number">70</span>   <span class="number">81</span>    <span class="number">2</span>   <span class="number">13</span>   <span class="number">24</span>   <span class="number">35</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; sum(A,<span class="number">2</span>) %行的和</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   <span class="number">369</span></span><br><span class="line">   <span class="number">369</span></span><br><span class="line">   <span class="number">369</span></span><br><span class="line">   <span class="number">369</span></span><br><span class="line">   <span class="number">369</span></span><br><span class="line">   <span class="number">369</span></span><br><span class="line">   <span class="number">369</span></span><br><span class="line">   <span class="number">369</span></span><br><span class="line">   <span class="number">369</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; sum(A,<span class="number">1</span>) %列的和</span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">   <span class="number">369</span>   <span class="number">369</span>   <span class="number">369</span>   <span class="number">369</span>   <span class="number">369</span>   <span class="number">369</span>   <span class="number">369</span>   <span class="number">369</span>   <span class="number">369</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; sum(sum(A.* eye(<span class="number">9</span>))) %对角线的和</span><br><span class="line">ans =  <span class="number">369</span></span><br><span class="line">&gt;&gt; pinv(A) %伪逆矩阵</span><br></pre></td></tr></table></figure>
<h1 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h1><figure class="highlight excel"><table><tr><td class="code"><pre><span class="line">&gt; <span class="built_in">t</span> = [<span class="number">0</span> <span class="symbol">:</span> <span class="number">0.01</span> <span class="symbol">:</span> <span class="number">0.98</span>];</span><br><span class="line">&gt;&gt; <span class="symbol">y1</span> = <span class="built_in">sin</span>(<span class="number">2</span>*<span class="built_in">pi</span>*<span class="number">4</span>*<span class="built_in">t</span>);</span><br><span class="line">&gt;&gt; plot(<span class="built_in">t</span>, <span class="symbol">y1</span>,'r')</span><br></pre></td></tr></table></figure>
<p><img src="1.png" alt=""><br>在一个画布上画两副如图<br><figure class="highlight excel"><table><tr><td class="code"><pre><span class="line">&gt;&gt; <span class="symbol">y1</span> = <span class="built_in">sin</span>(<span class="number">2</span>*<span class="built_in">pi</span>*<span class="number">4</span>*<span class="built_in">t</span>);</span><br><span class="line">&gt;&gt; <span class="symbol">y2</span> = <span class="built_in">cos</span>(<span class="number">2</span>*<span class="built_in">pi</span>*<span class="number">4</span>*<span class="built_in">t</span>);</span><br><span class="line">&gt;&gt; plot(<span class="built_in">t</span>,<span class="symbol">y2</span>)</span><br><span class="line">&gt;&gt; hold on;</span><br><span class="line">&gt;&gt; plot(<span class="built_in">t</span>, <span class="symbol">y1</span>,'r')</span><br></pre></td></tr></table></figure></p>
<p><img src="2.png" alt=""><br>并列显示两个图<br><figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span> subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;</span> plot(t,y1)</span><br><span class="line"><span class="meta">&gt;&gt;</span> subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;</span> plot(t,y2)</span><br><span class="line"><span class="meta">&gt;&gt;</span> axis([<span class="number">0</span>.<span class="number">5</span> <span class="number">1</span> -<span class="number">1</span> <span class="number">1</span>])</span><br></pre></td></tr></table></figure></p>
<p><img src="3.png" alt=""><br>绘制矩阵<br><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">&gt;&gt; A = magic(<span class="number">5</span>)</span><br><span class="line">A =</span><br><span class="line"></span><br><span class="line">   <span class="number">17</span>   <span class="number">24</span>    <span class="number">1</span>    <span class="number">8</span>   <span class="number">15</span></span><br><span class="line">   <span class="number">23</span>    <span class="number">5</span>    <span class="number">7</span>   <span class="number">14</span>   <span class="number">16</span></span><br><span class="line">    <span class="number">4</span>    <span class="number">6</span>   <span class="number">13</span>   <span class="number">20</span>   <span class="number">22</span></span><br><span class="line">   <span class="number">10</span>   <span class="number">12</span>   <span class="number">19</span>   <span class="number">21</span>    <span class="number">3</span></span><br><span class="line">   <span class="number">11</span>   <span class="number">18</span>   <span class="number">25</span>    <span class="number">2</span>    <span class="number">9</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; imagesc(A)</span><br></pre></td></tr></table></figure></p>
<p><img src="4.png" alt=""><br><figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span> imagesc(A),colorbar,colormap gray;</span><br></pre></td></tr></table></figure></p>
<p><img src="5.png" alt=""></p>
<h1 id="控制语句"><a href="#控制语句" class="headerlink" title="控制语句"></a>控制语句</h1><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">%for循环语句</span></span><br><span class="line">&gt;&gt; v = <span class="built_in">zeros</span>(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span>   <span class="number">0</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>: <span class="number">10</span></span><br><span class="line">v(<span class="built_in">i</span>) = <span class="number">2</span>^<span class="built_in">i</span>;</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line">&gt;&gt; v</span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">      <span class="number">2</span>      <span class="number">4</span>      <span class="number">8</span>     <span class="number">16</span>     <span class="number">32</span>     <span class="number">64</span>    <span class="number">128</span>    <span class="number">256</span>    <span class="number">512</span>   <span class="number">1024</span></span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">%while语句</span></span><br><span class="line">&gt;&gt; <span class="keyword">while</span> <span class="built_in">i</span> &lt; <span class="number">5</span>,</span><br><span class="line">v(<span class="built_in">i</span>) = <span class="number">100</span>;</span><br><span class="line"><span class="built_in">i</span> = <span class="built_in">i</span>+<span class="number">1</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">&gt;&gt; v</span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">      <span class="number">2</span>      <span class="number">4</span>      <span class="number">8</span>     <span class="number">16</span>     <span class="number">32</span>     <span class="number">64</span>    <span class="number">128</span>    <span class="number">256</span>    <span class="number">512</span>   <span class="number">1024</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%if break语句</span></span><br><span class="line">&gt;&gt; <span class="built_in">i</span> = <span class="number">1</span>;</span><br><span class="line">&gt;&gt; <span class="keyword">while</span> <span class="built_in">true</span>;</span><br><span class="line">v(<span class="built_in">i</span>) = <span class="number">999</span>;</span><br><span class="line"><span class="built_in">i</span> = <span class="built_in">i</span>+<span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">i</span>==<span class="number">6</span>,</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="built_in">i</span> =  <span class="number">2</span></span><br><span class="line"><span class="built_in">i</span> =  <span class="number">3</span></span><br><span class="line"><span class="built_in">i</span> =  <span class="number">4</span></span><br><span class="line"><span class="built_in">i</span> =  <span class="number">5</span></span><br><span class="line"><span class="built_in">i</span> =  <span class="number">6</span></span><br><span class="line">&gt;&gt; v</span><br><span class="line">v =</span><br><span class="line"></span><br><span class="line">    <span class="number">999</span>    <span class="number">999</span>    <span class="number">999</span>    <span class="number">999</span>    <span class="number">999</span>     <span class="number">64</span>    <span class="number">128</span>    <span class="number">256</span>    <span class="number">512</span>   <span class="number">1024</span></span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="keyword">if</span> v(<span class="number">1</span>) == <span class="number">1</span>,</span><br><span class="line"><span class="built_in">disp</span>(<span class="string">'The value is one!'</span>)</span><br><span class="line"><span class="keyword">elseif</span> v(<span class="number">1</span>) == <span class="number">2</span>,</span><br><span class="line"><span class="built_in">disp</span>(<span class="string">'The value is two!'</span>)</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">disp</span>(<span class="string">'The value is not one or two!'</span>)</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">The value is not one or two!</span><br><span class="line">&gt;&gt;</span><br></pre></td></tr></table></figure>
<h1 id="定义函数"><a href="#定义函数" class="headerlink" title="定义函数"></a>定义函数</h1><p>将函数定义写在文件中，并把文件名命名为‘函数名.m’，将文件放在当前路径下，或者用 addpath 将文件目录加入当前会话</p>
<blockquote>
<p>本章学习结束</p>
</blockquote>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>octave</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习（十二）</title>
    <url>/2018/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<p>十七、大规模机器学习(Large Scale Machine Learning)</p>
<h3 id="17-1-大型数据集的学习"><a href="#17-1-大型数据集的学习" class="headerlink" title="17.1 大型数据集的学习"></a>17.1 大型数据集的学习</h3><p>参考视频: 17 - 1 - Learning With Large Datasets (6 min).mkv</p>
<p>如果我们有一个低方差的模型，增加数据集的规模可以帮助你获得更好的结果。我们应该怎样应对一个有100万条记录的训练集？</p>
<p>以线性回归模型为例，每一次梯度下降迭代，我们都需要计算训练集的误差的平方和，如果我们的学习算法需要有20次迭代，这便已经是非常大的计算代价。</p>
<p>首先应该做的事是去检查一个这么大规模的训练集是否真的必要，也许我们只用1000个训练集也能获得较好的效果，我们可以绘制学习曲线来帮助判断。</p>
<p><img src="bdf069136b4b661dd14158496d1d1419.png" alt=""></p>
<h3 id="17-2-随机梯度下降法"><a href="#17-2-随机梯度下降法" class="headerlink" title="17.2 随机梯度下降法"></a>17.2 随机梯度下降法</h3><p>参考视频: 17 - 2 - Stochastic Gradient Descent (13 min).mkv</p>
<p>如果我们一定需要一个大规模的训练集，我们可以尝试使用随机梯度下降法来代替批量梯度下降法。</p>
<p>在随机梯度下降法中，我们定义代价函数为一个单一训练实例的代价：<br>

​                                                           $$cost\left(  \theta, \left( {x}^{(i)} , {y}^{(i)} \right)  \right) = \frac{1}{2}\left( {h}_{\theta}\left({x}^{(i)}\right)-{y}^{{(i)}} \right)^{2}$$

**随机**梯度下降算法为：首先对训练集随机“洗牌”，然后：
 Repeat (usually anywhere between1-10){

  **for** $i = 1:m${

 ​       $\theta:={\theta}_{j}-\alpha\left( {h}_{\theta}\left({x}^{(i)}\right)-{y}^{(i)} \right){{x}_{j}}^{(i)}$      

​        (**for** $j=0:n$)

 ​    }
 }

随机梯度下降算法在每一次计算之后便更新参数 ${{\theta }}$ ，而不需要首先将所有的训练集求和，在梯度下降算法还没有完成一次迭代时，随机梯度下降算法便已经走出了很远。但是这样的算法存在的问题是，不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊。
<br><img src="9710a69ba509a9dcbca351fccc6e7aae.jpg" alt=""></p>
<h3 id="17-3-小批量梯度下降"><a href="#17-3-小批量梯度下降" class="headerlink" title="17.3 小批量梯度下降"></a>17.3 小批量梯度下降</h3><p>参考视频: 17 - 3 - Mini-Batch Gradient Descent (6 min).mkv<br>
小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算常数$b$次训练实例，便更新一次参数  ${{\theta }}$ 。
 **Repeat** {

 **for** $i = 1:m${

 ​       $\theta:={\theta}_{j}-\alpha\frac{1}{b}\sum_\limits{k=i}^{i+b-1}\left( {h}_{\theta}\left({x}^{(k)}\right)-{y}^{(k)} \right){{x}_{j}}^{(k)}$      

​       (**for** $j=0:n$)

​      $ i +=10 $   

 ​     }
 }
<br>通常我们会令 $b$ 在 2-100 之间。这样做的好处在于，我们可以用向量化的方式来循环 $b$个训练实例，如果我们用的线性代数函数库比较好，能够支持平行处理，那么算法的总体表现将不受影响（与随机梯度下降相同）。</p>
<h3 id="17-4-随机梯度下降收敛"><a href="#17-4-随机梯度下降收敛" class="headerlink" title="17.4 随机梯度下降收敛"></a>17.4 随机梯度下降收敛</h3><p>参考视频: 17 - 4 - Stochastic Gradient Descent Convergence (12 min). mkv</p>
<p>现在我们介绍随机梯度下降算法的调试，以及学习率 $α$ 的选取。</p>
<p>在批量梯度下降中，我们可以令代价函数$J$为迭代次数的函数，绘制图表，根据图表来判断梯度下降是否收敛。但是，在大规模的训练集的情况下，这是不现实的，因为计算代价太大了。<br>
在随机梯度下降中，我们在每一次更新 ${{\theta }}$ 之前都计算一次代价，然后每$x$次迭代后，求出这$x$次对训练实例计算代价的平均值，然后绘制这些平均值与$x$次迭代的次数之间的函数图表。
<br><img src="76fb1df50bdf951f4b880fa66489e367.png" alt=""></p>
<p>当我们绘制这样的图表时，可能会得到一个颠簸不平但是不会明显减少的函数图像（如上面左下图中蓝线所示）。我们可以增加$α$来使得函数更加平缓，也许便能看出下降的趋势了（如上面左下图中红线所示）；或者可能函数图表仍然是颠簸不平且不下降的（如洋红色线所示），那么我们的模型本身可能存在一些错误。</p>
<p>如果我们得到的曲线如上面右下方所示，不断地上升，那么我们可能会需要选择一个较小的学习率$α$。</p>
<p>我们也可以令学习率随着迭代次数的增加而减小，例如令：</p>
<p>​                            $$\alpha = \frac{const1}{iterationNumber + const2}$$</p>
<p>随着我们不断地靠近全局最小值，通过减小学习率，我们迫使算法收敛而非在最小值附近徘徊。<br>但是通常我们不需要这样做便能有非常好的效果了，对$α$进行调整所耗费的计算通常不值得</p>
<p><img src="f703f371dbb80d22fd5e4aec48aa9fd4.jpg" alt=""></p>
<p>总结下，这段视频中，我们介绍了一种方法，近似地监测出随机梯度下降算法在最优化代价函数中的表现，这种方法不需要定时地扫描整个训练集，来算出整个样本集的代价函数，而是只需要每次对最后1000个，或者多少个样本，求一下平均值。应用这种方法，你既可以保证随机梯度下降法正在正常运转和收敛，也可以用它来调整学习速率$α$的大小。</p>
<h3 id="17-5-在线学习"><a href="#17-5-在线学习" class="headerlink" title="17.5 在线学习"></a>17.5 在线学习</h3><p>参考视频: 17 - 5 - Online Learning (13 min).mkv</p>
<p>在这个视频中，讨论一种新的大规模的机器学习机制，叫做在线学习机制。在线学习机制让我们可以模型化问题。</p>
<p>今天，许多大型网站或者许多大型网络公司，使用不同版本的在线学习机制算法，从大批的涌入又离开网站的用户身上进行学习。特别要提及的是，如果你有一个由连续的用户流引发的连续的数据流，进入你的网站，你能做的是使用一个在线学习机制，从数据流中学习用户的偏好，然后使用这些信息来优化一些关于网站的决策。</p>
<p>假定你有一个提供运输服务的公司，用户们来向你询问把包裹从<strong>A</strong>地运到<strong>B</strong>地的服务，同时假定你有一个网站，让用户们可多次登陆，然后他们告诉你，他们想从哪里寄出包裹，以及包裹要寄到哪里去，也就是出发地与目的地，然后你的网站开出运输包裹的的服务价格。比如，我会收取\$50来运输你的包裹，我会收取\$20之类的，然后根据你开给用户的这个价格，用户有时会接受这个运输服务，那么这就是个正样本，有时他们会走掉，然后他们拒绝购买你的运输服务，所以，让我们假定我们想要一个学习算法来帮助我们，优化我们想给用户开出的价格。</p>
<p>一个算法来从中学习的时候来模型化问题在线学习算法指的是对数据流而非离线的静态数据集的学习。许多在线网站都有持续不断的用户流，对于每一个用户，网站希望能在不将数据存储到数据库中便顺利地进行算法学习。</p>
<p>假使我们正在经营一家物流公司，每当一个用户询问从地点A至地点B的快递费用时，我们给用户一个报价，该用户可能选择接受（$y=1$）或不接受（$y=0$）。</p>
<p>现在，我们希望构建一个模型，来预测用户接受报价使用我们的物流服务的可能性。因此报价<br>是我们的一个特征，其他特征为距离，起始地点，目标地点以及特定的用户数据。模型的输出是:$p(y=1)$。</p>

在线学习的算法与随机梯度下降算法有些类似，我们对单一的实例进行学习，而非对一个提前定义的训练集进行循环。
 Repeat forever (as long as the website is running) {
  Get $\left(x,y\right)$ corresponding to the current user 
​        $\theta:={\theta}_{j}-\alpha\left( {h}_{\theta}\left({x}\right)-{y} \right){{x}_{j}}$
​       (**for** $j=0:n$) 
    }


<p>一旦对一个数据的学习完成了，我们便可以丢弃该数据，不需要再存储它了。这种方式的好处在于，我们的算法可以很好的适应用户的倾向性，算法可以针对用户的当前行为不断地更新模型以适应该用户。</p>
<p>每次交互事件并不只产生一个数据集，例如，我们一次给用户提供3个物流选项，用户选择2项，我们实际上可以获得3个新的训练实例，因而我们的算法可以一次从3个实例中学习并更新模型。</p>
<p>这些问题中的任何一个都可以被归类到标准的，拥有一个固定的样本集的机器学习问题中。或许，你可以运行一个你自己的网站，尝试运行几天，然后保存一个数据集，一个固定的数据集，然后对其运行一个学习算法。但是这些是实际的问题，在这些问题里，你会看到大公司会获取如此多的数据，真的没有必要来保存一个固定的数据集，取而代之的是你可以使用一个在线学习算法来连续的学习，从这些用户不断产生的数据中来学习。这就是在线学习机制，然后就像我们所看到的，我们所使用的这个算法与随机梯度下降算法非常类似，唯一的区别的是，我们不会使用一个固定的数据集，我们会做的是获取一个用户样本，从那个样本中学习，然后丢弃那个样本并继续下去，而且如果你对某一种应用有一个连续的数据流，这样的算法可能会非常值得考虑。当然，在线学习的一个优点就是，如果你有一个变化的用户群，又或者你在尝试预测的事情，在缓慢变化，就像你的用户的品味在缓慢变化，这个在线学习算法，可以慢慢地调试你所学习到的假设，将其调节更新到最新的用户行为。</p>
<h3 id="17-6-映射化简和数据并行"><a href="#17-6-映射化简和数据并行" class="headerlink" title="17.6 映射化简和数据并行"></a>17.6 映射化简和数据并行</h3><p>参考视频: 17 - 6 - Map Reduce and Data Parallelism (14 min).mkv</p>
<p>映射化简和数据并行对于大规模机器学习问题而言是非常重要的概念。之前提到，如果我们用批量梯度下降算法来求解大规模数据集的最优解，我们需要对整个训练集进行循环，计算偏导数和代价，再求和，计算代价非常大。如果我们能够将我们的数据集分配给不多台计算机，让每一台计算机处理数据集的一个子集，然后我们将计所的结果汇总在求和。这样的方法叫做映射简化。</p>
<p>具体而言，如果任何学习算法能够表达为，对训练集的函数的求和，那么便能将这个任务分配给多台计算机（或者同一台计算机的不同<strong>CPU</strong> 核心），以达到加速处理的目的。</p>
<p>例如，我们有400个训练实例，我们可以将批量梯度下降的求和任务分配给4台计算机进行处理：</p>
<p><img src="919eabe903ef585ec7d08f2895551a1f.jpg" alt="">           </p>
<p>很多高级的线性代数函数库已经能够利用多核<strong>CPU</strong>的多个核心来并行地处理矩阵运算，这也是算法的向量化实现如此重要的缘故（比调用循环快）。</p>
<h2 id="十八、应用实例：图片文字识别-Application-Example-Photo-OCR"><a href="#十八、应用实例：图片文字识别-Application-Example-Photo-OCR" class="headerlink" title="十八、应用实例：图片文字识别(Application Example: Photo OCR)"></a>十八、应用实例：图片文字识别(Application Example: Photo OCR)</h2><h3 id="18-1-问题描述和流程图"><a href="#18-1-问题描述和流程图" class="headerlink" title="18.1 问题描述和流程图"></a>18.1 问题描述和流程图</h3><p>参考视频: 18 - 1 - Problem Description and Pipeline (7 min).mkv</p>
<p>图像文字识别应用所作的事是，从一张给定的图片中识别文字。这比从一份扫描文档中识别文字要复杂的多。</p>
<p><img src="095e4712376c26ff7ffa260125760140.jpg" alt=""></p>
<p>为了完成这样的工作，需要采取如下步骤：</p>
<ol>
<li><p>文字侦测（<strong>Text detection</strong>）——将图片上的文字与其他环境对象分离开来</p>
</li>
<li><p>字符切分（<strong>Character segmentation</strong>）——将文字分割成一个个单一的字符</p>
</li>
<li><p>字符分类（<strong>Character classification</strong>）——确定每一个字符是什么<br>可以用任务流程图来表达这个问题，每一项任务可以由一个单独的小队来负责解决：</p>
</li>
</ol>
<p><img src="610fffb413d8d577882d6345c166a9fb.png" alt=""></p>
<h3 id="18-2-滑动窗口"><a href="#18-2-滑动窗口" class="headerlink" title="18.2 滑动窗口"></a>18.2 滑动窗口</h3><p>参考视频: 18 - 2 - Sliding Windows (15 min).mkv</p>
<p>滑动窗口是一项用来从图像中抽取对象的技术。假使我们需要在一张图片中识别行人，首先要做的是用许多固定尺寸的图片来训练一个能够准确识别行人的模型。然后我们用之前训练识别行人的模型时所采用的图片尺寸在我们要进行行人识别的图片上进行剪裁，然后将剪裁得到的切片交给模型，让模型判断是否为行人，然后在图片上滑动剪裁区域重新进行剪裁，将新剪裁的切片也交给模型进行判断，如此循环直至将图片全部检测完。</p>
<p>一旦完成后，我们按比例放大剪裁的区域，再以新的尺寸对图片进行剪裁，将新剪裁的切片按比例缩小至模型所采纳的尺寸，交给模型进行判断，如此循环。</p>
<p><img src="1e00d03719e20eeaf1f414f99d7f4109.jpg" alt=""></p>
<p>滑动窗口技术也被用于文字识别，首先训练模型能够区分字符与非字符，然后，运用滑动窗口技术识别字符，一旦完成了字符的识别，我们将识别得出的区域进行一些扩展，然后将重叠的区域进行合并。接着我们以宽高比作为过滤条件，过滤掉高度比宽度更大的区域（认为单词的长度通常比高度要大）。下图中绿色的区域是经过这些步骤后被认为是文字的区域，而红色的区域是被忽略的。</p>
<p><img src="bc48a4b0c7257591643eb50f2bf46db6.jpg" alt=""></p>
<p>以上便是文字侦测阶段。<br>下一步是训练一个模型来完成将文字分割成一个个字符的任务，需要的训练集由单个字符的图片和两个相连字符之间的图片来训练模型。</p>
<p><img src="0a930f2083bbeb85837f018b74fd0a02.jpg" alt=""></p>
<p><img src="0bde4f379c8a46c2074336ecce1a955f.jpg" alt=""></p>
<p>模型训练完后，我们仍然是使用滑动窗口技术来进行字符识别。</p>
<p>以上便是字符切分阶段。<br>最后一个阶段是字符分类阶段，利用神经网络、支持向量机或者逻辑回归算法训练一个分类器即可。</p>
<h3 id="18-3-获取大量数据和人工数据"><a href="#18-3-获取大量数据和人工数据" class="headerlink" title="18.3 获取大量数据和人工数据"></a>18.3 获取大量数据和人工数据</h3><p>参考视频: 18 - 3 - Getting Lots of Data and Artificial Data (16 min).mkv</p>
<p>如果我们的模型是低方差的，那么获得更多的数据用于训练模型，是能够有更好的效果的。问题在于，我们怎样获得数据，数据不总是可以直接获得的，我们有可能需要人工地创造一些数据。</p>
<p>以我们的文字识别应用为例，我们可以字体网站下载各种字体，然后利用这些不同的字体配上各种不同的随机背景图片创造出一些用于训练的实例，这让我们能够获得一个无限大的训练集。这是从零开始创造实例。</p>
<p>另一种方法是，利用已有的数据，然后对其进行修改，例如将已有的字符图片进行一些扭曲、旋转、模糊处理。只要我们认为实际数据有可能和经过这样处理后的数据类似，我们便可以用这样的方法来创造大量的数据。</p>
<p>有关获得更多数据的几种方法：</p>
<ol>
<li><p>人工数据合成</p>
</li>
<li><p>手动收集、标记数据</p>
</li>
<li><p>众包</p>
</li>
</ol>
<h3 id="18-4-上限分析：哪部分管道的接下去做"><a href="#18-4-上限分析：哪部分管道的接下去做" class="headerlink" title="18.4 上限分析：哪部分管道的接下去做"></a>18.4 上限分析：哪部分管道的接下去做</h3><p>参考视频: 18 - 4 - Ceiling Analysis_ What Part of the Pipeline to Work on Next<br>(14 min).mkv</p>
<p>在机器学习的应用中，我们通常需要通过几个步骤才能进行最终的预测，我们如何能够知道哪一部分最值得我们花时间和精力去改善呢？这个问题可以通过上限分析来回答。</p>
<p>回到我们的文字识别应用中，我们的流程图如下：</p>
<p><img src="55d41ee748680a62e755d6aa5b95b53c.png" alt=""></p>
<p>流程图中每一部分的输出都是下一部分的输入，上限分析中，我们选取一部分，手工提供100%正确的输出结果，然后看应用的整体效果提升了多少。假使我们的例子中总体效果为72%的正确率。</p>
<p>如果我们令文字侦测部分输出的结果100%正确，发现系统的总体效果从72%提高到了89%。这意味着我们很可能会希望投入时间精力来提高我们的文字侦测部分。</p>
<p>接着我们手动选择数据，让字符切分输出的结果100%正确，发现系统的总体效果只提升了1%，这意味着，我们的字符切分部分可能已经足够好了。</p>
<p>最后我们手工选择数据，让字符分类输出的结果100%正确，系统的总体效果又提升了10%，这意味着我们可能也会应该投入更多的时间和精力来提高应用的总体表现。</p>
<p><img src="f1ecee10884098f98032648da08f8937.jpg" alt=""></p>
<h2 id="十九、总结-Conclusion"><a href="#十九、总结-Conclusion" class="headerlink" title="十九、总结(Conclusion)"></a>十九、总结(Conclusion)</h2><h3 id="19-1-总结和致谢"><a href="#19-1-总结和致谢" class="headerlink" title="19.1 总结和致谢"></a>19.1 总结和致谢</h3><p>参考视频: 19 - 1 - Summary and Thank You (5 min).mkv</p>
<p>欢迎来到《机器学习》课的最后一段视频。我们已经一起学习很长一段时间了。在最后这段视频中，我想快速地回顾一下这门课的主要内容，然后简单说几句想说的话。<br>
作为这门课的结束时间，那么我们学到了些什么呢？在这门课中，我们花了大量的时间介绍了诸如线性回归、逻辑回归、神经网络、支持向量机等等一些监督学习算法，这类算法具有带标签的数据和样本，比如${{x}^{\left( i \right)}}$、${{y}^{\left( i \right)}}$。

然后我们也花了很多时间介绍无监督学习。例如 **K-均值**聚类、用于降维的主成分分析，以及当你只有一系列无标签数据 ${{x}^{\left( i \right)}}$ 时的异常检测算法。
</p>
<p>当然，有时带标签的数据，也可以用于异常检测算法的评估。此外，我们也花时间讨论了一些特别的应用或者特别的话题，比如说推荐系统。以及大规模机器学习系统，包括并行系统和映射化简方法，还有其他一些特别的应用。比如，用于计算机视觉技术的滑动窗口分类算法。</p>
<p>最后，我们还提到了很多关于构建机器学习系统的实用建议。这包括了怎样理解某个机器学习算法是否正常工作的原因，所以我们谈到了偏差和方差的问题，也谈到了解决方差问题的正则化，同时我们也讨论了怎样决定接下来怎么做的问题，也就是说当你在开发一个机器学习系统时，什么工作才是接下来应该优先考虑的问题。因此我们讨论了学习算法的评价法。介绍了评价矩阵，比如：查准率、召回率以及F1分数，还有评价学习算法比较实用的训练集、交叉验证集和测试集。我们也介绍了学习算法的调试，以及如何确保学习算法的正常运行，于是我们介绍了一些诊断法，比如学习曲线，同时也讨论了误差分析、上限分析等等内容。</p>
<p>所有这些工具都能有效地指引你决定接下来应该怎样做，让你把宝贵的时间用在刀刃上。现在你已经掌握了很多机器学习的工具，包括监督学习算法和无监督学习算法等等。</p>
<p>但除了这些以外，我更希望你现在不仅仅只是认识这些工具，更重要的是掌握怎样有效地利用这些工具来建立强大的机器学习系统。所以，以上就是这门课的全部内容。如果你跟着我们的课程一路走来，到现在，你应该已经感觉到自己已经成为机器学习方面的专家了吧？</p>
<p>我们都知道，机器学习是一门对科技、工业产生深远影响的重要学科，而现在，你已经完全具备了应用这些机器学习工具来创造伟大成就的能力。我希望你们中的很多人都能在相应的领域，应用所学的机器学习工具，构建出完美的机器学习系统，开发出无与伦比的产品和应用。并且我也希望你们通过应用机器学习，不仅仅改变自己的生活，有朝一日，还要让更多的人生活得更加美好！</p>
<p>我也想告诉大家，教这门课对我来讲是一种享受。所以，谢谢大家！</p>
<p>最后，在结束之前，我还想再多说一点：那就是，也许不久以前我也是一个学生，即使是现在，我也尽可能挤出时间听一些课，学一些新的东西。所以，我深知要坚持学完这门课是很需要花一些时间的，我知道，也许你是一个很忙的人，生活中有很多很多事情要处理。正因如此，你依然挤出时间来观看这些课程视频。我知道，很多视频的时间都长达数小时，你依然花了好多时间来做这些复习题。你们中好多人，还愿意花时间来研究那些编程练习，那些又长又复杂的编程练习。我对你们表示衷心的感谢！我知道你们很多人在这门课中都非常努力，很多人都在这门课上花了很多时间，很多人都为这门课贡献了自己的很多精力。所以，我衷心地希望你们能从这门课中有所收获！</p>
<p>最后我想说！再次感谢你们选修这门课程！</p>
<p><strong>Andew Ng</strong></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>正则化</title>
    <url>/2018/05/09/%E6%AD%A3%E5%88%99%E5%8C%96/</url>
    <content><![CDATA[<p>在机器学习中遇到了难题，就是对正则化的理解，通过查阅资料，记录下什么是正则化。</p>
<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>模型选择的典型方法是正则化（regularization）。正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项（regularizer）或罚项（penalty term）。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大。比如，正则化项可以是模型参数向量的==范数==。</p>
<p>在这里我又遇到了一个问题，什么是范数，哎，高数没学好，啥也不知道了。</p>
<blockquote>
<p>范数(norm)是数学中的一种基本概念。在泛函分析中，它定义在赋范线性空间中，并满足一定的条件，即①非负性；②齐次性；③三角不等式。它常常被用来度量某个向量空间（或矩阵）中的每个向量的长度或大小。</p>
</blockquote>
<p>看完定义又是头大，完全不理解啊！<br>定义和性质什么的都不重要了，这里我只需要知道范数所代表的函数意义：<br><figure class="highlight llvm"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span>-范数：║<span class="keyword">x</span>║<span class="number">1</span>=│<span class="keyword">x</span><span class="number">1</span>│+│<span class="keyword">x</span><span class="number">2</span>│+…+│xn│</span><br><span class="line"><span class="number">2</span>-范数：║<span class="keyword">x</span>║<span class="number">2</span>=（│<span class="keyword">x</span><span class="number">1</span>│<span class="number">2</span>+│<span class="keyword">x</span><span class="number">2</span>│<span class="number">2</span>+…+│xn│<span class="number">2</span>）<span class="number">1</span>/<span class="number">2</span></span><br><span class="line">∞-范数：║<span class="keyword">x</span>║∞=<span class="keyword">max</span>（│<span class="keyword">x</span><span class="number">1</span>│，│<span class="keyword">x</span><span class="number">2</span>│，…，│xn│）</span><br><span class="line">其中<span class="number">2</span>-范数就是通常意义下的距离。</span><br></pre></td></tr></table></figure></p>
<p>矩阵范数：<br><figure class="highlight gherkin"><table><tr><td class="code"><pre><span class="line">1-范数：</span><br><span class="line">║A║1 = max&#123; ∑|<span class="string">ai1</span>|<span class="string">，∑</span>|<span class="string">ai2</span>|<span class="string">，……，∑</span>|<span class="string">ain</span>|<span class="string"> &#125; （列和范数，A每一列元素绝对值之和的最大值）（其中∑</span>|<span class="string">ai1</span>|<span class="string">第一列元素绝对值的和∑</span>|<span class="string">ai1</span>|<span class="string">=</span>|<span class="string">a11</span>|<span class="string">+</span>|<span class="string">a21</span>|<span class="string">+...+</span>|<span class="string">an1</span>|<span class="string">，其余类似）；</span></span><br><span class="line"><span class="string">2-范数：</span></span><br><span class="line"><span class="string">║A║2 = A的最大奇异值 = (max&#123; λi(AH*A) &#125;) 1/2 （谱范数，即A^H*A特征值λi中最大者λ1的平方根，其中AH为A的转置共轭矩阵）；</span></span><br><span class="line"><span class="string">∞-范数：</span></span><br><span class="line"><span class="string">║A║∞ = max&#123; ∑</span>|<span class="string">a1j</span>|<span class="string">，∑</span>|<span class="string">a2j</span>|<span class="string">,...，∑</span>|<span class="string">amj</span>|<span class="string"> &#125; （行和范数，A每一行元素绝对值之和的最大值）（其中∑</span>|<span class="string">a1j</span>|<span class="string"> 为第一行元素绝对值的和，其余类似）；</span></span><br></pre></td></tr></table></figure></p>
<p>看了这几个例子大概理解了，若<br><img src="1.png" alt="">，那么<br><img src="2.png" alt=""><br>继续正则化的话题，正则化主要解决的问题：</p>
<p>1.正则化就是对最小化经验误差函数上加约束，这样的约束可以解释为先验知识(正则化参数等价于对参数引入先验分布)。约束有引导作用，在优化误差函数的时候倾向于选择满足约束的梯度减少的方向，使最终的解倾向于符合先验知识(如一般的l-norm先验，表示原问题更可能是比较简单的，这样的优化倾向于产生参数值量级小的解，一般对应于稀疏参数的平滑解)。</p>
<p>2.同时，正则化解决了逆问题的不适定性，产生的解是存在，唯一同时也依赖于数据的，噪声对不适定的影响就弱，解就不会过拟合，而且如果先验(正则化)合适，则解就倾向于是符合真解(更不会过拟合了)，即使训练集中彼此间不相关的样本数很少。</p>
<p>正则化一般具有如下形式：<br><img src="3.png" alt=""><br>其中第一项是经验风险，第二项是正则化项，λ&gt;=0为调整两者关系的系数。正则化项可以取不同的形式，例如，回归问题，损失函数是平方损失，正则化项可以使参数向量的2范类。<br>范类的记录大概就是这么多了。</p>
<h1 id="泰勒公式"><a href="#泰勒公式" class="headerlink" title="泰勒公式"></a>泰勒公式</h1><p>顺便说下泰勒公式，是在学习吴恩达的机器学习正则化时才想到的，我们的预测模型是一个多项式的和，当多项式过少会欠拟合，过多会过拟合，当多项式足够多的时候就会区分出所有的种类，这不就是泰勒公式展开式吗？</p>
<blockquote>
<p>数学中，泰勒公式是一个用函数在某点的信息描述其附近取值的公式。如果函数足够平滑的话，在已知函数在某一点的各阶导数值的情况之下，泰勒公式可以用这些导数值做系数构建一个多项式来近似函数在这一点的邻域中的值。</p>
</blockquote>
<p>泰勒公式是将一个在x=x0处具有n阶导数的函数f(x)利用关于(x-x0)的n次多项式来逼近函数的方法。<br>若函数f(x)在包含x0的某个闭区间[a,b]上具有n阶导数，且在开区间(a,b)上具有(n+1)阶导数，则对闭区间[a,b]上任意一点x，成立下式：<br><img src="4.png" alt="">其中,fn^(x)表示fn(x)的n阶导数，等号后的多项式称为函数f(x)在x0处的泰勒展开式，剩余的Rn(x)是泰勒公式的余项，是(x-x0)n的高阶无穷小。</p>
<p>参考：<br>李航 统计学习方法</p>
]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title>激活函数（Activation functions）</title>
    <url>/2019/03/27/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88Activation%20functions%EF%BC%89/</url>
    <content><![CDATA[<p>在使用神经网络时，需要决定使用哪种激活函数作用在隐藏层上，哪种用在输出节点上。普通的激活函数有sigmoid, tanh, Relu和Relu的优化版Leaky Relu。</p>
<h1 id="为什么需要非线性激活函数"><a href="#为什么需要非线性激活函数" class="headerlink" title="为什么需要非线性激活函数"></a>为什么需要非线性激活函数</h1><p>为什么神经网络需要非线性激活函数？事实证明：要让你的神经网络能够计算出有趣的函数，你必须使用非线性激活函数，如果是用线性激活函数或者叫恒等激励函数，那么神经网络只是把输入线性组合再输出。</p>
<p>事实证明，如果你使用线性激活函数或者没有使用一个激活函数，那么无论你的神经网络有多少层一直在做的只是计算线性函数，所以不如直接去掉全部隐藏层。事实证明如果你在隐藏层用线性激活函数，在输出层用sigmoid函数，那么这个模型的复杂度和没有任何隐藏层的标准Logistic回归是一样的。</p>
<p>只有一个地方可以使用线性激活函数，就是你在做机器学习中的回归问题。举个例子，比如你想预测房地产价格，目标结果就不是二分类任务0或1，而是一个实数，从0到正无穷。如果目标是个实数，那么在输出层用线性激活函数也许可行，你的输出也是一个实数，从负无穷到正无穷。</p>
<p>总而言之，不能在隐藏层用线性激活函数，可以用ReLU或者tanh或者leaky ReLU或者其他的非线性激活函数，唯一可以用线性激活函数的通常就是输出层；除了这种情况，会在隐层用线性函数的，除了一些特殊情况，比如与压缩有关的，那方面在这里将不深入讨论。在这之外，在隐层使用线性激活函数非常少见。因为房价都是非负数，所以我们也可以在输出层使用ReLU函数这样你的输出结果都大于等于0。</p>
<h1 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h1><p><img src="sigmoid.png" alt="sigmoid"></p>

$a=\frac{1}{1+e^x}$

<p>sigmoid激活函数是之前在学习逻辑回归的时候使用的激活函数，但它是一个基本不使用的激活函数，tanh函数（者双曲正切函数）是总体上都优于sigmoid函数的激活函数。我们只有在二分类任务中会使用，因为在二分类的问题中，对于输出层，目标的值是0或1，所以想让预测的数值介于0和1之间，而不是在-1和+1之间。所以需要使用sigmoid激活函数。</p>
<h2 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h2><p>其具体的求导如下：</p>

$\frac{d}{dz}g(z) = {\frac{1}{1 + e^{-z}} (1-\frac{1}{1 + e^{-z}})}=g(z)(1-g(z))$

<p>注：<br>
当$z=10$或$z=-10$ $\frac{d}{dz}g(z)\approx0$
</p>

当$z=0$,$\frac{d}{dz}g(z)\text{=g(z)(1-g(z))=}{1}/{4}$


在神经网络中$a= g(z)$;$g{{(z)}^{'}}=\frac{d}{dz}g(z)=a(1-a)$

<h1 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h1><p><img src="tanh.png" alt="tanh"></p>

$a=\frac{e^x-e^{-x}}{e^x+e^{-x}}$

<p>事实上，tanh函数是sigmoid的向下平移和伸缩后的结果。对它进行了变形后，穿过了点(0, 0)，并且值域介于+1和-1之间。</p>
<p>结果表明，如果在隐藏层上使用tanh函数效果总是优于sigmoid函数。因为函数值域在-1和+1的激活函数，其均值是更接近零均值的。在训练一个算法模型时，如果使用tanh函数代替sigmoid函数中心化数据，使得数据的平均值更接近0而不是0.5.</p>
<p>sigmoid函数和tanh函数两者共同的缺点是，在z特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度。</p>
<h2 id="导数-1"><a href="#导数-1" class="headerlink" title="导数"></a>导数</h2><p>其具体的求导如下：</p>

$g(z)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$


$\frac{d}{{d}z}g(z) = 1 - (tanh(z))^{2}$

<p>注：<br>
当$z=10$或$z=-10$ $\frac{d}{dz}g(z)\approx0$
</p>

当$z=0$,$\frac{d}{dz}g(z)\text{=1-(0)=}1$

<h1 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h1><p><img src="relu.png" alt="Relu"></p>

$a=\max\left(0,\ x\right)$

<p>在机器学习另一个很流行的函数是：修正线性单元的函数（ReLu）。只要是正值的情况下，导数恒等于1，当是负值的时候，导数恒等于0。从实际上来说，当使用的导数时，z=0的导数是没有定义的。但是当编程实现的时候，z的取值刚好等于0.00000001，这个值相当小，所以，在实践中，不需要担心这个值，是等于0的时候，假设一个导数是1或者0效果都可以。</p>
<p>这有一些选择激活函数的经验法则：</p>
<p>如果输出是0、1值（二分类问题），则输出层选择sigmoid函数，然后其它的所有单元都选择Relu函数。</p>
<p>这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会使用Relu激活函数。有时，也会使用tanh激活函数，但Relu的一个优点是：当z是负值的时候，导数等于0。</p>
<h2 id="导数-2"><a href="#导数-2" class="headerlink" title="导数"></a>导数</h2>
$g(z)^{'}=
  \begin{cases}
  0&	\text{if z < 0}\\
  1&	\text{if z > 0}\\
undefined&	\text{if z = 0}
\end{cases}$

<p>注：通常在z=0的时候给定其导数1,0；当然z=0的情况很少</p>
<h1 id="Leaky-Relu"><a href="#Leaky-Relu" class="headerlink" title="Leaky Relu"></a>Leaky Relu</h1><p><img src="leaky.png" alt="leaky"></p>

$a=\max\left(0.01x,\ x\right)$

<p>这里也有另一个版本的Relu被称为Leaky Relu。当是负值时，这个函数的值不是等于0，而是轻微的倾斜，这个函数通常比Relu激活函数效果要好，尽管在实际中Leaky ReLu使用的并不多。</p>
<p>两者的优点是：</p>
<p>第一，在的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个if-else语句，而sigmoid函数需要进行浮点四则运算，在实践中，使用ReLu激活函数神经网络通常会比使用sigmoid或者tanh激活函数学习的更快。</p>
<p>第二，sigmoid和tanh函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度弥散，而Relu和Leaky ReLu函数大于0部分都为常数，不会产生梯度弥散现象。(同时应该注意到的是，Relu进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而Leaky ReLu不会有这问题)</p>
<p>在ReLu的梯度一半都是0，但是，有足够的隐藏层使得z值大于0，所以对大多数的训练数据来说学习过程仍然可以很快。</p>
<h2 id="导数-3"><a href="#导数-3" class="headerlink" title="导数"></a>导数</h2>
$g(z)=\max(0.01z,z)$


$g(z)^{'}=
    \begin{cases}
    0.01& 	\text{if z < 0}\\
    1&	\text{if z > 0}\\
    undefined&	\text{if z = 0}
    \end{cases}$

<p>注：通常在z=0的时候给定其导数1,0.01；当然z=0的情况很少。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>概括一下不同激活函数的过程和结论。</p>
<p>sigmoid激活函数：除了输出层是一个二分类问题基本不会用它。</p>
<p>tanh激活函数：tanh是非常优秀的，几乎适合所有场合。</p>
<p>ReLu激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用ReLu或者Leaky ReLu。$g(z)=\max(0.01z,z)$为什么常数是0.01？当然，可以为学习算法选择不同的参数。</p>
<p>在选择自己神经网络的激活函数时，有一定的直观感受，在深度学习中的经常遇到一个问题：在编写神经网络的时候，会有很多选择：隐藏层单元的个数、激活函数的选择、初始化权值……这些选择想得到一个对比较好的指导原则是挺困难的。</p>
<p>鉴于以上三个原因，以及在工业界的见闻，提供一种直观的感受，哪一种工业界用的多，哪一种用的少。但是，自己的神经网络的应用，以及其特殊性，是很难提前知道选择哪些效果更好。所以通常的建议是：如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者发展集上进行评价。然后看哪一种表现的更好，就去使用它。</p>
<p>为自己的神经网络的应用测试这些不同的选择，会在以后检验自己的神经网络或者评估算法的时候，看到不同的效果。如果仅仅遵守使用默认的ReLu激活函数，而不要用其他的激励函数，那就可能在近期或者往后，每次解决问题的时候都使用相同的办法。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习（十）</title>
    <url>/2018/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%EF%BC%89/</url>
    <content><![CDATA[<h1 id="十三、聚类-Clustering"><a href="#十三、聚类-Clustering" class="headerlink" title="十三、聚类(Clustering)"></a>十三、聚类(Clustering)</h1><h2 id="无监督学习：简介"><a href="#无监督学习：简介" class="headerlink" title="无监督学习：简介"></a>无监督学习：简介</h2><p>参考视频: 13 - 1 - Unsupervised Learning_ Introduction (3 min).mkv</p>
<p>在这个视频中，我将开始介绍聚类算法。这将是一个激动人心的时刻，因为这是我们学习的第一个非监督学习算法。我们将要让计算机学习无标签数据，而不是此前的标签数据。</p>
<p>那么，什么是非监督学习呢？在课程的一开始，我曾简单的介绍过非监督学习，然而，我们还是有必要将其与监督学习做一下比较。</p>
<p>在一个典型的监督学习中，我们有一个有标签的训练集，我们的目标是找到能够区分正样本和负样本的决策边界，在这里的监督学习中，我们有一系列标签，我们需要据此拟合一个假设函数。与此不同的是，在非监督学习中，我们的数据没有附带任何标签，我们拿到的数据就是这样的：</p>
<p><img src="6709f5ca3cd2240d4e95dcc3d3e808d5.png" alt=""></p>
<p>在这里我们有一系列点，却没有标签。因此，我们的训练集可以写成只有$x^{(1)}$,$x^{(2)}$…..一直到$x^{(m)}$。我们没有任何标签$y$。因此，图上画的这些点没有标签信息。也就是说，在非监督学习中，我们需要将一系列无标签的训练数据，输入到一个算法中，然后我们告诉这个算法，快去为我们找找这个数据的内在结构给定数据。我们可能需要某种算法帮助我们寻找一种结构。图上的数据看起来可以分成两个分开的点集（称为簇），一个能够找到我圈出的这些点集的算法，就被称为聚类算法。</p>
<p><img src="6709f5ca3cd2240d4e95dcc3d3e808d5.png" alt=""></p>
<p>这将是我们介绍的第一个非监督学习算法。当然，此后我们还将提到其他类型的非监督学习算法，它们可以为我们找到其他类型的结构或者其他的一些模式，而不只是簇。</p>
<p>我们将先介绍聚类算法。此后，我们将陆续介绍其他算法。那么聚类算法一般用来做什么呢？</p>
<p><img src="ff180f091e9bad9ac185248721437526.png" alt=""></p>
<p>在这门课程的早些时候，我曾经列举过一些应用：比如市场分割。也许你在数据库中存储了许多客户的信息，而你希望将他们分成不同的客户群，这样你可以对不同类型的客户分别销售产品或者分别提供更适合的服务。社交网络分析：事实上有许多研究人员正在研究这样一些内容，他们关注一群人，关注社交网络，例如<strong>Facebook</strong>，<strong>Google+</strong>，或者是其他的一些信息，比如说：你经常跟哪些人联系，而这些人又经常给哪些人发邮件，由此找到关系密切的人群。因此，这可能需要另一个聚类算法，你希望用它发现社交网络中关系密切的朋友。我有一个朋友正在研究这个问题，他希望使用聚类算法来更好的组织计算机集群，或者更好的管理数据中心。因为如果你知道数据中心中，那些计算机经常协作工作。那么，你可以重新分配资源，重新布局网络。由此优化数据中心，优化数据通信。</p>
<p>最后，我实际上还在研究如何利用聚类算法了解星系的形成。然后用这个知识，了解一些天文学上的细节问题。好的，这就是聚类算法。这将是我们介绍的第一个非监督学习算法。在下一个视频中，我们将开始介绍一个具体的聚类算法。</p>
<h2 id="K-均值算法"><a href="#K-均值算法" class="headerlink" title="K-均值算法"></a>K-均值算法</h2><p>参考视频: 13 - 2 - K-Means Algorithm (13 min).mkv</p>
<p><strong>K-均值</strong>是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。</p>
<p><strong>K-均值</strong>是一个迭代算法，假设我们想要将数据聚类成n个组，其方法为:</p>
<p>首先选择$K$个随机的点，称为<strong>聚类中心</strong>（<strong>cluster centroids</strong>）；</p>
<p>对于数据集中的每一个数据，按照距离$K$个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。</p>
<p>计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。</p>
<p>重复步骤2-4直至中心点不再变化。</p>
<p>下面是一个聚类示例：</p>
<p><img src="ff1db77ec2e83b592bbe1c4153586120.jpg" alt=""></p>
<p>迭代 1 次</p>
<p><img src="acdb3ac44f1fe61ff3b5a77d5a4895a1.jpg" alt=""></p>
<p>迭代 3 次</p>
<p><img src="fe6dd7acf1a1eddcd09da362ecdf976f.jpg" alt=""></p>
<p>迭代 10 次</p>
<p>用$μ^1$,$μ^2$,…,$μ^k$ 来表示聚类中心，用$c^{(1)}$,$c^{(2)}$,…,$c^{(m)}$来存储与第$i$个实例数据最近的聚类中心的索引，<strong>K-均值</strong>算法的伪代码如下：</p>
<figure class="highlight cal"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Repeat</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> <span class="keyword">to</span> m</span><br><span class="line"></span><br><span class="line">c(i) := index (form <span class="number">1</span> <span class="keyword">to</span> K) <span class="keyword">of</span> cluster centroid closest <span class="keyword">to</span> x(i)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k = <span class="number">1</span> <span class="keyword">to</span> K</span><br><span class="line"></span><br><span class="line">μk := average (mean) <span class="keyword">of</span> points assigned <span class="keyword">to</span> cluster k</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>算法分为两个步骤，第一个<strong>for</strong>循环是赋值步骤，即：对于每一个样例$i$，计算其应该属于的类。第二个<strong>for</strong>循环是聚类中心的移动，即：对于每一个类$K$，重新计算该类的质心。</p>
<p><strong>K-均值</strong>算法也可以很便利地用于将数据分为许多不同组，即使在没有非常明显区分的组群的情况下也可以。下图所示的数据集包含身高和体重两项特征构成的，利用<strong>K-均值</strong>算法将数据分为三类，用于帮助确定将要生产的T-恤衫的三种尺寸。</p>
<p><img src="fed50a4e482cf3aae38afeb368141a97.png" alt=""></p>
<h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p>参考视频: 13 - 3 - Optimization Objective (7 min).mkv</p>
<p>K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此<br>K-均值的代价函数（又称<strong>畸变函数</strong> <strong>Distortion function</strong>）为</p>
$$J(c^{(1)},...,c^{(m)},μ_1,...,μ_K)=\dfrac {1}{m}\sum^{m}_{i=1}\left\| X^{\left( i\right) }-\mu_{c^{(i)}}\right\| ^{2}$$
其中${{\mu }_{{{c}^{(i)}}}}$代表与${{x}^{(i)}}$最近的聚类中心点。<br>我们的的优化目标便是找出使得代价函数最小的 $c^{(1)}$,$c^{(2)}$,...,$c^{(m)}$和$μ^1$,$μ^2$,...,$μ^k$
<p><img src="8605f0826623078a156d30a7782dfc3c.png" alt=""></p>
<p>回顾刚才给出的:<br><strong>K-均值</strong>迭代算法，我们知道，第一个循环是用于减小$c^{(i)}$引起的代价，而第二个循环则是用于减小${{\mu }_{i}}$引起的代价。<br>迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。</p>
<h2 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h2><p>参考视频: 13 - 4 - Random Initialization (8 min).mkv</p>
<p>在运行K-均值算法的之前，我们首先要随机初始化所有的聚类中心点，下面介绍怎样做：</p>
<ol>
<li><p>我们应该选择$K&lt;m$，即聚类中心点的个数要小于所有训练集实例的数量</p>
</li>
<li><p>随机选择$K$个训练实例，然后令$K$个聚类中心分别与这$K$个训练实例相等</p>
</li>
</ol>
<p><strong>K-均值</strong>的一个问题在于，它有可能会停留在一个局部最小值处，而这取决于初始化的情况。</p>
<p><img src="d4d2c3edbdd8915f4e9d254d2a47d9c7.png" alt=""></p>
<p>为了解决这个问题，我们通常需要多次运行<strong>K-均值</strong>算法，每一次都重新进行随机初始化，最后再比较多次运行<strong>K-均值</strong>的结果，选择代价函数最小的结果。<br>这种方法在$K$较小的时候（2–10）还是可行的，但是如果$K$较大，这么做也可能不会有明显地改善。</p>
<h2 id="选择聚类数"><a href="#选择聚类数" class="headerlink" title="选择聚类数"></a>选择聚类数</h2><p>参考视频: 13 - 5 - Choosing the Number of Clusters (8 min).mkv</p>
<p>没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考我们运用<strong>K-均值</strong>算法聚类的动机是什么，然后选择能最好服务于该目的标聚类数。</p>
<p>当人们在讨论，选择聚类数目的方法时，有一个可能会谈及的方法叫作“肘部法则”。关于“肘部法则”，我们所需要做的是改变$K$值，也就是聚类类别数目的总数。我们用一个聚类来运行<strong>K均值</strong>聚类方法。这就意味着，所有的数据都会分到一个聚类里，然后计算成本函数或者计算畸变函数$J$。$K$代表聚类数字。</p>
<p><img src="f3ddc6d751cab7aba7a6f8f44794e975.png" alt=""></p>
<p>我们可能会得到一条类似于这样的曲线。像一个人的肘部。这就是“肘部法则”所做的，让我们来看这样一个图，看起来就好像有一个很清楚的肘在那儿。好像人的手臂，如果你伸出你的胳膊，那么这就是你的肩关节、肘关节、手。这就是“肘部法则”。你会发现这种模式，它的畸变值会迅速下降，从1到2，从2到3之后，你会在3的时候达到一个肘点。<br>在此之后，畸变值就下降的非常慢，看起来就像使用3个聚类来进行聚类是正确的，这是因为那个点是曲线的肘点，畸变值下降得很快，$K=3$之后就下降得很慢，那么我们就选$K=3$。当你应用“肘部法则”的时候，如果你得到了一个像上面这样的图，那么这将是一种用来选择聚类个数的合理方法。</p>
例如，我们的 T-恤制造例子中，我们要将用户按照身材聚类，我们可以分成3个尺寸:$S,M,L$，也可以分成5个尺寸$XS,S,M,L,XL$，这样的选择是建立在回答“聚类后我们制造的T-恤是否能较好地适合我们的客户”这个问题的基础上作出的。<br><br><strong>聚类参考资料：</strong><br><br>1.相似度/距离计算方法总结<br><br>(1). 闵可夫斯基距离<strong>Minkowski</strong>（其中欧式距离：$p=2$)<br><br>$dist(X,Y)={{\left( {{\sum\limits_{i=1}^{n}{\left| {{x}_{i}}-{{y}_{i}} \right|}}^{p}} \right)}^{\frac{1}{p}}}$
<p>(2). 杰卡德相似系数(<strong>Jaccard</strong>)：</p>
$J(A,B)=\frac{\left| A\cap B \right|}{\left|A\cup B \right|}$
<p>(3). 余弦相似度(<strong>cosine similarity</strong>)：</p>
<p>$n$维向量$x$和$y$的夹角记做$\theta$，根据余弦定理，其余弦值为：</p>
$cos (\theta )=\frac{{{x}^{T}}y}{\left|x \right|\cdot \left| y \right|}=\frac{\sum\limits_{i=1}^{n}{{{x}_{i}}{{y}_{i}}}}{\sqrt{\sum\limits_{i=1}^{n}{{{x}_{i}}^{2}}}\sqrt{\sum\limits_{i=1}^{n}{{{y}_{i}}^{2}}}}$
<p>(4). Pearson皮尔逊相关系数：<br>${{\rho }_{XY}}=\frac{\operatorname{cov}(X,Y)}{{{\sigma }_{X}}{{\sigma }_{Y}}}=\frac{E[(X-{{\mu }_{X}})(Y-{{\mu }_{Y}})]}{{{\sigma }_{X}}{{\sigma }_{Y}}}=\frac{\sum\limits_{i=1}^{n}{(x-{{\mu }_{X}})(y-{{\mu }_{Y}})}}{\sqrt{\sum\limits_{i=1}^{n}{{{(x-{{\mu }_{X}})}^{2}}}}\sqrt{\sum\limits_{i=1}^{n}{{{(y-{{\mu }_{Y}})}^{2}}}}}$</p>
<p>Pearson相关系数即将$x$、$y$坐标向量各自平移到原点后的夹角余弦。</p>
<p>2.聚类的衡量指标</p>
<p>(1). 均一性：$p$</p>
<p>类似于精确率，一个簇中只包含一个类别的样本，则满足均一性。其实也可以认为就是正确率(每个 聚簇中正确分类的样本数占该聚簇总样本数的比例和)</p>
<p>(2). 完整性：$r$</p>
<p>类似于召回率，同类别样本被归类到相同簇中，则满足完整性;每个聚簇中正确分类的样本数占该<br>类型的总样本数比例的和</p>
<p>(3). <strong>V-measure</strong>:</p>
<p>均一性和完整性的加权平均 </p>
$V = \frac{(1+\beta^2)*pr}{\beta^2*p+r}$
<p>(4). 轮廓系数</p>
<p>样本$i$的轮廓系数：$s(i)$</p>
<p>簇内不相似度:计算样本$i$到同簇其它样本的平均距离为$a(i)$，应尽可能小。</p>
<p>簇间不相似度:计算样本$i$到其它簇$C_j$的所有样本的平均距离$b_{ij}$，应尽可能大。</p>
<p>轮廓系数：$s(i)$值越接近1表示样本$i$聚类越合理，越接近-1，表示样本$i$应该分类到 另外的簇中，近似为0，表示样本$i$应该在边界上;所有样本的$s(i)$的均值被成为聚类结果的轮廓系数。<br>$s(i) = \frac{b(i)-a(i)}{max\{a(i),b(i)\}}$</p>
<p>(5). <strong>ARI</strong></p>
<p>数据集$S$共有$N$个元素，  两个聚类结果分别是：</p>
$X=\{{{X}_{1}},{{X}_{2}},...,{{X}_{r}}\},Y=\{{{Y}_{1}},{{Y}_{2}},...,{{Y}_{s}}\}$
<p>$X$和$Y$的元素个数为：</p>
$a=\{{{a}_{1}},{{a}_{2}},...,{{a}_{r}}\},b=\{{{b}_{1}},{{b}_{2}},...,{{b}_{s}}\}$
记：${{n}_{ij}}=\left| {{X}_{i}}\cap {{Y}_{i}} \right|$
$ARI=\frac{\sum\limits_{i,j}{C_{{{n}_{ij}}}^{2}}-\left[ \left( \sum\limits_{i}{C_{{{a}_{i}}}^{2}} \right)\cdot \left( \sum\limits_{i}{C_{{{b}_{i}}}^{2}} \right) \right]/C_{n}^{2}}{\frac{1}{2}\left[ \left( \sum\limits_{i}{C_{{{a}_{i}}}^{2}} \right)+\left( \sum\limits_{i}{C_{{{b}_{i}}}^{2}} \right) \right]-\left[ \left( \sum\limits_{i}{C_{{{a}_{i}}}^{2}} \right)\cdot \left( \sum\limits_{i}{C_{{{b}_{i}}}^{2}} \right) \right]/C_{n}^{2}}$
<h1 id="十四、降维-Dimensionality-Reduction"><a href="#十四、降维-Dimensionality-Reduction" class="headerlink" title="十四、降维(Dimensionality Reduction)"></a>十四、降维(Dimensionality Reduction)</h1><h2 id="动机一：数据压缩"><a href="#动机一：数据压缩" class="headerlink" title="动机一：数据压缩"></a>动机一：数据压缩</h2><p>参考视频: 14 - 1 - Motivation I_ Data Compression (10 min).mkv</p>
<p>这个视频，我想开始谈论第二种类型的无监督学习问题，称为降维。有几个不同的的原因使你可能想要做降维。一是数据压缩，后面我们会看了一些视频后，数据压缩不仅允许我们压缩数据，因而使用较少的计算机内存或磁盘空间，但它也让我们加快我们的学习算法。</p>
<p>但首先，让我们谈论降维是什么。作为一种生动的例子，我们收集的数据集，有许多，许多特征，我绘制两个在这里。</p>
<p><img src="2373072a74d97a9f606981ffaf1dd53b.png" alt=""></p>
<p>假设我们未知两个的特征：$x_1$:长度：用厘米表示；$x_2$：是用英寸表示同一物体的长度。</p>
<p>所以，这给了我们高度冗余表示，也许不是两个分开的特征$x_1$和$x_2$，这两个基本的长度度量，也许我们想要做的是减少数据到一维，只有一个数测量这个长度。这个例子似乎有点做作，这里厘米英寸的例子实际上不是那么不切实际的，两者并没有什么不同。</p>
<p>将数据从二维降至一维：<br>假使我们要采用两种不同的仪器来测量一些东西的尺寸，其中一个仪器测量结果的单位是英寸，另一个仪器测量的结果是厘米，我们希望将测量的结果作为我们机器学习的特征。现在的问题的是，两种仪器对同一个东西测量的结果不完全相等（由于误差、精度等），而将两者都作为特征有些重复，因而，我们希望将这个二维的数据降至一维。</p>
<p>从这件事情我看到的东西发生在工业上的事。如果你有几百个或成千上万的特征，它是它这往往容易失去你需要的特征。有时可能有几个不同的工程团队，也许一个工程队给你二百个特征，第二工程队给你另外三百个的特征，第三工程队给你五百个特征，一千多个特征都在一起，它实际上会变得非常困难，去跟踪你知道的那些特征，你从那些工程队得到的。其实不想有高度冗余的特征一样。</p>
<p><img src="2c95b316a3c61cf076ef132d3d50b51c.png" alt=""></p>
<p>多年我一直在研究直升飞机自动驾驶。诸如此类。如果你想测量——如果你想做，你知道，做一个调查或做这些不同飞行员的测试——你可能有一个特征：$x_1$，这也许是他们的技能（直升机飞行员），也许$x_2$可能是飞行员的爱好。这是表示他们是否喜欢飞行，也许这两个特征将高度相关。你真正关心的可能是这条红线的方向，不同的特征，决定飞行员的能力。</p>
<p><img src="8274f0c29314742e9b4f15071ea7624a.png" alt=""></p>
<p>将数据从三维降至二维：<br>这个例子中我们要将一个三维的特征向量降至一个二维的特征向量。过程是与上面类似的，我们将三维向量投射到一个二维的平面上，强迫使得所有的数据都在同一个平面上，降至二维的特征向量。</p>
<p><img src="67e2a9d760300d33ac5e12ad2bd5523c.jpg" alt=""></p>
<p>这样的处理过程可以被用于把任何维度的数据降到任何想要的维度，例如将1000维的特征降至100维。</p>
<p>正如我们所看到的，最后，这将使我们能够使我们的一些学习算法运行也较晚，但我们会在以后的视频提到它。</p>
<h2 id="动机二：数据可视化"><a href="#动机二：数据可视化" class="headerlink" title="动机二：数据可视化"></a>动机二：数据可视化</h2><p>参考视频: 14 - 2 - Motivation II_ Visualization (6 min).mkv</p>
<p>在许多及其学习问题中，如果我们能将数据可视化，我们便能寻找到一个更好的解决方案，降维可以帮助我们。</p>
<p><img src="789d90327121d3391735087b9276db2a.png" alt=""></p>
<p>假使我们有有关于许多不同国家的数据，每一个特征向量都有50个特征（如<strong>GDP</strong>，人均<strong>GDP</strong>，平均寿命等）。如果要将这个50维的数据可视化是不可能的。使用降维的方法将其降至2维，我们便可以将其可视化了。</p>
<p><img src="ec85b79482c868eddc06ba075465fbcf.png" alt=""></p>
<p>这样做的问题在于，降维的算法只负责减少维数，新产生的特征的意义就必须由我们自己去发现了。</p>
<h2 id="主成分分析问题"><a href="#主成分分析问题" class="headerlink" title="主成分分析问题"></a>主成分分析问题</h2><p>参考视频: 14 - 3 - Principal Component Analysis Problem Formulation (9 min). mkv</p>
<p>主成分分析(<strong>PCA</strong>)是最常见的降维算法。</p>
<p>在<strong>PCA</strong>中，我们要做的是找到一个方向向量（<strong>Vector direction</strong>），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。</p>
<p><img src="a93213474b35ce393320428996aeecd9.jpg" alt=""></p>
<p>下面给出主成分分析问题的描述：</p>
<p>问题是要将$n$维数据降至$k$维，目标是找到向量$u^{(1)}$,$u^{(2)}$,…,$u^{(k)}$使得总的投射误差最小。主成分分析与线性回顾的比较：</p>
<p>主成分分析与线性回归是两种不同的算法。主成分分析最小化的是投射误差（<strong>Projected Error</strong>），而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。</p>
<p><img src="7e1389918ab9358d1432d20ed20f8142.png" alt=""></p>
<p>上图中，左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差（垂直于红线投影）。</p>
<p><strong>PCA</strong>将$n$个特征降维到$k$个，可以用来进行数据压缩，如果100维的向量最后可以用10维来表示，那么压缩率为90%。同样图像处理领域的<strong>KL变换</strong>使用<strong>PCA</strong>做图像压缩。但<strong>PCA</strong> 要保证降维后，还要保证数据的特性损失最小。</p>
<p><strong>PCA</strong>技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。</p>
<p><strong>PCA</strong>技术的一个很大的优点是，它是完全无参数限制的。在<strong>PCA</strong>的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。</p>
<p>但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。</p>
<h2 id="主成分分析算法"><a href="#主成分分析算法" class="headerlink" title="主成分分析算法"></a>主成分分析算法</h2><p>参考视频: 14 - 4 - Principal Component Analysis Algorithm (15 min).mkv</p>
<p><strong>PCA</strong> 减少$n$维到$k$维：</p>
第一步是均值归一化。我们需要计算出所有特征的均值，然后令 $x_j= x_j-μ_j$。如果特征是在不同的数量级上，我们还需要将其除以标准差 $σ^2$。<br><br>第二步是计算<strong>协方差矩阵</strong>（<strong>covariance matrix</strong>）$Σ$：<br>$\sum=\dfrac {1}{m}\sum^{n}_{i=1}\left( x^{(i)}\right) \left( x^{(i)}\right) ^{T}$
<p>第三步是计算协方差矩阵$Σ$的<strong>特征向量</strong>（<strong>eigenvectors</strong>）:</p>
<p>在 <strong>Octave</strong> 里我们可以利用<strong>奇异值分解</strong>（<strong>singular value decomposition</strong>）来求解，<code>[U, S, V]= svd(sigma)</code>。</p>
<p><img src="0918b38594709705723ed34bb74928ba.png" alt=""></p>
$$Sigma=\dfrac {1}{m}\sum^{n}_{i=1}\left( x^{(i)}\right) \left( x^{(i)}\right) ^{T}$$
<p><img src="01e1c4a2f29a626b5980a27fc7d6a693.png" alt=""></p>
<p>对于一个 $n×n$维度的矩阵，上式中的$U$是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从$n$维降至$k$维，我们只需要从$U$中选取前$k$个向量，获得一个$n×k$维度的矩阵，我们用$U_{reduce}$表示，然后通过如下计算获得要求的新特征向量$z^{(i)}$:<br>$$z^{(i)}=U^{T}_{reduce}*x^{(i)}$$</p>
<p>其中$x$是$n×1$维的，因此结果为$k×1$维度。注，我们不对方差特征进行处理。</p>
<h2 id="选择主成分的数量"><a href="#选择主成分的数量" class="headerlink" title="选择主成分的数量"></a>选择主成分的数量</h2><p>参考视频: 14 - 5 - Choosing The Number Of Principal Components (13 min).mkv</p>
<p>主要成分分析是减少投射的平均均方误差：</p>
<p>训练集的方差为：$\dfrac {1}{m}\sum^{m}_{i=1}\left\| x^{\left( i\right) }\right\| ^{2}$</p>
<p>我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的$k$值。</p>
<p>如果我们希望这个比例小于1%，就意味着原本数据的偏差有99%都保留下来了，如果我们选择保留95%的偏差，便能非常显著地降低模型中特征的维度了。</p>
<p>我们可以先令$k=1$，然后进行主要成分分析，获得$U_{reduce}$和$z$，然后计算比例是否小于1%。如果不是的话再令$k=2$，如此类推，直到找到可以使得比例小于1%的最小$k$ 值（原因是各个特征之间通常情况存在某种相关性）。</p>
<p>还有一些更好的方式来选择$k$，当我们在<strong>Octave</strong>中调用“<strong>svd</strong>”函数的时候，我们获得三个参数：<code>[U, S, V] = svd(sigma)</code>。</p>
<p><img src="a4477d787f876ae4e72cb416a2cb0b8a.jpg" alt=""></p>
<p>其中的$S$是一个$n×n$的矩阵，只有对角线上有值，而其它单元都是0，我们可以使用这个矩阵来计算平均均方误差与训练集方差的比例：<br>$$\dfrac {\dfrac {1}{m}\sum^{m}_{i=1}\left\| x^{\left( i\right) }-x^{\left( i\right) }_{approx}\right\| ^{2}}{\dfrac {1}{m}\sum^{m}_{i=1}\left\| x^{(i)}\right\| ^{2}}=1-\dfrac {\Sigma^{k}_{i=1}S_{ii}}{\Sigma^{m}_{i=1}S_{ii}}\leq 1\%$$</p>
<p>也就是：$$\frac {\Sigma^{k}_{i=1}s_{ii}}{\Sigma^{n}_{i=1}s_{ii}}\geq0.99$$</p>
<p>在压缩过数据后，我们可以采用如下方法来近似地获得原有的特征：$$x^{\left( i\right) }_{approx}=U_{reduce}z^{(i)}$$</p>
<h2 id="重建的压缩表示"><a href="#重建的压缩表示" class="headerlink" title="重建的压缩表示"></a>重建的压缩表示</h2><p>参考视频: 14 - 6 - Reconstruction from Compressed Representation (4 min).mkv</p>
<p>在以前的视频中，我谈论<strong>PCA</strong>作为压缩算法。在那里你可能需要把1000维的数据压缩100维特征，或具有三维数据压缩到一二维表示。所以，如果这是一个压缩算法，应该能回到这个压缩表示，回到你原有的高维数据的一种近似。</p>
<p>所以，给定的$z^{(i)}$，这可能100维，怎么回到你原来的表示$x^{(i)}$，这可能是1000维的数组？</p>
<p><img src="0a4edcb9c0d0a3812a50b3e95ef3912a.png" alt=""></p>
<p><strong>PCA</strong>算法，我们可能有一个这样的样本。如图中样本$x^{(1)}$,$x^{(2)}$。我们做的是，我们把这些样本投射到图中这个一维平面。然后现在我们需要只使用一个实数，比如$z^{(1)}$，指定这些点的位置后他们被投射到这一个三维曲面。给定一个点$z^{(1)}$，我们怎么能回去这个原始的二维空间呢？$x$为2维，z为1维，$z=U^{T}_{reduce}x$，相反的方程为：$x_{appox}=U_{reduce}\cdot z$,$x_{appox}\approx x$。如图：</p>
<p><img src="66544d8fa1c1639d80948006f7f4a8ff.png" alt=""></p>
<p>如你所知，这是一个漂亮的与原始数据相当相似。所以，这就是你从低维表示$z$回到未压缩的表示。我们得到的数据的一个之间你的原始数据 $x$，我们也把这个过程称为重建原始数据。</p>
<p>当我们认为试图重建从压缩表示 $x$ 的初始值。所以，给定未标记的数据集，您现在知道如何应用<strong>PCA</strong>，你的带高维特征$x$和映射到这的低维表示$z$。这个视频，希望你现在也知道如何采取这些低维表示$z$，映射到备份到一个近似你原有的高维数据。</p>
<p>现在你知道如何实施应用<strong>PCA</strong>，我们将要做的事是谈论一些技术在实际使用<strong>PCA</strong>很好，特别是，在接下来的视频中，我想谈一谈关于如何选择$k$。</p>
<h2 id="主成分分析法的应用建议"><a href="#主成分分析法的应用建议" class="headerlink" title="主成分分析法的应用建议"></a>主成分分析法的应用建议</h2><p>参考视频: 14 - 7 - Advice for Applying PCA (13 min).mkv</p>
<p>假使我们正在针对一张 100×100像素的图片进行某个计算机视觉的机器学习，即总共有10000 个特征。</p>
<ol>
<li><p>第一步是运用主要成分分析将数据压缩至1000个特征</p>
</li>
<li><p>然后对训练集运行学习算法</p>
<ol start="3">
<li>在预测时，采用之前学习而来的$U_{reduce}$将输入的特征$x$转换成特征向量$z$，然后再进行预测</li>
</ol>
</li>
</ol>
<p>注：如果我们有交叉验证集合测试集，也采用对训练集学习而来的$U_{reduce}$。</p>
<p>错误的主要成分分析情况：一个常见错误使用主要成分分析的情况是，将其用于减少过拟合（减少了特征的数量）。这样做非常不好，不如尝试正则化处理。原因在于主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。</p>
<p>另一个常见的错误是，默认地将主要成分分析作为学习过程中的一部分，这虽然很多时候有效果，最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习的参数调试</title>
    <url>/2019/04/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95/</url>
    <content><![CDATA[<p>在构建神经网络模型时，有许多的参数需要去调试，比如<strong>learning rate</strong> （学习率）、<strong>iterations</strong>(梯度下降法循环的数量)、<strong>L</strong>（隐藏层数目）、（隐藏层单元数目）、<strong>choice of activation function</strong>（激活函数的选择）除了这些基本的还有一些其他的参数，如<strong>momentum、mini batch size、regularization parameters</strong>等等。</p>
<p>在这里记录下常用的几个参数的选择。</p>
<h1 id="隐藏单元-hidden-units"><a href="#隐藏单元-hidden-units" class="headerlink" title="隐藏单元 hidden units"></a>隐藏单元 hidden units</h1><p>这是首先要考虑的问题，我们要构建多少个隐藏单元才会更适合？</p>
<p>首先要明白一点，hidden units不是越多越好，具体多少要通过具体的情况尝试，虽然有许多的论文写了各种数学公式来验证样本集合与神经元的关系，但是我觉得还是在具体情况下，根据直觉来设置一个，然后再删减找到最优。</p>
<p>通常的建议是设置小于输入的75%，但具体的情况具体考虑，例如我在做数字识别时，我们有0-9十个数字，设置十个隐藏单元要好于设置大于十和小于十。</p>
<h1 id="学习速率-learning-rate"><a href="#学习速率-learning-rate" class="headerlink" title="学习速率 learning rate"></a>学习速率 learning rate</h1><p>这是非常重要的参数，因为学习速率的选择对梯度下降的影响最大，它可能会在0到1之间，</p>
<p>所以在Python中，你可以这样做，使</p>
<p>python<br>r = -4*np.random.rand()<br>a = 10**r</p>
<p>因为$r \in \lbrack - 4,0\rbrack$所以$a \in \lbrack 10^{-4}, 10^{0}\rbrack$，这样我们就可以在0.0001和1之间随机取出一个值。</p>
<h1 id="batch-size"><a href="#batch-size" class="headerlink" title="batch size"></a>batch size</h1><p>在样本非常多的情况下会使用mini batch梯度下降，我们每次迭代选择多少size来计算呢？</p>
<p>首先，如果训练集较小，小于2000个样本，直接使用batch梯度下降法，也就是size=m（样本总数），样本集较小就没必要使用mini-batch梯度下降法，我们可以快速处理整个训练集，所以使用batch梯度下降法也很好。</p>
<p>样本数目较大的话，一般的mini batch大小为64到512，考虑到电脑内存设置和使用的方式，如果mini batch大小是2的次方，代码会运行地快一些，64就是2的6次方，以此类推，128是2的7次方，256是2的8次方，512是2的9次方。</p>
<p>最后需要注意的是在mini batch中，要确保和要符合CPU/GPU内存，取决于我们的应用方向以及训练集的大小。如果处理的mini batch和CPU/GPU内存不相符，不管用什么方法处理数据，算法的表现会急转直下变得惨不忍睹，我们需要做一些尝试，才能找到能够最有效地减少成本函数的那个，一般会尝试几个不同的值，几个不同的2次方，然后看能否找到一个让梯度下降优化算法最高效的大小。</p>
<h1 id="正则化参数"><a href="#正则化参数" class="headerlink" title="正则化参数"></a>正则化参数</h1><p>正则话可以防止过拟合，除了之前在逻辑回归中使用的的L2正则化，还可以使用随机失活（dropout）正则化，实施dropout，在计算机视觉领域很成功。计算视觉中的输入量非常大，输入太多像素，以至于没有足够的数据，所以dropout在计算机视觉中应用得比较频繁，有些计算机视觉研究人员非常喜欢用它，几乎成了默认的选择，但要牢记一点，dropout是一种正则化方法，它有助于预防过拟合，因此除非算法过拟合，不然我是不会使用dropout的，所以它在其它领域应用得比较少，主要存在于计算机视觉领域，因为我们通常没有足够的数据，所以一直存在过拟合，这就是有些计算机视觉研究人员如此钟情于dropout函数的原因。</p>
<p>dropout一大缺点就是代价函数不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。</p>
<p>扩充我们的数据可以防止过拟合，但扩增数据代价很高，而且有时候我们无法扩增数据，但我们可以通过添加这类图片来增加训练集。例如，水平翻转图片，扭曲图片，随意裁剪图片，并把它添加到训练集。所以现在训练集中有原图，还有变换后的这张图片，这虽然不如我们额外收集一组新图片那么好，但这样做节省了获取更多图片的花费。</p>
<p>还有另外一种常用的方法叫作early stopping，运行梯度下降时，我们可以绘制训练误差，或只绘制代价函数的优化过程，还可以绘制验证集误差，它可以是验证集上的分类误差，或验证集上的代价函数，逻辑损失和对数损失等，你会发现，验证集误差通常会先呈下降趋势，然后在某个节点处开始上升，我们在此停止训练吧，这并不是一个很好的建议。</p>
<h1 id="optimizer"><a href="#optimizer" class="headerlink" title="optimizer"></a>optimizer</h1><p>优化器的具体选择就比较复杂了，有许多的论文，具体可以看看这篇论文：<br><a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a></p>
<p>看两张动图直观上感受下算法的优化过程。第一张图为不同算法在损失平面等高线上随时间的变化情况，第二张图为不同算法在鞍点处的行为比较。</p>
<p><strong>optimization on loss surface contours</strong></p>
<p><img src="contours_evaluation_optimizers.gif" alt="optimization on loss surface contours"></p>
<p><strong>optimization on saddle point</strong></p>
<p><img src="saddle_point_evaluation_optimizers.gif" alt="optimization on saddle point"></p>
<p>tensorflow 提供了多个优化器的api，使用起来非常简单。 <a href="http://www.tensorfly.cn/tfdoc/api_docs/python/train.html#AUTOGENERATED-optimizers" target="_blank" rel="noopener">tf.train.Optimizer</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>独热编码One Hot Encoder</title>
    <url>/2018/09/29/%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81One%20Hot%20Encoder/</url>
    <content><![CDATA[<p>在机器学习时我们通常要进行归一化数据后再进行训练，还有一些其他处理方法比如使用独热编码。</p>
<h1 id="什么是独热码"><a href="#什么是独热码" class="headerlink" title="什么是独热码"></a>什么是独热码</h1><p>独热编码即 One-Hot 编码，又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都有它独立的寄存器位，并且在任意时候，其中只有一位有效。</p>
<p>可以这样理解，对于每一个特征，如果它有m个可能值，那么经过独热编码后，就变成了m个二元特征。并且，这些特征互斥，每次只有一个激活。因此，数据会变成稀疏的。</p>
<p>例如对六个状态进行编码：</p>
<p>自然顺序码为 000,001,010,011,100,101</p>
<p>独热编码则是 000001,000010,000100,001000,010000,100000</p>
<h1 id="独热码的优点"><a href="#独热码的优点" class="headerlink" title="独热码的优点"></a>独热码的优点</h1><p>有一些特征无法直接应用在需要数值计算的算法中，例如，用户的性别，爱好，住址等，一般简单粗暴的处理方式时直接将不同的类别映射为一个整数，比如男性为0，女性为1，其他为2，这种简单的实现最大的问题就在于各种类别的特征都被看成是有序的，这显然不符合实际场景。</p>
<p>使用独热码可以处理非连续型数值特征，并且在一定程度上扩充了特征。</p>
<p>1.使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。 </p>
<p>2.将离散特征通过one-hot编码映射到欧式空间，是因为，在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。 </p>
<p>3.将离散型特征使用one-hot编码，可以会让特征之间的距离计算更加合理。比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = (1), x_2 = (2), x_3 = (3)。两个工作之间的距离是，(x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。那如果使用one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。</p>
<h1 id="编码过程"><a href="#编码过程" class="headerlink" title="编码过程"></a>编码过程</h1><p>　　假如只有一个特征是离散值：</p>
<p>　　　　{sex：{male， female，other}}</p>
<p>　　该特征总共有3个不同的分类值，此时需要3个bit位表示该特征是什么值，对应bit位为1的位置对应原来的特征的值（一般情况下可以将原始的特征的取值进行排序，以便于后期使用），此时得到独热码为{100}男性 ，{010}女性，{001}其他</p>
<p>　　假如多个特征需要独热码编码，那么久按照上面的方法依次将每个特征的独热码拼接起来：</p>
<p>　　　　{sex：{male， female，other}}</p>
<p>　　　　{grade：{一年级， 二年级，三年级， 四年级}}</p>
<p>　　此时对于输入为{sex：male； grade： 四年级}进行独热编码，可以首先将sex按照上面的进行编码得到{100}，然后按照grade进行编码为{0001}，那么两者连接起来得到最后的独热码{1000001}；
　　</p>
<h1 id="sklearn中的One-Hot-Encoder"><a href="#sklearn中的One-Hot-Encoder" class="headerlink" title="sklearn中的One Hot Encoder"></a>sklearn中的One Hot Encoder</h1><p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" target="_blank" rel="noopener">官方文档</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">y = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]).T</span><br><span class="line">encoder = OneHotEncoder(sparse=<span class="keyword">False</span>)</span><br><span class="line">y_onehot = encoder.fit_transform(y)</span><br><span class="line">print(y_onehot)</span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">[[<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]]</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>OneHotEncoder</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习实战（十三）</title>
    <url>/2020/06/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[<h1 id="利用PCA来简化数据"><a href="#利用PCA来简化数据" class="headerlink" title="利用PCA来简化数据"></a>利用PCA来简化数据</h1><p>降维（dimensionality reduction），数据在低纬度时更容易处理。</p>
<h1 id="降维技术"><a href="#降维技术" class="headerlink" title="降维技术"></a>降维技术</h1><p>数据进行简化的原因：</p>
<pre><code>使得数据集更容易使用
降低很多算法的计算开销
去除噪声
使得结果易懂
</code></pre><p>降维方法，主成分分析（Principal Component Analysis，PCA），数据从原来的坐标系转换到新的坐标系，新的坐标系的选择由数据本身决定的。第一个新坐标轴选择的是原始数据中方差最大的方向，第二个新坐标轴的选择和第一个坐标轴正交且具有最大方差的方向。该过程一直重复，重复次数为原始数据中特征的数目。</p>
<p>另一种降维技术是，因子分析（Factor Analysis），在因子分析中，我们假设在观察数据的生成中有一些观察不到的隐变量（latent variable）。假设观察数据是这些隐变量和某些噪声的线性组合。那么隐变量的数据可能比观察数据的数目少，也就是说通过找到隐变量就可以实现数据的降维。</p>
<p>还有一种降维技术就是独立成分分析（Independent Component Analysis，ICA），ICA假设数据是从N个数据源生成的，这一点和因子分析有些类似。假设数据为多个数据源的混合观察结果，这些数据源之间在统计上是相互独立的，而在PCA中假设数据是不相关的。同因子分析一样，如果数据源的数目少于观察数据的数目，则可以实现降维过程。</p>
<h1 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h1><p>主成分分析</p>
<pre><code>优点：降低数据的复杂性，识别最重要的多个特征
缺点：不一定需要，且可能损失有用的信息
适用数据类型：数值型数据
</code></pre><h2 id="在Numpy中实现PCA"><a href="#在Numpy中实现PCA" class="headerlink" title="在Numpy中实现PCA"></a>在Numpy中实现PCA</h2><p>伪代码大致如下：</p>
<pre><code>去除平均值
计算协方差矩阵
计算协方差矩阵的特征值和特征向量
将特征值从大到小排序
保留最上面的N个特征向量
将数据转换到上述N个特征向量构建的空间中
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName, delim=<span class="string">'\t'</span>)</span>:</span></span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    stringArr = [line.strip().split(delim) <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines()]</span><br><span class="line">    datArr = [list(map(float, line)) <span class="keyword">for</span> line <span class="keyword">in</span> stringArr]</span><br><span class="line">    <span class="keyword">return</span> mat(datArr)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca</span><span class="params">(dataMat, topNfeat=<span class="number">9999999</span>)</span>:</span></span><br><span class="line">    meanVals = mean(dataMat, axis=<span class="number">0</span>)</span><br><span class="line">    meanRemoved = dataMat - meanVals <span class="comment"># 去平均值</span></span><br><span class="line">    covMat = cov(meanRemoved, rowvar=<span class="number">0</span>)</span><br><span class="line">    eigVals, eigVects = linalg.eig(mat(covMat))</span><br><span class="line">    eigValInd = argsort(eigVals)            <span class="comment"># 从小到大排序</span></span><br><span class="line">    eigValInd = eigValInd[: -(topNfeat+<span class="number">1</span>): <span class="number">-1</span>]  <span class="comment"># 去掉多余的</span></span><br><span class="line">    redEigVects = eigVects[:, eigValInd]</span><br><span class="line">    lowDDataMat = meanRemoved * redEigVects <span class="comment"># 将数据转换为新的维度</span></span><br><span class="line">    reconMat = (lowDDataMat * redEigVects.T) + meanVals</span><br><span class="line">    <span class="keyword">return</span> lowDDataMat, reconMat</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataMat = loadDataSet(<span class="string">'MLiA_SourceCode/Ch13/testSet.txt'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lowDMat, reconMat = pca(dataMat, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">shape(lowDMat)</span><br></pre></td></tr></table></figure>
<pre><code>(1000, 1)
</code></pre><p>将原始数据和降维后的数据绘制出来</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">ax.scatter(dataMat[:, <span class="number">0</span>].flatten().A[<span class="number">0</span>], dataMat[:, <span class="number">1</span>].flatten().A[<span class="number">0</span>], marker=<span class="string">'^'</span>, s=<span class="number">90</span>)</span><br><span class="line">ax.scatter(reconMat[:, <span class="number">0</span>].flatten().A[<span class="number">0</span>], reconMat[:, <span class="number">1</span>].flatten().A[<span class="number">0</span>], marker=<span class="string">'o'</span>, s=<span class="number">90</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89_10_0.png" alt="png"></p>
<h2 id="实例：利用PCA对半导体制造数据降维"><a href="#实例：利用PCA对半导体制造数据降维" class="headerlink" title="实例：利用PCA对半导体制造数据降维"></a>实例：利用PCA对半导体制造数据降维</h2><p>数据拥有590个特征，包含许多的缺失值，这些缺失值是以NaN标识的。用平均值来代替缺失值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replaceNanWithMean</span><span class="params">()</span>:</span> </span><br><span class="line">    datMat = loadDataSet(<span class="string">'MLiA_SourceCode/Ch13/secom.data'</span>, <span class="string">' '</span>)</span><br><span class="line">    numFeat = shape(datMat)[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeat):</span><br><span class="line">        meanVal = mean(datMat[nonzero(~isnan(datMat[:,i].A))[<span class="number">0</span>],i]) <span class="comment"># 计算所有非Nan的平均值</span></span><br><span class="line">        datMat[nonzero(isnan(datMat[:,i].A))[<span class="number">0</span>],i] = meanVal  <span class="comment"># 将所有的Nan设置为平均值</span></span><br><span class="line">    <span class="keyword">return</span> datMat</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataMat = replaceNanWithMean()</span><br></pre></td></tr></table></figure>
<p>观察pca()的工作过程</p>
<p>首先是去除均值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">meanVals = mean(dataMat, axis=<span class="number">0</span>)</span><br><span class="line">meanRemoved = dataMat - meanVals</span><br></pre></td></tr></table></figure>
<p>然后计算协方差矩阵：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">covMat = cov(meanRemoved, rowvar=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>对该矩阵进行特征值分析</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eigVals, eigVects = linalg.eig(mat(covMat))</span><br></pre></td></tr></table></figure>
<p>观察特征值结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eigVals</span><br></pre></td></tr></table></figure>
<pre><code>array([ 5.34151979e+07+0.00000000e+00j,  2.17466719e+07+0.00000000e+00j,
        8.24837662e+06+0.00000000e+00j,  2.07388086e+06+0.00000000e+00j,
        ...
        ...
        0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,
        0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,
        0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,
        0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,
        0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j])
</code></pre><p>我们可以看到一堆的值，发现其中有超过20%的特征值都是0。这就意味着这些特征都是其他特征的副本，也就是说，他们可以通过其他特征来表示，而本身并没有提供额外的信息。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>降维技术使得数据变得更易使用，并且它们往往能够去除数据中的噪声，使得其他机器学习任务更加精确。降维往往作为i预处理的步骤，在数据应用到其他算法之前清洗数据。很多技术可以用于数据降维，在这些技术中，独立成分分析，因子分析和主成分分析比较流行，其中又以主成分分析应用最广泛。</p>
<p>PCA可以从数据中识别其主要特征，他是通过沿着数据最大方差方向旋转坐标轴来实现的。选择方差最大的方向作为第一条坐标轴，后续坐标轴则与前面的坐标轴正交。协方差矩阵上的特征值分析可以用一系列的正交坐标轴来获取。</p>
]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>PCA</tag>
        <tag>降维</tag>
        <tag>主成分分析</tag>
      </tags>
  </entry>
  <entry>
    <title>随便玩玩的玩家</title>
    <url>/2018/04/07/%E9%9A%8F%E4%BE%BF%E7%8E%A9%E7%8E%A9%E7%9A%84%E7%8E%A9%E5%AE%B6/</url>
    <content><![CDATA[<p>如果从三岁看哥哥玩马戏团算起，游戏玩了也有二十个年头了，但还是个游戏菜鸟，已经古稀之年的斯皮尔伯格却要拍一部头号玩家，这使我怀着浓厚的兴趣走进了影院。</p>
<p>如果说最早看过斯皮尔伯格执导的电影那应该是大白鲨和侏罗纪公园了，但那时我还太小，大概七八岁，并不知道导演什么的，但是大白鲨却给我留下怕水的和鱼的心理阴影，小时候连鱼都不敢抓，对恐龙没有太大的兴趣所以侏罗纪公园没太大印象，到初中时候英语老师给放夺宝奇兵，介绍说是一个多么棒的导演拍的，没记住导演和剧情，只是被电影中的各种探险寻宝的过程深深吸引，然后化身中二少年与小伙伴们一起开始寻宝之旅。对电影的理解还是要到高中时期，一次回家的路上，谈到了电影，好友说他圣城上下载了《拯救大兵瑞恩》超清版的，连弹道都能看到，汤姆汉克斯演的不愧是最好的二战电影，另一位好友说是斯皮尔伯格拍的，吧啦吧啦。。。什么他们在说什么，我看电影都是只记得剧情和中国的演员，而他们能把外国的导演和演员如数家珍，为了不丢人于是回家恶补，结果发现斯皮尔伯格拍的电影大都看过，但却没关注过导演(lll￢ω￢)，我正是从那时起，看电影不再是看看剧情一笑而过了，而是欣赏导演和演员的艺术，看完后会去和好友聊聊不同的见解，这仿佛为我打开了新世界的大门，从此沉迷电影无法自拔了。</p>
<p>周末迎着呼呼的风一大早跑去电影院，途中遇到了因为风大而改变行程的两位好友一起随我到影院，虽然我们仨都是游戏迷，但只有我是影迷，他俩看完并没有什么感想。电影开场介绍了世界观与社会背景，当主角连接绿洲时，差点使我中二之魂爆发喊出了Link Start！，刀剑第一季是我最喜欢的动漫之一，接着闪过我的世界等游戏画面，介绍各种世界时让我想到了星际特工·千星之城，一个场景和女主不错，但故事有点老套的商业电影，这是吕克·贝松唯一让我看的有点瞌睡的电影，不能怪他，只怪那是一个翻拍无数遍的剧本，当时担心斯皮尔伯格会不会陷入这种情况，还好全程无尿点，剧情非常紧凑。</p>
<p>第一关是老司机开车，主角的座驾有些眼熟，后来看别人说了回到未来才想起来，当主角倒着开车时，我回想了下我有没有这样玩过呢？一个晕车的小孩当然也不会喜欢赛车游戏，在游戏厅看别人玩极品飞车的我因为跑不过人家有时会生气的倒着开，反正也追不上了就随便玩玩吧，并不是没有好胜心，而是想你们看我开倒了，输了也只是因为不想玩，呵呵。。在主角倒车到达终点时，我想我也经常不按游戏设计走，会瞎跑一会但都无疾而终，有没有倒着却赢的时候呢，思绪回到了2008年QQ飞车上线时，和小伙伴愉快的玩耍，我属于技术比较菜的，漂移都是360°的，在玩月牙湾的地图时，好友都快领先我一圈了，而我漂移后找不到方向，继续向前开他却迎面向我开来把我撞偏了，说我怎么反着开，我说我也不知到啊，系统也没提示我，反正我也分不清方向就继续开，然后我就第一了，还打破了记录，大家都很惊讶，我们仔细研究下发现了BUG，在过一个弯道后反向开回终点也算完成一圈但路途近了一半，于是我们开始刷这个BUG把自己的名次刷到了全服最高，后来玩家基本都知道了，跑图时会发现一半人在反向开，那时玩的还是很开心的，再后来BUG当然是被修复了，分数也被清零了，正在我想着当年一起开车的场景时，主角已经找到了第二条线索，闪灵是我最想看但最不敢看的恐怖片了，所以至今没有正眼看过，当我们的钢铁少年拿着球遇到了俩小萝莉时，我尽管没看过但我还是知道，要发生恐怖的事情了，当血水涌出的镜头又出现时，我想起了星爷的功夫，那部电影是我第一次知道了闪灵，知道了，一部电影可以向另一部电影致敬，而不能说是抄袭，还好这是一部带有喜剧色彩的电影并没有什么恐怖的镜头，然后剧情发展有些快，第三关IOI是怎么找到的，主角的也没有根据提示获取第三关的情报，不是说好闯过一关才有第二关吗？怎么看IOI都是一直在准备第三关，导演也没有给我们留下思考这个问题的时间，于是开始打雅达利的游戏机，雅达利还是通过敖厂长的介绍才知道的，当年雅达利就是因为开发了斯皮尔伯格拍的ET外星人游戏而走向衰落的，我以为导演要打这部游戏，不是更有趣，最后主角随便玩玩找到了彩蛋，（哈哈，我可不会在我写的代码里放上我的名字，那可就要被下个接手的人骂死了）又经过了最后的考验，当主角问哈利迪死了吗的时候又让我想到了刀剑的茅场晶彦，结局皆大欢喜，住居成功的出任CEO，迎娶白富美，走上人生巅峰。</p>
<p>看完电影让我明白了，游戏还是随便玩玩就好，游戏无法代替现实生活，随着年龄的增长游戏中的胜负显得没那么重要了，能和好朋友开开黑就很美了，还有一个从小就明白的道理，游戏还是要有队友才最好玩。</p>
]]></content>
      <categories>
        <category>影评</category>
      </categories>
  </entry>
  <entry>
    <title>虚拟机搭建Hadoop实验</title>
    <url>/2017/09/14/%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%90%AD%E5%BB%BAHadoop%E5%AE%9E%E9%AA%8C/</url>
    <content><![CDATA[<pre><code>在学习大数据的过程中搭建由三台虚拟机构成的Hadoop模型
</code></pre><h1 id="创建三台虚拟机"><a href="#创建三台虚拟机" class="headerlink" title="创建三台虚拟机"></a>创建三台虚拟机</h1><pre><code>使用软件：Vmware12
Centos7最小镜像
安装一台虚拟机，配置好网络与jdk，复制出两台同样的
</code></pre><p>方法/步骤</p>
<ol>
<li><p>首先我们安装后centos7最小化系统后，并进入系统执行命令ifconfig，会发现系统提示命令未找到。具体展示效果如下图所示。</p>
<p><img src="1.png" alt="image"></p>
</li>
<li><p>然后输入命令查看本机是否分配IP,执行命令ip addr ，可以发现系统的网卡没有分配IP地址，在此我们需要记住本机网卡的名称，用于下一步使用，本篇中我们的网卡为：eno16777736。具体效果如下图所示。</p>
<p><img src="2.png" alt="image"></p>
</li>
<li><p>然后我们进入网卡配置文件的目录。执行命令 cd /etc/sysconfig/network-scripts/ 然后查看下面的网卡文件。具体效果如下图所示。</p>
<p><img src="3.png" alt="image"></p>
</li>
<li><p>然后我们找到对应的网卡文件执行命令 vi ifcfg-eno16777736。进行修改网卡文件，不同机器网卡不同，本篇以自己电脑为例展示。</p>
<p><img src="4.png" alt="image"></p>
</li>
<li><p>我们需要首先找到ONBOOT=no ，需要修改为ONBOOT=yes然后保存退出。</p>
<p><img src="5.png" alt="image"></p>
</li>
<li><p>然后执行命令 service network restart 重启网卡服务。具体操作如下图所示。</p>
<p><img src="6.png" alt="image"></p>
</li>
<li><p>执行完成后，我们再次执行命令 ip addr 查看是否分配到IP地址，可以看到已经分配到IP地址。具体操作如下图所示。</p>
<p><img src="7.png" alt="image"></p>
</li>
<li><p>然后我们执行命令yum provides ifconfig 查看哪个包提供了ifconfig命令，然后可以看到net-tools包提供ifconfig包， 具体操作如下图所示。</p>
<p><img src="8.png" alt="image"></p>
</li>
<li><p>然后我们执行命令安装net-tools包，执行命令：yum install net-tools。具体操作如下图所示。</p>
<p><img src="9.png" alt="image"></p>
</li>
<li><p>然后我们执行命令ifconfig，可以看到可以使用了，而且展示了系统的网卡信息。具体操作如下图所示。</p>
<p><img src="10.png" alt="image"></p>
<figure class="highlight x86asm"><table><tr><td class="code"><pre><span class="line">安装jdk：</span><br><span class="line"><span class="number">1</span>.查看yum库中都有哪些jdk版本(暂时只发现了openjdk)</span><br><span class="line">[root@localhost ~]# yum search java|grep jdk</span><br><span class="line">ldapjdk-javadoc.x86_64 : Javadoc for ldapjdk</span><br><span class="line">java-<span class="number">1.6</span><span class="meta">.0</span>-openjdk.x86_64 : OpenJDK Runtime Environment</span><br><span class="line">java-<span class="number">1.6</span><span class="meta">.0</span>-openjdk-demo.x86_64 : OpenJDK Demos</span><br><span class="line">java-<span class="number">1.6</span><span class="meta">.0</span>-openjdk-devel.x86_64 : OpenJDK Development Environment</span><br><span class="line">java-<span class="number">1.6</span><span class="meta">.0</span>-openjdk-javadoc.x86_64 : OpenJDK API Documentation</span><br><span class="line">java-<span class="number">1.6</span><span class="meta">.0</span>-openjdk-src.x86_64 : OpenJDK Source Bundle</span><br><span class="line">java-<span class="number">1.7</span><span class="meta">.0</span>-openjdk.x86_64 : OpenJDK Runtime Environment</span><br><span class="line">java-<span class="number">1.7</span><span class="meta">.0</span>-openjdk-demo.x86_64 : OpenJDK Demos</span><br><span class="line">java-<span class="number">1.7</span><span class="meta">.0</span>-openjdk-devel.x86_64 : OpenJDK Development Environment</span><br><span class="line">java-<span class="number">1.7</span><span class="meta">.0</span>-openjdk-javadoc.noarch : OpenJDK API Documentation</span><br><span class="line">java-<span class="number">1.7</span><span class="meta">.0</span>-openjdk-src.x86_64 : OpenJDK Source Bundle</span><br><span class="line">java-<span class="number">1.8</span><span class="meta">.0</span>-openjdk.x86_64 : OpenJDK Runtime Environment</span><br><span class="line">java-<span class="number">1.8</span><span class="meta">.0</span>-openjdk-demo.x86_64 : OpenJDK Demos</span><br><span class="line">java-<span class="number">1.8</span><span class="meta">.0</span>-openjdk-devel.x86_64 : OpenJDK Development Environment</span><br><span class="line">java-<span class="number">1.8</span><span class="meta">.0</span>-openjdk-headless.x86_64 : OpenJDK Runtime Environment</span><br><span class="line">java-<span class="number">1.8</span><span class="meta">.0</span>-openjdk-javadoc.noarch : OpenJDK API Documentation</span><br><span class="line">java-<span class="number">1.8</span><span class="meta">.0</span>-openjdk-src.x86_64 : OpenJDK Source Bundle</span><br><span class="line">ldapjdk.x86_64 : The Mozilla LDAP Java SDK</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>.选择版本,进行安装</span><br><span class="line">//选择<span class="number">1.7</span>版本进行安装</span><br><span class="line">[root@localhost ~]# yum install java-<span class="number">1.7</span><span class="meta">.0</span>-openjdk</span><br><span class="line">//安装完之后，默认的安装目录是在: /usr/lib/jvm/java-<span class="number">1.7</span><span class="meta">.0</span>-openjdk-<span class="number">1.7</span><span class="meta">.0</span><span class="meta">.75</span>.x86_64</span><br><span class="line"></span><br><span class="line"><span class="number">3</span>.设置环境变量 （如果已经有java命令不用设置）</span><br><span class="line">[root@localhost ~]# vi /etc/profile</span><br><span class="line">在profile文件中添加如下内容</span><br><span class="line">#set java environment</span><br><span class="line">JAVA_HOME=/usr/lib/jvm/java-<span class="number">1.7</span><span class="meta">.0</span>-openjdk-<span class="number">1.7</span><span class="meta">.0</span><span class="meta">.75</span>.x86_64</span><br><span class="line">JRE_HOME=$JAVA_HOME/jre</span><br><span class="line">CLASS_PATH=.:$JAVA_HOME/lib/<span class="built_in">dt</span>.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib</span><br><span class="line">PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin</span><br><span class="line">export JAVA_HOME JRE_HOME CLASS_PATH PATH</span><br><span class="line"></span><br><span class="line"><span class="number">4</span>.让修改生效</span><br><span class="line">[root@localhost java]# source /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="number">5</span>.查看刚安装的Java版本信息。</span><br><span class="line">◆输入：java -version 可查看Java版本；</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>配置hosts<br>          vi /etc/hosts<br>          <img src="11.png" alt="image"><br>         说明：slaver217,slaver214作为datanode节点，master204作为namenode节点。另外，各datanode节点主机上只需配置如：172.16.51.214 slaver214。</p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title>微博爬虫</title>
    <url>/2019/07/25/%E7%AE%80%E5%8D%95%E7%9A%84%E5%BE%AE%E5%8D%9A%E7%88%AC%E8%99%AB/</url>
    <content><![CDATA[<p>准备写论文了，需要爬取些互联网的数据进行分析下，Pyhton爬虫是一个比较简单的方法，虽然有许多的爬虫框架但是我并不需要太复杂的爬虫，所以决定从零开始写一个简单的爬虫爬一下微博和评论。</p>
<h1 id="准备工具"><a href="#准备工具" class="headerlink" title="准备工具"></a>准备工具</h1><p>爬虫就是一个分析HTTP请求的工作，首先需要一些抓取http的工具，推荐使用 Fiddler，简单免费。</p>
<p>还需要浏览器 GoogleChrome</p>
<h1 id="分析微博网站"><a href="#分析微博网站" class="headerlink" title="分析微博网站"></a>分析微博网站</h1><p>微博有三个入口，分别是</p>
<pre><code>weibo.com
weibo.cn
m.weibo.cn
</code></pre><p>第一个是PC网页版，后两个是手机版，手机版网站结构相对简单爬取容易，所以选择<code>m.weibo.cn</code>为爬取对象。</p>
<p>打开浏览器开发者模式，观察Network，在热门频道刷新，发现一个API</p>
<p><img src="weibo1.png" alt="image"></p>
<pre><code>https://m.weibo.cn/api/container/getIndex?containerid=102803&amp;openApp=0
</code></pre><p>这个地址的response就是热门微博的数据，接下来寻找翻页的方法，向下刷新观察发现一个地址在变化。</p>
<pre><code>https://m.weibo.cn/api/container/getIndex?containerid=102803&amp;openApp=0&amp;since_id=1
</code></pre><p><img src="weibo2.png" alt="image"></p>
<p><code>since_id</code>这个参数是控制翻页的。</p>
<p>用同样的方法观察评论页面</p>
<pre><code>https://m.weibo.cn/detail/{微博编号}
</code></pre><p>这个地址就是具体微博的详细，</p>
<pre><code>https://m.weibo.cn/comments/hotflow?id={mblogid}&amp;mid={mblogid}&amp;max_id={max_id}&amp;max_id_type=0
</code></pre><p><code>mblogid</code>是微博的编号，<code>max_id</code>控制翻页，前一个request会告诉这个id的数，如果是0则表示最后一页。</p>
<p>微博页面分析完毕，接下来编写代码模仿浏览器发Http请求就OK了。</p>
<h1 id="编写代码"><a href="#编写代码" class="headerlink" title="编写代码"></a>编写代码</h1><p>首先我们需要用到<code>urllib</code>这个库来发送请求。<br>还需要<code>json</code>化一下数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># env python3.7</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 热门微博的url</span></span><br><span class="line">HotWeiBoUrl = <span class="string">'https://m.weibo.cn/api/container/getIndex?containerid=102803&amp;since_id=&#123;sinceid&#125;'</span></span><br><span class="line"><span class="comment"># 微博评论的Url</span></span><br><span class="line">WeiBoCommentsUrl = <span class="string">'https://m.weibo.cn/comments/hotflow?id=&#123;mblogid&#125;&amp;mid=&#123;mblogid&#125;&amp;max_id=&#123;max_id&#125;&amp;max_id_type=0'</span></span><br></pre></td></tr></table></figure>
<p>首先访问热门微博页面，解析当前页面的热门微博的response，提取出每个微博的内容和地址。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GetHotWeiBoData</span><span class="params">(url)</span>:</span></span><br><span class="line">    scheme_url = []</span><br><span class="line">    body_text = []</span><br><span class="line"></span><br><span class="line">    req = request.Request(url)</span><br><span class="line">    req.add_header(<span class="string">'User-Agent'</span>, <span class="string">'Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25'</span>)</span><br><span class="line">    <span class="keyword">with</span> request.urlopen(req) <span class="keyword">as</span> f:</span><br><span class="line">        print(<span class="string">'Status:'</span>, f.status, f.reason)</span><br><span class="line">        <span class="keyword">if</span> f.status == <span class="number">200</span>:</span><br><span class="line">            data = f.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">            data = json.loads(data)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> data[<span class="string">'data'</span>][<span class="string">'cards'</span>]:</span><br><span class="line">                scheme_url.append(i[<span class="string">'scheme'</span>])</span><br><span class="line">                body_text.append(i[<span class="string">'mblog'</span>][<span class="string">'text'</span>])</span><br><span class="line">            <span class="keyword">return</span> body_text, scheme_url</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">None</span>, <span class="keyword">None</span></span><br></pre></td></tr></table></figure>
<p>获取评论和下一页评论</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GetCommentsData</span><span class="params">(comments_url)</span>:</span></span><br><span class="line">    text = []</span><br><span class="line">    req = request.Request(comments_url)</span><br><span class="line">    req.add_header(<span class="string">'User-Agent'</span>, <span class="string">'Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25'</span>)</span><br><span class="line">    print(comments_url)</span><br><span class="line">    <span class="keyword">with</span> request.urlopen(req) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">if</span> f.status == <span class="number">200</span>:</span><br><span class="line">            data = f.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">            data = json.loads(data)</span><br><span class="line">            <span class="keyword">if</span> <span class="string">"data"</span> <span class="keyword">in</span> data:</span><br><span class="line">                data = data[<span class="string">"data"</span>]</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> data[<span class="string">"data"</span>]:</span><br><span class="line">                    text.append(i[<span class="string">"text"</span>])</span><br><span class="line">                next_id = data[<span class="string">'max_id'</span>]</span><br><span class="line">                <span class="keyword">return</span> text, next_id</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">None</span>, <span class="keyword">None</span></span><br></pre></td></tr></table></figure>
<p>循环执行上面两个函数，就可以了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GetCommentsUrl</span><span class="params">(url, max_id=<span class="number">0</span>)</span>:</span></span><br><span class="line">    parsed_tuple = parse.urlparse(url)</span><br><span class="line">    mblogid = parsed_tuple.path[<span class="number">8</span>:]</span><br><span class="line">    comments_url = WeiBoCommentsUrl.format(mblogid=mblogid, max_id=max_id)</span><br><span class="line">    <span class="keyword">return</span> comments_url, mblogid</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GetWeiBo</span><span class="params">(sinceid)</span>:</span></span><br><span class="line">    data = &#123;&#125;</span><br><span class="line">    url = HotWeiBoUrl.format(sinceid=<span class="number">0</span>)</span><br><span class="line">    body_text, scheme_url = GetHotWeiBoData(url)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(scheme_url)):</span><br><span class="line">        url = scheme_url[i]</span><br><span class="line">        body = body_text[i]</span><br><span class="line">        comments_url, mblogid = GetCommentsUrl(url)</span><br><span class="line">        comments, next_id = GetCommentsData(comments_url)</span><br><span class="line">        <span class="keyword">if</span> comments:</span><br><span class="line">            data[mblogid] = &#123;&#125;</span><br><span class="line">            data[mblogid][<span class="string">'comments'</span>] = []</span><br><span class="line">            data[mblogid][<span class="string">'comments'</span>].extend(comments)</span><br><span class="line">            data[mblogid][<span class="string">'body'</span>] = body</span><br><span class="line"></span><br><span class="line">        <span class="comment"># while next_id != 0:</span></span><br><span class="line">        <span class="comment">#    comments_url, mblogid = GetCommentsUrl(url, next_id)</span></span><br><span class="line">        <span class="comment">#    comments, next_id = GetCommentsData(comments_url)</span></span><br><span class="line">        <span class="comment">#    data[mblogid]['comments'].extend(comments)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>
<p>把数据保存起来</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SaveData</span><span class="params">(data)</span>:</span></span><br><span class="line">    data = json.dumps(data, ensure_ascii=<span class="keyword">False</span>) + <span class="string">"\n"</span></span><br><span class="line">    WeiBoData.write(data)</span><br><span class="line">    <span class="comment"># 关闭打开的文件</span></span><br><span class="line">    <span class="comment"># fo.close()</span></span><br></pre></td></tr></table></figure>
<p>让爬虫开始工作，为防止被禁，把爬取频率调低点。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 打开一个文件 a 已追加的方式打开，编码方式为 utf-8</span></span><br><span class="line">WeiBoData = open(<span class="string">"./WeiBoData.txt"</span>, <span class="string">"a"</span>, encoding=<span class="string">"utf-8"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    data = GetWeiBo(<span class="number">0</span>)</span><br><span class="line">    SaveData(data)</span><br><span class="line">    time.sleep(random.randint(<span class="number">280</span>, <span class="number">320</span>))</span><br></pre></td></tr></table></figure>
<p>如果想要爬取多页评论，需要登陆操作，设置 cookie。</p>
]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>House Prices Advanced Regression Techniques V2</title>
    <url>/2019/05/28/House_Prices_Advanced_Regression_Techniques_V2/</url>
    <content><![CDATA[<p><img src="https://storage.googleapis.com/kaggle-competitions/kaggle/5407/media/housesbanner.png" alt="替代文字"></p>
<p>Kaggle Competition 的练习</p>
<p><a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques" target="_blank" rel="noopener">房价预测</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 安装 vecstack</span></span><br><span class="line">!pip install vecstack</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据分析库</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 机器学习库</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC, LinearSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Perceptron</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score, mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV, cross_val_score, StratifiedKFold, learning_curve, KFold, train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ensemble Models</span></span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> LGBMRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># Package for stacking models</span></span><br><span class="line"><span class="keyword">from</span> vecstack <span class="keyword">import</span> stacking</span><br></pre></td></tr></table></figure>
<h1 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">'/train.csv'</span>, index_col=<span class="string">'Id'</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">'/test.csv'</span>, index_col=<span class="string">'Id'</span>)</span><br><span class="line"></span><br><span class="line">train.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MSSubClass</th>
      <th>MSZoning</th>
      <th>LotFrontage</th>
      <th>LotArea</th>
      <th>Street</th>
      <th>Alley</th>
      <th>LotShape</th>
      <th>LandContour</th>
      <th>Utilities</th>
      <th>LotConfig</th>
      <th>LandSlope</th>
      <th>Neighborhood</th>
      <th>Condition1</th>
      <th>Condition2</th>
      <th>BldgType</th>
      <th>HouseStyle</th>
      <th>OverallQual</th>
      <th>OverallCond</th>
      <th>YearBuilt</th>
      <th>YearRemodAdd</th>
      <th>RoofStyle</th>
      <th>RoofMatl</th>
      <th>Exterior1st</th>
      <th>Exterior2nd</th>
      <th>MasVnrType</th>
      <th>MasVnrArea</th>
      <th>ExterQual</th>
      <th>ExterCond</th>
      <th>Foundation</th>
      <th>BsmtQual</th>
      <th>BsmtCond</th>
      <th>BsmtExposure</th>
      <th>BsmtFinType1</th>
      <th>BsmtFinSF1</th>
      <th>BsmtFinType2</th>
      <th>BsmtFinSF2</th>
      <th>BsmtUnfSF</th>
      <th>TotalBsmtSF</th>
      <th>Heating</th>
      <th>HeatingQC</th>
      <th>CentralAir</th>
      <th>Electrical</th>
      <th>1stFlrSF</th>
      <th>2ndFlrSF</th>
      <th>LowQualFinSF</th>
      <th>GrLivArea</th>
      <th>BsmtFullBath</th>
      <th>BsmtHalfBath</th>
      <th>FullBath</th>
      <th>HalfBath</th>
      <th>BedroomAbvGr</th>
      <th>KitchenAbvGr</th>
      <th>KitchenQual</th>
      <th>TotRmsAbvGrd</th>
      <th>Functional</th>
      <th>Fireplaces</th>
      <th>FireplaceQu</th>
      <th>GarageType</th>
      <th>GarageYrBlt</th>
      <th>GarageFinish</th>
      <th>GarageCars</th>
      <th>GarageArea</th>
      <th>GarageQual</th>
      <th>GarageCond</th>
      <th>PavedDrive</th>
      <th>WoodDeckSF</th>
      <th>OpenPorchSF</th>
      <th>EnclosedPorch</th>
      <th>3SsnPorch</th>
      <th>ScreenPorch</th>
      <th>PoolArea</th>
      <th>PoolQC</th>
      <th>Fence</th>
      <th>MiscFeature</th>
      <th>MiscVal</th>
      <th>MoSold</th>
      <th>YrSold</th>
      <th>SaleType</th>
      <th>SaleCondition</th>
      <th>SalePrice</th>
    </tr>
    <tr>
      <th>Id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>60</td>
      <td>RL</td>
      <td>65.0</td>
      <td>8450</td>
      <td>Pave</td>
      <td>NaN</td>
      <td>Reg</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>Inside</td>
      <td>Gtl</td>
      <td>CollgCr</td>
      <td>Norm</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>2Story</td>
      <td>7</td>
      <td>5</td>
      <td>2003</td>
      <td>2003</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>VinylSd</td>
      <td>VinylSd</td>
      <td>BrkFace</td>
      <td>196.0</td>
      <td>Gd</td>
      <td>TA</td>
      <td>PConc</td>
      <td>Gd</td>
      <td>TA</td>
      <td>No</td>
      <td>GLQ</td>
      <td>706</td>
      <td>Unf</td>
      <td>0</td>
      <td>150</td>
      <td>856</td>
      <td>GasA</td>
      <td>Ex</td>
      <td>Y</td>
      <td>SBrkr</td>
      <td>856</td>
      <td>854</td>
      <td>0</td>
      <td>1710</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>Gd</td>
      <td>8</td>
      <td>Typ</td>
      <td>0</td>
      <td>NaN</td>
      <td>Attchd</td>
      <td>2003.0</td>
      <td>RFn</td>
      <td>2</td>
      <td>548</td>
      <td>TA</td>
      <td>TA</td>
      <td>Y</td>
      <td>0</td>
      <td>61</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0</td>
      <td>2</td>
      <td>2008</td>
      <td>WD</td>
      <td>Normal</td>
      <td>208500</td>
    </tr>
    <tr>
      <th>2</th>
      <td>20</td>
      <td>RL</td>
      <td>80.0</td>
      <td>9600</td>
      <td>Pave</td>
      <td>NaN</td>
      <td>Reg</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>FR2</td>
      <td>Gtl</td>
      <td>Veenker</td>
      <td>Feedr</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>1Story</td>
      <td>6</td>
      <td>8</td>
      <td>1976</td>
      <td>1976</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>MetalSd</td>
      <td>MetalSd</td>
      <td>None</td>
      <td>0.0</td>
      <td>TA</td>
      <td>TA</td>
      <td>CBlock</td>
      <td>Gd</td>
      <td>TA</td>
      <td>Gd</td>
      <td>ALQ</td>
      <td>978</td>
      <td>Unf</td>
      <td>0</td>
      <td>284</td>
      <td>1262</td>
      <td>GasA</td>
      <td>Ex</td>
      <td>Y</td>
      <td>SBrkr</td>
      <td>1262</td>
      <td>0</td>
      <td>0</td>
      <td>1262</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>TA</td>
      <td>6</td>
      <td>Typ</td>
      <td>1</td>
      <td>TA</td>
      <td>Attchd</td>
      <td>1976.0</td>
      <td>RFn</td>
      <td>2</td>
      <td>460</td>
      <td>TA</td>
      <td>TA</td>
      <td>Y</td>
      <td>298</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0</td>
      <td>5</td>
      <td>2007</td>
      <td>WD</td>
      <td>Normal</td>
      <td>181500</td>
    </tr>
    <tr>
      <th>3</th>
      <td>60</td>
      <td>RL</td>
      <td>68.0</td>
      <td>11250</td>
      <td>Pave</td>
      <td>NaN</td>
      <td>IR1</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>Inside</td>
      <td>Gtl</td>
      <td>CollgCr</td>
      <td>Norm</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>2Story</td>
      <td>7</td>
      <td>5</td>
      <td>2001</td>
      <td>2002</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>VinylSd</td>
      <td>VinylSd</td>
      <td>BrkFace</td>
      <td>162.0</td>
      <td>Gd</td>
      <td>TA</td>
      <td>PConc</td>
      <td>Gd</td>
      <td>TA</td>
      <td>Mn</td>
      <td>GLQ</td>
      <td>486</td>
      <td>Unf</td>
      <td>0</td>
      <td>434</td>
      <td>920</td>
      <td>GasA</td>
      <td>Ex</td>
      <td>Y</td>
      <td>SBrkr</td>
      <td>920</td>
      <td>866</td>
      <td>0</td>
      <td>1786</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>Gd</td>
      <td>6</td>
      <td>Typ</td>
      <td>1</td>
      <td>TA</td>
      <td>Attchd</td>
      <td>2001.0</td>
      <td>RFn</td>
      <td>2</td>
      <td>608</td>
      <td>TA</td>
      <td>TA</td>
      <td>Y</td>
      <td>0</td>
      <td>42</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0</td>
      <td>9</td>
      <td>2008</td>
      <td>WD</td>
      <td>Normal</td>
      <td>223500</td>
    </tr>
    <tr>
      <th>4</th>
      <td>70</td>
      <td>RL</td>
      <td>60.0</td>
      <td>9550</td>
      <td>Pave</td>
      <td>NaN</td>
      <td>IR1</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>Corner</td>
      <td>Gtl</td>
      <td>Crawfor</td>
      <td>Norm</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>2Story</td>
      <td>7</td>
      <td>5</td>
      <td>1915</td>
      <td>1970</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>Wd Sdng</td>
      <td>Wd Shng</td>
      <td>None</td>
      <td>0.0</td>
      <td>TA</td>
      <td>TA</td>
      <td>BrkTil</td>
      <td>TA</td>
      <td>Gd</td>
      <td>No</td>
      <td>ALQ</td>
      <td>216</td>
      <td>Unf</td>
      <td>0</td>
      <td>540</td>
      <td>756</td>
      <td>GasA</td>
      <td>Gd</td>
      <td>Y</td>
      <td>SBrkr</td>
      <td>961</td>
      <td>756</td>
      <td>0</td>
      <td>1717</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>Gd</td>
      <td>7</td>
      <td>Typ</td>
      <td>1</td>
      <td>Gd</td>
      <td>Detchd</td>
      <td>1998.0</td>
      <td>Unf</td>
      <td>3</td>
      <td>642</td>
      <td>TA</td>
      <td>TA</td>
      <td>Y</td>
      <td>0</td>
      <td>35</td>
      <td>272</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0</td>
      <td>2</td>
      <td>2006</td>
      <td>WD</td>
      <td>Abnorml</td>
      <td>140000</td>
    </tr>
    <tr>
      <th>5</th>
      <td>60</td>
      <td>RL</td>
      <td>84.0</td>
      <td>14260</td>
      <td>Pave</td>
      <td>NaN</td>
      <td>IR1</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>FR2</td>
      <td>Gtl</td>
      <td>NoRidge</td>
      <td>Norm</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>2Story</td>
      <td>8</td>
      <td>5</td>
      <td>2000</td>
      <td>2000</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>VinylSd</td>
      <td>VinylSd</td>
      <td>BrkFace</td>
      <td>350.0</td>
      <td>Gd</td>
      <td>TA</td>
      <td>PConc</td>
      <td>Gd</td>
      <td>TA</td>
      <td>Av</td>
      <td>GLQ</td>
      <td>655</td>
      <td>Unf</td>
      <td>0</td>
      <td>490</td>
      <td>1145</td>
      <td>GasA</td>
      <td>Ex</td>
      <td>Y</td>
      <td>SBrkr</td>
      <td>1145</td>
      <td>1053</td>
      <td>0</td>
      <td>2198</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>Gd</td>
      <td>9</td>
      <td>Typ</td>
      <td>1</td>
      <td>TA</td>
      <td>Attchd</td>
      <td>2000.0</td>
      <td>RFn</td>
      <td>3</td>
      <td>836</td>
      <td>TA</td>
      <td>TA</td>
      <td>Y</td>
      <td>192</td>
      <td>84</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0</td>
      <td>12</td>
      <td>2008</td>
      <td>WD</td>
      <td>Normal</td>
      <td>250000</td>
    </tr>
  </tbody>
</table>
</div>

<h1 id="浏览数据"><a href="#浏览数据" class="headerlink" title="浏览数据"></a>浏览数据</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train.info()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;
Int64Index: 1460 entries, 1 to 1460
Data columns (total 80 columns):
MSSubClass       1460 non-null int64
MSZoning         1460 non-null object
LotFrontage      1201 non-null float64
LotArea          1460 non-null int64
Street           1460 non-null object
Alley            91 non-null object
LotShape         1460 non-null object
LandContour      1460 non-null object
Utilities        1460 non-null object
LotConfig        1460 non-null object
LandSlope        1460 non-null object
Neighborhood     1460 non-null object
Condition1       1460 non-null object
Condition2       1460 non-null object
BldgType         1460 non-null object
HouseStyle       1460 non-null object
OverallQual      1460 non-null int64
OverallCond      1460 non-null int64
YearBuilt        1460 non-null int64
YearRemodAdd     1460 non-null int64
RoofStyle        1460 non-null object
RoofMatl         1460 non-null object
Exterior1st      1460 non-null object
Exterior2nd      1460 non-null object
MasVnrType       1452 non-null object
MasVnrArea       1452 non-null float64
ExterQual        1460 non-null object
ExterCond        1460 non-null object
Foundation       1460 non-null object
BsmtQual         1423 non-null object
BsmtCond         1423 non-null object
BsmtExposure     1422 non-null object
BsmtFinType1     1423 non-null object
BsmtFinSF1       1460 non-null int64
BsmtFinType2     1422 non-null object
BsmtFinSF2       1460 non-null int64
BsmtUnfSF        1460 non-null int64
TotalBsmtSF      1460 non-null int64
Heating          1460 non-null object
HeatingQC        1460 non-null object
CentralAir       1460 non-null object
Electrical       1459 non-null object
1stFlrSF         1460 non-null int64
2ndFlrSF         1460 non-null int64
LowQualFinSF     1460 non-null int64
GrLivArea        1460 non-null int64
BsmtFullBath     1460 non-null int64
BsmtHalfBath     1460 non-null int64
FullBath         1460 non-null int64
HalfBath         1460 non-null int64
BedroomAbvGr     1460 non-null int64
KitchenAbvGr     1460 non-null int64
KitchenQual      1460 non-null object
TotRmsAbvGrd     1460 non-null int64
Functional       1460 non-null object
Fireplaces       1460 non-null int64
FireplaceQu      770 non-null object
GarageType       1379 non-null object
GarageYrBlt      1379 non-null float64
GarageFinish     1379 non-null object
GarageCars       1460 non-null int64
GarageArea       1460 non-null int64
GarageQual       1379 non-null object
GarageCond       1379 non-null object
PavedDrive       1460 non-null object
WoodDeckSF       1460 non-null int64
OpenPorchSF      1460 non-null int64
EnclosedPorch    1460 non-null int64
3SsnPorch        1460 non-null int64
ScreenPorch      1460 non-null int64
PoolArea         1460 non-null int64
PoolQC           7 non-null object
Fence            281 non-null object
MiscFeature      54 non-null object
MiscVal          1460 non-null int64
MoSold           1460 non-null int64
YrSold           1460 non-null int64
SaleType         1460 non-null object
SaleCondition    1460 non-null object
SalePrice        1460 non-null int64
dtypes: float64(3), int64(34), object(43)
memory usage: 923.9+ KB
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train.describe(include=<span class="string">"O"</span>)</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MSZoning</th>
      <th>Street</th>
      <th>Alley</th>
      <th>LotShape</th>
      <th>LandContour</th>
      <th>Utilities</th>
      <th>LotConfig</th>
      <th>LandSlope</th>
      <th>Neighborhood</th>
      <th>Condition1</th>
      <th>Condition2</th>
      <th>BldgType</th>
      <th>HouseStyle</th>
      <th>RoofStyle</th>
      <th>RoofMatl</th>
      <th>Exterior1st</th>
      <th>Exterior2nd</th>
      <th>MasVnrType</th>
      <th>ExterQual</th>
      <th>ExterCond</th>
      <th>Foundation</th>
      <th>BsmtQual</th>
      <th>BsmtCond</th>
      <th>BsmtExposure</th>
      <th>BsmtFinType1</th>
      <th>BsmtFinType2</th>
      <th>Heating</th>
      <th>HeatingQC</th>
      <th>CentralAir</th>
      <th>Electrical</th>
      <th>KitchenQual</th>
      <th>Functional</th>
      <th>FireplaceQu</th>
      <th>GarageType</th>
      <th>GarageFinish</th>
      <th>GarageQual</th>
      <th>GarageCond</th>
      <th>PavedDrive</th>
      <th>PoolQC</th>
      <th>Fence</th>
      <th>MiscFeature</th>
      <th>SaleType</th>
      <th>SaleCondition</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1460</td>
      <td>1460</td>
      <td>91</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1452</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1423</td>
      <td>1423</td>
      <td>1422</td>
      <td>1423</td>
      <td>1422</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1459</td>
      <td>1460</td>
      <td>1460</td>
      <td>770</td>
      <td>1379</td>
      <td>1379</td>
      <td>1379</td>
      <td>1379</td>
      <td>1460</td>
      <td>7</td>
      <td>281</td>
      <td>54</td>
      <td>1460</td>
      <td>1460</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>5</td>
      <td>2</td>
      <td>2</td>
      <td>4</td>
      <td>4</td>
      <td>2</td>
      <td>5</td>
      <td>3</td>
      <td>25</td>
      <td>9</td>
      <td>8</td>
      <td>5</td>
      <td>8</td>
      <td>6</td>
      <td>8</td>
      <td>15</td>
      <td>16</td>
      <td>4</td>
      <td>4</td>
      <td>5</td>
      <td>6</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>6</td>
      <td>6</td>
      <td>6</td>
      <td>5</td>
      <td>2</td>
      <td>5</td>
      <td>4</td>
      <td>7</td>
      <td>5</td>
      <td>6</td>
      <td>3</td>
      <td>5</td>
      <td>5</td>
      <td>3</td>
      <td>3</td>
      <td>4</td>
      <td>4</td>
      <td>9</td>
      <td>6</td>
    </tr>
    <tr>
      <th>top</th>
      <td>RL</td>
      <td>Pave</td>
      <td>Grvl</td>
      <td>Reg</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>Inside</td>
      <td>Gtl</td>
      <td>NAmes</td>
      <td>Norm</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>1Story</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>VinylSd</td>
      <td>VinylSd</td>
      <td>None</td>
      <td>TA</td>
      <td>TA</td>
      <td>PConc</td>
      <td>TA</td>
      <td>TA</td>
      <td>No</td>
      <td>Unf</td>
      <td>Unf</td>
      <td>GasA</td>
      <td>Ex</td>
      <td>Y</td>
      <td>SBrkr</td>
      <td>TA</td>
      <td>Typ</td>
      <td>Gd</td>
      <td>Attchd</td>
      <td>Unf</td>
      <td>TA</td>
      <td>TA</td>
      <td>Y</td>
      <td>Gd</td>
      <td>MnPrv</td>
      <td>Shed</td>
      <td>WD</td>
      <td>Normal</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>1151</td>
      <td>1454</td>
      <td>50</td>
      <td>925</td>
      <td>1311</td>
      <td>1459</td>
      <td>1052</td>
      <td>1382</td>
      <td>225</td>
      <td>1260</td>
      <td>1445</td>
      <td>1220</td>
      <td>726</td>
      <td>1141</td>
      <td>1434</td>
      <td>515</td>
      <td>504</td>
      <td>864</td>
      <td>906</td>
      <td>1282</td>
      <td>647</td>
      <td>649</td>
      <td>1311</td>
      <td>953</td>
      <td>430</td>
      <td>1256</td>
      <td>1428</td>
      <td>741</td>
      <td>1365</td>
      <td>1334</td>
      <td>735</td>
      <td>1360</td>
      <td>380</td>
      <td>870</td>
      <td>605</td>
      <td>1311</td>
      <td>1326</td>
      <td>1340</td>
      <td>3</td>
      <td>157</td>
      <td>49</td>
      <td>1267</td>
      <td>1198</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="检查缺失数据"><a href="#检查缺失数据" class="headerlink" title="检查缺失数据"></a>检查缺失数据</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_missing = train.isnull().sum()</span><br><span class="line">train_missing = train_missing[train_missing &gt; <span class="number">0</span>]</span><br><span class="line">train_missing</span><br></pre></td></tr></table></figure>
<pre><code>LotFrontage      259
Alley           1369
MasVnrType         8
MasVnrArea         8
BsmtQual          37
BsmtCond          37
BsmtExposure      38
BsmtFinType1      37
BsmtFinType2      38
Electrical         1
FireplaceQu      690
GarageType        81
GarageYrBlt       81
GarageFinish      81
GarageQual        81
GarageCond        81
PoolQC          1453
Fence           1179
MiscFeature     1406
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_missing = test.isnull().sum()</span><br><span class="line">test_missing = test_missing[test_missing &gt; <span class="number">0</span>]</span><br><span class="line">test_missing</span><br></pre></td></tr></table></figure>
<pre><code>MSZoning           4
LotFrontage      227
Alley           1352
Utilities          2
Exterior1st        1
Exterior2nd        1
MasVnrType        16
MasVnrArea        15
BsmtQual          44
BsmtCond          45
BsmtExposure      44
BsmtFinType1      42
BsmtFinSF1         1
BsmtFinType2      42
BsmtFinSF2         1
BsmtUnfSF          1
TotalBsmtSF        1
BsmtFullBath       2
BsmtHalfBath       2
KitchenQual        1
Functional         2
FireplaceQu      730
GarageType        76
GarageYrBlt       78
GarageFinish      78
GarageCars         1
GarageArea         1
GarageQual        78
GarageCond        78
PoolQC          1456
Fence           1169
MiscFeature     1408
SaleType           1
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可视化缺失数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_missing</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="comment"># 寻找缺失的列</span></span><br><span class="line">    missing = df.isnull().sum()</span><br><span class="line">    missing = missing[missing &gt; <span class="number">0</span>]</span><br><span class="line">    missing.sort_values(inplace=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 画出缺失值的柱状图。</span></span><br><span class="line">    missing.plot.bar(figsize=(<span class="number">10</span>,<span class="number">8</span>))</span><br><span class="line">    plt.xlabel(<span class="string">'Columns with missing values'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Count'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 搜索缺失值</span></span><br><span class="line">    <span class="keyword">import</span> missingno <span class="keyword">as</span> msno</span><br><span class="line">    msno.matrix(df=df, figsize=(<span class="number">10</span>,<span class="number">8</span>))</span><br><span class="line">    <span class="comment"># 查看相关性</span></span><br><span class="line">    <span class="comment">#msno.heatmap(df=df,figsize=(10,8))</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_missing(train)</span><br></pre></td></tr></table></figure>
<p><img src="House_Prices_Advanced_Regression_Techniques_V2_13_0.png" alt="png"></p>
<p><img src="House_Prices_Advanced_Regression_Techniques_V2_13_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_missing(test)</span><br></pre></td></tr></table></figure>
<p><img src="House_Prices_Advanced_Regression_Techniques_V2_14_0.png" alt="png"></p>
<p><img src="House_Prices_Advanced_Regression_Techniques_V2_14_1.png" alt="png"></p>
<h1 id="分析概要"><a href="#分析概要" class="headerlink" title="分析概要"></a>分析概要</h1><p>以下是缺失比较多的feature</p>
<table>
<thead>
<tr>
<th style="text-align:center">Feature</th>
<th style="text-align:center">Train miss</th>
<th style="text-align:center">Test miss</th>
<th style="text-align:center">Dispos </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">LotFrontage</td>
<td style="text-align:center">259</td>
<td style="text-align:center">227</td>
<td style="text-align:center">填个中位数吧</td>
</tr>
<tr>
<td style="text-align:center">Alley</td>
<td style="text-align:center">1369</td>
<td style="text-align:center">1352</td>
<td style="text-align:center">删除 </td>
</tr>
<tr>
<td style="text-align:center">FireplaceQu</td>
<td style="text-align:center">690</td>
<td style="text-align:center">730</td>
<td style="text-align:center">fireplaceQU 和 fireplaces 有关 缺失项貌似都是没有fireplace的</td>
</tr>
<tr>
<td style="text-align:center">PoolQC</td>
<td style="text-align:center">1453</td>
<td style="text-align:center">1456</td>
<td style="text-align:center">删除</td>
</tr>
<tr>
<td style="text-align:center">Fence</td>
<td style="text-align:center">1179</td>
<td style="text-align:center">1169</td>
<td style="text-align:center">删除</td>
</tr>
<tr>
<td style="text-align:center">MiscFeature</td>
<td style="text-align:center">1406</td>
<td style="text-align:center">1408</td>
<td style="text-align:center">删除</td>
</tr>
</tbody>
</table>
<h1 id="处理缺失值"><a href="#处理缺失值" class="headerlink" title="处理缺失值"></a>处理缺失值</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 删除缺失过多的feature</span></span><br><span class="line">train = train.drop([<span class="string">'Alley'</span>, <span class="string">'PoolQC'</span>, <span class="string">'MiscFeature'</span>, <span class="string">'Fence'</span>], axis=<span class="number">1</span>)</span><br><span class="line">test = test.drop([<span class="string">'Alley'</span>, <span class="string">'PoolQC'</span>, <span class="string">'MiscFeature'</span>, <span class="string">'Fence'</span>], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># FireplaceQu 缺失的都认为是没有的</span></span><br><span class="line">train[<span class="string">'FireplaceQu'</span>] = train[<span class="string">'FireplaceQu'</span>].fillna(<span class="string">'NA'</span>)</span><br><span class="line">test[<span class="string">'FireplaceQu'</span>] = test[<span class="string">'FireplaceQu'</span>].fillna(<span class="string">'NA'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 填充其他缺失值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fill_missing_values</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="comment"># 查找缺失值</span></span><br><span class="line">    missing = df.isnull().sum()</span><br><span class="line">    missing = missing[missing &gt; <span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> column <span class="keyword">in</span> missing.index:</span><br><span class="line">        <span class="comment"># 类型为 object 的填众数</span></span><br><span class="line">        <span class="keyword">if</span> df[column].dtype == <span class="string">'object'</span>:</span><br><span class="line">            df[column].fillna(df[column].value_counts().index[<span class="number">0</span>], inplace=<span class="keyword">True</span>)</span><br><span class="line">        <span class="comment"># 其他类型填中位数</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            df[column].fillna(df[column].median(), inplace=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fill_missing_values(train)</span><br><span class="line">train.isnull().sum().max()</span><br></pre></td></tr></table></figure>
<pre><code>0
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fill_missing_values(test)</span><br><span class="line">test.isnull().sum().max()</span><br></pre></td></tr></table></figure>
<pre><code>0
</code></pre><p>好的，已经没有缺失数据了。</p>
<h2 id="整理-description-文件"><a href="#整理-description-文件" class="headerlink" title="整理 description 文件"></a>整理 description 文件</h2><p>数据描述文件记录了所有特征所代表的含义，其中许多特征是字符串，现在我们要整理为个字典，便于我们查询。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">description_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'/data_description.txt'</span>,<span class="string">'r'</span>) <span class="keyword">as</span> description:</span><br><span class="line">    description_data = description.read()</span><br><span class="line">    description.close()</span><br><span class="line">    </span><br><span class="line">description_data = description_data.split(<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> description_data:</span><br><span class="line">    <span class="keyword">if</span> <span class="string">':'</span> <span class="keyword">in</span> i:</span><br><span class="line">        key = i.split(<span class="string">':'</span>)[<span class="number">0</span>]</span><br><span class="line">        description_dict[key] = []</span><br><span class="line">    <span class="keyword">elif</span> i.split() <span class="keyword">and</span> <span class="string">'       '</span> <span class="keyword">in</span> i:</span><br><span class="line">        value = i.split()[<span class="number">0</span>]</span><br><span class="line">        description_dict[key].append(value)</span><br><span class="line"></span><br><span class="line">print(description_dict)</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;MSSubClass&apos;: [&apos;20&apos;, &apos;30&apos;, &apos;40&apos;, &apos;45&apos;, &apos;50&apos;, &apos;60&apos;, &apos;70&apos;, &apos;75&apos;, &apos;80&apos;, &apos;85&apos;, &apos;90&apos;, &apos;120&apos;, &apos;150&apos;, &apos;160&apos;, &apos;180&apos;, &apos;190&apos;], &apos;MSZoning&apos;: [&apos;A&apos;, &apos;C&apos;, &apos;FV&apos;, &apos;I&apos;, &apos;RH&apos;, &apos;RL&apos;, &apos;RP&apos;, &apos;RM&apos;], &apos;LotFrontage&apos;: [], &apos;LotArea&apos;: [], &apos;Street&apos;: [&apos;Grvl&apos;, &apos;Pave&apos;], &apos;Alley&apos;: [&apos;Grvl&apos;, &apos;Pave&apos;, &apos;NA&apos;], &apos;LotShape&apos;: [&apos;Reg&apos;, &apos;IR1&apos;, &apos;IR2&apos;, &apos;IR3&apos;], &apos;LandContour&apos;: [&apos;Lvl&apos;, &apos;Bnk&apos;, &apos;HLS&apos;, &apos;Low&apos;], &apos;Utilities&apos;: [&apos;AllPub&apos;, &apos;NoSewr&apos;, &apos;NoSeWa&apos;, &apos;ELO&apos;], &apos;LotConfig&apos;: [&apos;Inside&apos;, &apos;Corner&apos;, &apos;CulDSac&apos;, &apos;FR2&apos;, &apos;FR3&apos;], &apos;LandSlope&apos;: [&apos;Gtl&apos;, &apos;Mod&apos;, &apos;Sev&apos;], &apos;Neighborhood&apos;: [&apos;Blmngtn&apos;, &apos;Blueste&apos;, &apos;BrDale&apos;, &apos;BrkSide&apos;, &apos;ClearCr&apos;, &apos;CollgCr&apos;, &apos;Crawfor&apos;, &apos;Edwards&apos;, &apos;Gilbert&apos;, &apos;IDOTRR&apos;, &apos;MeadowV&apos;, &apos;Mitchel&apos;, &apos;Names&apos;, &apos;NoRidge&apos;, &apos;NPkVill&apos;, &apos;NridgHt&apos;, &apos;NWAmes&apos;, &apos;OldTown&apos;, &apos;SWISU&apos;, &apos;Sawyer&apos;, &apos;SawyerW&apos;, &apos;Somerst&apos;, &apos;StoneBr&apos;, &apos;Timber&apos;, &apos;Veenker&apos;], &apos;Condition1&apos;: [&apos;Artery&apos;, &apos;Feedr&apos;, &apos;Norm&apos;, &apos;RRNn&apos;, &apos;RRAn&apos;, &apos;PosN&apos;, &apos;PosA&apos;, &apos;RRNe&apos;, &apos;RRAe&apos;], &apos;Condition2&apos;: [&apos;Artery&apos;, &apos;Feedr&apos;, &apos;Norm&apos;, &apos;RRNn&apos;, &apos;RRAn&apos;, &apos;PosN&apos;, &apos;PosA&apos;, &apos;RRNe&apos;, &apos;RRAe&apos;], &apos;BldgType&apos;: [&apos;1Fam&apos;, &apos;2FmCon&apos;, &apos;Duplx&apos;, &apos;TwnhsE&apos;, &apos;TwnhsI&apos;], &apos;HouseStyle&apos;: [&apos;1Story&apos;], &apos;       1.5Fin\tOne and one-half story&apos;: [], &apos;       1.5Unf\tOne and one-half story&apos;: [&apos;2Story&apos;], &apos;       2.5Fin\tTwo and one-half story&apos;: [], &apos;       2.5Unf\tTwo and one-half story&apos;: [&apos;SFoyer&apos;, &apos;SLvl&apos;], &apos;OverallQual&apos;: [&apos;10&apos;, &apos;9&apos;, &apos;8&apos;, &apos;7&apos;, &apos;6&apos;, &apos;5&apos;, &apos;4&apos;, &apos;3&apos;, &apos;2&apos;, &apos;1&apos;], &apos;OverallCond&apos;: [&apos;10&apos;, &apos;9&apos;, &apos;8&apos;, &apos;7&apos;, &apos;6&apos;, &apos;5&apos;, &apos;4&apos;, &apos;3&apos;, &apos;2&apos;, &apos;1&apos;], &apos;YearBuilt&apos;: [], &apos;YearRemodAdd&apos;: [], &apos;RoofStyle&apos;: [&apos;Flat&apos;, &apos;Gable&apos;, &apos;Gambrel&apos;, &apos;Hip&apos;, &apos;Mansard&apos;, &apos;Shed&apos;], &apos;RoofMatl&apos;: [&apos;ClyTile&apos;, &apos;CompShg&apos;, &apos;Membran&apos;, &apos;Metal&apos;, &apos;Roll&apos;, &apos;Tar&amp;Grv&apos;, &apos;WdShake&apos;, &apos;WdShngl&apos;], &apos;Exterior1st&apos;: [&apos;AsbShng&apos;, &apos;AsphShn&apos;, &apos;BrkComm&apos;, &apos;BrkFace&apos;, &apos;CBlock&apos;, &apos;CemntBd&apos;, &apos;HdBoard&apos;, &apos;ImStucc&apos;, &apos;MetalSd&apos;, &apos;Other&apos;, &apos;Plywood&apos;, &apos;PreCast&apos;, &apos;Stone&apos;, &apos;Stucco&apos;, &apos;VinylSd&apos;, &apos;Wd&apos;, &apos;WdShing&apos;], &apos;Exterior2nd&apos;: [&apos;AsbShng&apos;, &apos;AsphShn&apos;, &apos;BrkComm&apos;, &apos;BrkFace&apos;, &apos;CBlock&apos;, &apos;CemntBd&apos;, &apos;HdBoard&apos;, &apos;ImStucc&apos;, &apos;MetalSd&apos;, &apos;Other&apos;, &apos;Plywood&apos;, &apos;PreCast&apos;, &apos;Stone&apos;, &apos;Stucco&apos;, &apos;VinylSd&apos;, &apos;Wd&apos;, &apos;WdShing&apos;], &apos;MasVnrType&apos;: [&apos;BrkCmn&apos;, &apos;BrkFace&apos;, &apos;CBlock&apos;, &apos;None&apos;, &apos;Stone&apos;], &apos;MasVnrArea&apos;: [], &apos;ExterQual&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;], &apos;ExterCond&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;], &apos;Foundation&apos;: [&apos;BrkTil&apos;, &apos;CBlock&apos;, &apos;PConc&apos;, &apos;Slab&apos;, &apos;Stone&apos;, &apos;Wood&apos;], &apos;BsmtQual&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;, &apos;NA&apos;], &apos;BsmtCond&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;, &apos;NA&apos;], &apos;BsmtExposure&apos;: [&apos;Gd&apos;, &apos;Av&apos;, &apos;Mn&apos;, &apos;No&apos;, &apos;NA&apos;], &apos;BsmtFinType1&apos;: [&apos;GLQ&apos;, &apos;ALQ&apos;, &apos;BLQ&apos;, &apos;Rec&apos;, &apos;LwQ&apos;, &apos;Unf&apos;, &apos;NA&apos;], &apos;BsmtFinSF1&apos;: [], &apos;BsmtFinType2&apos;: [&apos;GLQ&apos;, &apos;ALQ&apos;, &apos;BLQ&apos;, &apos;Rec&apos;, &apos;LwQ&apos;, &apos;Unf&apos;, &apos;NA&apos;], &apos;BsmtFinSF2&apos;: [], &apos;BsmtUnfSF&apos;: [], &apos;TotalBsmtSF&apos;: [], &apos;Heating&apos;: [&apos;Floor&apos;, &apos;GasA&apos;, &apos;GasW&apos;, &apos;Grav&apos;, &apos;OthW&apos;, &apos;Wall&apos;], &apos;HeatingQC&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;], &apos;CentralAir&apos;: [&apos;N&apos;, &apos;Y&apos;], &apos;Electrical&apos;: [&apos;SBrkr&apos;, &apos;FuseA&apos;, &apos;FuseF&apos;, &apos;FuseP&apos;, &apos;Mix&apos;], &apos;1stFlrSF&apos;: [], &apos;2ndFlrSF&apos;: [], &apos;LowQualFinSF&apos;: [], &apos;GrLivArea&apos;: [], &apos;BsmtFullBath&apos;: [], &apos;BsmtHalfBath&apos;: [], &apos;FullBath&apos;: [], &apos;HalfBath&apos;: [], &apos;Bedroom&apos;: [], &apos;Kitchen&apos;: [], &apos;KitchenQual&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;], &apos;TotRmsAbvGrd&apos;: [], &apos;Functional&apos;: [&apos;Typ&apos;, &apos;Min1&apos;, &apos;Min2&apos;, &apos;Mod&apos;, &apos;Maj1&apos;, &apos;Maj2&apos;, &apos;Sev&apos;, &apos;Sal&apos;], &apos;Fireplaces&apos;: [], &apos;FireplaceQu&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;, &apos;NA&apos;], &apos;GarageType&apos;: [&apos;2Types&apos;, &apos;Attchd&apos;, &apos;Basment&apos;, &apos;BuiltIn&apos;, &apos;CarPort&apos;, &apos;Detchd&apos;, &apos;NA&apos;], &apos;GarageYrBlt&apos;: [], &apos;GarageFinish&apos;: [&apos;Fin&apos;, &apos;RFn&apos;, &apos;Unf&apos;, &apos;NA&apos;], &apos;GarageCars&apos;: [], &apos;GarageArea&apos;: [], &apos;GarageQual&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;, &apos;NA&apos;], &apos;GarageCond&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;, &apos;NA&apos;], &apos;PavedDrive&apos;: [&apos;Y&apos;, &apos;P&apos;, &apos;N&apos;], &apos;WoodDeckSF&apos;: [], &apos;OpenPorchSF&apos;: [], &apos;EnclosedPorch&apos;: [], &apos;3SsnPorch&apos;: [], &apos;ScreenPorch&apos;: [], &apos;PoolArea&apos;: [], &apos;PoolQC&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;NA&apos;], &apos;Fence&apos;: [&apos;GdPrv&apos;, &apos;MnPrv&apos;, &apos;GdWo&apos;, &apos;MnWw&apos;, &apos;NA&apos;], &apos;MiscFeature&apos;: [&apos;Elev&apos;, &apos;Gar2&apos;, &apos;Othr&apos;, &apos;Shed&apos;, &apos;TenC&apos;, &apos;NA&apos;], &apos;MiscVal&apos;: [], &apos;MoSold&apos;: [], &apos;YrSold&apos;: [], &apos;SaleType&apos;: [&apos;WD&apos;, &apos;CWD&apos;, &apos;VWD&apos;, &apos;New&apos;, &apos;COD&apos;, &apos;Con&apos;, &apos;ConLw&apos;, &apos;ConLI&apos;, &apos;ConLD&apos;, &apos;Oth&apos;], &apos;SaleCondition&apos;: [&apos;Normal&apos;, &apos;Abnorml&apos;, &apos;AdjLand&apos;, &apos;Alloca&apos;, &apos;Family&apos;, &apos;Partial&apos;]}
</code></pre><h2 id="字符串类型的-feature-重编码"><a href="#字符串类型的-feature-重编码" class="headerlink" title="字符串类型的 feature 重编码"></a>字符串类型的 feature 重编码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这个函数的作用是得到数据集中非数字的feature column</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_object_column</span><span class="params">(df)</span>:</span></span><br><span class="line">    object_column = []</span><br><span class="line">    <span class="keyword">for</span> column <span class="keyword">in</span> df.columns:</span><br><span class="line">        <span class="keyword">if</span> df[column].dtype == <span class="string">'object'</span>:</span><br><span class="line">            object_column.append(column)</span><br><span class="line">    <span class="keyword">return</span> object_column</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#object_column = get_object_column(train)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这个函数的作用是把 description_dict 中的 value 转换为对应数字的字典</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_map</span><span class="params">(map_list, end_index=<span class="number">1</span>)</span>:</span></span><br><span class="line">    d = &#123;&#125;</span><br><span class="line">    j = len(map_list) - end_index</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> map_list:</span><br><span class="line">        d[i] = j</span><br><span class="line">        j -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_order_feature</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> get_object_column(df):</span><br><span class="line">        order_map = generate_map(description_dict[i], <span class="number">0</span>)</span><br><span class="line">        df[i] = df[i].map(order_map)</span><br><span class="line">        df[i] = df[i].fillna(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train = preprocess_order_feature(train)</span><br><span class="line">test = preprocess_order_feature(test)</span><br></pre></td></tr></table></figure>
<h1 id="观察数据"><a href="#观察数据" class="headerlink" title="观察数据"></a>观察数据</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MSSubClass</th>
      <th>MSZoning</th>
      <th>LotFrontage</th>
      <th>LotArea</th>
      <th>Street</th>
      <th>LotShape</th>
      <th>LandContour</th>
      <th>Utilities</th>
      <th>LotConfig</th>
      <th>LandSlope</th>
      <th>Neighborhood</th>
      <th>Condition1</th>
      <th>Condition2</th>
      <th>BldgType</th>
      <th>HouseStyle</th>
      <th>OverallQual</th>
      <th>OverallCond</th>
      <th>YearBuilt</th>
      <th>YearRemodAdd</th>
      <th>RoofStyle</th>
      <th>RoofMatl</th>
      <th>Exterior1st</th>
      <th>Exterior2nd</th>
      <th>MasVnrType</th>
      <th>MasVnrArea</th>
      <th>ExterQual</th>
      <th>ExterCond</th>
      <th>Foundation</th>
      <th>BsmtQual</th>
      <th>BsmtCond</th>
      <th>BsmtExposure</th>
      <th>BsmtFinType1</th>
      <th>BsmtFinSF1</th>
      <th>BsmtFinType2</th>
      <th>BsmtFinSF2</th>
      <th>BsmtUnfSF</th>
      <th>TotalBsmtSF</th>
      <th>Heating</th>
      <th>HeatingQC</th>
      <th>CentralAir</th>
      <th>Electrical</th>
      <th>1stFlrSF</th>
      <th>2ndFlrSF</th>
      <th>LowQualFinSF</th>
      <th>GrLivArea</th>
      <th>BsmtFullBath</th>
      <th>BsmtHalfBath</th>
      <th>FullBath</th>
      <th>HalfBath</th>
      <th>BedroomAbvGr</th>
      <th>KitchenAbvGr</th>
      <th>KitchenQual</th>
      <th>TotRmsAbvGrd</th>
      <th>Functional</th>
      <th>Fireplaces</th>
      <th>FireplaceQu</th>
      <th>GarageType</th>
      <th>GarageYrBlt</th>
      <th>GarageFinish</th>
      <th>GarageCars</th>
      <th>GarageArea</th>
      <th>GarageQual</th>
      <th>GarageCond</th>
      <th>PavedDrive</th>
      <th>WoodDeckSF</th>
      <th>OpenPorchSF</th>
      <th>EnclosedPorch</th>
      <th>3SsnPorch</th>
      <th>ScreenPorch</th>
      <th>PoolArea</th>
      <th>MiscVal</th>
      <th>MoSold</th>
      <th>YrSold</th>
      <th>SaleType</th>
      <th>SaleCondition</th>
      <th>SalePrice</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.00000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>56.897260</td>
      <td>2.825342</td>
      <td>69.863699</td>
      <td>10516.828082</td>
      <td>1.004110</td>
      <td>3.591781</td>
      <td>3.814384</td>
      <td>3.998630</td>
      <td>4.583562</td>
      <td>2.937671</td>
      <td>10.747945</td>
      <td>6.969178</td>
      <td>6.993151</td>
      <td>4.334247</td>
      <td>0.497260</td>
      <td>6.099315</td>
      <td>5.575342</td>
      <td>1971.267808</td>
      <td>1984.865753</td>
      <td>4.589726</td>
      <td>6.924658</td>
      <td>5.958904</td>
      <td>5.271918</td>
      <td>2.552740</td>
      <td>103.117123</td>
      <td>3.39589</td>
      <td>3.083562</td>
      <td>4.603425</td>
      <td>4.565068</td>
      <td>4.010959</td>
      <td>2.656164</td>
      <td>4.571233</td>
      <td>443.639726</td>
      <td>2.273288</td>
      <td>46.549315</td>
      <td>567.240411</td>
      <td>1057.429452</td>
      <td>4.963699</td>
      <td>4.145205</td>
      <td>1.065068</td>
      <td>4.889726</td>
      <td>1162.626712</td>
      <td>346.992466</td>
      <td>5.844521</td>
      <td>1515.463699</td>
      <td>0.425342</td>
      <td>0.057534</td>
      <td>1.565068</td>
      <td>0.382877</td>
      <td>2.866438</td>
      <td>1.046575</td>
      <td>3.511644</td>
      <td>6.517808</td>
      <td>7.841781</td>
      <td>0.613014</td>
      <td>2.825342</td>
      <td>4.791781</td>
      <td>1978.589041</td>
      <td>2.771233</td>
      <td>1.767123</td>
      <td>472.980137</td>
      <td>3.976712</td>
      <td>3.975342</td>
      <td>2.856164</td>
      <td>94.244521</td>
      <td>46.660274</td>
      <td>21.954110</td>
      <td>3.409589</td>
      <td>15.060959</td>
      <td>2.758904</td>
      <td>43.489041</td>
      <td>6.321918</td>
      <td>2007.815753</td>
      <td>9.509589</td>
      <td>5.417808</td>
      <td>180921.195890</td>
    </tr>
    <tr>
      <th>std</th>
      <td>42.300571</td>
      <td>1.020174</td>
      <td>22.027677</td>
      <td>9981.264932</td>
      <td>0.063996</td>
      <td>0.582296</td>
      <td>0.606509</td>
      <td>0.052342</td>
      <td>0.773448</td>
      <td>0.276232</td>
      <td>7.565716</td>
      <td>0.878349</td>
      <td>0.248272</td>
      <td>1.555218</td>
      <td>0.500164</td>
      <td>1.382997</td>
      <td>1.112799</td>
      <td>30.202904</td>
      <td>20.645407</td>
      <td>0.834998</td>
      <td>0.599127</td>
      <td>4.426038</td>
      <td>4.263353</td>
      <td>1.046204</td>
      <td>180.731373</td>
      <td>0.57428</td>
      <td>0.351054</td>
      <td>0.722394</td>
      <td>0.678071</td>
      <td>0.284178</td>
      <td>1.039123</td>
      <td>2.070649</td>
      <td>456.098091</td>
      <td>0.869859</td>
      <td>161.319273</td>
      <td>441.866955</td>
      <td>438.705324</td>
      <td>0.295124</td>
      <td>0.959501</td>
      <td>0.246731</td>
      <td>0.394658</td>
      <td>386.587738</td>
      <td>436.528436</td>
      <td>48.623081</td>
      <td>525.480383</td>
      <td>0.518911</td>
      <td>0.238753</td>
      <td>0.550916</td>
      <td>0.502885</td>
      <td>0.815778</td>
      <td>0.220338</td>
      <td>0.663760</td>
      <td>1.625393</td>
      <td>0.667698</td>
      <td>0.644666</td>
      <td>1.810877</td>
      <td>1.759864</td>
      <td>23.997022</td>
      <td>0.811835</td>
      <td>0.747315</td>
      <td>213.804841</td>
      <td>0.241665</td>
      <td>0.232860</td>
      <td>0.496592</td>
      <td>125.338794</td>
      <td>66.256028</td>
      <td>61.119149</td>
      <td>29.317331</td>
      <td>55.757415</td>
      <td>40.177307</td>
      <td>496.123024</td>
      <td>2.703626</td>
      <td>1.328095</td>
      <td>1.368616</td>
      <td>1.475209</td>
      <td>79442.502883</td>
    </tr>
    <tr>
      <th>min</th>
      <td>20.000000</td>
      <td>0.000000</td>
      <td>21.000000</td>
      <td>1300.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1872.000000</td>
      <td>1950.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>2.00000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>334.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>334.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>1900.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>2006.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>34900.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>20.000000</td>
      <td>3.000000</td>
      <td>60.000000</td>
      <td>7553.500000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>3.000000</td>
      <td>4.000000</td>
      <td>7.000000</td>
      <td>7.000000</td>
      <td>5.000000</td>
      <td>0.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>1954.000000</td>
      <td>1967.000000</td>
      <td>5.000000</td>
      <td>7.000000</td>
      <td>3.000000</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>3.00000</td>
      <td>3.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>223.000000</td>
      <td>795.750000</td>
      <td>5.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>5.000000</td>
      <td>882.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1129.500000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>5.000000</td>
      <td>8.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>1962.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>334.500000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>3.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>5.000000</td>
      <td>2007.000000</td>
      <td>10.000000</td>
      <td>6.000000</td>
      <td>129975.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>50.000000</td>
      <td>3.000000</td>
      <td>69.000000</td>
      <td>9478.500000</td>
      <td>1.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>5.000000</td>
      <td>3.000000</td>
      <td>10.000000</td>
      <td>7.000000</td>
      <td>7.000000</td>
      <td>5.000000</td>
      <td>0.000000</td>
      <td>6.000000</td>
      <td>5.000000</td>
      <td>1973.000000</td>
      <td>1994.000000</td>
      <td>5.000000</td>
      <td>7.000000</td>
      <td>3.000000</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>3.00000</td>
      <td>3.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>4.000000</td>
      <td>2.000000</td>
      <td>5.000000</td>
      <td>383.500000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>477.500000</td>
      <td>991.500000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>1.000000</td>
      <td>5.000000</td>
      <td>1087.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1464.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>6.000000</td>
      <td>8.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>6.000000</td>
      <td>1980.000000</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>480.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>3.000000</td>
      <td>0.000000</td>
      <td>25.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>6.000000</td>
      <td>2008.000000</td>
      <td>10.000000</td>
      <td>6.000000</td>
      <td>163000.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>70.000000</td>
      <td>3.000000</td>
      <td>79.000000</td>
      <td>11601.500000</td>
      <td>1.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>5.000000</td>
      <td>3.000000</td>
      <td>18.000000</td>
      <td>7.000000</td>
      <td>7.000000</td>
      <td>5.000000</td>
      <td>1.000000</td>
      <td>7.000000</td>
      <td>6.000000</td>
      <td>2000.000000</td>
      <td>2004.000000</td>
      <td>5.000000</td>
      <td>7.000000</td>
      <td>9.000000</td>
      <td>9.000000</td>
      <td>4.000000</td>
      <td>164.250000</td>
      <td>4.00000</td>
      <td>3.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>4.000000</td>
      <td>3.000000</td>
      <td>7.000000</td>
      <td>712.250000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>808.000000</td>
      <td>1298.250000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>1.000000</td>
      <td>5.000000</td>
      <td>1391.250000</td>
      <td>728.000000</td>
      <td>0.000000</td>
      <td>1776.750000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>4.000000</td>
      <td>7.000000</td>
      <td>8.000000</td>
      <td>1.000000</td>
      <td>5.000000</td>
      <td>6.000000</td>
      <td>2001.000000</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>576.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>3.000000</td>
      <td>168.000000</td>
      <td>68.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>8.000000</td>
      <td>2009.000000</td>
      <td>10.000000</td>
      <td>6.000000</td>
      <td>214000.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>190.000000</td>
      <td>6.000000</td>
      <td>313.000000</td>
      <td>215245.000000</td>
      <td>2.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>5.000000</td>
      <td>3.000000</td>
      <td>25.000000</td>
      <td>9.000000</td>
      <td>9.000000</td>
      <td>5.000000</td>
      <td>1.000000</td>
      <td>10.000000</td>
      <td>9.000000</td>
      <td>2010.000000</td>
      <td>2010.000000</td>
      <td>6.000000</td>
      <td>8.000000</td>
      <td>17.000000</td>
      <td>17.000000</td>
      <td>5.000000</td>
      <td>1600.000000</td>
      <td>5.00000</td>
      <td>5.000000</td>
      <td>6.000000</td>
      <td>6.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>7.000000</td>
      <td>5644.000000</td>
      <td>7.000000</td>
      <td>1474.000000</td>
      <td>2336.000000</td>
      <td>6110.000000</td>
      <td>6.000000</td>
      <td>5.000000</td>
      <td>2.000000</td>
      <td>5.000000</td>
      <td>4692.000000</td>
      <td>2065.000000</td>
      <td>572.000000</td>
      <td>5642.000000</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>8.000000</td>
      <td>3.000000</td>
      <td>5.000000</td>
      <td>14.000000</td>
      <td>8.000000</td>
      <td>3.000000</td>
      <td>6.000000</td>
      <td>7.000000</td>
      <td>2010.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>1418.000000</td>
      <td>6.000000</td>
      <td>6.000000</td>
      <td>3.000000</td>
      <td>857.000000</td>
      <td>547.000000</td>
      <td>552.000000</td>
      <td>508.000000</td>
      <td>480.000000</td>
      <td>738.000000</td>
      <td>15500.000000</td>
      <td>12.000000</td>
      <td>2010.000000</td>
      <td>10.000000</td>
      <td>6.000000</td>
      <td>755000.000000</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="观察-feature-之间的相关性"><a href="#观察-feature-之间的相关性" class="headerlink" title="观察 feature 之间的相关性"></a>观察 feature 之间的相关性</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">corr_mat = train[[<span class="string">"SalePrice"</span>,<span class="string">"MSSubClass"</span>,<span class="string">"MSZoning"</span>,<span class="string">"LotFrontage"</span>,<span class="string">"LotArea"</span>, <span class="string">"BldgType"</span>,</span><br><span class="line">                       <span class="string">"OverallQual"</span>, <span class="string">"OverallCond"</span>,<span class="string">"YearBuilt"</span>, <span class="string">"BedroomAbvGr"</span>, <span class="string">"PoolArea"</span>, <span class="string">"GarageArea"</span>,</span><br><span class="line">                       <span class="string">"SaleType"</span>, <span class="string">"MoSold"</span>]].corr()</span><br><span class="line"><span class="comment"># corr_mat = train.corr()</span></span><br><span class="line">f, ax = plt.subplots(figsize=(<span class="number">16</span>, <span class="number">8</span>))</span><br><span class="line">sns.heatmap(corr_mat, vmax=<span class="number">1</span> , square=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb6c086fa20&gt;
</code></pre><p><img src="House_Prices_Advanced_Regression_Techniques_V2_35_1.png" alt="png"></p>
<p>观察热力图，颜色越浅相关性越大。关于热力图-&gt; <a href="https://www.youtube.com/watch?v=oMtDyOn2TCc" target="_blank" rel="noopener">this video</a>.</p>
<h2 id="观察年限和售价的规律"><a href="#观察年限和售价的规律" class="headerlink" title="观察年限和售价的规律"></a>观察年限和售价的规律</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f, ax = plt.subplots(figsize=(<span class="number">16</span>, <span class="number">8</span>))</span><br><span class="line">sns.lineplot(x=<span class="string">'YearBuilt'</span>, y=<span class="string">'SalePrice'</span>, data=train)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb6a5211320&gt;
</code></pre><p><img src="House_Prices_Advanced_Regression_Techniques_V2_38_1.png" alt="png"></p>
<p>在二十世纪的时候价格增长迅速。</p>
<h2 id="综合质量和售价有明显的相关性"><a href="#综合质量和售价有明显的相关性" class="headerlink" title="综合质量和售价有明显的相关性"></a>综合质量和售价有明显的相关性</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f, ax = plt.subplots(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">sns.lineplot(x=<span class="string">'OverallQual'</span>, y=<span class="string">'SalePrice'</span>, color=<span class="string">'green'</span>,data=train)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb6a3e6d8d0&gt;
</code></pre><p><img src="House_Prices_Advanced_Regression_Techniques_V2_41_1.png" alt="png"></p>
<p>我们可以看到，随着房屋整体质量的提高，销售价格快速上涨，这是非常合理的。</p>
<h2 id="观察售价"><a href="#观察售价" class="headerlink" title="观察售价"></a>观察售价</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">f, ax = plt.subplots(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">sns.distplot(train[<span class="string">'SalePrice'</span>])</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb6a25cdba8&gt;
</code></pre><p><img src="House_Prices_Advanced_Regression_Techniques_V2_44_1.png" alt="png"></p>
<p>大部分的房屋售价在 100000 到  200000 之间。</p>
<h1 id="构建预测模型"><a href="#构建预测模型" class="headerlink" title="构建预测模型"></a>构建预测模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = train.drop(<span class="string">'SalePrice'</span>, axis=<span class="number">1</span>)</span><br><span class="line">y = np.ravel(np.array(train[[<span class="string">'SalePrice'</span>]]))</span><br><span class="line">print(y.shape)</span><br><span class="line">y</span><br></pre></td></tr></table></figure>
<pre><code>(1460,)





array([208500, 181500, 223500, ..., 266500, 142125, 147500])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Use train_test_split from sci-kit learn to segment our data into train and a local testset</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="定义评估函数"><a href="#定义评估函数" class="headerlink" title="定义评估函数"></a>定义评估函数</h2><p>评估函数基于预测值的对数与观察到的销售价格的对数之间的均方根误差(RMSE)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmse</span><span class="params">(y, y_pred)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(mean_squared_error(np.log(y), np.log(y_pred)))</span><br></pre></td></tr></table></figure>
<h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">random_forest = RandomForestRegressor(n_estimators=<span class="number">1200</span>,</span><br><span class="line">                                      max_depth=<span class="number">15</span>,</span><br><span class="line">                                      min_samples_split=<span class="number">5</span>,</span><br><span class="line">                                      min_samples_leaf=<span class="number">5</span>,</span><br><span class="line">                                      max_features=<span class="keyword">None</span>,</span><br><span class="line">                                      random_state=<span class="number">42</span>,</span><br><span class="line">                                      oob_score=<span class="keyword">True</span>)</span><br><span class="line">kf = KFold(n_splits=<span class="number">5</span>)</span><br><span class="line">y_pred = cross_val_score(random_forest, X, y, cv=kf)</span><br><span class="line">y_pred.mean()</span><br></pre></td></tr></table></figure>
<pre><code>0.8500001566166802
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">random_forest.fit(X, y)</span><br></pre></td></tr></table></figure>
<pre><code>RandomForestRegressor(bootstrap=True, criterion=&apos;mse&apos;, max_depth=15,
                      max_features=None, max_leaf_nodes=None,
                      min_impurity_decrease=0.0, min_impurity_split=None,
                      min_samples_leaf=5, min_samples_split=5,
                      min_weight_fraction_leaf=0.0, n_estimators=1200,
                      n_jobs=None, oob_score=True, random_state=42, verbose=0,
                      warm_start=False)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rf_pred = random_forest.predict(test)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rf_pred</span><br></pre></td></tr></table></figure>
<pre><code>array([126945.71699684, 153924.56961003, 182182.80294353, ...,
       156066.28489667, 117296.65091637, 224995.13115853])
</code></pre><h2 id="XG-Boost"><a href="#XG-Boost" class="headerlink" title="XG Boost"></a>XG Boost</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">xg_boost = XGBRegressor(learning_rate=<span class="number">0.01</span>,</span><br><span class="line">                        n_estimators=<span class="number">6000</span>,</span><br><span class="line">                        max_depth=<span class="number">4</span>, </span><br><span class="line">                        min_child_weight=<span class="number">1</span>,</span><br><span class="line">                        gamma=<span class="number">0.6</span>,</span><br><span class="line">                        subsample=<span class="number">0.7</span>,</span><br><span class="line">                        colsample_bytree=<span class="number">0.2</span>,</span><br><span class="line">                        objective=<span class="string">'reg:linear'</span>,</span><br><span class="line">                        nthread=<span class="number">-1</span>,</span><br><span class="line">                        scale_pos_weight=<span class="number">1</span>,</span><br><span class="line">                        seed=<span class="number">27</span>,</span><br><span class="line">                        reg_alpha=<span class="number">0.00006</span>)</span><br><span class="line"></span><br><span class="line">kf = KFold(n_splits=<span class="number">5</span>)</span><br><span class="line">y_pred = cross_val_score(xg_boost, X, y, cv=kf)</span><br><span class="line">y_pred.mean()</span><br></pre></td></tr></table></figure>
<pre><code>[03:11:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
[03:11:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
[03:11:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
[03:12:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
[03:12:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.





0.8959027545454475
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">xg_boost.fit(X, y)</span><br></pre></td></tr></table></figure>
<pre><code>[03:12:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.





XGBRegressor(base_score=0.5, booster=&apos;gbtree&apos;, colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=0.2, gamma=0.6,
             importance_type=&apos;gain&apos;, learning_rate=0.01, max_delta_step=0,
             max_depth=4, min_child_weight=1, missing=None, n_estimators=6000,
             n_jobs=1, nthread=-1, objective=&apos;reg:linear&apos;, random_state=0,
             reg_alpha=6e-05, reg_lambda=1, scale_pos_weight=1, seed=27,
             silent=None, subsample=0.7, verbosity=1)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">xgb_pred = xg_boost.predict(test)</span><br><span class="line">xgb_pred</span><br></pre></td></tr></table></figure>
<pre><code>array([127263.77 , 163056.02 , 193208.25 , ..., 173218.75 , 115712.914,
       211616.88 ], dtype=float32)
</code></pre><h2 id="Gradient-Boost-Regressor-GBM"><a href="#Gradient-Boost-Regressor-GBM" class="headerlink" title="Gradient Boost Regressor(GBM)"></a>Gradient Boost Regressor(GBM)</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g_boost = GradientBoostingRegressor(n_estimators=<span class="number">6000</span>,</span><br><span class="line">                                    learning_rate=<span class="number">0.01</span>,</span><br><span class="line">                                    max_depth=<span class="number">5</span>,</span><br><span class="line">                                    max_features=<span class="string">'sqrt'</span>,</span><br><span class="line">                                    min_samples_leaf=<span class="number">15</span>,</span><br><span class="line">                                    min_samples_split=<span class="number">10</span>,</span><br><span class="line">                                    loss=<span class="string">'ls'</span>,</span><br><span class="line">                                    random_state=<span class="number">42</span></span><br><span class="line">                                    )</span><br><span class="line"></span><br><span class="line">kf = KFold(n_splits=<span class="number">5</span>)</span><br><span class="line">y_pred = cross_val_score(g_boost, X, y, cv=kf)</span><br><span class="line">y_pred.mean()</span><br></pre></td></tr></table></figure>
<pre><code>0.8905525972502479
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g_boost.fit(X, y)</span><br></pre></td></tr></table></figure>
<pre><code>GradientBoostingRegressor(alpha=0.9, criterion=&apos;friedman_mse&apos;, init=None,
                          learning_rate=0.01, loss=&apos;ls&apos;, max_depth=5,
                          max_features=&apos;sqrt&apos;, max_leaf_nodes=None,
                          min_impurity_decrease=0.0, min_impurity_split=None,
                          min_samples_leaf=15, min_samples_split=10,
                          min_weight_fraction_leaf=0.0, n_estimators=6000,
                          n_iter_no_change=None, presort=&apos;auto&apos;,
                          random_state=42, subsample=1.0, tol=0.0001,
                          validation_fraction=0.1, verbose=0, warm_start=False)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gbm_pred = g_boost.predict(test)</span><br><span class="line">gbm_pred</span><br></pre></td></tr></table></figure>
<pre><code>array([125909.87154943, 163184.8652622 , 187872.72372976, ...,
       176429.22616544, 119620.23912638, 211953.00041747])
</code></pre><h2 id="Light-GBM"><a href="#Light-GBM" class="headerlink" title="Light GBM"></a>Light GBM</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lightgbm = LGBMRegressor(objective=<span class="string">'regression'</span>,</span><br><span class="line">                         num_leaves=<span class="number">6</span>,</span><br><span class="line">                         learning_rate=<span class="number">0.01</span>,</span><br><span class="line">                         n_estimators=<span class="number">6400</span>,</span><br><span class="line">                         verbose=<span class="number">-1</span>,</span><br><span class="line">                         bagging_fraction=<span class="number">0.8</span>,</span><br><span class="line">                         bagging_freq=<span class="number">4</span>,</span><br><span class="line">                         bagging_seed=<span class="number">6</span>,</span><br><span class="line">                         feature_fraction=<span class="number">0.2</span>,</span><br><span class="line">                         feature_fraction_seed=<span class="number">7</span></span><br><span class="line">                         )</span><br><span class="line">kf = KFold(n_splits=<span class="number">5</span>)</span><br><span class="line">y_pred = cross_val_score(lightgbm, X, y, cv=kf)</span><br><span class="line">y_pred.mean()</span><br></pre></td></tr></table></figure>
<pre><code>0.8881867437856128
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lightgbm.fit(X,y)</span><br></pre></td></tr></table></figure>
<pre><code>LGBMRegressor(bagging_fraction=0.8, bagging_freq=4, bagging_seed=6,
              boosting_type=&apos;gbdt&apos;, class_weight=None, colsample_bytree=1.0,
              feature_fraction=0.2, feature_fraction_seed=7,
              importance_type=&apos;split&apos;, learning_rate=0.01, max_depth=-1,
              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
              n_estimators=6400, n_jobs=-1, num_leaves=6,
              objective=&apos;regression&apos;, random_state=None, reg_alpha=0.0,
              reg_lambda=0.0, silent=True, subsample=1.0,
              subsample_for_bin=200000, subsample_freq=0, verbose=-1)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lgb_pred = lightgbm.predict(test)</span><br><span class="line">lgb_pred</span><br></pre></td></tr></table></figure>
<pre><code>array([124759.4703253 , 161206.70919126, 187680.444818  , ...,
       168310.83365532, 123698.90457326, 206480.92047866])
</code></pre><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logreg = LogisticRegression()</span><br><span class="line">kf = KFold(n_splits=<span class="number">5</span>)</span><br><span class="line">y_pred = cross_val_score(logreg, X, y, cv=kf)</span><br><span class="line">y_pred.mean()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">logreg.fit(X, y)</span><br></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to &apos;auto&apos; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  &quot;the number of iterations.&quot;, ConvergenceWarning)





LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class=&apos;warn&apos;, n_jobs=None, penalty=&apos;l2&apos;,
                   random_state=None, solver=&apos;warn&apos;, tol=0.0001, verbose=0,
                   warm_start=False)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">round(logreg.score(X, y) * <span class="number">100</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>88.9
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">log_pred = logreg.predict(test)</span><br><span class="line">log_pred</span><br></pre></td></tr></table></figure>
<pre><code>array([135500, 128950, 175000, ..., 133900, 190000, 187500])
</code></pre><h1 id="模型的叠加"><a href="#模型的叠加" class="headerlink" title="模型的叠加"></a>模型的叠加</h1><p>叠加(也称为元集成)是一种模型集成技术，用于组合来自多个预测模型的信息，生成性能更好的新模型。在这个项目中，我们使用名为vecstack的python包，它可以帮助我们对前面导入的模型进行堆栈。它实际上非常容易使用，可以查看文档了解更多信息。<a href="https://github.com/vecxoz/vecstack" target="_blank" rel="noopener">vecstack</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">models = [g_boost, xg_boost, lightgbm, random_forest]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Strain, S_test = stacking(models,</span><br><span class="line">                          X_train,</span><br><span class="line">                          y_train,</span><br><span class="line">                          X_test,</span><br><span class="line">                          regression=<span class="keyword">True</span>,</span><br><span class="line">                          mode=<span class="string">'oof_pred_bag'</span>,</span><br><span class="line">                          metric=rmse,</span><br><span class="line">                          n_folds=<span class="number">5</span>,</span><br><span class="line">                          random_state=<span class="number">25</span>,</span><br><span class="line">                          verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>task:         [regression]
metric:       [rmse]
mode:         [oof_pred_bag]
n_models:     [4]

model  0:     [GradientBoostingRegressor]
    fold  0:  [0.12653004]
    fold  1:  [0.13818165]
    fold  2:  [0.10747644]
    fold  3:  [0.14980732]
    fold  4:  [0.11127270]
    ----
    MEAN:     [0.12665363] + [0.01595833]
    FULL:     [0.12764756]

model  1:     [XGBRegressor]
[03:23:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    fold  0:  [0.11631560]
[03:24:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    fold  1:  [0.14701253]
[03:24:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    fold  2:  [0.10450330]
[03:24:12] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    fold  3:  [0.14328067]
[03:24:18] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
    fold  4:  [0.10632026]
    ----
    MEAN:     [0.12348647] + [0.01817552]
    FULL:     [0.12481458]

model  2:     [LGBMRegressor]
    fold  0:  [0.12668239]
    fold  1:  [0.14251415]
    fold  2:  [0.11409004]
    fold  3:  [0.15394461]
    fold  4:  [0.11576550]
    ----
    MEAN:     [0.13059934] + [0.01545902]
    FULL:     [0.13150292]

model  3:     [RandomForestRegressor]
    fold  0:  [0.13803357]
    fold  1:  [0.16746496]
    fold  2:  [0.13370269]
    fold  3:  [0.17907099]
    fold  4:  [0.13625091]
    ----
    MEAN:     [0.15090463] + [0.01867560]
    FULL:     [0.15204350]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Strain, S_test</span><br></pre></td></tr></table></figure>
<pre><code>(array([[145154.57609501, 140247.640625  , 144708.92814448,
         136304.80002144],
        [441586.84575012, 453786.875     , 476049.8262998 ,
         433615.5690085 ],
        [205559.38156983, 199459.953125  , 204548.62741617,
         189012.87710637],
        ...,
        [229773.83814053, 245324.03125   , 222988.34529258,
         233726.54189435],
        [ 78529.68615301,  81706.46875   ,  74919.86211206,
          94651.91862458],
        [126564.42955093, 118016.921875  , 131591.97464745,
         134449.34870648]]),
 array([[156946.11019358, 162235.903125  , 156274.58204718,
         174271.19166786],
        [168719.74644755, 171368.26875   , 172660.15892698,
         168988.4858204 ],
        [165875.73697659, 167827.703125  , 166511.09786993,
         144720.7621456 ],
        ...,
        [235105.18179731, 240780.803125  , 236012.18528746,
         224310.44197611],
        [311340.99357469, 306275.29375   , 305036.50098821,
         319953.95931372],
        [100400.26285948,  97430.8671875 ,  97576.05463032,
         109458.70614352]]))
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Initialize 2nd level model</span></span><br><span class="line">xgb_lev2 = XGBRegressor(learning_rate=<span class="number">0.1</span>, </span><br><span class="line">                        n_estimators=<span class="number">500</span>,</span><br><span class="line">                        max_depth=<span class="number">3</span>,</span><br><span class="line">                        n_jobs=<span class="number">-1</span>,</span><br><span class="line">                        random_state=<span class="number">17</span></span><br><span class="line">                       )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the 2nd level model on the output of level 1</span></span><br><span class="line">xgb_lev2.fit(Strain, y_train)</span><br></pre></td></tr></table></figure>
<pre><code>[03:25:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.





XGBRegressor(base_score=0.5, booster=&apos;gbtree&apos;, colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0,
             importance_type=&apos;gain&apos;, learning_rate=0.1, max_delta_step=0,
             max_depth=3, min_child_weight=1, missing=None, n_estimators=500,
             n_jobs=-1, nthread=None, objective=&apos;reg:linear&apos;, random_state=17,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
             silent=None, subsample=1, verbosity=1)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Make predictions on the localized test set</span></span><br><span class="line">stacked_pred = xgb_lev2.predict(S_test)</span><br><span class="line">print(<span class="string">"RMSE of Stacked Model: &#123;&#125;"</span>.format(rmse(y_test,stacked_pred)))</span><br></pre></td></tr></table></figure>
<pre><code>RMSE of Stacked Model: 0.12290530277450722
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y1_pred_L1 = models[<span class="number">0</span>].predict(test)</span><br><span class="line">y2_pred_L1 = models[<span class="number">1</span>].predict(test)</span><br><span class="line">y3_pred_L1 = models[<span class="number">2</span>].predict(test)</span><br><span class="line">y4_pred_L1 = models[<span class="number">3</span>].predict(test)</span><br><span class="line">S_test_L1 = np.c_[y1_pred_L1, y2_pred_L1, y3_pred_L1, y4_pred_L1]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_stacked_pred = xgb_lev2.predict(S_test_L1)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Save the predictions in form of a dataframe</span></span><br><span class="line">submission = pd.DataFrame()</span><br><span class="line"></span><br><span class="line">submission[<span class="string">'Id'</span>] = np.array(test.index)</span><br><span class="line">submission[<span class="string">'SalePrice'</span>] = test_stacked_pred</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">submission.to_csv(<span class="string">'/submissionV2.csv'</span>, index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<h1 id="混合较好得分的-submission"><a href="#混合较好得分的-submission" class="headerlink" title="混合较好得分的 submission"></a>混合较好得分的 submission</h1><p>因为不知道最终的测试集合的正真数据是什么，只能一遍一遍提交去蒙，看到别人的方法是混合他人较好的提交去验证，尝试下看看。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">submission_v1 = pd.read_csv(<span class="string">'/House_price_submission_v44.csv'</span>)</span><br><span class="line">submission_v2 = pd.read_csv(<span class="string">'/submissionV19.csv'</span>)</span><br><span class="line">submission_v3 = pd.read_csv(<span class="string">'/blended_submission.csv'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">final_blend = <span class="number">0.5</span>*submission_v1.SalePrice.values + <span class="number">0.2</span>*submission_v2.SalePrice.values + <span class="number">0.3</span>*submission_v3.SalePrice.values</span><br><span class="line"></span><br><span class="line">blended_submission = pd.DataFrame()</span><br><span class="line"></span><br><span class="line">blended_submission[<span class="string">'Id'</span>] = submission_v1.Id.values</span><br><span class="line">blended_submission[<span class="string">'SalePrice'</span>] = final_blend</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">blended_submission.to_csv(<span class="string">'/submissionV20.csv'</span>, index=<span class="keyword">False</span>)</span><br><span class="line">blended_submission</span><br></pre></td></tr></table></figure>
<p><img src="score.png" alt="Top4"></p>
<p>呃，就这样吧，最终和前十名差不到0.004。</p>
<h1 id="用-Tensorfolw-试试"><a href="#用-Tensorfolw-试试" class="headerlink" title="用 Tensorfolw 试试"></a>用 Tensorfolw 试试</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">tf.logging.set_verbosity(tf.logging.ERROR)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correlation_dataframe = train.copy()</span><br><span class="line">saleprice_corr = correlation_dataframe.corr()[<span class="string">'SalePrice'</span>]</span><br><span class="line">saleprice_corr = saleprice_corr[saleprice_corr &gt; <span class="number">0</span>]</span><br><span class="line">corr_feature = saleprice_corr.index</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">corr_feature</span><br></pre></td></tr></table></figure>
<pre><code>Index([&apos;MSZoning&apos;, &apos;LotFrontage&apos;, &apos;LotArea&apos;, &apos;Utilities&apos;, &apos;BldgType&apos;,
       &apos;OverallQual&apos;, &apos;YearBuilt&apos;, &apos;YearRemodAdd&apos;, &apos;MasVnrType&apos;, &apos;MasVnrArea&apos;,
       &apos;ExterQual&apos;, &apos;ExterCond&apos;, &apos;BsmtQual&apos;, &apos;BsmtCond&apos;, &apos;BsmtExposure&apos;,
       &apos;BsmtFinType1&apos;, &apos;BsmtFinSF1&apos;, &apos;BsmtUnfSF&apos;, &apos;TotalBsmtSF&apos;, &apos;Heating&apos;,
       &apos;HeatingQC&apos;, &apos;Electrical&apos;, &apos;1stFlrSF&apos;, &apos;2ndFlrSF&apos;, &apos;GrLivArea&apos;,
       &apos;BsmtFullBath&apos;, &apos;FullBath&apos;, &apos;HalfBath&apos;, &apos;BedroomAbvGr&apos;, &apos;KitchenQual&apos;,
       &apos;TotRmsAbvGrd&apos;, &apos;Functional&apos;, &apos;Fireplaces&apos;, &apos;FireplaceQu&apos;, &apos;GarageType&apos;,
       &apos;GarageYrBlt&apos;, &apos;GarageFinish&apos;, &apos;GarageCars&apos;, &apos;GarageArea&apos;, &apos;GarageQual&apos;,
       &apos;GarageCond&apos;, &apos;PavedDrive&apos;, &apos;WoodDeckSF&apos;, &apos;OpenPorchSF&apos;, &apos;3SsnPorch&apos;,
       &apos;ScreenPorch&apos;, &apos;PoolArea&apos;, &apos;MoSold&apos;, &apos;SalePrice&apos;],
      dtype=&apos;object&apos;)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_feature</span><span class="params">(df, corr_feature)</span>:</span></span><br><span class="line">    newdf = pd.DataFrame()</span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> corr_feature:</span><br><span class="line">        newdf[feature] = df[feature]</span><br><span class="line">    <span class="keyword">return</span> newdf</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = preprocess_feature(train, corr_feature).drop(<span class="string">'SalePrice'</span>, axis=<span class="number">1</span>)</span><br><span class="line">y = np.ravel(np.array(train[[<span class="string">'SalePrice'</span>]]))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MSZoning</th>
      <th>LotFrontage</th>
      <th>LotArea</th>
      <th>Utilities</th>
      <th>BldgType</th>
      <th>OverallQual</th>
      <th>YearBuilt</th>
      <th>YearRemodAdd</th>
      <th>MasVnrType</th>
      <th>MasVnrArea</th>
      <th>ExterQual</th>
      <th>ExterCond</th>
      <th>BsmtQual</th>
      <th>BsmtCond</th>
      <th>BsmtExposure</th>
      <th>BsmtFinType1</th>
      <th>BsmtFinSF1</th>
      <th>BsmtUnfSF</th>
      <th>TotalBsmtSF</th>
      <th>Heating</th>
      <th>HeatingQC</th>
      <th>Electrical</th>
      <th>1stFlrSF</th>
      <th>2ndFlrSF</th>
      <th>GrLivArea</th>
      <th>BsmtFullBath</th>
      <th>FullBath</th>
      <th>HalfBath</th>
      <th>BedroomAbvGr</th>
      <th>KitchenQual</th>
      <th>TotRmsAbvGrd</th>
      <th>Functional</th>
      <th>Fireplaces</th>
      <th>FireplaceQu</th>
      <th>GarageType</th>
      <th>GarageYrBlt</th>
      <th>GarageFinish</th>
      <th>GarageCars</th>
      <th>GarageArea</th>
      <th>GarageQual</th>
      <th>GarageCond</th>
      <th>PavedDrive</th>
      <th>WoodDeckSF</th>
      <th>OpenPorchSF</th>
      <th>3SsnPorch</th>
      <th>ScreenPorch</th>
      <th>PoolArea</th>
      <th>MoSold</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.00000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
      <td>1460.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>2.825342</td>
      <td>69.863699</td>
      <td>10516.828082</td>
      <td>3.998630</td>
      <td>4.334247</td>
      <td>6.099315</td>
      <td>1971.267808</td>
      <td>1984.865753</td>
      <td>2.552740</td>
      <td>103.117123</td>
      <td>3.39589</td>
      <td>3.083562</td>
      <td>4.565068</td>
      <td>4.010959</td>
      <td>2.656164</td>
      <td>4.571233</td>
      <td>443.639726</td>
      <td>567.240411</td>
      <td>1057.429452</td>
      <td>4.963699</td>
      <td>4.145205</td>
      <td>4.889726</td>
      <td>1162.626712</td>
      <td>346.992466</td>
      <td>1515.463699</td>
      <td>0.425342</td>
      <td>1.565068</td>
      <td>0.382877</td>
      <td>2.866438</td>
      <td>3.511644</td>
      <td>6.517808</td>
      <td>7.841781</td>
      <td>0.613014</td>
      <td>2.825342</td>
      <td>4.791781</td>
      <td>1978.589041</td>
      <td>2.771233</td>
      <td>1.767123</td>
      <td>472.980137</td>
      <td>3.976712</td>
      <td>3.975342</td>
      <td>2.856164</td>
      <td>94.244521</td>
      <td>46.660274</td>
      <td>3.409589</td>
      <td>15.060959</td>
      <td>2.758904</td>
      <td>6.321918</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.020174</td>
      <td>22.027677</td>
      <td>9981.264932</td>
      <td>0.052342</td>
      <td>1.555218</td>
      <td>1.382997</td>
      <td>30.202904</td>
      <td>20.645407</td>
      <td>1.046204</td>
      <td>180.731373</td>
      <td>0.57428</td>
      <td>0.351054</td>
      <td>0.678071</td>
      <td>0.284178</td>
      <td>1.039123</td>
      <td>2.070649</td>
      <td>456.098091</td>
      <td>441.866955</td>
      <td>438.705324</td>
      <td>0.295124</td>
      <td>0.959501</td>
      <td>0.394658</td>
      <td>386.587738</td>
      <td>436.528436</td>
      <td>525.480383</td>
      <td>0.518911</td>
      <td>0.550916</td>
      <td>0.502885</td>
      <td>0.815778</td>
      <td>0.663760</td>
      <td>1.625393</td>
      <td>0.667698</td>
      <td>0.644666</td>
      <td>1.810877</td>
      <td>1.759864</td>
      <td>23.997022</td>
      <td>0.811835</td>
      <td>0.747315</td>
      <td>213.804841</td>
      <td>0.241665</td>
      <td>0.232860</td>
      <td>0.496592</td>
      <td>125.338794</td>
      <td>66.256028</td>
      <td>29.317331</td>
      <td>55.757415</td>
      <td>40.177307</td>
      <td>2.703626</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>21.000000</td>
      <td>1300.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1872.000000</td>
      <td>1950.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>2.00000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>334.000000</td>
      <td>0.000000</td>
      <td>334.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>1900.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>3.000000</td>
      <td>60.000000</td>
      <td>7553.500000</td>
      <td>4.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>1954.000000</td>
      <td>1967.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>3.00000</td>
      <td>3.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>223.000000</td>
      <td>795.750000</td>
      <td>5.000000</td>
      <td>3.000000</td>
      <td>5.000000</td>
      <td>882.000000</td>
      <td>0.000000</td>
      <td>1129.500000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>3.000000</td>
      <td>5.000000</td>
      <td>8.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>1962.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>334.500000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>3.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>5.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>3.000000</td>
      <td>69.000000</td>
      <td>9478.500000</td>
      <td>4.000000</td>
      <td>5.000000</td>
      <td>6.000000</td>
      <td>1973.000000</td>
      <td>1994.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>3.00000</td>
      <td>3.000000</td>
      <td>5.000000</td>
      <td>4.000000</td>
      <td>2.000000</td>
      <td>5.000000</td>
      <td>383.500000</td>
      <td>477.500000</td>
      <td>991.500000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>1087.000000</td>
      <td>0.000000</td>
      <td>1464.000000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>3.000000</td>
      <td>6.000000</td>
      <td>8.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>6.000000</td>
      <td>1980.000000</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>480.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>3.000000</td>
      <td>0.000000</td>
      <td>25.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>6.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>3.000000</td>
      <td>79.000000</td>
      <td>11601.500000</td>
      <td>4.000000</td>
      <td>5.000000</td>
      <td>7.000000</td>
      <td>2000.000000</td>
      <td>2004.000000</td>
      <td>4.000000</td>
      <td>164.250000</td>
      <td>4.00000</td>
      <td>3.000000</td>
      <td>5.000000</td>
      <td>4.000000</td>
      <td>3.000000</td>
      <td>7.000000</td>
      <td>712.250000</td>
      <td>808.000000</td>
      <td>1298.250000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>1391.250000</td>
      <td>728.000000</td>
      <td>1776.750000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>4.000000</td>
      <td>7.000000</td>
      <td>8.000000</td>
      <td>1.000000</td>
      <td>5.000000</td>
      <td>6.000000</td>
      <td>2001.000000</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>576.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>3.000000</td>
      <td>168.000000</td>
      <td>68.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>8.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>6.000000</td>
      <td>313.000000</td>
      <td>215245.000000</td>
      <td>4.000000</td>
      <td>5.000000</td>
      <td>10.000000</td>
      <td>2010.000000</td>
      <td>2010.000000</td>
      <td>5.000000</td>
      <td>1600.000000</td>
      <td>5.00000</td>
      <td>5.000000</td>
      <td>6.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>7.000000</td>
      <td>5644.000000</td>
      <td>2336.000000</td>
      <td>6110.000000</td>
      <td>6.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
      <td>4692.000000</td>
      <td>2065.000000</td>
      <td>5642.000000</td>
      <td>3.000000</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>8.000000</td>
      <td>5.000000</td>
      <td>14.000000</td>
      <td>8.000000</td>
      <td>3.000000</td>
      <td>6.000000</td>
      <td>7.000000</td>
      <td>2010.000000</td>
      <td>4.000000</td>
      <td>4.000000</td>
      <td>1418.000000</td>
      <td>6.000000</td>
      <td>6.000000</td>
      <td>3.000000</td>
      <td>857.000000</td>
      <td>547.000000</td>
      <td>508.000000</td>
      <td>480.000000</td>
      <td>738.000000</td>
      <td>12.000000</td>
    </tr>
  </tbody>
</table>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_input_fn</span><span class="params">(features, targets, batch_size=<span class="number">1</span>, shuffle=True, num_epochs=None)</span>:</span></span><br><span class="line">    features = &#123;key: np.array(value) <span class="keyword">for</span> key, value <span class="keyword">in</span> dict(features).items()&#125;</span><br><span class="line">    </span><br><span class="line">    ds = Dataset.from_tensor_slices((features, targets))</span><br><span class="line">    ds = ds.batch(batch_size).repeat(num_epochs)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        ds = ds.shuffle(<span class="number">10000</span>)</span><br><span class="line">    </span><br><span class="line">    features, labels = ds.make_one_shot_iterator().get_next()</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">construct_feature_columns</span><span class="params">(input_features)</span>:</span></span><br><span class="line">    <span class="string">"""构造TensorFlow特征列</span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            input_features:要使用的数字输入特性的名称。</span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            一个 feature columns 集合</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> set([tf.feature_column.numeric_column(my_feature) </span><br><span class="line">                <span class="keyword">for</span> my_feature <span class="keyword">in</span> input_features])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">    strps,</span></span></span><br><span class="line"><span class="function"><span class="params">    batch_size,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    training_targets,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_examples,</span></span></span><br><span class="line"><span class="function"><span class="params">    validation_targets)</span>:</span></span><br><span class="line">    <span class="string">"""训练多元特征的线性回归模型</span></span><br><span class="line"><span class="string">        除训练外，此功能还打印训练进度信息，</span></span><br><span class="line"><span class="string">        以及随着时间的推移而失去的训练和验证。</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        learning_rate:一个float，表示学习率</span></span><br><span class="line"><span class="string">        steps:一个非零的int，训练步骤的总数。训练步骤</span></span><br><span class="line"><span class="string">            由使用单个批处理的向前和向后传递组成。</span></span><br><span class="line"><span class="string">        batch_size:一个非零的int</span></span><br><span class="line"><span class="string">        training_example: DataFrame 包含一个或多个列</span></span><br><span class="line"><span class="string">        '  california_housing_dataframe '作为训练的输入feature</span></span><br><span class="line"><span class="string">        training_targets:一个' DataFrame '，它只包含一列</span></span><br><span class="line"><span class="string">        ' california_housing_dataframe '作为训练的目标。</span></span><br><span class="line"><span class="string">        validation_example: ' DataFrame '包含一个或多个列</span></span><br><span class="line"><span class="string">        ' california_housing_dataframe '作为验证的输入feature</span></span><br><span class="line"><span class="string">        validation_targets: ' DataFrame '，仅包含来自其中的一列</span></span><br><span class="line"><span class="string">        ' california_housing_dataframe '作为验证的目标。</span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">        在训练数据上训练的“线性回归器”对象</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    periods = <span class="number">10</span></span><br><span class="line">    steps_per_period = strps / periods</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建一个线性回归对象</span></span><br><span class="line">    my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)</span><br><span class="line">    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line">    linear_regressor = tf.estimator.LinearRegressor(</span><br><span class="line">        feature_columns=construct_feature_columns(training_examples),</span><br><span class="line">        optimizer=my_optimizer</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建输入函数</span></span><br><span class="line">    training_input_fn = <span class="keyword">lambda</span>: my_input_fn(</span><br><span class="line">        training_examples,</span><br><span class="line">        training_targets,</span><br><span class="line">        batch_size=batch_size)</span><br><span class="line">    </span><br><span class="line">    predict_training_input_fn = <span class="keyword">lambda</span>: my_input_fn(</span><br><span class="line">      training_examples, </span><br><span class="line">      training_targets, </span><br><span class="line">      num_epochs=<span class="number">1</span>, </span><br><span class="line">      shuffle=<span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">    predict_validation_input_fn = <span class="keyword">lambda</span>: my_input_fn(</span><br><span class="line">        validation_examples, </span><br><span class="line">        validation_targets,</span><br><span class="line">        num_epochs=<span class="number">1</span>,</span><br><span class="line">        shuffle=<span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#训练模型，但要在循环中进行，这样我们才能定期评估</span></span><br><span class="line">    <span class="comment">#损失指标</span></span><br><span class="line">    print(<span class="string">"Training model..."</span>)</span><br><span class="line">    print(<span class="string">"RMSE (on training data):"</span>)</span><br><span class="line">    training_rmse = []</span><br><span class="line">    validation_rmse = []</span><br><span class="line">    <span class="keyword">for</span> period <span class="keyword">in</span> range (<span class="number">0</span>, periods):</span><br><span class="line">      <span class="comment"># Train the model, starting from the prior state.</span></span><br><span class="line">      linear_regressor.train(</span><br><span class="line">          input_fn=training_input_fn,</span><br><span class="line">          steps=steps_per_period,</span><br><span class="line">      )</span><br><span class="line">      <span class="comment"># Take a break and compute predictions.</span></span><br><span class="line">      training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn)</span><br><span class="line">      training_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> training_predictions])</span><br><span class="line">      </span><br><span class="line">      validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)</span><br><span class="line">      validation_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> validation_predictions])</span><br><span class="line">      </span><br><span class="line"></span><br><span class="line">      <span class="comment"># Compute training and validation loss.</span></span><br><span class="line">      training_root_mean_squared_error = math.sqrt(</span><br><span class="line">          metrics.mean_squared_error(training_predictions, training_targets))</span><br><span class="line">      validation_root_mean_squared_error = math.sqrt(</span><br><span class="line">          metrics.mean_squared_error(validation_predictions, validation_targets))</span><br><span class="line">      <span class="comment"># Occasionally print the current loss.</span></span><br><span class="line">      print(<span class="string">"  period %02d : %0.2f"</span> % (period, training_root_mean_squared_error))</span><br><span class="line">      <span class="comment"># Add the loss metrics from this period to our list.</span></span><br><span class="line">      training_rmse.append(training_root_mean_squared_error)</span><br><span class="line">      validation_rmse.append(validation_root_mean_squared_error)</span><br><span class="line">    print(<span class="string">"Model training finished."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Output a graph of loss metrics over periods.</span></span><br><span class="line">    plt.ylabel(<span class="string">"RMSE"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Periods"</span>)</span><br><span class="line">    plt.title(<span class="string">"Root Mean Squared Error vs. Periods"</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.plot(training_rmse, label=<span class="string">"training"</span>)</span><br><span class="line">    plt.plot(validation_rmse, label=<span class="string">"validation"</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> linear_regressor</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">linear_regressor = train_model(</span><br><span class="line">    learning_rate=<span class="number">0.5</span>,</span><br><span class="line">    strps=<span class="number">200</span>,</span><br><span class="line">    batch_size=<span class="number">5</span>,</span><br><span class="line">    training_examples=X_train,</span><br><span class="line">    training_targets=y_train,</span><br><span class="line">    validation_examples=X_test,</span><br><span class="line">    validation_targets=y_test)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
  period 00 : 115236.06
  period 01 : 135729.29
  period 02 : 94007.13
  period 03 : 94940.56
  period 04 : 78776.40
  period 05 : 73407.86
  period 06 : 79100.02
  period 07 : 76995.32
  period 08 : 94657.55
  period 09 : 55721.83
Model training finished.
</code></pre><p><img src="House_Prices_Advanced_Regression_Techniques_V2_99_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">linear_regressor2 = train_model(</span><br><span class="line">    learning_rate=<span class="number">0.25</span>,</span><br><span class="line">    strps=<span class="number">500</span>,</span><br><span class="line">    batch_size=<span class="number">5</span>,</span><br><span class="line">    training_examples=X_train,</span><br><span class="line">    training_targets=y_train,</span><br><span class="line">    validation_examples=X_test,</span><br><span class="line">    validation_targets=y_test)</span><br></pre></td></tr></table></figure>
<pre><code>Training model...
RMSE (on training data):
  period 00 : 126663.18
  period 01 : 89900.76
  period 02 : 67300.53
  period 03 : 73597.13
  period 04 : 59429.95
  period 05 : 60645.34
  period 06 : 57056.42
  period 07 : 55974.51
  period 08 : 59490.64
  period 09 : 59963.44
Model training finished.
</code></pre><p><img src="House_Prices_Advanced_Regression_Techniques_V2_100_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">predict_test_input_fn = <span class="keyword">lambda</span>: my_input_fn(</span><br><span class="line">      test, </span><br><span class="line">      test, </span><br><span class="line">      num_epochs=<span class="number">1</span>, </span><br><span class="line">      shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">test_predictions = linear_regressor.predict(input_fn=predict_test_input_fn)</span><br><span class="line">test_predictions = np.array([item[<span class="string">'predictions'</span>][<span class="number">0</span>] <span class="keyword">for</span> item <span class="keyword">in</span> test_predictions])</span><br><span class="line">test_predictions</span><br></pre></td></tr></table></figure>
<pre><code>array([152403.8 , 178793.02, 186845.23, ..., 203018.39, 142496.02,
       197809.12], dtype=float32)
</code></pre><p>就先这样吧。</p>
]]></content>
      <categories>
        <category>Kaggle</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>【转】机器学习法则：ML工程的最佳实践</title>
    <url>/2018/05/21/%5B%E8%BD%AC%5D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B3%95%E5%88%99%EF%BC%9AML%E5%B7%A5%E7%A8%8B%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[<p>机器学习法则：ML工程的最佳实践</p>
<p></p><p style="color: rgb(105, 115, 117);font-size: 14px;letter-spacing: 1px;background-color: rgb(254, 255, 255);line-height: 2em;"><span style="text-align: right;background-color: rgb(255, 255, 255);font-size: 13px;color: rgb(61, 170, 214);font-family: Optima-Regular, PingFangTC-light;">作者</span></p><p style="color: rgb(105, 115, 117);font-size: 14px;letter-spacing: 1px;background-color: rgb(254, 255, 255);line-height: 2em;"><span style="background-color: rgb(255, 255, 255);color: rgb(136, 136, 136);font-family: Optima-Regular, PingFangTC-light;font-size: 13px;letter-spacing: 0.5px;">无邪</span></p><p style="color: rgb(105, 115, 117);font-size: 14px;letter-spacing: 1px;background-color: rgb(254, 255, 255);line-height: 2em;"><span style="background-color: rgb(255, 255, 255);color: rgb(136, 136, 136);font-family: Optima-Regular, PingFangTC-light;font-size: 13px;letter-spacing: 0.5px;"><span style=";">机器学习研究者，人工智障推进者。</span></span></p><p style="white-space: normal;color: rgb(51, 51, 51);text-align: center;"><br></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Martin Zinkevich 在2016年将 google 内容多年关于机器学习相关的经验分享了出来，这篇文章是对该分享的一些翻译+解读，如果想查看原文请参见：<a href="https://developers.google.com/machine-learning/rules-of-ml/&nbsp;。" target="_blank" rel="noopener">https://developers.google.com/machine-learning/rules-of-ml/&nbsp;。</a></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><br></span></p><section><section class="Powered-by-XIUMI V5" style="" powered-by="xiumi.us"><section class="" style="margin-top: 10px;margin-bottom: 10px;text-align: center;"><section class="" style="display: inline-block;vertical-align: top;"><section style="width: 8px;height: 8px;float: left;border-top: 1px solid rgb(249, 110, 87);border-radius: 4px 0px 0px;border-left: 1px solid rgb(249, 110, 87);"></section><section style="width: 8px;height: 8px;float: right;border-top: 1px solid rgb(249, 110, 87);border-right: 1px solid rgb(249, 110, 87);border-radius: 0px 4px 0px 0px;"></section><section style="clear: both;"></section><section class="" style="margin-top: -5px;margin-bottom: -5px;padding-right: 8px;padding-left: 8px;line-height: 1.3;"><span style="font-family: Optima-Regular, PingFangTC-light;font-size: 15px;font-weight: 700;">术语</span></section><section style="width: 8px;height: 8px;float: left;border-bottom: 1px solid rgb(249, 110, 87);border-left: 1px solid rgb(249, 110, 87);border-radius: 0px 0px 0px 4px;"></section><section style="width: 8px;height: 8px;float: right;border-bottom: 1px solid rgb(249, 110, 87);border-right: 1px solid rgb(249, 110, 87);border-radius: 0px 0px 4px;"></section><section style="clear: both;"></section></section><p><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"></span><br></p><p style="margin-bottom: 16px;"><span style="font-family: Optima-Regular, PingFangTC-light;font-size: 15px;">在说到具体的相关经验之前，先来了解下常用的术语。</span><br></p><ul class=" list-paddingleft-2" style=""><li><p style="text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">示例（Instance）：那些你要为其做出预测的事物称为示例。例如，示例可能是一个网页，你要将其归为“关于猫的”网页或者“不是关于猫的”网页。</span></p></li><li><p style="text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">标签（Label）：预测任务的答案或结果称为标签。无论是机器学习系统的答案或结果，还是训练数据的答案或结果，都可以称为标签。例如，将网页标记为“关于猫的”。</span></p></li><li><p style="text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">特征（Feature）：预测任务中示例的属性即为“特征”。例如，网页可以有“包含词汇‘猫’”的特征。</span></p></li><li><p style="text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">特征栏（Feature Column）：特征栏是相关特征的集合，如用户所住地区存在的所有可能国籍的集合。在同一个样本的同一个特征栏中可能有一个或多个特征。特征栏相当于（雅虎或微软的）虚拟机系统的 “命名空间（namespace）”或“域（field）”。</span></p></li><li><p style="text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">样本（Example）：样本包含示例（具有各种特征）和一个标签。</span></p></li><li><p style="text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">模型（Model）：模型是预测任务的数学表达形式。先是通过样本训练模型，而后利用模型做出预测。</span></p></li><li><p style="text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">指标（Metric）：指标是指一系列的数字，这些数字直接或间接的都被优化过。</span></p></li><li><p style="text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">目标（Objective）：目标是指算法经过优化，努力要达到的度量标准。</span></p></li><li><p style="text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">工作流（Pipeline）：工作流指的是围绕机器学习算法而存在的基础架构。从前端搜集数据、将搜集到的数据放入训练数据文件夹、训练一个或多个模型以及将模型用于生产等过程，都属于工作流。</span></p></li><li><p style="text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">点击率（Click-through Rate）：用户浏览网页时对包含广告链接的点击次数占浏览次数的百分比。</span></p><p style="text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><br></span></p></li></ul><section><section class="Powered-by-XIUMI V5" style="" powered-by="xiumi.us"><section class="" style="margin-top: 10px;margin-bottom: 10px;text-align: center;"><section class="" style="display: inline-block;vertical-align: top;"><section style="width: 8px;height: 8px;float: left;border-top: 1px solid rgb(249, 110, 87);border-radius: 4px 0px 0px;border-left: 1px solid rgb(249, 110, 87);"></section><section style="width: 8px;height: 8px;float: right;border-top: 1px solid rgb(249, 110, 87);border-right: 1px solid rgb(249, 110, 87);border-radius: 0px 4px 0px 0px;"></section><section style="clear: both;"></section><section class="" style="margin-top: -5px;margin-bottom: -5px;padding-right: 8px;padding-left: 8px;line-height: 1.3;"><span style="font-family: Optima-Regular, PingFangTC-light;font-size: 15px;font-weight: 700;">概述</span></section><section style="width: 8px;height: 8px;float: left;border-bottom: 1px solid rgb(249, 110, 87);border-left: 1px solid rgb(249, 110, 87);border-radius: 0px 0px 0px 4px;"></section><section style="width: 8px;height: 8px;float: right;border-bottom: 1px solid rgb(249, 110, 87);border-right: 1px solid rgb(249, 110, 87);border-radius: 0px 0px 4px;"></section><section style="clear: both;"></section></section><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">要想创造出优秀的产品：</span></p><p style="margin-bottom: 16px;"><span style="font-weight: 700;font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">你需要以一位优秀工程师的身份去运用深度学习！记住！你不单单是一位机器学习的研究者！</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">事实上，你所面临的大多数问题都是工程问题。即便拥有足以媲美机器学习专家的理论知识，要想有所突破，大多数情况下都在依赖示例的良好特征，而非优秀的机器学习算法。因此，基本方法如下：</span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">确保你的工作流各连接端十分可靠</span></p></li><li><p style="text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">从树立合理的目标开始</span></p></li><li><p style="text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">用简单的方式，添加符合常识的特征</span></p></li><li><p style="text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">确保你的工作流始终可靠</span></p></li></ol><p style="margin-bottom: 16px;"><br></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">这种方法能带来相当多的盈利，也能在较长时间里令许多人都满意，甚至还可能实现双赢。只有在简单技巧不发挥任何作用的情况下，才考虑使用复杂的一些的方法。方法越复杂，产品最终输出速度慢。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">当所有的简单技巧用完后，很可能就要考虑最前沿机器学习术了。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">本文档主要由四部分组成：</span></p><ol class=" list-paddingleft-2" style=""><li><p style="margin-top: 16px;margin-bottom: 16px;font-size: 13px;line-height: 1.5;text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">第一部分：帮助你明白是否到了需要构建一个机器学习系统</span></p></li><li><p style="margin-top: 16px;margin-bottom: 16px;font-size: 13px;line-height: 1.5;text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">第二部分：部署你的第一个工作流</span></p></li><li><p style="margin-top: 16px;margin-bottom: 16px;font-size: 13px;line-height: 1.5;text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">第三部分：往工作流增加新特征时的发布和迭代，以及如何评价模型和训练-服务倾斜（training-serving shew)</span></p></li><li><p style="margin-top: 16px;margin-bottom: 16px;font-size: 13px;line-height: 1.5;text-align: justify;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">第四部分：达到稳定阶段后该继续做什么。</span></p></li></ol><p><br></p><section><section class="Powered-by-XIUMI V5" style="" powered-by="xiumi.us"><section class="" style="margin-top: 10px;margin-bottom: 10px;text-align: center;"><section class="" style="display: inline-block;vertical-align: top;"><section style="width: 8px;height: 8px;float: left;border-top: 1px solid rgb(249, 110, 87);border-radius: 4px 0px 0px;border-left: 1px solid rgb(249, 110, 87);"></section><section style="width: 8px;height: 8px;float: right;border-top: 1px solid rgb(249, 110, 87);border-right: 1px solid rgb(249, 110, 87);border-radius: 0px 4px 0px 0px;"></section><section style="clear: both;"></section><section class="" style="margin-top: -5px;margin-bottom: -5px;padding-right: 8px;padding-left: 8px;line-height: 1.3;"><span style="font-family: Optima-Regular, PingFangTC-light;font-size: 15px;font-weight: 700;">在机器学习之前</span></section><section style="width: 8px;height: 8px;float: left;border-bottom: 1px solid rgb(249, 110, 87);border-left: 1px solid rgb(249, 110, 87);border-radius: 0px 0px 0px 4px;"></section><section style="width: 8px;height: 8px;float: right;border-bottom: 1px solid rgb(249, 110, 87);border-right: 1px solid rgb(249, 110, 87);border-radius: 0px 0px 4px;"></section><section style="clear: both;"></section></section><p><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"></span><br></p><p style="caret-color: rgb(51, 51, 51);color: rgb(51, 51, 51);"><br></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #1:Don’t be afraid to launch a product without machine learning.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 1:不要害怕发布一款没有用到机器学习的产品</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">机器学习是很酷，但它需要数据。如果你认为机器学习可以提高 100% 收益，那么启发式规则可以获得 50% 收益。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #2: First, design and implement metrics.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则2：首先需要设计和实现评估指标</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">在构建具体的机器学习系统之前，首先在当前系统中记录尽量详细的历史信息。原因如下：</span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">在早期，更容易获得系统用户的权限许可（获得系统用户权限后，更容易收集各种数据）。</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">如果你觉得某个问题以后会受到关注，最好是从现状开始就搜集历史数据。</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">如果设计系统的时候考虑了评估指标，这对将来会大有益处。具体来说，这是为了让你以后不用在日志文件中寻找相关的字符串。</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">你能够注意到什么（随着时间）改变了，什么（随着时间）没有改变。举个例子，假设你想要直接优化一天的活跃用户量。然而在早期对系统的处理中可能会发现用户体验的变化并没有显著改变活跃用户量的度量。</span></p></li></ol><p style="caret-color: rgb(51, 51, 51);color: rgb(51, 51, 51);"><br></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #3: Choose machine learning over a complex heuristic.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则3：优先选择机器学习而不是复杂的启发式规则</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">简单的启发式方法可以轻松应用到产品上，而复杂的启发式方法却难以维护。一旦你拥有了足够的数据，并且对要实现的目标有了基本的概念，那就转向机器学习吧。在大多数软件工程中，不管使用的是启发方法还是机器学习模型，都需要经常更新算法。但是你会发现，使用机器学习的模型更容易更新和维护。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><br></span></p><section><section class="Powered-by-XIUMI V5" style="" powered-by="xiumi.us"><section class="" style="margin-top: 10px;margin-bottom: 10px;text-align: center;"><section class="" style="display: inline-block;vertical-align: top;"><section style="width: 8px;height: 8px;float: left;border-top: 1px solid rgb(249, 110, 87);border-radius: 4px 0px 0px;border-left: 1px solid rgb(249, 110, 87);"></section><section style="width: 8px;height: 8px;float: right;border-top: 1px solid rgb(249, 110, 87);border-right: 1px solid rgb(249, 110, 87);border-radius: 0px 4px 0px 0px;"></section><section style="clear: both;"></section><section class="" style="margin-top: -5px;margin-bottom: -5px;padding-right: 8px;padding-left: 8px;line-height: 1.3;"><span style="font-family: Optima-Regular, PingFangTC-light;font-size: 15px;font-weight: 700;">机器学习阶段 1：第一条工作流</span></section><section style="width: 8px;height: 8px;float: left;border-bottom: 1px solid rgb(249, 110, 87);border-left: 1px solid rgb(249, 110, 87);border-radius: 0px 0px 0px 4px;"></section><section style="width: 8px;height: 8px;float: right;border-bottom: 1px solid rgb(249, 110, 87);border-right: 1px solid rgb(249, 110, 87);border-radius: 0px 0px 4px;"></section><section style="clear: both;"></section></section><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><br></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">构建第一个机器学习工作流时，一定要更多关注系统基础架构的建设。虽然机器学习的算法令人激动，但是因为基础架构不给力找不到问题时会令人抓狂。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #4: Keep the first model simple and get the infrastructure right.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">法则4：第一个模型要简单，但是基础架构要正确</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">第一个模型对你的产品提高最大，因此它不需要有多花哨。相反，你会碰到比你想象的多的基础架构方面的问题。在别人使用你的的新机器学习系统前，你需要确定：</span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">如何为你的学习算法得到样本</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">为你的系统初步定义“好”与“坏”的标准</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">如何将模型集成到应用程序中。你可以直接将模型应用到在线应用程序中，也可以在离线样本的基础上对模型进行预计算（pre－compute），然后把与计算的结果储存在表格中。</span></p></li></ol><p style="caret-color: rgb(51, 51, 51);color: rgb(51, 51, 51);"><br></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">选择简单的特征，这样会更容易确保：</span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">特征正确应用到算法中</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">模型能够学习到合理的权重</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">特征正确应用到服务器模型（也就是生产环境的模型）中</span></p></li></ol><p style="caret-color: rgb(51, 51, 51);color: rgb(51, 51, 51);"><br></p><p style="caret-color: rgb(51, 51, 51);color: rgb(51, 51, 51);"><br></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">你的系统如果能够可靠地遵守这三点，你就完成了大多数工作。你的简单模型能够提供基准指标和基准行为，你可以用来测量更加复杂的模型。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #5: Test the infrastructure independently from the machine learning.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则5：独立于机器学习来测试架构流程</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">不仅需要确保基础架构的可测试性，还需要确保系统的学习部分（learning part）是封装好的（encapsulated），这样才能测试所有与之相关的部件。具体来说：</span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">测试输入到算法中的数据。检查应该填充的特征栏是否正确填充。</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">测试模型在训练算法之外的运行情况。确保模型的训练环境中和服务环境中的得分相同。</span></p></li></ol><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">机器学习的一个特点就是不可预测性。因此，你必须确保在训练和实际运行中创造样本的代码能被测试，并且在实际运行中始终使用同一个固定的模型。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #6: Be careful about dropped data when copying pipelines.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则6：复制工作流时留意丢失的数据</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">我们有时候会通过复制已经存在的工作流来创建一个新的工作流。在新的工作流中需要的数据，很可能在旧的数据流就丢弃了。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #7: Turn heuristics into features, or handle them externally.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 7: 将启发规则转化为特征，或者在外部处理它们</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">机器学习系统解决的问题通常都不是新问题，而是对已有问题的进一步优化。这意味着有很多已有的规则或者启发式规则可供使用。这部分信息应该被充分利用。下面是几种启发式规则可以被使用的方式：</span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">用启发式规则进行预处理。 若特征相当完美，则可以采用这个方法。举个例子，在垃圾邮件过滤器中，如果发件人已经被加入黑名单了，则可以不用重新学习“黑名单”的概念。直接阻止该信息就可以！这种方法在二元分类（binary classification）任务中很有用。</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">创建特征。 直接从启发式规则中创建特征会很便捷。举个例子，若要用启发式规则为某个查询结果计算相关度，你可以把分数纳入特征的值中。接下来，用机器学习的方法来处理这些值（例如，把这些值转化为由一系列离散值组成的有限集，或者也可以与其它特征相结合），但是要从启发式方法生成的原始数据入手。</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">挖掘启发式方法的原始输入数据。 对于某款 app，若存在一个启发式方法，其包含安装量、文本字符数和当天日期等要素，可以考虑将这些原始信息单独作为特征使用。</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">修改标签。当你发觉启发式方法捕捉了一些信息，而这些信息没有包含在标记中，这时可以考虑该选项。举个例子，如果你想让下载量达到最大，但同时对内容的质量有要求，那么可以用 app 的平均评级乘以标记来解决问题。</span></p></li></ol><h3 style="margin-top: 1em;margin-bottom: 16px;font-weight: 700;font-size: 1.5em;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">监控</span></h3><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">一般来说，所有系统都要设置良好的警示程序，警报系统需要顺利执行，或者设置一个仪表板页面（dashboard page）。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #8: Know the freshness requirements of your system.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 8: 了解你系统对新鲜度的要求</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">如果你使用的是一天前的旧模型，运行状况会下降多少？如果是一周前的呢？或一个季度前的呢？知道何时该刷新系统能帮助你划分监控的优先级。如果你的模型一天没有更新，受益便下降 10%，因此有必要指派一名工程师时时关注它的动态。大多数广告服务系统每天都会有新的广告需要处理和更新。此外，要留意系统对新鲜度的要求会随着时间变化，特别是在添加或移除特征栏的时候，需要尤为注意。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #9: Detect problems before exporting models.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 9: 输出（发布）模型前发现问题</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">许多机器学习系统都存在这样一个阶段：直接把模型输出运行。如果问题出现在模型输出之后，那么这个问题就是用户所面临的问题。而如果问题出现在模型输出之前，就是训练过程中的问题，用户不会发现。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">输出模型之前请做好完整性检查（sanity check）。具体来讲，确保模型在留存数据上运行合理，例如AUC。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #10: Watch for silent failures.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则10：注意隐藏性故障</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">比起其它系统，机器学习系统更容易出现潜在的问题。假设系统的某个特定的表格不再进行更新，整个系统通过调整仍会保持良好的运行水准，但是会慢慢衰减。有时有些表格几个月都不会刷新一次，而只需简单的刷新就能大幅度提升系统的运行水准，效果甚至超过该季度最新发布的那些模型！例如，由于系统实现（implementation）发生变化，特征的覆盖范围也会发生相应的变化：比如，某个特征栏刚开始可能包含 90%的样本，接下来却可能突然下降到 60％。解决方法是是对关键数据的统计信息进行监控，并且周期性对关键数据进行人工检查。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #11: Give feature columns owners and documentation.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 11：为特征栏指定负责人并记录文档</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">如果系统的规模比较大，并且特征栏比较多，那么必须清楚每个特征栏的创建者或者维护者。如果某个了解该特征栏的人离开了，一定要确保另外还有人了解这部分信息。虽然很多特征栏的名字非常直观，但最好还是使用更详尽的文档来描述这些特征的内容、来自哪里以及它们的作用。</span></p><h3 style="margin-top: 1em;margin-bottom: 16px;font-weight: 700;font-size: 1.5em;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">你的第一个目标（Objective）</span></h3><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">objective 是模型试图优化的值，而 metric 指的是任何用来评估系统的值。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #12: Don’t overthink which objective you choose to directly optimize.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 12: 不要过于纠结该优化哪个目标</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">你有成千上万关心的指标，这些指标也值得你去测试。但是，在机器学习过程的早期，你会发现，即使你并没有直接去优化，他们也都会上升。比如，你关心点击次数，停留时间以及每日活跃用户数。如果仅优化了点击次数，通常也会看到停留时间增加了。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">所以，当提高所有的指标都不难的时候，就没必要花心思来如何权衡不同的指标。不过过犹不及：不要混淆了你的目标和系统的整体健康度。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #13: Choose a simple, observable and attributable metric for your first objective.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 13：选择一个简单、可观测并且可归类的评估指标（metric）作为你的第一个目标（objective）</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">有时候你自以为你清楚真实的目标,但随着你对数据的观察，对老系统和新的机器学习系统的分析，你会发现你又想要调整。而且，不同的团队成员对于真实目标并不能达成一致。机器学习的目标必须是能很容易测量的，并且一定是“真实”目标的代言。因此，在简单的机器学习目标上训练，并创建一个“决策层”，以允许你在上面增加额外的逻辑（这些逻辑，越简单越好）来形成最后的排序。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">最容易建模的是那些可以直接观察并可归属到系统的某个动作的用户行为：</span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">排序的链接被点击了吗？</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">排序的物品被下载了吗？</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">排序的物品被转发/回复/邮件订阅了吗？</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">排序的物品被评价了吗？</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">展示的物品是否被标注为垃圾/色情/暴力？</span></p></li></ol><p style="caret-color: rgb(51, 51, 51);color: rgb(51, 51, 51);"><br></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">最开始要避免对间接效果建模：</span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">用户第二天会来访吗？</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">用户访问时间是多长？</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">每日活跃用户是什么样的？</span></p></li></ol><p style="caret-color: rgb(51, 51, 51);color: rgb(51, 51, 51);"><br></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">间接效果是非常重要的指标，在A/B test和发布决定的时候可以使用。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">最后，不要试图让机器学习来回答以下问题：</span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">用户使用你的产品是否开心</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">用户是否有满意的体验</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">产品是否提高了用户的整体幸福感</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">这些是否影响了公司的整体健康度</span></p></li></ol><p style="caret-color: rgb(51, 51, 51);color: rgb(51, 51, 51);"><br></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">这些都很重要，但太难评估了。与其如此，不如考虑其他代替的：比如，用户如果高兴，那停留时间就应该更长。如果用户满意，他就会再次造访。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #14: Starting with an interpretable model makes debugging easier.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 14：从容易解释的模型入手会让调试过程更加容易</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">线性回归，逻辑回归和泊松回归直接由概率模型激发。每个预测可解释为概率或期望值。这使得他们比那些使用目标来直接优化分类准确性和排序性能的模型要更容易调试。比如，如果训练时的概率和预测时的概率，或者生产系统上的查看到的概率有偏差，那说明存在某种问题。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #15: Separate Spam Filtering and Quality Ranking in a Policy Layer.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 15：在策略层将垃圾信息过滤和质量排名分开</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">质量排名是一门艺术，而垃圾过滤是一场战争。那些使用你系统的人非常清楚你采用什么来评价一篇帖子的质量，所以他们会想尽办法来使得他们的帖子具有这些属性。因此，质量排序应该关注对哪些诚实发布的内容进行排序。如果将垃圾邮件排高名次，那质量排序学习器就大打折扣。同理也要将粗俗的内容从质量排序中拿出分开处理。垃圾过滤就是另外一回事。你必须考虑到要生成的特征会经常性的改变。你会输入很多明显的规则到系统中。至少要保证你的模型是每日更新的。同时，要重点考虑内容创建者的信誉问题。</span></p><p><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"></span></p><section><section class="Powered-by-XIUMI V5" style="" powered-by="xiumi.us"><section class="" style="margin-top: 10px;margin-bottom: 10px;text-align: center;"><section class="" style="display: inline-block;vertical-align: top;"><section style="width: 8px;height: 8px;float: left;border-top: 1px solid rgb(249, 110, 87);border-radius: 4px 0px 0px;border-left: 1px solid rgb(249, 110, 87);"></section><section style="width: 8px;height: 8px;float: right;border-top: 1px solid rgb(249, 110, 87);border-right: 1px solid rgb(249, 110, 87);border-radius: 0px 4px 0px 0px;"></section><section style="clear: both;"></section><section class="" style="margin-top: -5px;margin-bottom: -5px;padding-right: 8px;padding-left: 8px;line-height: 1.3;"><span style="font-family: Optima-Regular, PingFangTC-light;font-size: 15px;font-weight: 700;">机器学习阶段 2：特征工程</span></section><section style="width: 8px;height: 8px;float: left;border-bottom: 1px solid rgb(249, 110, 87);border-left: 1px solid rgb(249, 110, 87);border-radius: 0px 0px 0px 4px;"></section><section style="width: 8px;height: 8px;float: right;border-bottom: 1px solid rgb(249, 110, 87);border-right: 1px solid rgb(249, 110, 87);border-radius: 0px 0px 4px;"></section><section style="clear: both;"></section></section></section></section></section><p><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"></span><br></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">在机器学习系统研发周期的第一阶段，重点是把训练数据导入学习系统，得到感兴趣的评价指标，并创建基础架构。当你有了一个端对端的系统，并且该系统的单元和测试都仪表化之后，第二阶段便开始了。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">第二阶段需要纳入尽可能多的有效特征，并依据直观的感觉组合起来。在这个阶段，所有的评估指标仍然会上升。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #16: Plan to launch and iterate.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则16：做好持续迭代上线的准备</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">不要期望现在发布的这个模型是最终的模型。因此，考虑你给当前这个模型增加的复杂度会不会减慢后续的发布。许多团队每季度推出一个模型或者更多年。之所以不断发布新模型，有三个基本原因：</span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">你会不断地想到新的特征。</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">你会不断地调整并以新的方式组合旧的特征。</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">你会不断调优目标。</span></p></li></ol><p style="caret-color: rgb(51, 51, 51);color: rgb(51, 51, 51);"><br></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #17: Start with directly observed and reported features as opposed to learned features.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 17：优先使用直接观测或收集到的特征，而不是学习出来的特征（learned features）</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">先描述一下什么是学习出来的特征（learned features）。学习出来的特征（learned features）是由外部系统（比如无监督聚类系统）或学习者本身（比如因子模型、深度学习）生成的特征。两种方式生成的特征都很有用，但也有很多问题，因此不应当用在第一个模型中。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #18: Explore with features of content that generalize across contexts.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 18：探索使用可以跨场景的内容特征</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">通常情况下，机器学习只占到一个大系统中的很小一部分，因此你必须要试着从不同角度审视一个用户行为。比如热门推荐这一场景，一般情况下论坛里“热门推荐”里的帖子都会有许多评论、分享和阅读量，如果利用这些统计数据对模型展开训练，然后对一个新帖子进行优化，就有可能使其成为热门帖子。另一方面，YouTube上自动播放的下一个视频也有许多选择，例如可以根据大部分用户的观看顺序推荐，或者根据用户评分推荐等。总之，如果你将一个用户行为用作模型的标记（label），那么在不同的上下文条件下审视这一行为，可能会得到更丰富的特征（feature），也就更利于模型的训练。需要注意的是这与个性化不同：个性化是确定用户是否在特定的上下文环境中喜欢某一内容，并发现哪些用户喜欢，喜欢的程度如何。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #19: Use very specific features when you can.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 19：尽量使用非常具体的特征</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">在海量数据的支持下，即使学习数百万个简单的特征也比仅仅学习几个复杂的特征要容易实现。由于被检索的文本标识与规范化的查询并不会提供太多的归一化信息，只会调整头部查询中的标记排序。因此你不必担心虽然整体的数据覆盖率高达90%以上，但针对每个特征组里的单一特征却没有多少训练数据可用的情况。另外，你也可以尝试正则化的方法来增加每个特征所对应的样本数。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #20: Combine and modify existing features to create new features in human–understandable ways.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 20: 用人类可理解的方式对已有特征进行组合和修改</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">有很多种方法组合和改良特征。像 TensorFlow 这样的机器学习系统，它允许通过 transformations 预处理数据。其最标准的两种方法分别是“discretization（离散化）”和“crosses（叉积）”。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Discretization 会根据一个连续的特征创建许多离散的特征。假定年龄是一个连续的特征。我们可以创建如下特征，当年龄小于 18 时记为 1，或者当年龄在 18 到35 岁之间时为 1，以此类推。不用过多考虑这些数据的边界问题：简单的数字可以给你最直观的冲击。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Cross由两个或多个特征栏组成。根据TensorFlow给出的解释， 特征栏是一组同类的特征。（如｛男，女｝、｛美国，加拿大，墨西哥｝等）。而Cross是一个新的特征栏，可以用｛男，女｝×｛美国，加拿大，墨西哥｝等来简单的表示。新的<br>特征栏会包含以下特征，如｛男，加拿大｝。使用TensorFlow时可以让它帮你创建cross。｛男，加拿大｝可以在样本中代表男性加拿大人。注意若模型使用三个以上的特征栏组成的cross，则需要大量的数据来训练模型。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Cross 会产生庞大的特征栏，有可能导致过拟合现象。举个例子，假设你要做某种搜索。检索词构成一个特征栏，文档中的词构成另一个特征栏。你可以通过cross 来组合它们，但这样会出现很多特征。处理文本时，有两个替代性方案。最苛刻的方案是 dot product（点积）。点积仅统计检索词和文档词中的公共词汇。得到的特征可以被离散化。另一种方案是取intersection（交集）：</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">因此，我们有一个特征来表示当“pony（色情）”这个词同时出现在文档和检索词中，另一个特征表示“the”同时出现在文档和检索词中。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #21: The number of feature weights you can learn in a linear model is roughly proportional to the amount of data you have.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 21：线性模型中的特征权重的数量应大致和样本数量形成一定的比例</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">关于模型究竟多复杂才合适，统计学习理论有许多有趣的结论。但总的来说，这一条法则就足够了。我曾和一些人交流过，在他们看来，要想学到些东西，一千个样本远远不够，至少需要一百万个样本。这是因为，他们被特定的学习方法束缚了手脚。而诀窍就是，根据数据大小调整学习方法：</span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">如果你在开发一个搜索排名系统，并且有数百万不同的词汇存在于文档和检索词中，而你仅有 1000 个带有标记的样本。那么你应该使用文档和检索词的点积特征、 TF－IDF 以及其它六个人工设计的特征。 1000 个样本，对应 12个左右的特征。</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">如果有一百万个的样本，那就通过 regularization 或特征 selection，取文档特征栏和检索词特征栏的交集。这样你能得到数百万个特征，但 regularization会帮你减少些许的特征。一千万个样本，对应大约十万个特征。</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">如果有十亿个乃至几千亿个样本，你可以通过 regularization 和特征选取，取文档特征栏和 query token 的叉积。如果有十亿个样本，那么你会得到一千万个特征。</span></p></li></ol><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #22: Clean up features you are no longer using.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则22：清理不再使用的特征</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">当决定要清除哪些特征时，需要考虑其覆盖率，即该项特征覆盖了多少样本。举个例子，如果你有一些比较特别的特征，但只有 8% 的用户与之相关，那么这些特征就无足轻重了。同时，有些特征可能超越它们的权重。比如某个特征仅覆盖 1% 的数据，但 90% 的正样本都含有这种特征。那么，也应当将这个特征添加进来。</span></p><h3 style="margin-top: 1em;margin-bottom: 16px;font-weight: 700;font-size: 1.5em;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">系统的人工分析</span></h3><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">在进入机器学习第三阶段前，有一些在机器学习课程上学习不到的内容也非常值得关注：如何检测一个模型并改进它。这与其说是门科学，还不如说是一门艺术。这里再介绍几种要避免的反模式（anti-patterns）</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #23: You are not a typical end user.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 23: 你并非典型终端用户</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">这可能是让一个团队陷入困境的最简单的方法。虽然fishfooding（只在团队内部使用原型）和dogfooding（只在公司内部使用原型）都有许多优点，但无论哪一种，开发者都应该首先确认这种方式是否符合性能要求。要避免使用一个明显不好的改变，同时，任何看起来合理的产品策略也应该进一步的测试，不管是通过让非专业人士来回答问题，还是通过一个对真实用户的线上实验。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #24: Measure the delta between models.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则24：测量模型间的差异</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">在将你的模型发布上线前，一个最简单，有时也是最有效的测试是比较你当前的模型和已经交付的模型生产的结果之间的差异。如果差异很小，那不再需要做实验，你也知道你这个模型不会带来什么改变。如果差异很大，那就要继续确定这种改变是不是好的。检查对等差分很大的查询能帮助理解改变的性质（是变好，还是变坏）。但是，使用不同模型进行比较前，需要确保该模型和它本身比较，这个差异很小（理想情况应该是无任何差异）。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #25: When choosing models, utilitarian performance trumps predictive power.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 25: 选择模型时，性能表现比预测力更重要</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">虽然我们训练模型时 objective 一般都是 logloss，也就是说实在追求模型的预测能力。但是我们在上层应用中却可能有多种用途，例如可能会用来排序，那么这时具体的预测能力就不如排序能力重要；如果用来划定阈值然后跟根据阈值判断垃圾邮件，那么准确率就更重要。当然大多数情况下这几个指标是一致的。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #26: Look for patterns in the measured errors, and create new features.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 26: 在错误中寻找规律，然后创建新特征</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">假设你的模型在某个样本中预测错误。在分类任务中，这可能是误报或漏报。在排名任务中，这可能是一个正向判断弱于逆向判断的组。但更重要的是，在这个样本中机器学习系统知道它错了，需要修正。如果你此时给模型一个允许它修复的特征，那么模型将尝试自行修复这个错误。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">另一方面，如果你尝试基于未出错的样本创建特征，那么该特征将很可能被系统忽略。例如，假设在 Google Play商店的应用搜索中，有人搜索“免费游戏”，但其中一个排名靠前的搜索结果却是一款其他App，所以你为其他App创建了一个特征。但如果你将其他App的安装数最大化，即人们在搜索免费游戏时安装了其他App，那么这个其他App的特征就不会产生其应有的效果。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">所以，正确的做法是一旦出现样本错误，那么应该在当前的特征集之外寻找解决方案。例如，如果你的系统降低了内容较长的帖子的排名，那就应该普遍增加帖子的长度。而且也不要拘泥于太具体的细节。例如你要增加帖子的长度，就不要猜测长度的具体含义，而应该直接添加几个相关的特征，交给模型自行处理，这才是最简单有效的方法。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #27: Try to quantify observed undesirable behavior.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 27：尝试量化观察到的异常行为</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">如果在系统中观察到了模型没有优化到的问题，典型的例如推荐系统逼格不够这种问题，这时应该努力将这种不满意转化为具体的数字，具体来讲可以通过人工标注等方法标注出不满意的物品，然后进行统计。如果问题可以被量化，后面就可以将其用作特征、objective或者metric。整体原则就是“<span style="font-family: Optima-Regular, PingFangTC-light;font-size: 15px;font-weight: 700;">先量化，再优化</span>”。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #28: Be aware that identical short-term behavior does not imply identical long-term behavior.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 28：短期行为相同并不代表长期行为也相同</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">假设你有一个新系统，它可以查看每个doc_id和exact_query，然后根据每个文档的每次查询行为计算其点击率。你发现它的行为几乎与当前系统的并行和A/B测试结果完全相同，而且它很简单，于是你启动了这个系统。却没有新的应用显示，为什么？由于你的系统只基于自己的历史查询记录显示文档，所以不知道应该显示一个新的文档。<br>要了解一个系统在长期行为中如何工作的唯一办法，就是让它只基于当前的模型数据展开训练。这一点非常困难。</span></p><h3 style="margin-top: 1em;margin-bottom: 16px;font-weight: 700;font-size: 1.5em;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>训练偏差（Training－Serving Skew）</strong></span></h3><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">训练偏差是指训练时的表现和在生产环境中实际运行时的表现的差别。这种偏差可能由以下因素引起：</span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">在训练时和在实际工作流中用不同的方式处理数据。</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">训练中的数据和在实际运行中的分布不同。</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">模型和算法之间存在反馈循环。</span></p><p style="text-align: left;"><br></p></li></ol><p style="caret-color: rgb(51, 51, 51);color: rgb(51, 51, 51);"><br></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">解决这类问题的核心是对系统和数据的变化进行监控，确保一切差异都在监控之内，不会悄悄进入系统。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #29: The best way to make sure that you train like you serve is to save the set of features used at serving time, and then pipe those features to a log to use them at training time.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 29： 要让实际产品和训练时表现一样好，最好的方法是实际运行中保留特征集，并记录到日志中以便训练中使用</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">即使你不能对每个样例都这样做，做一小部分也比什么也不做好，这样你就可以验证服务和训练之间的一致性（见规则37）。在 Google 采取了这项措施的团队有时候会对其效果感到惊讶。比如YouTube主页在服务时会切换到日志记录特征，这不仅大大提高了服务质量，而且减少了代码复杂度。目前有许多团队都已经在其基础设施上采用了这种策略。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #30: Importance-weight sampled data, don’t arbitrarily drop it!</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则30：给抽样数据按重要性赋权重，不要随意丢弃它们</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">当我们有太多训练数据时，我们会只取其中的一部分。但这是错误的。正确的做法是，如果你给某条样本30%的采样权重，那么在训练时就给它10/3的训练权重。通过这样的重要性赋权（importance weight），整个训练结果的校准性（calibration）就还能够保证。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #31: Beware that if you join data from a table at training and serving time, the data in the table may change.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 31：如果要从表格中组合数据，注意训练时和实际运行时表格可能发生改变</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">假设你要把文档 id 和包含文档特征的表格（比如评论或点击的数量）结合起来。从训练和实际运行，表格中的特征可能会改变（例如用户对物品的评论数），模型对同一文档做的预测也能不同。要避免这这类问题，最简单的办法就是记录所有实际运行时的特征。若表格只是缓慢的变化，你也可以按照每小时或每天的频率对其做出记录，得到足够相近的数据。注意这样不能完美的解决问题。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #32: Re-use code between your training pipeline and your serving pipeline whenever possible.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 32: 尽量在训练流和实际运行流中使用重复代码</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">首先需要明确一点：批处理和在线处理并不一样。在线处理中，你必须及时处理每一个请求（比如，必须为每个查询单独查找），而批处理，你可以合并完成。服务时，你要做的是在线处理，而训练是批处理任务。尽管如此，还是有很多可以重用代码的地方。比如说，你可以创建特定于系统的对象，其中的所有联结和查询结果都以人类可读的方式存储，错误也可以被简单地测试。然后，一旦在服务或训练期间收集了所有信息，你就可以通过一种通用方法在这个特定对象和机器学习系统需要的格式之间形成互通，训练和服务的偏差也得以消除。因此，尽量不要在训练时和服务时使用不同的变成语言，毕竟这样会让你没法重用代码。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #33: If you produce a model based on the data until January 5th, test the model on the data from January 6th and after.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 33: 如果训练数据是1月5日之前的，那么测试数据要从1月6日开始</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">测试模型时应当使用的比训练模型时更加新的数据，因为这更能反映你的系统实际运行表现。如果你用 1 月 5 日前的数据生成了一个模型，那就得用 1月 6 号之后的数据测试它。你会发现，在新的数据下模型表现得没那么好，但也不会差到哪里去。这个结果更加接近真实运行时的表现。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #34: In binary classification for filtering (such as spam detection or determining interesting emails), make small short-term sacrifices in performance for very clean data.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 34：在过滤类的任务中，被标记为负的样本是不会展示给用户的，例如可能会把75%标记为负的样本阻拦住不展现给用户。但如果你只从展示给用户的结果中获取下次训练的样本，显然你的训练样本是有偏的</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">更好的做法是使用一定比例的流量（例如1%）专门收集训练数据，在这部分流量中的用户会看到所有的样本。这样显然会影响线上的真实过滤效果，但是会收集到更好的数据，更有利于系统的长远发展。否则系统会越训练越偏，慢慢就不可用了。同时还能保证至少过滤掉74%的负样本，对系统的影响也不是很大。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">但是如果你的系统会过滤掉95%或者更多的负样本，这种做法就不那么可行了。即使如此，为了准确衡量模型的效果，你仍然可以通过构造一个更小的数据集（0.1%或者更小）来测试。十万级别的样本足够给出准确的评价指标了。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #35: Beware of the inherent skew in ranking problems.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 35: 注意排序问题存在固有偏差</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">当你对排序算法做出足够多的改动时，一方面会引起完全不同的排序结果，另一方面也可能在很大程度上改变算法未来可能要处理的数据。这会引入一些固有偏差，因此你必须事先充分认识到这一点。以下这些方法可以有效帮你优化训练数据。</span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">对涵盖更多查询的特征进行更高的正则化，而不是那些只覆盖单一查询的特征。这种方式使得模型更偏好那些针对个别查询的特征，而不是那些能够泛化到全部查询的特征。这种方式能够帮助阻止非常流行的结果进入不相关查询。这点和更传统的建议不一样，传统建议应该对更独特的特征集进行更高的正则化。</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">只允许特征具有正向权重，这样一来就能保证任何好特征都会比未知特征合适。</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">不要有那些仅仅偏文档（document-only）的特征。这是法则1的极端版本。比如，不管搜索请求是什么，即使一个给定的应用程序是当前的热门下载，你也不会想在所有地方都显示它。没有仅仅偏文档类特征，这会很容易实现。</span></p></li></ol><p style="caret-color: rgb(51, 51, 51);color: rgb(51, 51, 51);"><br></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #36: Avoid feedback loops with positional features.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 36：用位置特征来避免反馈回路</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">大家都知道排序位置本身就会影响用户是否会对物品产生互动，例如点击。所以如果模型中没有位置特征，本来由于位置导致的影响会被算到其他特征头上去，导致模型不够准。可以用加入位置特征的方法来避免这种问题，具体来讲，在训练时加入位置特征，预测时去掉位置特征，或者给所有样本一样的位置特征。这样会让模型更正确地分配特征的权重。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">需要注意的是，位置特征要保持相对独立，不要与其他特征发生关联。可以将位置相关的特征用一个函数表达，然后将其他特征用另外的函数表达，然后组合起来。具体应用中，可以通过位置特征不与任何其他特征交叉来实现这个目的。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Measure Training/Serving Skew.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 37: 衡量训练和服务之间的差异</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">很多情况会引起偏差。大致上分为一些几种：</span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">训练集和测试集之间的差异。这种差异会经常存在，而且不一定是坏事。</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">测试集和“第二天”数据间的差异。这种差异也会一直存在，而这个“第二天”数据上的表现是我们应该努力优化的，例如通过正则化。这两者之间差异如果过大，可能是因为用到了一些时间敏感的特征，导致模型效果变化明显。</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">“第二天”数据和线上数据间的差异。如果同样一条样本，在训练时给出的结果和线上服务时给出的结果不一致，那么这意味着工程实现中出现了bug。</span></p></li></ol><p style="caret-color: rgb(51, 51, 51);color: rgb(51, 51, 51);"><br></p><section><section class="Powered-by-XIUMI V5" style="" powered-by="xiumi.us"><section class="" style="margin-top: 10px;margin-bottom: 10px;text-align: center;"><section class="" style="display: inline-block;vertical-align: top;"><section style="width: 8px;height: 8px;float: left;border-top: 1px solid rgb(249, 110, 87);border-radius: 4px 0px 0px;border-left: 1px solid rgb(249, 110, 87);"></section><section style="width: 8px;height: 8px;float: right;border-top: 1px solid rgb(249, 110, 87);border-right: 1px solid rgb(249, 110, 87);border-radius: 0px 4px 0px 0px;"></section><section style="clear: both;"></section><section class="" style="margin-top: -5px;margin-bottom: -5px;padding-right: 8px;padding-left: 8px;line-height: 1.3;"><span style="font-family: Optima-Regular, PingFangTC-light;font-size: 15px;font-weight: 700;">机器学习第三阶段：放慢速度、优化细化和复杂的模型</span></section><section style="width: 8px;height: 8px;float: left;border-bottom: 1px solid rgb(249, 110, 87);border-left: 1px solid rgb(249, 110, 87);border-radius: 0px 0px 0px 4px;"></section><section style="width: 8px;height: 8px;float: right;border-bottom: 1px solid rgb(249, 110, 87);border-right: 1px solid rgb(249, 110, 87);border-radius: 0px 0px 4px;"></section><section style="clear: both;"></section></section></section></section></section><p style="margin-bottom: 16px;"><span style="font-family: Optima-Regular, PingFangTC-light;font-size: 15px;">一般会有一些明确的信号来标识第二阶段的尾声。首先，每月的提升会逐步降低。你开始在不同指标之间做权衡，有的上升有的下降。</span><br></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">这将会变得越来越有趣。增长越来越难实现，必须要考虑更加复杂的机器学习。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">警告：相对于前面两个阶段，这部分会有很多开放式的法则。第一阶段和第二阶段的机器学习是快乐的。当到了第三阶段，每个团队就不能不去找到他们自己的途径了。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #38: Don’t waste time on new features if unaligned objectives have become the issue.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 38： 如果目标没有达成一致，就不要在新特征上浪费时间</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">当达到评估指标瓶颈，你的团队开始关注机器学习系统目标范围之外的问题。如同之前提到的，如果产品目标没有包括在算法目标之内，你就得修改其中一个。比如说，你也许优化的是点击数、点赞或者下载量，但发布决策还是依赖于人类评估者。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #39: Launch decisions are a proxy for long-term product goals.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 39：模型发布决策是长期产品目标的代理</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">这个法则字面上有点难以理解，其实作者核心就是在讲一件事情：系统、产品甚至公司的长远发展需要通过多个指标来综合衡量，而新模型是否上线要综合考虑这些指标。所谓代理，指的就是优化这些综合指标就是在优化产品、公司的长远目标。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">决策只有在所有指标都在变好的情况下才会变得简单。但常常事情没那么简单，尤其是当不同指标之间无法换算的时候，例如A系统有一百万日活和四百万日收入，B系统有两百万日活和两百万日收入，你会从A切换到B吗？或者反过来？答案是或许都不会，因为你不知道某个指标的提升是否会cover另外一个指标的下降。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">关键是，没有任何一个指标能回答：“五年后我的产品在哪里”？</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">而每个个体，尤其是工程师们，显然更喜欢能够直接优化的目标，而这也是机器学习系统常见的场景 。现在也有一些多目标学习系统在试图解决这种问题。但仍然有很多目标无法建模为机器学习问题，比如用户为什么会来访问你的网站等等。作者说这是个AI-complete问题，也常被称为强AI问题，简单来说就是不能用某个单一算法解决的问题。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #40: Keep ensembles simple.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 40: 保持模型集合（ensembles）的简单性</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">接收原始特征、直接对内容排序的统一模型，是最容易理解、最容易修补漏洞的模型。但是，一个集成模型（一个把其他模型得分组合在一起的“模型”）的效果会更好。为保持简洁，每个模型应该要么是一个只接收其他模型的输入的集成模型，要么是一个有多种特征的基础模型，但不能两者皆是。如果你有单独训练、基于其它模型的模型，把它们组合到一起会导致不好的行为。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">只使用简单模型来集成那些仅仅把你的基础模型输出当做输入。你同样想要给这些集成模型加上属性。比如，基础模型生成得分的提高，不应该降低集成模型的分数。另外，如果连入模型在语义上可解释（比如校准了的）就最好了，这样其下层模型的改变不会影响集成模型。此外，强行让下层分类器预测的概率升高，不会降低集成模型的预测概率。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #41: When performance plateaus, look for qualitatively new sources of information to add rather than refining existing signals.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 41：当效果进入瓶颈期，寻找本质上新的信息源，而不是优化已有的信号。</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">你先是添加了一些用户的人口信息，又添加了一些文档词汇的信息，接着你又浏览了一遍模版，而后又调整了规则，但是最后，关键度量却只提升了不到 1%。现在怎么办？</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">这时候应该用完全不同的特征搭建基础架构，比如用户昨天／上周／去年访问的文档的历史记录。利用 wikidata 或对公司来说比较重要的东西（比如 Google 的知识图）。你或许需要使用深度学习。开始调整你对投资回报的期望，并作出相应努力。如同所有工程项目，你需要平衡新增加的特征与提高的复杂度。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #42: Don’t expect diversity, personalization, or relevance to be as correlated with popularity as you think they are.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 42：不要期望多样性、个性化、相关性和受欢迎程度之间有紧密联系</strong></span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">一系列内容的多样性能意味着许多东西，内容来源的多样性最为普遍。个性化意味着每个用户都能获得它自己感兴趣的结果。相关性意味着一个特定的查询对于某个查询总比其他更合适。显然，这三个属性的定义和标准都不相同。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">问题是标准很难打破。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">注意：如果你的系统在统计点击量、耗费时间、浏览数、点赞数、分享数等等，你事实上在衡量内容的受欢迎程度。有团队试图学习具备多样性的个性化模型。为个性化，他们加入允许系统进行个性化的特征（有的特征代表用户兴趣），或者加入多样性（表示该文档与其它返回文档有相同特征的特征，比如作者和内容），然后发现这些特征比他们预想的得到更低的权重（有时是不同的信号）。</span></p><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">这不意味着多样性、个性化和相关性就不重要。就像之前的规则指出的，你可以通过后处理来增加多样性或者相关性。如果你看到更长远的目标增长了，那至少你可以声称，除了受欢迎度，多样性/相关性是有价值的。你可以继续使用后处理，或者你也可以基于多样性或相关性直接修改你的目标。</span></p><blockquote style="margin-bottom: 16px;padding: 0.5em;border-left-width: 4px;border-color: rgb(51, 51, 51);"><p style="font-size: 13px;line-height: 1.5;text-align: left;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Rule #43: Your friends tend to be the same across different products. Your interests tend not to be.</span></p></blockquote><p style="margin-bottom: 16px;"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;"><strong>法则 43: 在不同的产品中，你的朋友可能相同，但兴趣却不尽然</strong></span></p><p style=";"><span style="font-size: 15px;font-family: Optima-Regular, PingFangTC-light;">Google 经常在不同产品上使用同样的好友关系预测模型，并且取得了很好的效果，这证明不同的产品上好友关系是可以迁移的，毕竟他们是固定的同一批人。但他们尝试将一个产品上的个性化特征使用到另外一个产品上时却常常得不到好结果。可行的做法是使用一个数据源上的原始数据来预测另外数据源上的行为，而不是使用加工后的特征。此外，用户在另一个数据源上的行为历史也会有用。</span></p>
</section></section></section></section></section></section></section></section></section></section></section></section>]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>【转】从机器学习谈起</title>
    <url>/2018/03/21/%E4%BB%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%B0%88%E8%B5%B7/</url>
    <content><![CDATA[<p>从决定向AI方向转已经过了半年了，书买了好多，还没有拆封，四月了，这次是必须下决心了，从博客搭建开始，记录学习过程，首先从这里开始，是最初看了这篇博客，开始了解机器学习，感谢博主的帮助</p>
<p>博客原地址：<br><a href="https://www.cnblogs.com/subconscious/p/4107357.html#first" target="_blank" rel="noopener">https://www.cnblogs.com/subconscious/p/4107357.html#first</a></p>
<p>作者：计算机的潜意识</p>
<p><a id="cb_post_title_url" class="postTitle2" href="http://www.cnblogs.com/subconscious/p/4107357.html" target="_blank" rel="noopener">从机器学习谈起</a><br>        <br>        <div class="clear"></div><br>        <div class="postBody"><br>            <div id="cnblogs_post_body" class="blogpost-body"><p><span style="font-size: 14px;">　　</span><span style="font-size: 14px;">在本篇文章中，我将对机器学习做个概要的介绍。本文的目的是能让即便完全不了解机器学习的人也能了解机器学习，并且上手相关的实践。这篇文档也算是EasyPR开发的番外篇，从这里开始，必须对机器学习了解才能进一步介绍EasyPR的内核。当然，本文也面对一般读者，不会对阅读有相关的前提要求。</span></p></div></div></p>
<p><span style="font-size: 14px;">　　在进入正题前，我想读者心中可能会有一个疑惑：机器学习有什么重要性，以至于要阅读完这篇非常长的文章呢？</span></p><br><p><span style="font-size: 14px;">　　我并不直接回答这个问题前。相反，我想请大家看两张图，下图是图一：</span></p><br><p style="text-align: center;"><img src="https://images0.cnblogs.com/blog/673793/201412/291235317785665.png" alt="" width="844" height="399"><br>&nbsp;<span style="font-size: 14px;"><span style="font-family: 黑体;">图1 机器学习界的执牛耳者与互联网界的大鳄的联姻</span>　　</span></p><br><p>　　这幅图上上的三人是当今机器学习界的执牛耳者。中间的是Geoffrey Hinton, 加拿大多伦多大学的教授，如今被聘为&ldquo;Google大脑&rdquo;的负责人。右边的是Yann LeCun, 纽约大学教授，如今是Facebook人工智能实验室的主任。而左边的大家都很熟悉，Andrew Ng，中文名吴恩达，斯坦福大学副教授，如今也是&ldquo;百度大脑&rdquo;的负责人与百度首席科学家。这三位都是目前业界炙手可热的大牛，被互联网界大鳄求贤若渴的聘请，足见他们的重要性。而他们的研究方向，则全部都是机器学习的子类–深度学习。<br><br>　　下图是图二：</p><br><p style="text-align: center;"><span style="font-size: 14px;"><img style="display: block; margin-left: auto; margin-right: auto;" src="https://images0.cnblogs.com/blog/673793/201411/190845269685692.jpg" alt="" width="582" height="353"><span style="font-family: 黑体;">图2 </span>语音助手产品</span></p><br><p><span style="font-size: 14px;">　　这幅图上描述的是什么？Windows Phone上的语音助手Cortana，名字来源于《光环》中士官长的助手。相比其他竞争对手，微软很迟才推出这个服务。Cortana背后的核心技术是什么，为什么它能够听懂人的语音？事实上，这个技术正是机器学习。机器学习是所有语音助手产品(包括Apple的siri与Google的Now)能够跟人交互的关键技术。</span></p><br><p><span style="font-size: 14px;">　　通过上面两图，我相信大家可以看出机器学习似乎是一个很重要的，有很多未知特性的技术。学习它似乎是一件有趣的任务。实际上，学习机器学习不仅可以帮助我们了解互联网界最新的趋势，同时也可以知道伴随我们的便利服务的实现技术。</span></p><br><p><span style="font-size: 14px;">　　机器学习是什么，为什么它能有这么大的魔力，这些问题正是本文要回答的。同时，本文叫做&ldquo;从机器学习谈起&rdquo;，因此会以漫谈的形式介绍跟机器学习相关的所有内容，包括学科(如数据挖掘、计算机视觉等)，算法(神经网络，svm)等等。本文的主要目录如下：<br><br>　　1.<a href="#first">一个故事说明什么是机器学习</a><br><br>　　2.<a href="#second">机器学习的定义</a><br><br>　　3.<a href="#third">机器学习的范围</a><br><br>　　4.<a href="#four">机器学习的方法</a><br><br>　　5.<a href="#five">机器学习的应用–大数据</a><br><br>　　6.<a href="#six">机器学习的子类–深度学习</a><br><br>　　7.<a href="#seven">机器学习的父类–人工智能</a><br><br>　　8.<a href="#eight">机器学习的思考–计算机的潜意识</a><br><br>　　9.<a href="#nine">总结</a><br><br>　　10.<a href="#ten">后记</a><br><br></span></p><br><p><span style="font-size: 14px;"><strong><a name="first"></a>1.<span style="font-size: 14px;">一个故事说明什么是机器学习</span></strong></span></p><br><p>　　机器学习这个词是让人疑惑的，首先它是英文名称Machine Learning(简称ML)的直译，在计算界Machine一般指计算机。这个名字使用了拟人的手法，说明了这门技术是让机器&ldquo;学习&rdquo;的技术。但是计算机是死的，怎么可能像人类一样&ldquo;学习&rdquo;呢？<br><br>　　传统上如果我们想让计算机工作，我们给它一串指令，然后它遵照这个指令一步步执行下去。有因有果，非常明确。但这样的方式在机器学习中行不通。机器学习根本不接受你输入的指令，相反，它接受你输入的数据! 也就是说，机器学习是一种让计算机利用数据而不是指令来进行各种工作的方法。这听起来非常不可思议，但结果上却是非常可行的。&ldquo;统计&rdquo;思想将在你学习&ldquo;机器学习&rdquo;相关理念时无时无刻不伴随，相关而不是因果的概念将是支撑机器学习能够工作的核心概念。你会颠覆对你以前所有程序中建立的因果无处不在的根本理念。<br><br>　　下面我通过一个故事来简单地阐明什么是机器学习。这个故事比较适合用在知乎上作为一个概念的阐明。在这里，这个故事没有展开，但相关内容与核心是存在的。如果你想简单的了解一下什么是机器学习，那么看完这个故事就足够了。如果你想了解机器学习的更多知识以及与它关联紧密的当代技术，那么请你继续往下看，后面有更多的丰富的内容。<br><br>　　这个例子来源于我真实的生活经验，我在思考这个问题的时候突然发现它的过程可以被扩充化为一个完整的机器学习的过程，因此我决定使用这个例子作为所有介绍的开始。这个故事称为&ldquo;等人问题&rdquo;。<br><br>　　我相信大家都有跟别人相约，然后等人的经历。现实中不是每个人都那么守时的，于是当你碰到一些爱迟到的人，你的时间不可避免的要浪费。我就碰到过这样的一个例子。<br><br>　　对我的一个朋友小Y而言，他就不是那么守时，最常见的表现是他经常迟到。当有一次我跟他约好3点钟在某个麦当劳见面时，在我出门的那一刻我突然想到一个问题：我现在出发合适么？我会不会又到了地点后，花上30分钟去等他？我决定采取一个策略解决这个问题。<br><br>　　要想解决这个问题，有好几种方法。第一种方法是采用知识：我搜寻能够解决这个问题的知识。但很遗憾，没有人会把如何等人这个问题作为知识传授，因此我不可能找到已有的知识能够解决这个问题。第二种方法是问他人：我去询问他人获得解决这个问题的能力。但是同样的，这个问题没有人能够解答，因为可能没人碰上跟我一样的情况。第三种方法是准则法：我问自己的内心，我有否设立过什么准则去面对这个问题？例如，无论别人如何，我都会守时到达。但我不是个死板的人，我没有设立过这样的规则。<br><br>　　事实上，我相信有种方法比以上三种都合适。我把过往跟小Y相约的经历在脑海中重现一下，看看跟他相约的次数中，迟到占了多大的比例。而我利用这来预测他这次迟到的可能性。如果这个值超出了我心里的某个界限，那我选择等一会再出发。假设我跟小Y约过5次，他迟到的次数是1次，那么他按时到的比例为80%，我心中的阈值为70%，我认为这次小Y应该不会迟到，因此我按时出门。如果小Y在5次迟到的次数中占了4次，也就是他按时到达的比例为20%，由于这个值低于我的阈值，因此我选择推迟出门的时间。这个方法从它的利用层面来看，又称为经验法。在经验法的思考过程中，我事实上利用了以往所有相约的数据。因此也可以称之为依据数据做的判断。<br><br>　　<strong>依据数据所做的判断跟机器学习的思想根本上是一致的。</strong><br><br>　　刚才的思考过程我只考虑&ldquo;频次&rdquo;这种属性。在真实的机器学习中，这可能都不算是一个应用。一般的机器学习模型至少考虑两个量：一个是因变量，也就是我们希望预测的结果，在这个例子里就是小Y迟到与否的判断。另一个是自变量，也就是用来预测小Y是否迟到的量。假设我把时间作为自变量，譬如我发现小Y所有迟到的日子基本都是星期五，而在非星期五情况下他基本不迟到。于是我可以建立一个模型，来模拟小Y迟到与否跟日子是否是星期五的概率。见下图：</p><br><p>&nbsp;</p><br><p style="text-align: center;"><img src="https://images0.cnblogs.com/blog/673793/201412/302230238567547.jpg" alt="" width="701" height="365"></p><br><p style="text-align: center;"><span style="font-size: 14px;"><span style="font-family: 黑体;">图3 决策树模型</span></span></p><br><p>　　这样的图就是一个最简单的机器学习模型，称之为决策树。<br><br>　　当我们考虑的自变量只有一个时，情况较为简单。如果把我们的自变量再增加一个。例如小Y迟到的部分情况时是在他开车过来的时候(你可以理解为他开车水平较臭，或者路较堵)。于是我可以关联考虑这些信息。建立一个更复杂的模型，这个模型包含两个自变量与一个因变量。<br><br>　　再更复杂一点，小Y的迟到跟天气也有一定的原因，例如下雨的时候，这时候我需要考虑三个自变量。<br><br>　　如果我希望能够预测小Y迟到的具体时间，我可以把他每次迟到的时间跟雨量的大小以及前面考虑的自变量统一建立一个模型。于是我的模型可以预测值，例如他大概会迟到几分钟。这样可以帮助我更好的规划我出门的时间。在这样的情况下，决策树就无法很好地支撑了，因为决策树只能预测离散值。我们可以用节2所介绍的线型回归方法建立这个模型。<br><br>　　如果我把这些建立模型的过程交给电脑。比如把所有的自变量和因变量输入，然后让计算机帮我生成一个模型，同时让计算机根据我当前的情况，给出我是否需要迟出门，需要迟几分钟的建议。那么计算机执行这些辅助决策的过程就是机器学习的过程。<br><br>　　<strong>机器学习方法是计算机利用已有的数据(经验)，得出了某种模型(迟到的规律)，并利用此模型预测未来(是否迟到)的一种方法。</strong><br><br>　　通过上面的分析，可以看出机器学习与人类思考的经验过程是类似的，不过它能考虑更多的情况，执行更加复杂的计算。事实上，机器学习的一个主要目的就是把人类思考归纳经验的过程转化为计算机通过对数据的处理计算得出模型的过程。经过计算机得出的模型能够以近似于人的方式解决很多灵活复杂的问题。<br><br>　　下面，我会开始对机器学习的正式介绍，包括定义、范围，方法、应用等等，都有所包含。</p><br><p>&nbsp;</p><br><p><span style="font-size: 14px;"><strong><a name="second"></a>2.<span style="font-size: 14px;">机器学习的定义</span></strong></span><br><br>　　从广义上来说，机器学习是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法。但从实践的意义上来说，机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。</p><br><p>　　让我们具体看一个例子。<br><br><span style="font-size: 14px;"><img style="display: block; margin-left: auto; margin-right: auto;" src="data:image/png;base64,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" alt="" width="705" height="368"></span></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图4 房价的例子</span></p><br><p><span style="font-size: 14px;">　　拿国民话题的房子来说。现在我手里有一栋房子需要售卖，我应该给它标上多大的价格？房子的面积是100平方米，价格是100万，120万，还是140万？</span></p><br><p><span style="font-size: 14px;">　　很显然，我希望获得房价与面积的某种规律。那么我该如何获得这个规律？用报纸上的房价平均数据么？还是参考别人面积相似的？无论哪种，似乎都并不是太靠谱。</span></p><br><p><span style="font-size: 14px;">　　我现在希望获得一个合理的，并且能够最大程度的反映面积与房价关系的规律。于是我调查了周边与我房型类似的一些房子，获得一组数据。这组数据中包含了大大小小房子的面积与价格，如果我能从这组数据中找出面积与价格的规律，那么我就可以得出房子的价格。<br></span></p><br><p><span style="font-size: 14px;">　　对规律的寻找很简单，拟合出一条直线，让它&ldquo;穿过&rdquo;所有的点，并且与各个点的距离尽可能的小。</span></p><br><p><span style="font-size: 14px;">　　通过这条直线，我获得了一个能够最佳反映房价与面积规律的规律。这条直线同时也是一个下式所表明的函数：</span></p><br><div class="cnblogs_code"><br><pre>　　房价 = 面积 <em> a + b</em></pre><br></div><br><p><span style="font-size: 14px;">　　上述中的a、b都是直线的参数。获得这些参数以后，我就可以计算出房子的价格。</span></p><br><p><span style="font-size: 14px;">　　假设a = 0.75,b = 50，则房价 = 100  0.75 + 50 = 125万。这个结果与我前面所列的100万，120万，140万都不一样。由于这条直线综合考虑了大部分的情况，因此从&ldquo;统计&rdquo;意义上来说，这是一个最合理的预测。<br><br>　　在求解过程中透露出了两个信息：<br>　　1.房价模型是根据拟合的函数类型决定的。如果是直线，那么拟合出的就是直线方程。如果是其他类型的线，例如抛物线，那么拟合出的就是抛物线方程。机器学习有众多算法，一些强力算法可以拟合出复杂的非线性模型，用来反映一些不是直线所能表达的情况。<br>　　2.如果我的数据越多，我的模型就越能够考虑到越多的情况，由此对于新情况的预测效果可能就越好。这是机器学习界&ldquo;数据为王&rdquo;思想的一个体现。一般来说(不是绝对)，数据越多，最后机器学习生成的模型预测的效果越好。</span></p><br><p><span style="font-size: 14px;">　　通过我拟合直线的过程，我们可以对机器学习过程做一个完整的回顾。首先，我们需要在计算机中存储历史的数据。接着，我们将这些 数据通过机器学习算法进行处理，这个过程在机器学习中叫做&ldquo;训练&rdquo;，处理的结果可以被我们用来对新的数据进行预测，这个结果一般称之为&ldquo;模型&rdquo;。对新数据 的预测过程在机器学习中叫做&ldquo;预测&rdquo;。&ldquo;训练&rdquo;与&ldquo;预测&rdquo;是机器学习的两个过程，&ldquo;模型&rdquo;则是过程的中间输出结果，&ldquo;训练&rdquo;产生&ldquo;模型&rdquo;，&ldquo;模型&rdquo;指导 &ldquo;预测&rdquo;。<br></span></p><br><p><span style="font-size: 14px;">　　让我们把机器学习的过程与人类对历史经验归纳的过程做个比对。</span></p><br><p style="text-align: center;"><img src="https://images0.cnblogs.com/blog/673793/201412/221227224214301.png" alt="" width="796" height="383"></p><br><p style="text-align: center;"><span style="font-family: 黑体; font-size: 14px;">图5 <span style="font-family: 黑体; font-size: 14px;">机器学习</span>与人类思考的类比</span></p><br><p><br>　　人类在成长、生活过程中积累了很多的历史与经验。人类定期地对这些经验进行&ldquo;归纳&rdquo;，获得了生活的&ldquo;规律&rdquo;。当人类遇到未知的问题或者需要对未来进行&ldquo;推测&rdquo;的时候，人类使用这些&ldquo;规律&rdquo;，对未知问题与未来进行&ldquo;推测&rdquo;，从而指导自己的生活和工作。</p><br><p>　　机器学习中的&ldquo;训练&rdquo;与&ldquo;预测&rdquo;过程可以对应到人类的&ldquo;归纳&rdquo;和&ldquo;推测&rdquo;过程。通过这样的对应，我们可以发现，机器学习的思想并不复杂，仅仅是对人类在生活中学习成长的一个模拟。由于机器学习不是基于编程形成的结果，因此它的处理过程不是因果的逻辑，而是通过归纳思想得出的相关性结论。</p><br><p>&nbsp;　　这也可以联想到人类为什么要学习历史，历史实际上是人类过往经验的总结。有句话说得很好，&ldquo;历史往往不一样，但历史总是惊人的相似&rdquo;。通过学习历史，我们从历史中归纳出人生与国家的规律，从而指导我们的下一步工作，这是具有莫大价值的。当代一些人忽视了历史的本来价值，而是把其作为一种宣扬功绩的手段，这其实是对历史真实价值的一种误用。</p><br><p>　　</p><br><p><span style="font-size: 14px;"><strong><a name="third"></a>3.机器学习的范围</strong></span></p><br><p><span style="font-size: 14px;">　　上文虽然说明了机器学习是什么，但是并没有给出机器学习的范围。</span></p><br><p><span style="font-size: 14px;">　　其实，机器学习跟模式识别，统计学习，数据挖掘，计算机视觉，语音识别，自然语言处理等领域有着很深的联系。</span></p><br><p><span style="font-size: 14px;">　　从范围上来说，机器学习跟模式识别，统计学习，数据挖掘是类似的，同时，机器学习与其他领域的处理技术的结合，形成了计算机视觉、语音识别、自然语言处理等交叉学科。因此，一般说数据挖掘时，可以等同于说机器学习。同时，我们平常所说的机器学习应用，应该是通用的，不仅仅局限在结构化数据，还有图像，音频等应用。<br><br>　　在这节对机器学习这些相关领域的介绍有助于我们理清机器学习的应用场景与研究范围，更好的理解后面的算法与应用层次。<br></span></p><br><p><span style="font-size: 14px;">　　下图是机器学习所牵扯的一些相关范围的学科与研究领域。</span></p><br><p style="text-align: center;"><img src="https://images0.cnblogs.com/blog/673793/201412/221213379524716.png" alt="" width="855" height="490"><br><span style="font-size: 14px;"><span style="font-family: 黑体; font-size: 14px;">图6 机器学习与相关学科</span></span></p><br><p>　　模式识别<br>　　模式识别=机器学习。两者的主要区别在于前者是从工业界发展起来的概念，后者则主要源自计算机学科。在著名的《Pattern Recognition And Machine Learning》这本书中，Christopher M. Bishop在开头是这样说的&ldquo;模式识别源自工业界，而机器学习来自于计算机学科。不过，它们中的活动可以被视为同一个领域的两个方面，同时在过去的10年间，它们都有了长足的发展&rdquo;。<br>　　<br>　　数据挖掘<br>　　数据挖掘=机器学习+数据库。这几年数据挖掘的概念实在是太耳熟能详。几乎等同于炒作。但凡说数据挖掘都会吹嘘数据挖掘如何如何，例如从数据中挖出金子，以及将废弃的数据转化为价值等等。但是，我尽管可能会挖出金子，但我也可能挖的是&ldquo;石头&rdquo;啊。这个说法的意思是，数据挖掘仅仅是一种思考方式，告诉我们应该尝试从数据中挖掘出知识，但不是每个数据都能挖掘出金子的，所以不要神话它。一个系统绝对不会因为上了一个数据挖掘模块就变得无所不能(这是IBM最喜欢吹嘘的)，恰恰相反，一个拥有数据挖掘思维的人员才是关键，而且他还必须对数据有深刻的认识，这样才可能从数据中导出模式指引业务的改善。大部分数据挖掘中的算法是机器学习的算法在数据库中的优化。<br><br>　　统计学习<br>　　统计学习近似等于机器学习。统计学习是个与机器学习高度重叠的学科。因为机器学习中的大多数方法来自统计学，甚至可以认为，统计学的发展促进机器学习的繁荣昌盛。例如著名的支持向量机算法，就是源自统计学科。但是在某种程度上两者是有分别的，这个分别在于：统计学习者重点关注的是统计模型的发展与优化，偏数学，而机器学习者更关注的是能够解决问题，偏实践，因此机器学习研究者会重点研究学习算法在计算机上执行的效率与准确性的提升。<br>　　　　<br>　　计算机视觉<br>　　计算机视觉=图像处理+机器学习。图像处理技术用于将图像处理为适合进入机器学习模型中的输入，机器学习则负责从图像中识别出相关的模式。计算机视觉相关的应用非常的多，例如百度识图、手写字符识别、车牌识别等等应用。这个领域是应用前景非常火热的，同时也是研究的热门方向。随着机器学习的新领域深度学习的发展，大大促进了计算机图像识别的效果，因此未来计算机视觉界的发展前景不可估量。<br>　　<br>　　语音识别<br>　　语音识别=语音处理+机器学习。语音识别就是音频处理技术与机器学习的结合。语音识别技术一般不会单独使用，一般会结合自然语言处理的相关技术。目前的相关应用有苹果的语音助手siri等。<br><br>　　自然语言处理<br>　　自然语言处理=文本处理+机器学习。自然语言处理技术主要是让机器理解人类的语言的一门领域。在自然语言处理技术中，大量使用了编译原理相关的技术，例如词法分析，语法分析等等，除此之外，在理解这个层面，则使用了语义理解，机器学习等技术。作为唯一由人类自身创造的符号，自然语言处理一直是机器学习界不断研究的方向。按照百度机器学习专家余凯的说法&ldquo;听与看，说白了就是阿猫和阿狗都会的，而只有语言才是人类独有的&rdquo;。如何利用机器学习技术进行自然语言的的深度理解，一直是工业和学术界关注的焦点。<br><br>　　可以看出机器学习在众多领域的外延和应用。机器学习技术的发展促使了很多智能领域的进步，改善着我们的生活。</p><br><p>&nbsp;</p><br><p><span style="font-size: 14px;"><strong><a name="four"></a>4.机器学习的方法</strong></span></p><br><p>　　通过上节的介绍我们知晓了机器学习的大致范围，那么机器学习里面究竟有多少经典的算法呢？在这个部分我会简要介绍一下机器学习中的经典代表方法。这部分介绍的重点是这些方法内涵的思想，数学与实践细节不会在这讨论。<br><br>　　<strong>1、回归算法</strong></p><br><p>　　在大部分机器学习课程中，回归算法都是介绍的第一个算法。原因有两个：一.回归算法比较简单，介绍它可以让人平滑地从统计学迁移到机器学习中。二.回归算法是后面若干强大算法的基石，如果不理解回归算法，无法学习那些强大的算法。回归算法有两个重要的子类：即线性回归和逻辑回归。</p><br><p>　　线性回归就是我们前面说过的房价求解问题。如何拟合出一条直线最佳匹配我所有的数据？一般使用&ldquo;最小二乘法&rdquo;来求解。&ldquo;最小二乘法&rdquo;的思想是这样的，假设我们拟合出的直线代表数据的真实值，而观测到的数据代表拥有误差的值。为了尽可能减小误差的影响，需要求解一条直线使所有误差的平方和最小。最小二乘法将最优问题转化为求函数极值问题。函数极值在数学上我们一般会采用求导数为0的方法。但这种做法并不适合计算机，可能求解不出来，也可能计算量太大。</p><br><p>　　计算机科学界专门有一个学科叫&ldquo;数值计算&rdquo;，专门用来提升计算机进行各类计算时的准确性和效率问题。例如，著名的&ldquo;梯度下降&rdquo;以及&ldquo;牛顿法&rdquo;就是数值计算中的经典算法，也非常适合来处理求解函数极值的问题。梯度下降法是解决回归模型中最简单且有效的方法之一。从严格意义上来说，由于后文中的神经网络和推荐算法中都有线性回归的因子，因此梯度下降法在后面的算法实现中也有应用。<br><br>　　逻辑回归是一种与线性回归非常类似的算法，但是，从本质上讲，线型回归处理的问题类型与逻辑回归不一致。线性回归处理的是数值问题，也就是最后预测出的结果是数字，例如房价。而逻辑回归属于分类算法，也就是说，逻辑回归预测结果是离散的分类，例如判断这封邮件是否是垃圾邮件，以及用户是否会点击此广告等等。</p><br><p>　　实现方面的话，逻辑回归只是对对线性回归的计算结果加上了一个Sigmoid函数，将数值结果转化为了0到1之间的概率(Sigmoid函数的图像一般来说并不直观，你只需要理解对数值越大，函数越逼近1，数值越小，函数越逼近0)，接着我们根据这个概率可以做预测，例如概率大于0.5，则这封邮件就是垃圾邮件，或者肿瘤是否是恶性的等等。从直观上来说，逻辑回归是画出了一条分类线，见下图。</p><br><p style="text-align: center;"><img src="https://images0.cnblogs.com/blog/673793/201412/221217185156265.png" alt="" width="877" height="424"><br><span style="font-size: 14px;">　　<span style="font-family: 黑体; font-size: 14px;">图7 逻辑回归的直观解释</span></span></p><br><p style="text-align: left;">　　假设我们有一组肿瘤患者的数据，这些患者的肿瘤中有些是良性的(图中的蓝色点)，有些是恶性的(图中的红色点)。这里肿瘤的红蓝色可以被称作数据的&ldquo;标签&rdquo;。同时每个数据包括两个&ldquo;特征&rdquo;：患者的年龄与肿瘤的大小。我们将这两个特征与标签映射到这个二维空间上，形成了我上图的数据。<br><br>　　当我有一个绿色的点时，我该判断这个肿瘤是恶性的还是良性的呢？根据红蓝点我们训练出了一个逻辑回归模型，也就是图中的分类线。这时，根据绿点出现在分类线的左侧，因此我们判断它的标签应该是红色，也就是说属于恶性肿瘤。<br><br>　　逻辑回归算法划出的分类线基本都是线性的(也有划出非线性分类线的逻辑回归，不过那样的模型在处理数据量较大的时候效率会很低)，这意味着当两类之间的界线不是线性时，逻辑回归的表达能力就不足。下面的两个算法是机器学习界最强大且重要的算法，都可以拟合出非线性的分类线。<br><br><span style="font-size: 14px;">　　<strong>2、神经网络</strong></span></p><br><p style="text-align: left;">　　神经网络(也称之为人工神经网络，ANN)算法是80年代机器学习界非常流行的算法，不过在90年代中途衰落。现在，携着&ldquo;深度学习&rdquo;之势，神经网络重装归来，重新成为最强大的机器学习算法之一。<br><br>　　神经网络的诞生起源于对大脑工作机理的研究。早期生物界学者们使用神经网络来模拟大脑。机器学习的学者们使用神经网络进行机器学习的实验，发现在视觉与语音的识别上效果都相当好。在BP算法(加速神经网络训练过程的数值算法)诞生以后，神经网络的发展进入了一个热潮。BP算法的发明人之一是前面介绍的机器学习大牛Geoffrey Hinton(图1中的中间者)。<br><br>　　具体说来，神经网络的学习机理是什么？简单来说，就是分解与整合。在著名的Hubel-Wiesel试验中，学者们研究猫的视觉分析机理是这样的。</p><br><p style="text-align: center;"><br><img src="https://images0.cnblogs.com/blog/673793/201412/221209098439656.png" alt="" width="956" height="364"><br>　　<span style="font-size: 14px;">&nbsp;<span style="font-family: 黑体;">图8 Hubel-Wiesel试验与大脑视觉机理</span></span></p><br><p>　　比方说，一个正方形，分解为四个折线进入视觉处理的下一层中。四个神经元分别处理一个折线。每个折线再继续被分解为两条直线，每条直线再被分解为黑白两个面。于是，一个复杂的图像变成了大量的细节进入神经元，神经元处理以后再进行整合，最后得出了看到的是正方形的结论。这就是大脑视觉识别的机理，也是神经网络工作的机理。<br><br>　　让我们看一个简单的神经网络的逻辑架构。在这个网络中，分成输入层，隐藏层，和输出层。输入层负责接收信号，隐藏层负责对数据的分解与处理，最后的结果被整合到输出层。每层中的一个圆代表一个处理单元，可以认为是模拟了一个神经元，若干个处理单元组成了一个层，若干个层再组成了一个网络，也就是”神经网络”。</p><br><p style="text-align: center;"><img src="https://images0.cnblogs.com/blog/673793/201412/221224194835024.png" alt=""><br><span style="font-size: 14px;"><span style="font-family: 黑体;">图9 神经网络的逻辑架构</span></span></p><br><p><br>　　在神经网络中，每个处理单元事实上就是一个逻辑回归模型，逻辑回归模型接收上层的输入，把模型的预测结果作为输出传输到下一个层次。通过这样的过程，神经网络可以完成非常复杂的非线性分类。</p><br><p>　　下图会演示神经网络在图像识别领域的一个著名应用，这个程序叫做LeNet，是一个基于多个隐层构建的神经网络。通过LeNet可以识别多种手写数字，并且达到很高的识别精度与拥有较好的鲁棒性。</p><br><p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://images0.cnblogs.com/blog/673793/201412/242108049057308.gif" alt="" width="488" height="302"></p><br><p style="text-align: center;"><img style="display: block; margin-left: auto; margin-right: auto;" src="https://images0.cnblogs.com/blog/673793/201412/242108416558480.gif" alt="" width="482" height="118"><span style="font-size: 14px;"><span style="font-family: 黑体;">图10 LeNet的效果展示</span></span></p><br><p>　　右下方的方形中显示的是输入计算机的图像，方形上方的红色字样&ldquo;answer&rdquo;后面显示的是计算机的输出。左边的三条竖直的图像列显示的是神经网络中三个隐藏层的输出，可以看出，随着层次的不断深入，越深的层次处理的细节越低，例如层3基本处理的都已经是线的细节了。LeNet的发明人就是前文介绍过的机器学习的大牛Yann LeCun(图1右者)。</p><br><p>　　进入90年代，神经网络的发展进入了一个瓶颈期。其主要原因是尽管有BP算法的加速，神经网络的训练过程仍然很困难。因此90年代后期支持向量机(SVM)算法取代了神经网络的地位。<br><br><span style="font-size: 14px;">　　<strong>3、SVM（支持向量机）</strong></span></p><br><p><span>　　支持向量机算法是诞生于统计学习界，同时在机器学习界大放光彩的经典算法。</span></p><br><p><span>　　支持向量机算法从某种意义上来说是逻辑回归算法的强化：通过给予逻辑回归算法更严格的优化条件，支持向量机算法可以获得比逻辑回归更好的分类界线。但是如果没有某类函数技术，则支持向量机算法最多算是一种更好的线性分类技术。</span></p><br><p><span>　　但是，通过跟高斯&ldquo;核&rdquo;的结合，支持向量机可以表达出非常复杂的分类界线，从而达成很好的的分类效果。&ldquo;核&rdquo;事实上就是一种特殊的函数，最典型的特征就是可以将</span>低维的空间映射到高维的空间。</p><br><p><span>　　例如下图所示：</span></p><br><p><span><img style="display: block; margin-left: auto; margin-right: auto;" src="https://images0.cnblogs.com/blog/673793/201412/100907289157780.png" alt="" width="359" height="282">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</span></p><br><p style="text-align: center;"><em id="__mceDel"><span style="font-size: 14px;">&nbsp;</span></em><span style="font-size: 14px;"><span style="font-family: 黑体;">图11 支持向量机图例<br></span></span></p><br><p><br><span style="font-size: 14px;">　　我们如何在二维平面划分出一个圆形的分类界线？在二维平面可能会很困难，但是通过&ldquo;核&rdquo;可以将二维空间映射到三维空间，然后使用一个线性平面就可以达成类似效果。也就是说，二维平面划分出的非线性分类界线可以等价于三维平面的线性分类界线。于是，我们可以通过在三维空间中进行简单的线性划分就可以达到在二维平面中的非线性划分效果。</span><br><img style="display: block; margin-left: auto; margin-right: auto;" src="https://images0.cnblogs.com/blog/673793/201412/100907449932793.gif" alt=""></p><br><p style="text-align: center;"><span style="font-family: 黑体;"><em id="__mceDel">&nbsp;</em>图12 三维空间的切割<br></span></p><br><p><br>　　支持向量机是一种数学成分很浓的机器学习算法（相对的，神经网络则有生物科学成分）。在算法的核心步骤中，有一步证明，即将数据从低维映射到高维不会带来最后计算复杂性的提升。于是，通过支持向量机算法，既可以保持计算效率，又可以获得非常好的分类效果。因此支持向量机在90年代后期一直占据着机器学习中最核心的地位，基本取代了神经网络算法。直到现在神经网络借着深度学习重新兴起，两者之间才又发生了微妙的平衡转变。<br><br><span style="font-size: 14px;">　　<strong>4、聚类算法</strong></span></p><br><p>　　前面的算法中的一个显著特征就是我的训练数据中包含了标签，训练出的模型可以对其他未知数据预测标签。在下面的算法中，训练数据都是不含标签的，而算法的目的则是通过训练，推测出这些数据的标签。这类算法有一个统称，即无监督算法(前面有标签的数据的算法则是有监督算法)。无监督算法中最典型的代表就是聚类算法。<br><br>　　让我们还是拿一个二维的数据来说，某一个数据包含两个特征。我希望通过聚类算法，给他们中不同的种类打上标签，我该怎么做呢？简单来说，聚类算法就是计算种群中的距离，根据距离的远近将数据划分为多个族群。<br><br>　　聚类算法中最典型的代表就是K-Means算法。</p><br><p><span style="font-size: 14px;">　　<strong>5、降维算法</strong></span></p><br><p><span style="font-size: 14px;">　　降维算法也是一种无监督学习算法，其主要特征是将数据从高维降低到低维层次。在这里，维度其实表示的是数据的特征量的大小，例如，房价包含房子的长、宽、面积与房间数量四个特征，也就是维度为4维的数据。可以看出来，长与宽事实上与面积表示的信息重叠了，例如面积=长 &times; 宽。通过降维算法我们就可以去除冗余信息，将特征减少为面积与房间数量两个特征，即从4维的数据压缩到2维。于是我们将数据从高维降低到低维，不仅利于表示，同时在计算上也能带来加速。<br><br>　　刚才说的降维过程中减少的维度属于肉眼可视的层次，同时压缩也不会带来信息的损失(因为信息冗余了)。如果肉眼不可视，或者没有冗余的特征，降维算法也能工作，不过这样会带来一些信息的损失。但是，降维算法可以从数学上证明，从高维压缩到的低维中最大程度地保留了数据的信息。因此，使用降维算法仍然有很多的好处。<br><br>　　降维算法的主要作用是压缩数据与提升机器学习其他算法的效率。通过降维算法，可以将具有几千个特征的数据压缩至若干个特征。另外，降维算法的另一个好处是数据的可视化，例如将5维的数据压缩至2维，然后可以用二维平面来可视。降维算法的主要代表是PCA算法(即主成分分析算法)。<br><br>　　<strong>6、推荐算法</strong></span></p><br><p><span style="font-size: 14px;">　　推荐算法是目前业界非常火的一种算法，在电商界，如亚马逊，天猫，京东等得到了广泛的运用。推荐算法的主要特征就是可以自动向用户推荐他们最感兴趣的东西，从而增加购买率，提升效益。推荐算法有两个主要的类别：</span></p><br><p><span style="font-size: 14px;">　　一类是基于物品内容的推荐，是将与用户购买的内容近似的物品推荐给用户，这样的前提是每个物品都得有若干个标签，因此才可以找出与用户购买物品类似的物品，这样推荐的好处是关联程度较大，但是由于每个物品都需要贴标签，因此工作量较大。</span></p><br><p><span style="font-size: 14px;">　　<span style="font-size: 14px;">另一类是基于用户相似度的推荐，</span>则是将与目标用户兴趣相同的其他用户购买的东西推荐给目标用户，例如小A历史上买了物品B和C，经过算法分析，发现另一个与小A近似的用户小D购买了物品E，于是将物品E推荐给小A。</span></p><br><p><span style="font-size: 14px;">　　两类推荐都有各自的优缺点，在一般的电商应用中，一般是两类混合使用。推荐算法中最有名的算法就是协同过滤算法。<br><br>　　<strong>7、其他</strong></span></p><br><p><span style="font-size: 14px;">　　除了以上算法之外，机器学习界还有其他的如高斯判别，朴素贝叶斯，决策树等等算法。但是上面列的六个算法是使用最多，影响最广，种类最全的典型。机器学习界的一个特色就是算法众多，发展百花齐放。<br><br>　　下面做一个总结，按照训练的数据有无标签，可以将上面算法分为监督学习算法和无监督学习算法，但推荐算法较为特殊，既不属于监督学习，也不属于非监督学习，是单独的一类。</span></p><br><p><span style="font-size: 14px;">　　<strong>监督学习算法：</strong><br>　　线性回归，逻辑回归，神经网络，SVM<br><br>　　<strong>无监督学习算法：</strong><br>　　聚类算法，降维算法<br><br>　　<strong>特殊算法：</strong><br>　　推荐算法<br></span></p><br><p><span style="font-size: 14px;">　　除了这些算法以外，有一些算法的名字在机器学习领域中也经常出现。但他们本身并不算是一个机器学习算法，而是为了解决某个子问题而诞生的。你可以理解他们为以上算法的子算法，用于大幅度提高训练过程。其中的代表有：梯度下降法，主要运用在线型回归，逻辑回归，神经网络，推荐算法中；牛顿法，主要运用在线型回归中；BP算法，主要运用在神经网络中；SMO算法，主要运用在SVM中。</span></p><br><p><span style="font-size: 14px;"><br><strong><a name="five"></a>5.机器学习的应用–大数据</strong><br><br>　　说完机器学习的方法，下面要谈一谈机器学习的应用了。无疑，在2010年以前，机器学习的应用在某些特定领域发挥了巨大的作用，如车牌识别，网络攻击防范，手写字符识别等等。但是，从2010年以后，随着大数据概念的兴起，机器学习大量的应用都与大数据高度耦合，几乎可以认为大数据是机器学习应用的最佳场景。<br><br>　　譬如，但凡你能找到的介绍大数据魔力的文章，都会说大数据如何准确准确预测到了某些事。例如经典的Google利用大数据预测了H1N1在美国某小镇的爆发。</span></p><br><p><span style="font-size: 14px;">　<img style="display: block; margin-left: auto; margin-right: auto;" src="https://images0.cnblogs.com/blog/673793/201412/242044433744851.png" alt=""></span></p><br><p style="text-align: center;"><span style="font-size: 14px;"><span style="font-family: 黑体;">图13 Google成功预测H1N1</span></span></p><br><p><span style="font-size: 14px;"><br>　　百度预测2014年世界杯，从淘汰赛到决赛全部预测正确。<br><br></span></p><br><p style="text-align: center;"><span style="font-size: 14px;"><img style="display: block; margin-left: auto; margin-right: auto;" src="https://images0.cnblogs.com/blog/673793/201412/242049525625891.jpg" alt=""><span style="font-size: 14px;"><span style="font-family: 黑体;">图14 百度世界杯成功预测了所有比赛结果</span></span></span></p><br><p><span style="font-size: 14px;">　　这些实在太神奇了，那么究竟是什么原因导致大数据具有这些魔力的呢？简单来说，就是机器学习技术。正是基于机器学习技术的应用，数据才能发挥其魔力。</span></p><br><p><span style="font-size: 14px;">　　大数据的核心是利用数据的价值，机器学习是利用数据价值的关键技术，对于大数据而言，机器学习是不可或缺的。相反，对于机器学习而言，越多的数据会越 可能提升模型的精确性，同时，复杂的机器学习算法的计算时间也迫切需要分布式计算与内存计算这样的关键技术。因此，机器学习的兴盛也离不开大数据的帮助。 大数据与机器学习两者是互相促进，相依相存的关系。</span></p><br><p><span style="font-size: 14px;">　　机器学习与大数据紧密联系。但是，必须清醒的认识到，大数据并不等同于机器学习，同理，机器学习也不等同于大数据。大数据中包含有分布式计算，内存数据库，多维分析等等多种技术。单从分析方法来看，大数据也包含以下四种分析方法：<br><br>　　1.<strong><span style="color: #000000;">大数据，小分析：</span></strong>即数据仓库领域的OLAP分析思路，也就是多维分析思想。<br>　　2.<strong><span style="color: #000000;">大数据，大分析：</span></strong>这个代表的就是数据挖掘与机器学习分析法。<br>　　3.<strong><span style="color: #000000;">流式分析：</span></strong>这个主要指的是事件驱动架构。<br>　　4.<strong><span style="color: #000000;">查询分析：</span></strong>经典代表是NoSQL数据库。<br><br>　　也就是说，机器学习仅仅是大数据分析中的一种而已。尽管机器学习的一些结果具有很大的魔力，在某种场合下是大数据价值最好的说明。但这并不代表机器学习是大数据下的唯一的分析方法。<br><br>　　机器学习与大数据的结合产生了巨大的价值。基于机器学习技术的发展，数据能够&ldquo;预测&rdquo;。对人类而言，积累的经验越丰富，阅历也广泛，对未来的判断越准确。例如常说的&ldquo;经验丰富&rdquo;的人比&ldquo;初出茅庐&rdquo;的小伙子更有工作上的优势，就在于经验丰富的人获得的规律比他人更准确。而在机器学习领域，根据著名的一个实验，有效的证实了机器学习界一个理论：即机器学习模型的数据越多，机器学习的预测的效率就越好。见下图：</span></p><br><p><span style="font-size: 14px;"><img style="display: block; margin-left: auto; margin-right: auto;" src="https://images0.cnblogs.com/blog/673793/201412/301631492162492.png" alt=""></span></p><br><p><span style="font-size: 14px;"><img style="display: block; margin-left: auto; margin-right: auto;" src="https://images0.cnblogs.com/blog/673793/201412/301631172002773.png" alt="" width="885" height="111"></span></p><br><p style="text-align: center;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-family: 黑体;">图15 机器学习准确率与数据的关系</span></span></span></span></p><br><p>　　通过这张图可以看出，各种不同算法在输入的数据量达到一定级数后，都有相近的高准确度。于是诞生了机器学习界的名言：<strong>成功的机器学习应用不是拥有最好的算法，而是拥有最多的数据！</strong><br><br>　　在大数据的时代，有好多优势促使机器学习能够应用更广泛。例如随着物联网和移动设备的发展，我们拥有的数据越来越多，种类也包括图片、文本、视频等非结构化数据，这使得机器学习模型可以获得越来越多的数据。同时大数据技术中的分布式计算Map-Reduce使得机器学习的速度越来越快，可以更方便的使用。种种优势使得在大数据时代，机器学习的优势可以得到最佳的发挥。<br><br></p><br><p><span style="font-size: 14px;"><strong><a name="six"></a>6.机器学习的子类–深度学习</strong><br><br>　　近来，机器学习的发展产生了一个新的方向，即&ldquo;深度学习&rdquo;。<br><br>　　虽然深度学习这四字听起来颇为高大上，但其理念却非常简单，就是传统的神经网络发展到了多隐藏层的情况。<br><br>　　在上文介绍过，自从90年代以后，神经网络已经消寂了一段时间。但是BP算法的发明人Geoffrey Hinton一直没有放弃对神经网络的研究。由于神经网络在隐藏层扩大到两个以上，其训练速度就会非常慢，因此实用性一直低于支持向量机。2006年，Geoffrey Hinton在科学杂志《Science》上发表了一篇文章，论证了两个观点：</span></p><br><p><strong><span style="font-size: 14px;">　　1.多隐层的神经网络具有优异的特征学习能力，学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类；</span></strong></p><br><p><strong><span style="font-size: 14px;">　　2.深度神经网络在训练上的难度，可以通过&ldquo;逐层初始化&rdquo; 来有效克服。</span></strong></p><br><p style="text-align: center;"><span style="font-size: 14px;"><br><img src="https://images0.cnblogs.com/blog/673793/201412/242050441877753.png" alt="" width="856" height="382"><br><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-family: 黑体;">图16 <span style="font-size: 14px;">Geoffrey Hinton</span>与他的学生在Science上发表文章<br></span></span></span></span></p><br><p><span style="font-size: 14px;">　　通过这样的发现，不仅解决了神经网络在计算上的难度，同时也说明了深层神经网络在学习上的优异性。从此，神经网络重新成为了机器学习界中的主流强大学习技术。同时，具有多个隐藏层的神经网络被称为深度神经网络，基于深度神经网络的学习研究称之为深度学习。<br><br>　　由于深度学习的重要性质，在各方面都取得极大的关注，按照时间轴排序，有以下四个标志性事件值得一说：<br><br>　　2012年6月，《纽约时报》披露了Google Brain项目，这个项目是由Andrew Ng和Map-Reduce发明人Jeff Dean共同主导，用16000个CPU Core的并行计算平台训练一种称为&ldquo;深层神经网络&rdquo;的机器学习模型，在语音识别和图像识别等领域获得了巨大的成功。Andrew Ng就是文章开始所介绍的机器学习的大牛(图1中左者)。</span></p><br><p><span style="font-size: 14px;">　　2012年11月，微软在中国天津的一次活动上公开演示了一个全自动的同声传译系统，讲演者用英文演讲，后台的计算机一气呵成自动完成语音识别、英中机器翻译，以及中文语音合成，效果非常流畅，其中支撑的关键技术是深度学习；</span></p><br><p><span style="font-size: 14px;">　　2013年1月，在百度的年会上，创始人兼CEO李彦宏高调宣布要成立百度研究院，其中第一个重点方向就是深度学习，并为此而成立深度学习研究院(IDL)。</span></p><br><p><span style="font-size: 14px;">　　2013年4月，《麻省理工学院技术评论》杂志将深度学习列为2013年十大突破性技术(Breakthrough Technology)之首。<br></span></p><br><p><span style="font-size: 14px;"><img style="display: block; margin-left: auto; margin-right: auto;" src="https://images0.cnblogs.com/blog/673793/201412/242119267339172.png" alt="" width="717" height="338"><br></span></p><br><p style="text-align: center;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-family: 黑体;">图17 <span style="font-size: 14px;">深度学习的发展热潮</span></span></span></span></span></span></p><br><p>　　文章开头所列的三位机器学习的大牛，不仅都是机器学习界的专家，更是深度学习研究领域的先驱。因此，使他们担任各个大型互联网公司技术掌舵者的原因不仅在于他们的技术实力，更在于他们研究的领域是前景无限的深度学习技术。<br><br>　　目前业界许多的图像识别技术与语音识别技术的进步都源于深度学习的发展，除了本文开头所提的Cortana等语音助手，还包括一些图像识别应用，其中典型的代表就是下图的百度识图功能。</p><br><p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://images0.cnblogs.com/blog/673793/201412/301558029039641.png" alt="" width="646" height="327"></p><br><p style="text-align: center;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-family: 黑体;">图18 <span style="font-size: 14px;">百度识图</span></span></span></span></span></span></p><br><p>　　深度学习属于机器学习的子类。基于深度学习的发展极大的促进了机器学习的地位提高，更进一步地，推动了业界对机器学习父类人工智能梦想的再次重视。</p><br><p style="text-align: left;">&nbsp;</p><br><p><span style="font-size: 14px;"><span style="font-size: 14px;"><strong><a name="seven"></a>7.机器学习的父类–人工智能</strong></span></span></p><br><p>　　人工智能是机器学习的父类。深度学习则是机器学习的子类。如果把三者的关系用图来表明的话，则是下图：</p><br><p style="text-align: center;"><img src="https://images0.cnblogs.com/blog/673793/201412/301611254811216.png" alt="" width="426" height="410"><br><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-family: 黑体;">图19 <span style="font-size: 14px;">深度学习、机器学习、人工智能三者关系</span></span></span></span></span></span></p><br><p>　　毫无疑问，人工智能(AI)是人类所能想象的科技界最突破性的发明了，某种意义上来说，人工智能就像游戏最终幻想的名字一样，是人类对于科技界的最终梦想。从50年代提出人工智能的理念以后，科技界，产业界不断在探索，研究。这段时间各种小说、电影都在以各种方式展现对于人工智能的想象。人类可以发明类似于人类的机器，这是多么伟大的一种理念！但事实上，自从50年代以后，人工智能的发展就磕磕碰碰，未有见到足够震撼的科学技术的进步。<br><br>　　总结起来，人工智能的发展经历了如下若干阶段，从早期的逻辑推理，到中期的专家系统，这些科研进步确实使我们离机器的智能有点接近了，但还有一大段距离。直到机器学习诞生以后，人工智能界感觉终于找对了方向。基于机器学习的图像识别和语音识别在某些垂直领域达到了跟人相媲美的程度。机器学习使人类第一次如此接近人工智能的梦想。</p><br><p>　　事实上，如果我们把人工智能相关的技术以及其他业界的技术做一个类比，就可以发现机器学习在人工智能中的重要地位不是没有理由的。</p><br><p>　　人类区别于其他物体，植物，动物的最主要区别，作者认为是<strong>&ldquo;智慧&rdquo;</strong>。而<span style="color: #ff0000;">智慧的最佳体现</span>是什么？</p><br><p>　　是计算能力么，应该不是，心算速度快的人我们一般称之为天才。<br>　　是反应能力么，也不是，反应快的人我们称之为灵敏。<br>　　是记忆能力么，也不是，记忆好的人我们一般称之为过目不忘。<br>　　是推理能力么，这样的人我也许会称他智力很高，类似&ldquo;福尔摩斯&rdquo;，但不会称他拥有智慧。<br>　　是知识能力么，这样的人我们称之为博闻广，也不会称他拥有智慧。</p><br><p>　　想想看我们一般形容谁有大智慧？圣人，诸如庄子，老子等。<strong>智慧是对生活的感悟，是对人生的积淀与思考</strong>，这与我们机器学习的思想何其相似？通过经验获取规律，指导人生与未来。没有经验就没有智慧。</p><br><p>&nbsp;</p><br><p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://images0.cnblogs.com/blog/673793/201412/301636181536962.png" alt="" width="847" height="349"></p><br><p style="text-align: center;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-family: 黑体;">图20 <span style="font-size: 14px;">机器学习与智慧</span></span></span></span></span></span></p><br><p>　　</p><br><p>　　那么，从计算机来看，以上的种种能力都有种种技术去应对。</p><br><p>　　例如计算能力我们有分布式计算，反应能力我们有事件驱动架构，检索能力我们有搜索引擎，知识存储能力我们有数据仓库，逻辑推理能力我们有专家系统，但是，唯有对应智慧中最显著特征的归纳与感悟能力，只有机器学习与之对应。这也是机器学习能力最能表征智慧的根本原因。<br><br>　　让我们再看一下机器人的制造，在我们具有了强大的计算，海量的存储，快速的检索，迅速的反应，优秀的逻辑推理后我们如果再配合上一个强大的智慧大脑，一个真正意义上的人工智能也许就会诞生，这也是为什么说在机器学习快速发展的现在，人工智能可能不再是梦想的原因。<br><br>　　人工智能的发展可能不仅取决于机器学习，更取决于前面所介绍的深度学习，深度学习技术由于深度模拟了人类大脑的构成，在视觉识别与语音识别上显著性的突破了原有机器学习技术的界限，因此极有可能是真正实现人工智能梦想的关键技术。无论是谷歌大脑还是百度大脑，都是通过海量层次的深度学习网络所构成的。也许借助于深度学习技术，在不远的将来，一个具有人类智能的计算机真的有可能实现。</p><br><p><span style="font-size: 14px;">　　最后再说一下题外话，由于人工智能借助于深度学习技术的快速发展，已经在某些地方引起了传统技术界达人的担忧。真实世界的&ldquo;钢铁侠&rdquo;，特斯拉CEO马斯克就是其中之一。最近马斯克在参加MIT讨论会时，就表达了对于人工智能的担忧。&ldquo;人工智能的研究就类似于召唤恶魔，我们必须在某些地方加强注意。&rdquo;</span></p><br><p>&nbsp;<span style="font-size: 14px;"><img style="display: block; margin-left: auto; margin-right: auto;" src="https://images0.cnblogs.com/blog/673793/201412/292123300283048.png" alt="" width="696" height="435"></span></p><br><p style="text-align: center;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-size: 14px;"><span style="font-family: 黑体;">图21<span style="font-size: 14px;"> 马斯克与人工智能</span></span></span></span></span></span></span></span></span></p><br><p><span style="font-size: 14px;">　　尽管马斯克的担心有些危言耸听，但是马斯克的推理不无道理。&ldquo;如果人工智能想要消除垃圾邮件的话，可能它最后的决定就是消灭人类。&rdquo;马斯克认为预防此类现象的方法是引入政府的监管。在这里作者的观点与马斯克类似，在人工智能诞生之初就给其加上若干规则限制可能有效，也就是不应该使用单纯的机器学习，而应该是机器学习与规则引擎等系统的综合能够较好的解决这类问题。因为如果学习没有限制，极有可能进入某个误区，必须要加上某些引导。正如人类社会中，法律就是一个最好的规则，杀人者死就是对于人类在探索提高生产力时不可逾越的界限。</span></p><br><p><span style="font-size: 14px;">　　在这里，必须提一下这里的规则与机器学习引出的规律的不同，规律不是一个严格意义的准则，其代表的更多是概率上的指导，而规则则是神圣不可侵犯，不可修改的。规律可以调整，但规则是不能改变的。有效的结合规律与规则的特点，可以引导出一个合理的，可控的学习型人工智能。</span></p><br><p>&nbsp;</p><br><p><span style="font-size: 14px;"><strong><a name="eight"></a>8.机器学习的思考–计算机的潜意识</strong><br><br>　　最后，作者想谈一谈关于机器学习的一些思考。主要是作者在日常生活总结出来的一些感悟。<br></span></p><br><p><span style="font-size: 14px;">　　回想一下我在节1里所说的故事，我把小Y过往跟我相约的经历做了一个罗列。但是这种罗列以往所有经历的方法只有少数人会这么做，大部分的人采用的是更直接的方法，即利用直觉。那么，直觉是什么？其实直觉也是你在潜意识状态下思考经验后得出的规律。就像你通过机器学习算法，得到了一个模型，那么你下次只要直接使用就行了。那么这个规律你是什么时候思考的？可能是在你无意识的情况下，例如睡觉，走路等情况。这种时候，大脑其实也在默默地做一些你察觉不到的工作。</span></p><br><p><span style="font-size: 14px;">　　这种直觉与潜意识，我把它与另一种人类思考经验的方式做了区分。如果一个人勤于思考，例如他会每天做一个小结，譬如&ldquo;吾日三省吾身&rdquo;，或者他经常与同伴讨论最近工作的得失，那么他这种训练模型的方式是直接的，明意识的思考与归纳。这样的效果很好，记忆性强，并且更能得出有效反应现实的规律。但是大部分的人可能很少做这样的总结，那么他们得出生活中规律的方法使用的就是潜意识法。</span></p><br><p><span style="font-size: 14px;">　　举一个作者本人关于潜意识的例子。作者本人以前没开过车，最近一段时间买了车后，天天开车上班。我每天都走固定的路线。有趣的是，在一开始的几天，我非常紧张的注意着前方的路况，而现在我已经在无意识中就把车开到了目标。这个过程中我的眼睛是注视着前方的，我的大脑是没有思考，但是我手握着的方向盘会自动的调整方向。也就是说。随着我开车次数的增多，我已经把我开车的动作交给了潜意识。这是非常有趣的一件事。在这段过程中，我的大脑将前方路况的图像记录了下来，同时大脑也记忆了我转动方向盘的动作。经过大脑自己的潜意识思考，最后生成的潜意识可以直接根据前方的图像调整我手的动作。假设我们将前方的录像交给计算机，然后让计算机记录与图像对应的驾驶员的动作。经过一段时间的学习，计算机生成的机器学习模型就可以进行自动驾驶了。这很神奇，不是么。其实包括Google、特斯拉在内的自动驾驶汽车技术的原理就是这样。<br><br>　　除了自动驾驶汽车以外，潜意识的思想还可以扩展到人的交际。譬如说服别人，一个最佳的方法就是给他展示一些信息，然后让他自己去归纳得出我们想要的结论。<strong>这</strong>就好比在<strong>阐述一个观点时，用一个事实，或者一个故事，比大段的道理要好很多。</strong>古往今来，但凡优秀的说客，无不采用的是这种方法。春秋战国时期，各国合纵连横，经常有各种说客去跟一国之君交流，直接告诉君主该做什么，无异于自寻死路，但是跟君主讲故事，通过这些故事让君主恍然大悟，就是一种正确的过程。这里面有许多杰出的代表，如墨子，苏秦等等。<br><br>　　基本上所有的交流过程，使用故事说明的效果都要远胜于阐述道义之类的效果好很多。为什么用故事的方法比道理或者其他的方法好很多，这是因为在人成长的过程，经过自己的思考，已经形成了很多规律与潜意识。如果你告诉的规律与对方的不相符，很有可能出于保护，他们会本能的拒绝你的新规律，但是如果你跟他讲一个故事，传递一些信息，输送一些数据给他，他会思考并自我改变。他的思考过程实际上就是机器学习的过程，他把新的数据纳入到他的旧有的记忆与数据中，经过重新训练。如果你给出的数据的信息量非常大，大到调整了他的模型，那么他就会按照你希望的规律去做事。有的时候，他会本能的拒绝执行这个思考过程，但是数据一旦输入，无论他希望与否，他的大脑都会在潜意识状态下思考，并且可能改变他的看法。<br><br>　　如果计算机也拥有潜意识(正如本博客的名称一样)，那么会怎么样？譬如让计算机在工作的过程中，逐渐产生了自身的潜意识，于是甚至可以在你不需要告诉它做什么时它就会完成那件事。这是个非常有意思的设想，这里留给各位读者去发散思考吧。</span></p><br><p><span style="font-size: 14px;"><br><strong><a name="nine"></a>9.总结</strong><br></span></p><br><p><span style="font-size: 14px;">　　本文首先介绍了互联网界与机器学习大牛结合的趋势，以及使用机器学习的相关应用，接着以一个&ldquo;等人故事&rdquo;展开对机器学习的介绍。介绍中首先是机器学习的概念与定义，然后是机器学习的相关学科，机器学习中包含的各类学习算法，接着介绍机器学习与大数据的关系，机器学习的新子类深度学习，最后探讨了一下机器学习与人工智能发展的联系以及机器学习与潜意识的关联。经过本文的介绍，相信大家对机器学习技术有一定的了解，例如机器学习是什么，它的内核思想是什么(即统计和归纳)，通过了解机器学习与人类思考的近似联系可以知晓机器学习为什么具有智慧能力的原因等等。其次，本文漫谈了机器学习与外延学科的关系，机器学习与大数据相互促进相得益彰的联系，机器学习界最新的深度学习的迅猛发展，以及对于人类基于机器学习开发智能机器人的一种展望与思考，最后作者简单谈了一点关于让计算机拥有潜意识的设想。<br><br>　　机器学习是目前业界最为Amazing与火热的一项技术，从网上的每一次淘宝的购买东西，到自动驾驶汽车技术，以及网络攻击抵御系统等等，都有机器学习的因子在内，同时机器学习也是最有可能使人类完成AI dream的一项技术，各种人工智能目前的应用，如微软小冰聊天机器人，到计算机视觉技术的进步，都有机器学习努力的成分。作为一名当代的计算机领域的开发或管理人员，以及身处这个世界，使用者IT技术带来便利的人们，最好都应该了解一些机器学习的相关知识与概念，因为这可以帮你更好的理解为你带来莫大便利技术的背后原理，以及让你更好的理解当代科技的进程。<br><br><strong><a name="ten"></a>10.后记</strong><br><br><span style="font-size: 14px;">　　这篇文档花了作者两个月的时间，终于在2014年的最后一天的前一天基本完成。通过这篇文章，作者希望对机器学习在国内的普及做一点贡献，同时也是作者本人自己对于所学机器学习知识的一个融汇贯通，整体归纳的提高过程。作者把这么多的知识经过自己的大脑思考，训练出了一个模型，形成了这篇文档，可以说这也是一种机器学习的过程吧(笑)。</span></span></p><br><p><span style="font-size: 14px;"><span style="font-size: 14px;">　　作者所在的行业会接触到大量的数据，因此对于数据的处理和分析是平常非常重要的工作，机器学习课程的思想和理念对于作者日常的工作指引作用极大，几乎导致了作者对于数据价值的重新认识。想想半年前，作者还对机器学习似懂非懂，如今也可以算是一个机器学习的Expert了(笑)。但作者始终认为，机器学习的真正应用不是通过概念或者思想的方式，而是通过实践。只有当把机器学习技术真正应用时，才可算是对机器学习的理解进入了一个层次。正所谓再&ldquo;阳春白雪&rdquo;的技术，也必须落到&ldquo;下里巴人&rdquo;的场景下运用。目前有一种风气，国内外研究机器学习的某些学者，有一种高贵的逼格，认为自己的研究是普通人无法理解的，但是这样的理念是根本错误的，没有在真正实际的地方发挥作用，凭什么证明你的研究有所价值呢？作者认为必须将高大上的技术用在改变普通人的生活上，才能发挥其根本的价值。一些简单的场景，恰恰是实践机器学习技术的最好地方。<br></span></span></p>
]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>【转】神经网络浅讲：从神经元到深度学习</title>
    <url>/2018/05/30/%E3%80%90%E8%BD%AC%E3%80%91%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%B5%85%E8%AE%B2%EF%BC%9A%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%85%83%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>这又是我常看的大神写的一篇博客，从最初的从机器学习谈起使我入门机器学习，到现在学到了神经网络，又找到了他的博客，希望对我学习神经网络有所帮助。</p>
<p>原文地址：<a href="https://www.cnblogs.com/subconscious/p/5058741.html#first" target="_blank" rel="noopener">https://www.cnblogs.com/subconscious/p/5058741.html#first</a></p>
<p>作者：计算机的潜意识</p>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body><div id="main"><br>    <div id="mainContent"><br>    <div class="forFlow"><br><br><div id="post_detail"><br><!--done--><br><div id="topics"><br>    <div class="post"><br>        <div class="clear"></div><br>        <div class="postBody"><br>            <div id="cnblogs_post_body" class="blogpost-body"><p class="p">　　神经网络是一门重要的机器学习技术。它是目前最为火热的研究方向<span style="font-family: Verdana;">–</span>深度学习的基础。学习神经网络不仅可以让你掌握一门强大的机器学习方法，同时也可以更好地帮助你理解深度学习技术。</p><br><p class="p">　　本文以一种简单的，循序的方式讲解神经网络。适合对神经网络了解不多的同学。本文对阅读没有一定的前提要求，但是懂一些<a title="从机器学习谈起" href="http://www.cnblogs.com/subconscious/p/4107357.html" target="_blank">机器学习</a>基础会更好地帮助理解本文。</p><br><p class="p">　　神经网络是一种模拟人脑的神经网络以期能够实现类人工智能的机器学习技术。人脑中的神经网络是一个非常复杂的组织。成人的大脑中估计有<span style="font-family: Verdana;">1000</span><span style="font-family: 宋体;">亿个神经元之多。</span></p><br><p style="text-align: center;"><img src="673793-20151229110952448-2017198041.jpg" alt="" width="429" height="321"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图1 人脑神经网络</span></p><br><p style="text-align: center;">&nbsp;</p><br><p>　　那么机器学习中的神经网络是如何实现这种模拟的，并且达到一个惊人的良好效果的？通过本文，你可以了解到这些问题的答案，同时还能知道神经网络的历史，以及如何较好地学习它。</p><br><p>　　由于本文较长，为方便读者，以下是本文的目录：</p><br><p>　　一.<a href="https://www.cnblogs.com/subconscious/p/5058741.html#first" target="_blank" rel="noopener">前言</a></p><br><p>　　二.<a href="https://www.cnblogs.com/subconscious/p/5058741.html#second" target="_blank" rel="noopener">神经元</a></p><br><p>　　三.<a href="https://www.cnblogs.com/subconscious/p/5058741.html#third" target="_blank" rel="noopener">单层神经网络（感知器）</a></p><br><p>　　四.<a href="https://www.cnblogs.com/subconscious/p/5058741.html#fourth" target="_blank" rel="noopener">两层神经网络（多层感知器）</a></p><br><p>　　五.<a href="https://www.cnblogs.com/subconscious/p/5058741.html#fifth" target="_blank" rel="noopener">多层神经网络（深度学习）</a></p><br><p>　　六.<a href="https://www.cnblogs.com/subconscious/p/5058741.html#sixth" target="_blank" rel="noopener">回顾</a></p><br><p>　　七.<a href="https://www.cnblogs.com/subconscious/p/5058741.html#seventh" target="_blank" rel="noopener">展望</a><a href="https://www.cnblogs.com/subconscious/p/5058741.html#sixth" target="_blank" rel="noopener"><br></a></p><br><p>　　八.<a href="https://www.cnblogs.com/subconscious/p/5058741.html#eighth" target="_blank" rel="noopener">总结</a><a href="https://www.cnblogs.com/subconscious/p/5058741.html#nineth" target="_blank" rel="noopener"><br></a></p><br><p>　　九.<a href="https://www.cnblogs.com/subconscious/p/5058741.html#nineth" target="_blank" rel="noopener">后记</a></p><br><p>　　十.<a href="https://www.cnblogs.com/subconscious/p/5058741.html#tenth" target="_blank" rel="noopener">备注</a><a href="https://www.cnblogs.com/subconscious/p/5058741.html#tenth" target="_blank" rel="noopener"><br></a></p><br><p>&nbsp;</p><br><p><strong><a name="first"></a>一. 前言</strong></p><br><p class="p">　　让我们来看一个经典的神经网络。这是一个包含三个层次的神经网络。红色的是<strong>输入层</strong>，绿色的是<strong>输出层</strong>，紫色的是<strong>中间层</strong>（也叫<strong>隐藏层</strong>）。输入层有<span style="font-family: Verdana;">3</span><span style="font-family: 宋体;">个输入</span>单元，隐藏层有<span style="font-family: Verdana;">4</span><span style="font-family: 宋体;">个</span>单元，输出层有<span style="font-family: Verdana;">2</span><span style="font-family: 宋体;">个</span>单元。后文中，我们统一使用这种颜色来表达神经网络的结构。</p><br><p style="text-align: center;"><span style="font-family: 宋体;"><img src="673793-20151219151604318-1557737289.jpg" alt="" width="329" height="463"></span></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图2 神经网络结构图</span></p><br><p style="text-align: center;">&nbsp;</p><br><p class="p">　　在开始介绍前，有一些知识可以先记在心里：</p><br><ol><br><li>设计一个神经网络时，输入层与输出层的节点数往往是固定的，中间层则可以自由指定；</li><br><li>神经网络结构图中的拓扑与箭头代表着<strong>预测</strong>过程时数据的流向，跟<strong>训练</strong>时的数据流有一定的区别；</li><br><li><span style="font-family: 宋体;">结构图里的关键不是圆圈（代表“神经元”），而是连接线（代表“神经元”之间的连接）。每个连接线对应一个不同的<strong>权重</strong>（其值称为<strong>权值</strong>），这是需要训练得到的</span>。<span style="font-family: Verdana;">&nbsp;</span>&nbsp;</li><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br></ol><br><p><span style="font-family: 宋体;">　　除了从左到右的形式表达的结构图，还有一种常见的表达形式是从下到上来表示一个神经网络。这时候，输入层在图的最下方。输出层则在图的最上方，如下图：</span></p><br><p style="text-align: center;"><img src="673793-20151219174631693-775181930.jpg" alt="" width="486" height="313"></p><br><p style="text-align: center;"><span style="line-height: 1.5; font-family: 黑体;">图3&nbsp;从下到上的神经网络结构图</span>&nbsp;</p><br><p style="text-align: center;">&nbsp;</p><br><p style="text-align: left;"><span style="line-height: 1.5;">　　从左到右的表达形式以</span><span style="font-family: Verdana;">Andrew&nbsp;Ng</span><span style="font-family: 宋体;">和</span><span style="font-family: Verdana;">LeCun</span><span style="font-family: 宋体;">的文献使用较多，</span><span style="font-family: Verdana;">Caffe</span><span style="font-family: 宋体;">里使用的则是从下到上的表达。在本文中使用</span><span style="font-family: Verdana;">Andrew&nbsp;Ng</span><span style="font-family: 宋体;">代表的从左到右的表达形式。</span></p><br><p class="p"><span style="font-family: 宋体;">　　</span><span style="font-family: 宋体;">下面从简单的神经元开始说起，一步一步介绍神经网络复杂结构的形成。</span></p><br><p class="p">&nbsp;</p><br><p><strong><a name="second"></a>二.&nbsp;神经元</strong></p><br><p>　　<strong>1.引子</strong><strong><strong>　</strong></strong></p><br><p>　　对于神经元的研究由来已久，1904年生物学家就已经知晓了神经元的组成结构。</p><br><p>　　一个神经元通常具有多个<strong>树突</strong>，主要用来接受传入信息；而<strong>轴突</strong>只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做<span style="font-family: 宋体;">“<strong>突触</strong>”</span>。</p><br><p>　　人脑中的神经元形状可以用下图做简单的说明：</p><br><p style="text-align: center;"><img src="673793-20151229121248198-818698949.jpg" alt="" width="477" height="284"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图4 神经元</span></p><br><p style="text-align: center;">&nbsp;</p><br><p>&nbsp;　　1943年，心理学家McCulloch和数学家Pitts参考了生物神经元的结构，发表了抽象的神经元模型MP。在下文中，我们会具体介绍神经元模型。</p><br><p style="text-align: center;"><img src="673793-20151219175550990-1730772549.jpg" alt="" width="181" height="238">&nbsp;&nbsp;&nbsp;<img src="673793-20151219175600006-1000051743.jpg" alt="" width="167" height="237"></p><br><p style="text-align: center;"><span style="font-family: 黑体; font-size: 14px;">图5 Warren McCulloch（左）和&nbsp;Walter Pitts（右）</span>&nbsp;&nbsp;</p><br><p><strong>　　2.结构</strong>&nbsp;</p><br><p>　　神经元模型是一个包含输入，输出与计算功能的模型。输入可以类比为神经元的树突，而输出可以类比为神经元的轴突，计算则可以类比为细胞核。</p><br><p>　　下图是一个典型的神经元模型：包含有3个输入，1个输出，以及2个计算功能。</p><br><p>　　注意中间<span style="font-family: 宋体;">的箭头线。这些线称为“连接”。每个上有一个“权值”</span>。</p><br><p style="text-align: center;"><img src="673793-20151219153856802-307732621.jpg" alt="" width="608" height="354"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图6 神经元模型</span>&nbsp;</p><br><p style="text-align: center;">&nbsp;</p><br><p>　　连接是神经元中最重要的东西。每一个连接上都有一个权重。</p><br><p>　　一个神经网络的训练算法就是让权重的值调整到最佳，以使得整个网络的预测效果最好。</p><br><p>　　我们使用a来表示输入，用w来表示权值。一个表示连接的有向箭头可以这样理解：在初端，传递的信号大小仍然是a，端中间有加权参数w，经过这个加权后的信号会变成a<em>w，因此在连接的末端，信号的大小就变成了a</em>w。</p><br><p>　　在其他绘图模型里，有向箭头可能表示的是值的不变传递。而在神经元模型里，每个有向箭头表示的是值的加权传递。</p><br><p style="text-align: center;"><img src="673793-20151219180614819-1652574235.jpg" alt="" width="445" height="256"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图7 连接（connection）&nbsp;</span>&nbsp;</p><br><p style="text-align: center;">&nbsp;</p><br><p>　　如果我们将神经元图中的所有变量用符号表示，并且写出输出的计算公式的话，就是下图。</p><br><p style="text-align: center;"><img src="673793-20151230201441792-1505283920.jpg" alt="" width="625" height="334"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图8 神经元计算&nbsp;</span>&nbsp;</p><br><p style="text-align: center;">&nbsp;</p><br><p>　　可见z是在输入和权值的线性加权和叠加了一个<strong>函数g</strong>的值。在MP模型里，函数g是sgn函数，也就是取符号函数。这个函数当输入大于0时，输出1，否则输出0。</p><br><p>　　下面对神经元模型的图进行一些扩展。首先将sum函数与sgn函数合并到一个圆圈里，代表神经元的内部计算。其次，把输入a与输出z写到连接线的左上方，便于后面画复杂的网络。最后说明，一个神经元可以引出多个代表输出的有向箭头，但值都是一样的。</p><br><p>　　神经元可以看作一个计算与存储单元。计算是神经元对其的输入进行计算功能。存储是神经元会暂存计算结果，并传递到下一层。</p><br><p style="text-align: center;"><img src="673793-20151230204036479-461440948.jpg" alt="" width="828" height="315"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图9 神经元扩展</span>&nbsp;</p><br><p style="text-align: center;">&nbsp;</p><br><p style="text-align: left;">　　<span style="font-family: 宋体;">当我们用“神经元”组成网络以后，描述网络中的某个“神经元”时，我们更多地会用“<strong>单元</strong>”（<span style="font-family: verdana, geneva;">unit</span>）来指代。同时由于神经网络的表现形式是一个有向图，有时也会用“<strong>节点</strong>”（<span style="font-family: verdana, geneva;">node</span>）来表达同样的意思。</span>&nbsp;</p><br><p><strong>　　3.效果</strong>&nbsp;</p><br><p>　　神经元模型的使用可以这样理解：</p><br><p>　　我们有一个数据，称之为样本。样本有四个属性，其中三个属性已知，一个属性未知。我们需要做的就是通过三个已知属性<strong>预测</strong>未知属性。</p><br><p>　　具体办法就是使用神经元的公式进行计算。三个已知属性的值是a<sub>1</sub>，a<sub>2</sub>，a<sub>3</sub>，未知属性的值是z。z可以通过公式计算出来。</p><br><p>　　这里，已知的属性称之为<strong>特征</strong>，未知的属性称之为<strong>目标</strong>。假设特征与目标之间确实是线性关系，并且我们已经得到表示这个关系的权值w<sub>1</sub>，w<sub>2</sub>，w<sub>3</sub>。那么，我们就可以通过神经元模型预测新样本的目标。</p><br><p><strong>　　4.影响</strong></p><br><p>　　1943年发布的MP模型，虽然简单，但已经建立了神经网络大厦的地基。但是，MP模型中，权重的值都是预先设置的，因此不能学习。</p><br><p>　　1949年心理学家Hebb提出了Hebb学习率，认为人脑神经细胞的<strong>突触</strong>（也就是连接）上的强度上可以变化的。于是计算科学家们开始考虑用调整权值的方法来让机器学习。这为后面的学习算法奠定了基础。</p><br><p style="text-align: center;"><img src="673793-20151219175742677-1785435491.gif" alt="" width="179" height="231"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图10 Donald Olding Hebb</span>&nbsp;</p><br><p style="text-align: center;">&nbsp;</p><br><p>　　尽管神经元模型与Hebb学习律都已诞生，但限于当时的计算机能力，直到接近10年后，第一个真正意义的神经网络才诞生。</p><br><p>&nbsp;</p><br><p><strong><a name="third"></a>三. 单层神经网络（感知器）</strong></p><br><p><strong>　　1.引子</strong><strong>　</strong>　</p><br><p>　　1958年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络。他给它起了一个名字–<span style="font-family: 宋体;">“感知器”</span>（Perceptron）（有的文献翻译成<span style="font-family: 宋体;">“感知机”，下文统一用“感知器”来指代</span>）。</p><br><p>　　感知器是当时首个可以学习的人工神经网络。Rosenblatt现场演示了其学习识别简单图像的过程，在当时的社会引起了轰动。</p><br><p>　　人们认为已经发现了智能的奥秘，许多学者和科研机构纷纷投入到神经网络的研究中。美国军方大力资助了神经网络的研究，并认为神经网络比<span style="font-family: 宋体;">“原子弹工程”</span>更重要。这段时间直到1969年才结束，这个时期可以看作神经网络的第一次高潮。</p><br><p style="text-align: center;"><img src="673793-20151221153812609-1784157068.jpg" alt="" width="520" height="239"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图11 Rosenblat与感知器&nbsp;</span></p><br><p><strong>　　2.结构</strong></p><br><p>　　下面来说明感知器模型。</p><br><p>　　在原来MP模型的<span style="font-family: 宋体;">“输入”</span>位置添加神经元节点，标志其为<span style="font-family: 宋体;">“输入单元”</span>。其余不变，于是我们就有了下图：从本图开始，我们将权值<span style="font-family: verdana, geneva;">w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub></span>写到<span style="font-family: 宋体;">“连接线”的中间</span>。</p><br><p style="text-align: center;"><img src="673793-20151221151959015-1876891081.jpg" alt="" width="308" height="339"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图12 单层神经网络</span>&nbsp;</p><br><p style="text-align: center;">&nbsp;</p><br><p>　　<span style="font-family: 宋体;">在“感知器”中</span>，有两个层次。分别是输入层和输出层。<span style="font-family: 宋体;">输入层里的“输入单元”只负责传输数据，</span>不做计算。<span style="font-family: 宋体;">输出层里的“输出单元”则需要对前面一层的输入进行计算。</span></p><br><p>　　我们把需要计算的层次称之为<span style="font-family: 宋体;">“计算层”</span>，并把拥有一个计算层的网络称之为<span style="font-family: 宋体;">“单层神经网络”</span>。有一些文献会按照网络拥有的层数来命名，例如把<span style="font-family: 宋体;">“感知器”称为两层神经网络。但在本文里，</span>我们根据计算层的数量来命名。</p><br><p>　　假如我们要预测的目标不再是一个值，而是一个向量，例如[2,3]。那么可以在输出层再增加一个<span style="font-family: 宋体;">“输出单元”</span>。</p><br><p>　　下图显示了带有两个输出单元的单层神经网络，其中输出单元z<sub>1</sub>的计算公式如下图。</p><br><p style="text-align: center;"><img src="673793-20151230204223917-579926148.jpg" alt="" width="583" height="365"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图13 单层神经网络(Z<sub>1</sub>)</span></p><br><p style="text-align: center;">&nbsp;</p><br><p>　　可以看到，z<sub>1</sub>的计算跟原先的z并没有区别。</p><br><p>　　我们已知一个神经元的输出可以向多个神经元传递，因此z<sub>2</sub>的计算公式如下图。</p><br><p style="text-align: center;"><img src="673793-20151230204258057-82126781.jpg" alt="" width="591" height="367"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图14 单层神经网络(Z<sub>2</sub>)</span></p><br><p style="text-align: center;">&nbsp;</p><br><p>　　可以看到，z<sub>2</sub>的计算中除了三个新的权值：<span style="font-family: verdana, geneva;">w<sub>4</sub>，w<sub>5</sub>，w<sub>6</sub>以外，其他与z<sub>1</sub>是一样的</span>。</p><br><p>　　整个网络的输出如下图。</p><br><p style="text-align: center;"><img src="673793-20151230204606760-610264555.jpg" alt="" width="604" height="414"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图15 单层神经网络(Z<sub>1</sub>和Z<sub>2</sub>)</span></p><br><p style="text-align: center;">&nbsp;</p><br><p>　　<span style="font-family: &#39;times new roman&#39;, times;">目前的表达公式有一点不让人满意的就是：<span style="font-family: verdana, geneva;">w<sub>4</sub>，w<sub>5</sub>，w<sub>6</sub></span>是后来加的，很难表现出跟原先的<span style="font-family: verdana, geneva;">w<sub>1</sub>，w<sub>2</sub>，w<sub>3</sub></span>的关系。</span></p><br><p><span style="font-family: &#39;times new roman&#39;, times;">　　因此我们改用二维的下标，用<span style="font-family: verdana, geneva;">w<sub>x,y</sub>来表达一个权值</span>。下标中的<span style="font-family: verdana, geneva;">x</span>代表后一层神经元的序号，而<span style="font-family: verdana, geneva;">y</span>代表前一层神经元的序号（序号的顺序从上到下）。</span></p><br><p><span style="font-family: &#39;times new roman&#39;, times;">　　例如，</span><span style="font-family: &#39;times new roman&#39;, times;"><span style="font-family: verdana, geneva;">w<sub>1,2</sub></span>代表后一层的第1个神经元与前一层的第2个神经元的连接的权值（这种标记方式参照了<span style="font-family: verdana, geneva;">Andrew Ng</span>的课件）。根据以上方法标记，我们有了下图。</span></p><br><p style="text-align: center;"><img src="673793-20151230205437995-673856644.jpg" alt="" width="630" height="417"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图16 单层神经网络(扩展)</span></p><br><p style="text-align: center;">&nbsp;</p><br><p><span style="font-family: 宋体;">　　如果我们仔细看输出的计算公式，会发现这两个公式就是线性代数方程组。因此可以用矩阵乘法来表达这两个公式。</span></p><br><p><span style="font-family: 宋体;">　　例如，输入的变量是<span style="font-family: verdana, geneva;">[a<sub>1</sub>，a<sub>2</sub>，a<sub>3</sub>]<sup>T</sup></span>（代表由<span style="font-family: verdana, geneva;">a<sub>1</sub>，a<sub>2</sub>，a<sub>3</sub></span>组成的列向量），用向量<span style="font-family: verdana, geneva;"><strong>a</strong>来表示。</span>方程的左边是<span style="font-family: verdana, geneva;">[z<sub>1</sub>，z<sub>2</sub>]<sup>T</sup></span>，用向量<span style="font-family: verdana, geneva;"><strong>z</strong></span>来表示。</span></p><br><p><span style="font-family: 宋体;">　　系数则是矩阵<span style="font-family: verdana, geneva;"><strong>W</strong></span>（2行3列的矩阵，排列形式与公式中的一样）。</span></p><br><p><span style="font-family: 宋体;">　　于是，输出公式可以改写成：</span></p><br><p style="text-align: center;"><span style="font-family: 宋体;"><span style="font-family: verdana, geneva;">g(<strong>W</strong> <em> <strong>a</strong>) = <strong>z</strong>;</em></span><br></span></p><br><p style="text-align: center;">&nbsp;</p><br><p><span style="font-family: 宋体;">　　这个公式就是神经网络中从前一层计算后一层的<strong>矩阵运算。</strong><br></span></p><br><p><strong>　　3.效果</strong></p><br><p>　　与神经元模型不同，感知器中的权值是通过训练得到的。因此，根据以前的知识我们知道，感知器类似一个<strong>逻辑回归</strong>模型，可以做线性分类任务。</p><br><p>　　我们可以用<strong>决策分界</strong>来形象的表达分类的效果。决策分界就是在二维的数据平面中划出一条直线，当数据的维度是3维的时候，就是划出一个平面，当数据的维度是n维时，就是划出一个n-1维的超平面。</p><br><p>　　下图显示了在二维平面中划出决策分界的效果，也就是感知器的分类效果。</p><br><p style="text-align: center;"><img src="673793-20151231073138323-962584420.png" alt="" width="540" height="447"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图17 单层神经网络（决策分界）</span></p><br><p>　　</p><br><p><strong style="line-height: 1.5;">　　4.影响</strong><span style="line-height: 1.5;">　</span></p><br><p>　　感知器只能做简单的线性分类任务。但是当时的人们热情太过于高涨，并没有人清醒的认识到这点。于是，当人工智能领域的巨擘Minsky指出这点时，事态就发生了变化。</p><br><p>　　Minsky在1969年出版了一本叫《Perceptron》的书，里面用详细的数学证明了感知器的弱点，尤其是感知器对XOR（异或）这样的简单分类任务都无法解决。</p><br><p>　　Minsky认为，如果将计算层增加到两层，计算量则过大，而且没有有效的学习算法。所以，他认为研究更深层的网络是没有价值的。（本文成文后一个月，即2016年1月，<a href="http://cacm.acm.org/news/197529-in-memoriam-marvin-minsky-1927-2016/fulltext" target="_blank">Minsky在美国去世</a>。谨在本文中纪念这位著名的计算机研究专家与大拿。）</p><br><p style="text-align: center;"><img src="673793-20151221213056031-351364155.jpg" alt="" width="198" height="203">&nbsp; &nbsp;</p><br><p style="text-align: center;"><span style="font-family: 黑体;">图18 Marvin Minsky</span></p><br><p>　　</p><br><p>　　由于Minsky的巨大影响力以及书中呈现的悲观态度，让很多学者和实验室纷纷放弃了神经网络的研究。神经网络的研究陷入了冰河期。这个时期又被称为<span style="font-family: 宋体;">“<span style="font-family: verdana, geneva;">AI winter</span>”</span>。</p><br><p>　　接近10年以后，对于两层神经网络的研究才带来神经网络的复苏。</p><br><p>&nbsp;</p><br><p><strong><a name="fourth"></a>四. 两层神经网络（多层感知器）</strong></p><br><p><strong>　　1.引子</strong></p><br><p class="p">　　两层神经网络是本文的重点，因为正是在这时候，神经网络开始了大范围的推广与使用。</p><br><p class="p">　　Minsky说过单层神经网络无法解决异或问题。但是当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。不过两层神经网络的计算是一个问题，没有一个较好的解法。</p><br><p class="p">　　<span style="font-family: Verdana;">1986</span><span style="font-family: 宋体;">年，</span><span style="font-family: Verdana;">Rumelhar</span><span style="font-family: 宋体;">和</span><span style="font-family: Verdana;">Hinton</span><span style="font-family: 宋体;">等人提出了反向传播（</span><span style="font-family: Verdana;">Backpropagation</span><span style="font-family: 宋体;">，</span><span style="font-family: Verdana;">BP</span><span style="font-family: 宋体;">）算法，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。</span><span style="font-family: 宋体;"><span style="font-family: verdana, Arial, Helvetica, sans-serif; line-height: 1.5;">目前，大量的教授神经网络的教材，都是重点介绍两层（带一个隐藏层）神经网络的内容。</span><span style="font-family: Verdana;">&nbsp;</span></span></p><br><p class="p"><span style="font-family: 宋体;"><span style="font-family: Verdana;">　　这时候的Hinton还很年轻，30年以后，正是他重新定义了神经网络，带来了神经网络复苏的又一春。</span></span></p><br><p style="text-align: center;">&nbsp; &nbsp;&nbsp;<img src="673793-20151222162306171-1025091923.jpg" alt="" width="138" height="208">&nbsp; &nbsp;&nbsp;<img src="673793-20151222163144687-406696976.jpg" alt="" width="170" height="209"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图19 David&nbsp;Rumelhart（左）以及&nbsp;Geoffery Hinton（右）</span></p><br><p style="text-align: center;">&nbsp;</p><br><p><strong>　　2.结构</strong></p><br><p>　　两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。我们扩展上节的单层神经网络，在右边新加一个层次（只含有一个节点）。</p><br><p>　　现在，我们的权值矩阵增加到了两个，我们用上标来区分不同层次之间的变量。</p><br><p>　　例如a<sub>x</sub><sup>(y)</sup>代表第y层的第x个节点。z<sub>1</sub>，z<sub>2</sub>变成了a<sub>1</sub><sup>(2)</sup>，a<sub>2</sub><sup>(2)</sup>。下图给出了a<sub>1</sub><sup>(2)</sup>，a<sub>2</sub><sup>(2)</sup>的计算公式。</p><br><p style="text-align: center;"><img src="673793-20151222164731249-360921014.jpg" alt="" width="568" height="390"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图20 两层神经网络（中间层计算）</span></p><br><p>&nbsp;</p><br><p>　　计算最终输出z的方式是利用了中间层的a<sub>1</sub><sup>(2)</sup>，a<sub>2</sub><sup>(2)</sup>和第二个权值矩阵计算得到的，如下图。</p><br><p style="text-align: center;"><img src="673793-20151222171056156-387680541.jpg" alt="" width="577" height="389"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图21 两层神经网络（输出层计算）</span></p><br><p style="text-align: center;">&nbsp;</p><br><p>　　<span style="font-family: 宋体;">假设我们的预测目标是一个向量，那么与前面类似，只需要在“输出层”再增加节点即可。</span></p><br><p><span style="font-family: 宋体;">　　我们使用向量和矩阵来表示层次中的变量。<span style="font-family: verdana, geneva;"><strong>a</strong><sup>(1)</sup>，<strong>a</strong><sup>(2)</sup>，<strong>z</strong></span>是网络中传输的向量数据。<span style="font-family: verdana, geneva;"><strong>W</strong><sup>(1)</sup>和<strong>W</strong><sup>(2)</sup></span>是网络的矩阵参数。</span><span style="font-family: 宋体;">如下图。</span></p><br><p style="text-align: center;"><img src="673793-20151222171328140-1303075636.jpg" alt="" width="423" height="389"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图22 两层神经网络（向量形式）</span></p><br><p style="text-align: center;">&nbsp;</p><br><p><span style="font-family: 宋体;">　　使用矩阵运算来表达整个计算公式的话如下：</span></p><br><p style="text-align: center;"><span style="font-family: 宋体;">&nbsp; <span style="font-family: verdana, geneva;">g(<strong>W</strong><sup>(1)</sup>&nbsp;&nbsp;<strong>a</strong><sup>(1)</sup>) =&nbsp;<strong>a</strong><sup>(2)</sup>;&nbsp;</span></span></p><br><p style="text-align: center;"><span style="font-family: verdana, geneva;">g(<strong>W</strong><sup>(2)</sup>&nbsp;<em>&nbsp;<strong>a</strong><sup>(2)</sup>) =&nbsp;<strong>z</strong>;</em></span></p><br><p style="text-align: center;">&nbsp;</p><br><p><span style="font-family: 宋体;">　　由此可见，使用矩阵运算来表达是很简洁的，而且也不会受到节点数增多的影响（无论有多少节点参与运算，乘法两端都只有一个变量）。因此神经网络的教程中大量使用矩阵运算来描述。</span></p><br><p><span style="font-family: 宋体;">　　需要说明的是，至今为止，我们对神经网络的结构图的讨论中都没有提到偏置节点（<span style="font-family: verdana, geneva;">bias unit</span>）。事实上，这些节点是默认存在的。它本质上是一个只含有存储功能，且存储值永远为1的单元。</span><span style="font-family: 宋体;">在神经网络的每个层次中，除了输出层以外，都会含有这样一个偏置单元。正如线性回归模型与逻辑回归模型中的一样。</span></p><br><p><span style="font-family: 宋体;">　　偏置单元与后一层的所有节点都有连接，我们设这些参数值为向量<span style="font-family: verdana, geneva;"><strong>b</strong></span>，称之为偏置。如下图。</span></p><br><p style="text-align: center;"><img src="673793-20151226111144687-604911384.jpg" alt="" width="431" height="512"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图23 两层神经网络（考虑偏置节点）</span></p><br><p style="text-align: center;">&nbsp;</p><br><p style="text-align: left;">　　可以看出，偏置节点很好认，因为其没有输入（前一层中没有箭头指向它）。有些神经网络的结构图中会把偏置节点明显画出来，有些不会。一般情况下，我们都不会明确画出偏置节点。&nbsp;</p><br><p>　　在考虑了偏置以后的一个神经网络的矩阵运算如下：</p><br><p style="text-align: center;">&nbsp;&nbsp;g(<strong>W</strong><sup>(1)</sup>&nbsp;&nbsp;<strong>a</strong><sup>(1)&nbsp;</sup>+ <strong>b</strong><sup>(1)</sup>) =&nbsp;<strong>a</strong><sup>(2)</sup>;&nbsp;</p><br><p style="text-align: center;">g(<strong>W</strong><sup>(2)</sup>&nbsp;<em>&nbsp;<strong>a</strong><sup>(2)&nbsp;</sup>+ <strong>b</strong><sup>(2)</sup>) =&nbsp;<strong>z</strong>;</em></p><br><p style="text-align: center;">&nbsp;</p><br><p>　　需要说明的是，在两层神经网络中，我们不再使用sgn函数作为函数g，而是使用平滑函数sigmoid作为函数g。我们把函数g也称作激活函数（active&nbsp;function）。</p><br><p>　　事实上，神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系。初学者可能认为画神经网络的结构图是为了在程序中实现这些圆圈与线，但在一个神经网络的程序中，既没有“线”这个对象，也没有“单元”这个对象。实现一个神经网络最需要的是线性代数库。</p><br><p><strong>　　3.效果</strong></p><br><p>　　与单层神经网络不同。理论证明，两层神经网络可以无限逼近任意连续函数。</p><br><p>　　这是什么意思呢？也就是说，面对复杂的非线性分类任务，两层（带一个隐藏层）神经网络可以分类的很好。</p><br><p>　　下面就是一个例子（此两图来自colah的<a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" target="_blank">博客</a>），红色的线与蓝色的线代表数据。而红色区域和蓝色区域代表由神经网络划开的区域，两者的分界线就是决策分界。</p><br><p style="text-align: center;"><img src="673793-20151231073619073-461403542.png" alt=""></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图24 两层神经网络（决策分界）</span></p><br><p>　　</p><br><p>　　可以看到，这个两层神经网络的决策分界是非常平滑的曲线，而且分类的很好。有趣的是，前面已经学到过，单层网络只能做线性分类任务。而两层神经网络中的后一层也是线性分类层，应该只能做线性分类任务。为什么两个线性分类任务结合就可以做非线性分类任务？</p><br><p>　　我们可以把输出层的决策分界单独拿出来看一下。就是下图。</p><br><p style="text-align: center;"><img src="673793-20151231074314604-2050732128.png" alt=""></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图25 两层神经网络（空间变换）</span></p><br><p style="text-align: center;">&nbsp;</p><br><p>　　可以看到，输出层的决策分界仍然是直线。关键就是，从输入层到隐藏层时，数据发生了空间变换。也就是说，两层神经网络中，隐藏层对原始的数据进行了一个空间变换，使其可以被线性分类，然后输出层的决策分界划出了一个线性分类分界线，对其进行分类。</p><br><p>　　这样就导出了两层神经网络可以做非线性分类的关键–隐藏层。联想到我们一开始推导出的矩阵公式，我们知道，矩阵和向量相乘，本质上就是对向量的坐标空间进行一个变换。因此，隐藏层的参数矩阵的作用就是使得数据的原始坐标空间从线性不可分，转换成了线性可分。</p><br><p>　　两层神经网络通过两层的线性模型模拟了数据内真实的非线性函数。因此，多层的神经网络的本质就是复杂函数拟合。</p><br><p>　　下面来讨论一下隐藏层的节点数设计。在设计一个神经网络时，输入层的节点数需要与特征的维度匹配，输出层的节点数要与目标的维度匹配。而中间层的节点数，却是由设计者指定的。<span style="font-family: 宋体;">因此，“自由”把握在设计者的手中。</span>但是，节点数设置的多少，却会影响到整个模型的效果。如何决定这个自由层的节点数呢？目前业界没有完善的理论来指导这个决策。一般是根据经验来设置。较好的方法就是预先设定几个可选值，通过切换这几个值来看整个模型的预测效果，选择效果最好的值作为最终选择。这种方法又叫做<span style="font-family: Verdana;">Grid&nbsp;Search</span><span style="font-family: 宋体;">（网格搜索）。</span></p><br><p style="text-align: left;">　　了解了两层神经网<span style="line-height: 1.5;">络的结构以后，我们就可以看懂其它类似的结构图。例如EasyPR字符识别网络架构（下图）。</span></p><br><p style="text-align: center;"><img src="673793-20151226122337406-1923048422.jpg" alt="" width="550" height="393"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图26 EasyPR字符识别网络</span></p><br><p style="text-align: center;">&nbsp;</p><br><p style="text-align: left;">　　EasyPR使用了字符的图像去进行字符文字的识别。输入是120维的向量。输出是要预测的文字类别，共有65类。根据实验，我们测试了一些隐藏层数目，发现当值为40时，整个网络在测试集上的效果较好，因此选择网络的最终结构就是120，40，65。</p><br><p><strong>　　4.训练</strong></p><br><p>　　下面简单介绍一下两层神经网络的训练。</p><br><p>　　在Rosenblat提出的感知器模型中，模型中的参数可以被训练，但是使用的方法较为简单，并没有使用目前机器学习中通用的方法，这导致其扩展性与适用性非常有限。从两层神经网络开始，神经网络的研究人员开始使用机器学习相关的技术进行神经网络的训练。例如用大量的数据（1000-10000左右），使用算法进行优化等等，从而使得模型训练可以获得性能与数据利用上的双重优势。</p><br><p>　　机器学习模型训练的目的，就是使得参数尽可能的与真实的模型逼近。具体做法是这样的。首先给所有参数赋上随机值。我们使用这些随机生成的参数值，来预测训练数据中的样本。样本的预测目标为<span style="font-family: verdana, geneva;">y<sub>p</sub></span>，真实目标为<span style="font-family: verdana, geneva;">y</span>。那么，定义一个值loss，计算公式如下。</p><br><p style="text-align: center;">loss = (y<sub>p&nbsp;</sub>- y)<sup>2</sup></p><br><p style="text-align: center;">&nbsp;</p><br><p>　　这个值称之为<strong>损失</strong>（loss），我们的目标就是使对所有训练数据的损失和尽可能的小。</p><br><p>　　如果将先前的神经网络预测的矩阵公式带入到y<sub>p</sub>中（因为有z=y<sub>p</sub>），那么我们可以把损失写为关于参数（parameter）的函数，这个函数称之为<strong>损失函数</strong>（loss function）。下面的问题就是求：如何优化参数，能够让损失函数的值最小。</p><br><p>　　此时这个问题就被转化为一个优化问题。一个常用方法就是高等数学中的求导，但是这里的问题由于参数不止一个，求导后计算导数等于0的运算量很大，所以一般来说解决这个优化问题使用的是<strong>梯度下降</strong>算法。梯度下降算法每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。</p><br><p>　　在神经网络模型中，由于结构复杂，每次计算梯度的代价很大。因此还需要使用<strong>反向传播</strong>算法。反向传播算法是利用了神经网络的结构进行的计算。不一次计算所有参数的梯度，而是从后往前。首先计算输出层的梯度，然后是第二个参数矩阵的梯度，接着是中间层的梯度，再然后是第一个参数矩阵的梯度，最后是输入层的梯度。计算结束以后，所要的两个参数矩阵的梯度就都有了。</p><br><p>　　反向传播算法可以直观的理解为下图。梯度的计算从后往前，一层层反向传播。前缀E代表着相对导数的意思。</p><br><p style="text-align: center;"><img src="673793-20151229120754198-2003498733.jpg" alt="" width="403" height="387"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图27 反向传播算法</span></p><br><p>&nbsp;</p><br><p><span style="line-height: 1.5;">　　反向传播算法的启示是数学中的<strong>链式法则</strong>。在此需要说明的是，尽管早期神经网络的研究人员努力从生物学中得到启发，但从BP算法开始，研究者们更多地从数学上寻求问题的最优解。不再盲目模拟人脑网络是神经网络研究走向成熟的标志。正如科学家们可以从鸟类的飞行中得到启发，但没有必要一定要完全模拟鸟类的飞行方式，也能制造可以飞天的飞机。</span></p><br><p><span style="line-height: 1.5;">　　优化问题只是训练中的一个部分。机器学习问题之所以称为学习问题，而不是优化问题，就是因为它不仅要求数据在训练集上求得一个较小的误差，在测试集上也要表现好。因为模型最终是要部署到没有见过训练数据的真实场景。提升模型在测试集上的预测效果的主题叫做<strong>泛化</strong>（generalization），相关方法被称作正则化（regularization）。</span><span style="line-height: 1.5;">神经网络中常用的泛化技术有<strong>权重衰减</strong>等。</span></p><br><p><strong>　　5.影响</strong></p><br><p class="p">　　两层神经网络在多个地方的应用说明了其效用与价值。10年前困扰神经网络界的异或问题被轻松解决。神经网络在这个时候，已经可以发力于语音识别，图像识别，自动驾驶等多个领域。</p><br><p class="p">　　历史总是惊人的相似，神经网络的学者们再次登上了《纽约时报》的专访。人们认为神经网络可以解决许多问题。就连娱乐界都开始受到了影响，当年的《终结者》电影中的阿诺都赶时髦地说一句：我的CPU是一个神经网络处理器，一个会学习的计算机。</p><br><p>　　但是神经网络仍然存在若干的问题：尽管使用了BP算法，一次神经网络的训练仍然耗时太久，而且困扰训练优化的一个问题就是局部最优解问题，这使得神经网络的优化较为困难。同时，隐藏层的节点数需要调参，这使得使用不太方便，工程和研究人员对此多有抱怨。</p><br><p>　　90年代中期，由Vapnik等人发明的SVM（Support Vector Machines，支持向量机）算法诞生，很快就在若干个方面体现出了对比神经网络的优势：无需调参；高效；全局最优解。基于以上种种理由，SVM迅速打败了神经网络算法成为主流。</p><br><p style="text-align: center;"><img src="673793-20151224114218312-1112152935.jpg" alt="" width="122" height="182"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图28 Vladimir Vapnik</span></p><br><p style="text-align: center;">&nbsp;</p><br><p>　　神经网络的研究再次陷入了冰河期。当时，只要你的论文中包含神经网络相关的字眼，非常容易被会议和期刊拒收，研究界那时对神经网络的不待见可想而知。</p><br><p>&nbsp;</p><br><p><strong><a name="fifth"></a>五. 多层神经网络（深度学习）</strong></p><br><p><strong>　　1.引子</strong><strong>　</strong>　</p><br><p class="p">　　在被人摒弃的10年中，有几个学者仍然在坚持研究。这其中的棋手就是加拿大多伦多大学的Geoffery Hinton教授。</p><br><p class="p">　　<span style="line-height: 1.5;">2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了</span><span style="font-family: 宋体;">“深度信念网络”的概念</span><span style="line-height: 1.5;">。</span><span style="font-family: 宋体;">与传统的训练方式不同，“深度信念网络”有一个“<strong>预训练</strong>”（<span style="font-family: verdana, geneva;">pre-training</span>）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“<strong>微调</strong>”(<span style="font-family: verdana, geneva;">fine-tuning</span>)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词–“<strong>深度学习</strong>”。</span></p><br><p class="p">&nbsp;　　很快，深度学习在语音识别领域暂露头角。接着，2012年，深度学习技术又在图像识别领域大展拳脚。Hinton<span style="font-family: 宋体;">与他的学生在</span><span style="font-family: verdana, geneva;">ImageNet</span>竞赛中，用多层的卷积神经网络成功地对包含一千类别的一百万张图片进行了训练，取得了分类错误率<span style="font-family: &#39;Times New Roman&#39;;"><span style="font-family: verdana, geneva;">15</span>%</span><span style="font-family: 宋体;">的好成绩，这个成绩比第二名高了近</span><span style="font-family: verdana, geneva;">11</span><span style="font-family: 宋体;">个百分点，充分证明了多层神经网络识别效果的优越性</span><span style="font-family: 宋体;">。</span></p><br><p class="p"><span style="font-family: 宋体;">　　在这之后，关于深度神经网络的研究与应用不断涌现。</span></p><br><p style="text-align: center;"><img src="673793-20151224145544656-1225191900.jpg" alt="" width="167" height="223"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图29 Geoffery Hinton&nbsp;</span></p><br><p style="text-align: center;">&nbsp;</p><br><p class="p">　　由于篇幅原因，本文不介绍CNN（Conventional Neural Network，卷积神经网络）与RNN（Recurrent Neural Network，递归神经网络）的架构，下面我们只讨论普通的多层神经网络。</p><br><p><strong>　　2.结构</strong></p><br><p>　　我们延续两层神经网络的方式来设计一个多层神经网络。</p><br><p>　　在两层神经网络的输出层后面，继续添加层次。原来的输出层变成中间层，新加的层次成为新的输出层。所以可以得到下图。</p><br><p style="text-align: center;"><img src="673793-20151224204339234-1994620313.jpg" alt="" width="540" height="351"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图30 多层神经网络</span></p><br><p style="text-align: center;">&nbsp;</p><br><p>　　依照这样的方式不断添加，我们可以得到更多层的多层神经网络。公式推导的话其实跟两层神经网络类似，使用矩阵运算的话就仅仅是加一个公式而已。</p><br><p>　　在已知输入<strong>a</strong><sup>(1)</sup>，参数<strong>W</strong><sup>(1)</sup>，<strong>W</strong><sup>(2)</sup>，<strong>W</strong><sup>(3)</sup>的情况下，输出<strong>z</strong>的推导公式如下：</p><br><p style="text-align: center;">&nbsp; &nbsp; &nbsp;g(<strong>W</strong><sup>(1)</sup>&nbsp;&nbsp;<strong>a</strong><sup>(1)</sup>) =&nbsp;<strong>a</strong><sup>(2)</sup>;&nbsp;</p><br><p style="text-align: center;">&nbsp; &nbsp; g(<strong>W</strong><sup>(2)</sup>&nbsp;<em>&nbsp;<strong>a</strong><sup>(2)</sup>) =&nbsp;<strong>a</strong><sup>(3)</sup>;</em></p><br><p style="text-align: center;">g(<strong>W</strong><sup>(3)</sup>&nbsp;&nbsp;<strong>a</strong><sup>(3)</sup>) = <strong>z</strong>;</p><br><p style="text-align: center;">&nbsp;</p><br><p>　　多层神经网络中，输出也是按照一层一层的方式来计算。从最外面的层开始，算出所有单元的值以后，再继续计算更深一层。只有当前层所有单元的值都计算完毕以后，才会算下一层。有点像计算向前不断推进的感觉。所以这个过程叫做<span style="font-family: 宋体;">“正向传播”</span>。</p><br><p>　　下面讨论一下多层神经网络中的参数。</p><br><p>　　首先我们看第一张图，可以看出<strong>W</strong><sup>(1)</sup>中有6个参数，<strong>W</strong><sup>(2)</sup>中有4个参数，<strong>W</strong><sup>(3)</sup>中有6个参数，所以整个神经网络中的参数有16个（这里我们不考虑偏置节点，下同）。</p><br><p style="text-align: center;"><img src="673793-20151224212531484-570745053.jpg" alt="" width="591" height="463">&nbsp;</p><br><p style="text-align: center;"><span style="font-family: 黑体;">图31 多层神经网络（较少参数）</span></p><br><p style="text-align: center;">&nbsp;</p><br><p>　　假设我们将中间层的节点数做一下调整。第一个中间层改为3个单元，第二个中间层改为4个单元。</p><br><p>　　经过调整以后，整个网络的参数变成了33个。</p><br><p style="text-align: center;"><img src="673793-20151224213620234-1075501325.jpg" alt="" width="626" height="496">&nbsp;</p><br><p style="text-align: center;"><span style="font-family: 黑体;">图32 多层神经网络（较多参数）</span></p><br><p style="text-align: center;">&nbsp;</p><br><p>　　虽然层数保持不变，但是第二个神经网络的参数数量却是第一个神经网络的接近两倍之多，从而带来了更好的表示（represention）能力。表示能力是多层神经网络的一个重要性质，下面会做介绍。</p><br><p>　　在参数一致的情况下，我们也可以获得一个<span style="font-family: 宋体;">“更深”</span>的网络。</p><br><p style="text-align: center;"><img src="673793-20151224213703109-813423001.jpg" alt="" width="681" height="454">&nbsp;</p><br><p style="text-align: center;"><span style="font-family: 黑体;">图33 多层神经网络（更深的层次）</span></p><br><p>&nbsp;</p><br><p>　　上图的网络中，虽然参数数量仍然是33，但却有4个中间层，是原来层数的接近两倍。这意味着一样的参数数量，可以用更深的层次去表达。</p><br><p><strong>　　3.效果</strong></p><br><p>　　与两层层神经网络不同。多层神经网络中的层数增加了很多。</p><br><p>　　增加更多的层次有什么好处？更深入的表示特征，以及更强的函数模拟能力。</p><br><p>　　更深入的表示特征可以这样理解，随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是<span style="font-family: 宋体;">“边缘”</span>的特征，第二个隐藏层学习到的是由<span style="font-family: 宋体;">“边缘”组成的“形状”</span>的特征，<span style="font-family: 宋体;">第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征</span>。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。</p><br><p>　　关于逐层特征学习的例子，可以参考下图。</p><br><p style="text-align: center;"><img src="673793-20151231075103229-1126297331.png" alt="">&nbsp;</p><br><p style="text-align: center;"><span style="font-family: 黑体;">图34 多层神经网络（特征学习）</span></p><br><p style="text-align: center;">&nbsp;</p><br><p>　　更强的函数模拟能力是由于随着层数的增加，整个网络的参数就越多。而神经网络其实本质就是模拟特征与目标之间的真实关系函数的方法，更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的<strong>容量</strong>（capcity）去拟合真正的关系。</p><br><p>　　通过研究发现，在参数数量一样的情况下，更深的网络往往具有比浅层的网络更好的识别效率。这点也在ImageNet的多次大赛中得到了证实。从2012年起，每年获得ImageNet冠军的深度神经网络的层数逐年增加，2015年最好的方法GoogleNet是一个多达22层的神经网络。</p><br><p>　　在最新一届的ImageNet大赛上，目前拿到最好成绩的MSRA团队的方法使用的更是一个深达152层的网络！关于这个方法更多的信息有兴趣的可以查阅ImageNet网站。</p><br><p><strong>　　4.训练</strong></p><br><p>　　在单层神经网络时，我们使用的激活函数是sgn函数。到了两层神经网络时，我们使用的最多的是sigmoid函数。而到了多层神经网络时，通过一系列的研究发现，ReLU函数在训练多层神经网络时，更容易收敛，并且预测性能更好。因此，目前在深度学习中，最流行的非线性函数是ReLU函数。ReLU函数不是传统的非线性函数，而是分段线性函数。其表达式非常简单，就是y=max(x,0)。简而言之，在x大于0，输出就是输入，而在x小于0时，输出就保持为0。这种函数的设计启发来自于生物神经元对于激励的线性响应，以及当低于某个阈值后就不再响应的模拟。</p><br><p>　　在多层神经网络中，训练的主题仍然是优化和泛化。当使用足够强的计算芯片（例如GPU图形加速卡）时，梯度下降算法以及反向传播算法在多层神经网络中的训练中仍然工作的很好。目前学术界主要的研究既在于开发新的算法，也在于对这两个算法进行不断的优化，例如，增加了一种带动量因子（momentum）的梯度下降算法。　</p><br><p>　　在深度学习中，泛化技术变的比以往更加的重要。这主要是因为神经网络的层数增加了，参数也增加了，表示能力大幅度增强，很容易出现<strong>过拟合现象</strong>。<span style="line-height: 1.5;">因此正则化技术就显得十分重要。目前，Dropout技术，以及数据扩容（Data-Augmentation）技术是目前使用的最多的正则化技术。</span></p><br><p><strong>　　5.影响</strong></p><br><p class="p">　　目前，深度神经网络在人工智能界占据统治地位。但凡有关人工智能的产业报道，必然离不开深度学习。神经网络界当下的四位引领者除了前文所说的Ng，Hinton以外，还有CNN的发明人Yann Lecun，以及《Deep Learning》的作者Bengio。</p><br><p class="p">　　前段时间一直对人工智能持谨慎态度的马斯克，搞了一个<a href="http://news.cnblogs.com/n/534878/" target="_blank">OpenAI项目</a>，邀请Bengio作为高级顾问。马斯克认为，人工智能技术不应该掌握在大公司如Google，Facebook的手里，更应该作为一种开放技术，让所有人都可以参与研究。马斯克的这种精神值得让人敬佩。</p><br><p style="text-align: center;"><img src="673793-20151228131815214-368589404.png" alt="" width="160" height="188">&nbsp; &nbsp;<img src="673793-20151228131718948-1881649621.png" alt="" width="166" height="189"></p><br><p style="text-align: center;"><span style="font-family: 黑体; font-size: 14px;">图35 Yann LeCun（左）和&nbsp;Yoshua Bengio（右）</span></p><br><p style="text-align: center;">&nbsp;</p><br><p class="p">　　多层神经网络的研究仍在进行中。现在最为火热的研究技术包括RNN，LSTM等，研究方向则是图像理解方面。图像理解技术是给计算机一幅图片，让它用语言来表达这幅图片的意思。ImageNet竞赛也在不断召开，有更多的方法涌现出来，刷新以往的正确率。</p><br><p class="p">&nbsp;</p><br><p><strong><a name="sixth"></a>六. 回顾</strong></p><br><p><strong>　　1.影响</strong><strong>　</strong>　</p><br><p class="p">　　我们回顾一下神经网络发展的历程。<span style="line-height: 1.5;">神经网络的发展历史曲折荡漾，既有被人捧上天的时刻，也有摔落在街头无人问津的时段，中间经历了数次大起大落。</span></p><br><p class="p">　　从单层神经网络（感知器）开始，到包含一个隐藏层的两层神经网络，再到多层的深度神经网络，一共有三次兴起过程。详见下图。</p><br><p style="text-align: center;"><img src="673793-20151228170208120-1856567090.jpg" alt="" width="964" height="523">&nbsp;</p><br><p style="text-align: center;"><span style="font-family: 黑体; font-size: 14px;">图36 三起三落的神经网络</span></p><br><p style="text-align: center;">&nbsp;</p><br><p>　　上图中的顶点与谷底可以看作神经网络发展的高峰与低谷。图中的横轴是时间，以年为单位。纵轴是一个神经网络影响力的示意表示。如果把1949年Hebb模型提出到1958年的感知机诞生这个10年视为落下（没有兴起）的话，那么神经网络算是经历了<span style="font-family: 宋体;">“三起三落”</span>这样一个过程，跟<span style="font-family: 宋体;">“小平”</span>同志类似。俗话说，天将降大任于斯人也，必先苦其心志，劳其筋骨。经历过如此多波折的神经网络能够在现阶段取得成功也可以被看做是磨砺的积累吧。</p><br><p>　　历史最大的好处是可以给现在做参考。科学的研究呈现螺旋形上升的过程，不可能一帆风顺。同时，这也给现在过分热衷深度学习与人工智能的人敲响警钟，因为这不是第一次人们因为神经网络而疯狂了。1958年到1969年，以及1985年到1995，这两个十年间人们对于神经网络以及人工智能的期待并不现在低，可结果如何大家也能看的很清楚。</p><br><p>　　因此，冷静才是对待目前深度学习热潮的最好办法。如果因为深度学习火热，或者可以有<span style="font-family: 宋体;">“钱景”</span>就一窝蜂的涌入，那么最终的受害人只能是自己。神经网络界已经两次有被人们捧上天了的境况，相信也对于捧得越高，摔得越惨这句话深有体会。因此，神经网络界的学者也必须给这股热潮浇上一盆水，不要让媒体以及投资家们过分的高看这门技术。很有可能，三十年河东，三十年河西，在几年后，神经网络就再次陷入谷底。根据上图的历史曲线图，这是很有可能的。</p><br><p><strong>　　2.效果</strong><strong>　</strong>　</p><br><p>　　下面说一下神经网络为什么能这么火热？简而言之，就是其学习效果的强大。随着神经网络的发展，其表示性能越来越强。</p><br><p>　　从单层神经网络，到两层神经网络，再到多层神经网络，下图说明了，随着网络层数的增加，以及激活函数的调整，神经网络所能拟合的决策分界平面的能力。</p><br><p style="text-align: center;"><img src="673793-20151228134016120-1091351096.jpg" alt="" width="984" height="653">&nbsp;</p><br><p style="text-align: center;"><span style="font-family: 黑体;">图37 表示能力不断增强</span></p><br><p style="text-align: center;">&nbsp;</p><br><p>　　可以看出，随着层数增加，其非线性分界拟合能力不断增强。图中的分界线并不代表真实训练出的效果，更多的是示意效果。</p><br><p>　　神经网络的研究与应用之所以能够不断地火热发展下去，与其强大的函数拟合能力是分不开关系的。</p><br><p><strong>　　3.外因</strong><strong>　</strong>　</p><br><p>　　当然，光有强大的内在能力，并不一定能成功。一个成功的技术与方法，不仅需要内因的作用，还需要时势与环境的配合。神经网络的发展背后的外在原因可以被总结为：更强的计算性能，更多的数据，以及更好的训练方法。只有满足这些条件时，神经网络的函数拟合能力才能得已体现，见下图。</p><br><p style="text-align: center;"><img src="673793-20151228170149135-2107087462.jpg" alt="" width="857" height="537">&nbsp;</p><br><p style="text-align: center;"><span style="font-family: 黑体;">图38 发展的外在原因<br></span></p><br><p style="text-align: center;">&nbsp;</p><br><p style="text-align: left;">　　之所以在单层神经网络年代，Rosenblat无法制作一个双层分类器，就在于当时的计算性能不足，Minsky也以此来打压神经网络。但是Minsky没有料到，仅仅10年以后，计算机CPU的快速发展已经使得我们可以做两层神经网络的训练，并且还有快速的学习算法BP。</p><br><p style="text-align: left;">　　但是在两层神经网络快速流行的年代。更高层的神经网络由于计算性能的问题，以及一些计算方法的问题，其优势无法得到体现。直到2012年，研究人员发现，用于高性能计算的图形加速卡（GPU）可以极佳地匹配神经网络训练所需要的要求：高并行性，高存储，没有太多的控制需求，配合预训练等算法，神经网络才得以大放光彩。</p><br><p style="text-align: left;">　　互联网时代，大量的数据被收集整理，更好的训练方法不断被发现。所有这一切都满足了多层神经网络发挥能力的条件。</p><br><p style="text-align: left;">　　<span style="font-family: 宋体;">“</span><span style="font-family: 宋体;">时势造英雄”，正如<span style="font-family: verdana, geneva;">Hinton</span>在<span style="font-family: verdana, geneva;">2006年</span>的论文里说道的</span></p><br><p style="text-align: left;"><span style="font-family: 宋体;">　　“</span><strong>…&nbsp;provided&nbsp;that&nbsp;computers&nbsp;were&nbsp;fast&nbsp;enough,&nbsp;data&nbsp;sets&nbsp;were&nbsp;big&nbsp;enough,&nbsp;and&nbsp;the&nbsp;initial&nbsp;weights&nbsp;were&nbsp;close&nbsp;enough&nbsp;to&nbsp;a&nbsp;good&nbsp;solution.&nbsp;All&nbsp;three&nbsp;conditions&nbsp;are&nbsp;now&nbsp;satisfied.</strong><span style="font-family: 宋体;">”</span>，</p><br><p style="text-align: left;">&nbsp;</p><br><p class="p">　　外在条件的满足也是神经网络从神经元得以发展到目前的深度神经网络的重要因素。</p><br><p>　　除此以外，一门技术的发扬没有<span style="font-family: 宋体;">“伯乐”</span>也是不行的。在神经网络漫长的历史中，正是由于许多研究人员的锲而不舍，不断钻研，才能有了现在的成就。前期的Rosenblat，Rumelhart没有见证到神经网络如今的流行与地位。但是在那个时代，他们为神经网络的发展所打下的基础，却会永远流传下去，不会退色。</p><br><p>&nbsp;</p><br><p class="p"><strong><a name="seventh"></a>七. 展望</strong></p><br><p class="p"><strong><strong>　　1.量子计算</strong></strong></p><br><p class="p">　　回到我们对神经网络历史的讨论，根据历史趋势图来看，神经网络以及深度学习会不会像以往一样再次陷入谷底？作者认为，这个过程可能取决于量子计算机的发展。</p><br><p class="p">　　根据一些最近的研究发现，人脑内部进行的计算可能是类似于量子计算形态的东西。而且目前已知的最大神经网络跟人脑的神经元数量相比，仍然显得非常小，仅不及1%左右。所以未来真正想实现人脑神经网络的模拟，可能需要借助量子计算的强大计算能力。</p><br><p class="p">　　各大研究组也已经认识到了量子计算的重要性。谷歌就在开展量子计算机D-wave的研究，希望用量子计算来进行机器学习，并且在前段时间有了突破性的<a href="http://news.cnblogs.com/n/535307/" target="_blank">进展</a>。国内方面，阿里和中科院合作成立了<a href="http://news.sciencenet.cn/htmlnews/2015/7/323963.shtm" target="_blank">量子计算实验室</a>，意图进行量子计算的研究。</p><br><p class="p">　　如果量子计算发展不力，仍然需要数十年才能使我们的计算能力得以突飞猛进的发展，那么缺少了强大计算能力的神经网络可能会无法一帆风顺的发展下去。这种情况可以类比为80-90年时期神经网络因为计算能力的限制而被低估与忽视。假设量子计算机真的能够与神经网络结合，并且助力真正的人工智能技术的诞生，而且量子计算机发展需要10年的话，那么神经网络可能还有10年的发展期。直到那时期以后，神经网络才能真正接近实现AI这一目标。</p><br><p style="text-align: center;"><img src="673793-20151227175114031-277810977.jpg" alt="" width="419" height="329">&nbsp;</p><br><p style="text-align: center;"><span style="font-family: 黑体; font-size: 14px;">图39 量子计算</span></p><br><p style="text-align: center;">&nbsp;</p><br><p class="p">　　<strong><strong>2.人工智能</strong></strong></p><br><p class="p">　　最后，作者想简单地谈谈对目前人工智能的看法。虽然现在人工智能非常火热，但是距离真正的人工智能还有很大的距离。就拿计算机视觉方向来说，面对稍微复杂一些的场景，以及易于混淆的图像，计算机就可能难以识别。因此，这个方向还有很多的工作要做。</p><br><p class="p">　　就普通人看来，这么辛苦的做各种实验，以及投入大量的人力就是为了实现一些不及孩童能力的视觉能力，未免有些不值。但是这只是第一步。虽然计算机需要很大的运算量才能完成一个普通人简单能完成的识图工作，但计算机最大的优势在于并行化与批量推广能力。使用计算机以后，我们可以很轻易地将以前需要人眼去判断的工作交给计算机做，而且几乎没有任何的推广成本。这就具有很大的价值。正如火车刚诞生的时候，有人嘲笑它又笨又重，速度还没有马快。但是很快规模化推广的火车就替代了马车的使用。人工智能也是如此。这也是为什么目前世界上各著名公司以及政府都对此热衷的原因。</p><br><p class="p">　　目前看来，神经网络要想实现人工智能还有很多的路要走，但方向至少是正确的，下面就要看后来者的不断努力了。</p><br><p style="text-align: center;"><img src="673793-20151231172950167-1599238944.jpg" alt="" width="420" height="280"></p><br><p style="text-align: center;"><span style="font-family: 黑体;">图40 人工智能</span></p><br><p style="text-align: center;">&nbsp;</p><br><p class="p"><strong><a name="eighth"></a>八 总结</strong></p><br><p class="p">　　本文回顾了神经网络的发展历史，从神经元开始，历经单层神经网络，两层神经网络，直到多层神经网络。在历史介绍中穿插讲解神经网络的结构，分类效果以及训练方法等。本文说明了神经网络内部实际上就是矩阵计算，<span style="font-family: 宋体;">在程序中的实现没有“点”和“线”的对象</span>。本文说明了神经网络强大预测能力的根本，就是多层的神经网络可以无限逼近真实的对应函数，从而模拟数据之间的真实关系。除此之外，本文回顾了神经网络发展的历程，分析了神经网络发展的外在原因，包括计算能力的增强，数据的增多，以及方法的创新等。最后，本文对神经网络的未来进行了展望，包括量子计算与神经网络结合的可能性，以及探讨未来人工智能发展的前景与价值。</p><br><p style="text-align: center;">&nbsp;</p><br><p class="p"><strong><a name="nineth"></a>九.&nbsp;后记</strong></p><br><p>　　本篇文章可以视为作者一年来对神经网络的理解与总结，包括实验的体会，书籍的阅读，以及思考的火花等。神经网络虽然重要，但学习并不容易。这主要是由于其结构图较为难懂，以及历史发展的原因，导致概念容易混淆，一些介绍的博客与网站内容新旧不齐。本篇文章着眼于这些问题，没有太多的数学推导，意图以一种简单的，直观的方式对神经网络进行讲解。在2015年最后一天终于写完。希望本文可以对各位有所帮助。</p><br><p>&nbsp;</p><br><p>&nbsp;</p><br><p>　　<strong>作者很感谢能够阅读到这里的读者。如果看完觉得好的话，还请轻轻点一下赞，你们的鼓励就是作者继续行文的动力。本文的备注部分是一些对神经网络学习的建议，供补充阅读与参考。</strong></p><br><p>　　</p><br><p>　　<em>目前为止，<a href="https://github.com/liuruoze/EasyPR" target="_blank">EasyPR</a>的1.4版已经将神经网络（ANN）训练的模块加以开放，开发者们可以使用这个模块来进行自己的字符模型的训练。有兴趣的可以<a href="https://github.com/liuruoze/EasyPR" target="_blank">下载</a>。</em></p><br><p>&nbsp;</p><br><p><strong><a name="tenth"></a>十. 备注</strong></p><br><p><strong>　　</strong>神经网络虽然很重要，但是对于神经网络的学习，却并不容易。这些学习困难主要来自以下三个方面：概念，类别，教程。下面简单说明这三点。</p><br><p><strong>　　1.概念</strong></p><br><p>　　对于一门技术的学习而言，首先最重要的是弄清概念。只有将概念理解清楚，才能顺畅的进行后面的学习。由于神经网络漫长的发展历史，经常会有一些概念容易混淆，让人学习中产生困惑。这里面包括历史的术语，不一致的说法，以及被遗忘的研究等。　</p><br><p class="p">　　<strong>历史的术语</strong></p><br><p>　　这个的代表就是多层感知器（MLP）这个术语。起初看文献时很难理解的一个问题就是，为什么神经网络又有另一个名称：MLP。其实MLP（Multi-Layer Perceptron）的名称起源于50-60年代的感知器（Perceptron）。由于我们在感知器之上又增加了一个计算层，因此称为多层感知器。值得注意的是，虽然叫“多层”，MLP一般都指的是两层（带一个隐藏层的）神经网络。</p><br><p>　　MLP这个术语属于历史遗留的产物。现在我们一般就说神经网络，以及深度神经网络。前者代表带一个隐藏层的两层神经网络，也是EasyPR目前使用的识别网络，后者指深度学习的网络。</p><br><p>　　<strong>不一致的说法<br></strong></p><br><p>　　这个最明显的代表就是损失函数loss&nbsp;function，这个还有两个说法是跟它完全一致的意思，分别是残差函数error&nbsp;function，以及代价函数cost&nbsp;function。loss&nbsp;function是目前深度学习里用的较多的一种说法，caffe里也是这么叫的。cost&nbsp;function则是Ng在coursera教学视频里用到的统一说法。这三者都是同一个意思，都是优化问题所需要求解的方程。虽然在使用的时候不做规定，但是在听到各种讲解时要心里明白。</p><br><p>　　再来就是权重weight和参数parameter的说法，神经网络界由于以前的惯例，一般会将训练得到的参数称之为权重，而不像其他机器学习方法就称之为参数。这个需要记住就好。不过在目前的使用惯例中，也有这样一种规定。那就是非偏置节点连接上的值称之为权重，而偏置节点上的值称之为偏置，两者统一起来称之为参数。</p><br><p>　　另外一个同义词就是激活函数active function和转移函数transfer function了。同样，他们代表一个意思，都是叠加的非线性函数的说法。</p><br><p>　　<strong>被遗忘的研究<br></strong></p><br><p>　　由于神经网络发展历史已经有70年的漫长历史，因此在研究过程中，必然有一些研究分支属于被遗忘阶段。这里面包括各种不同的网络，例如SOM（Self-Organizing Map，自组织特征映射网络），SNN（Synergetic Neural Network，协同神经网络），ART（Adaptive Resonance Theory，自适应共振理论网络）等等。所以看历史文献时会看到许多没见过的概念与名词。</p><br><p>　　有些历史网络甚至会重新成为新的研究热点，例如RNN与LSTM就是80年代左右开始的研究，目前已经是深度学习研究中的重要一门技术，在语音与文字识别中有很好的效果。　</p><br><p>　　对于这些易于混淆以及弄错的概念，务必需要多方参考文献，理清上下文，这样才不会在学习与阅读过程中迷糊。</p><br><p>　　<strong>2.类别</strong></p><br><p>　　下面谈一下关于神经网络中的不同类别。</p><br><p>　　其实本文的名字“神经网络浅讲”并不合适，因为本文并不是讲的是“神经网络”的内容，而是其中的一个子类，也是目前最常说的<strong>前馈神经网络</strong>。根据下图的分类可以看出。</p><br><p style="text-align: center;"><img src="673793-20151227190543499-2614280.jpg" alt="" width="839" height="371">&nbsp;</p><br><p style="text-align: center;"><span style="font-family: 黑体;">图41 神经网络的类别</span></p><br><p>&nbsp;</p><br><p>　　神经网络其实是一个非常宽泛的称呼，它包括两类，一类是用计算机的方式去模拟人脑，这就是我们常说的ANN（人工神经网络），另一类是研究生物学上的神经网络，又叫生物神经网络。对于我们计算机人士而言，肯定是研究前者。</p><br><p>　　在人工神经网络之中，又分为前馈神经网络和反馈神经网络这两种。那么它们两者的区别是什么呢？这个其实在于它们的结构图。我们可以把结构图看作是一个有向图。其中神经元代表顶点，连接代表有向边。对于前馈神经网络中，这个有向图是没有回路的。你可以仔细观察本文中出现的所有神经网络的结构图，确认一下。而对于反馈神经网络中，结构图的有向图是有回路的。反馈神经网络也是一类重要的神经网络。其中Hopfield网络就是反馈神经网络。深度学习中的RNN也属于一种反馈神经网络。</p><br><p>　　具体到前馈神经网络中，就有了本文中所分别描述的三个网络：单层神经网络，双层神经网络，以及多层神经网络。深度学习中的CNN属于一种特殊的多层神经网络。另外，在一些Blog中和文献中看到的BP神经网络是什么？其实它们就是使用了反向传播BP算法的两层前馈神经网络。也是最普遍的一种两层神经网络。</p><br><p>　　通过以上分析可以看出，神经网络这种说法其实是非常广义的，具体在文章中说的是什么网络，需要根据文中的内容加以区分。</p><br><p><strong>　　3.教程</strong></p><br><p>　　如何更好的学习神经网络，认真的学习一门课程或者看一本著作都是很有必要的。</p><br><p>　　说到网络教程的话，这里必须说一下Ng的机器学习课程。对于一个初学者而言，Ng的课程视频是非常有帮助的。Ng一共开设过两门机器学习公开课程：一个是2003年在Standford开设的，面向全球的学生，这个视频现在可以在网易公开课上找到；另一个是2010年专门为Coursera上的用户开设的，需要登陆Coursera上才能学习。</p><br><p>　　但是，需要注意点是，这两个课程对待神经网络的态度有点不同。早些的课程一共有20节课，Ng花了若干节课去专门讲SVM以及SVM的推导，而当时的神经网络，仅仅放了几段视频，花了大概不到20分钟（一节课60分钟左右）。而到了后来的课程时，总共10节的课程中，Ng给了完整的两节给神经网络，详细介绍了神经网络的反向传播算法。同时给SVM只有一节课，并且没有再讲SVM的推导过程。下面两张图分别是Ng介绍神经网络的开篇，可以大致看出一些端倪。</p><br><p style="text-align: center;"><img src="673793-20151230155838057-551533589.jpg" alt="" width="854" height="413">&nbsp;</p><br><p style="text-align: center;"><span style="font-family: 黑体;">图42 Ng与神经网络</span></p><br><p>&nbsp;</p><br><p class="p">　　为什么Ng对待神经网络的反应前后相差那么大？事实上就是深度学习的原因。Ng实践了深度学习的效果，认识到深度学习的基础–神经网络的重要性。这就是他在后面重点介绍神经网络的原因。总之，对于神经网络的学习而言，我更推荐Coursera上的。因为在那个时候，Ng才是真正的把神经网络作为一门重要的机器学习方法去传授。你可以从他上课的态度中感受到他的重视，以及他希望你能学好的期望。</p><br><p>&nbsp;</p><br><p><strong>版权说明：</strong></p><br><p><strong>　　本文中的所有文字，图片，代码的版权都是属于作者和博客园共同所有。欢迎转载，但是务必注明作者与出处。任何未经允许的剽窃以及爬虫抓取都属于侵权，作者和博客园保留所有权利。</strong></p><br><p>&nbsp;&nbsp;</p><br><p><strong>参考文献：</strong></p><br><p><strong>　　</strong>1.<a href="https://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html" target="_blank">Neural Networks</a></p><br><p>　　2.<a href="http://ufldl.stanford.edu/wiki/index.php/Neural_Networks" target="_blank">Andrew Ng&nbsp;Neural Networks&nbsp;</a></p><br><p>　　3.<a href="http://www.36dsj.com/archives/20804" target="_blank">神经网络简史</a><a href="http://www.36dsj.com/archives/20804" target="_blank"><br></a></p><br><p>　　4.<a href="http://www.intsci.ac.cn/shizz/course/kd08.ppt" target="_blank">中科院 史忠植 神经网络&nbsp;讲义</a></p><br><p>　　5.<a href="http://caai.cn/contents/118/1934.html" target="_blank">深度学习 胡晓林</a></p><br><p>&nbsp;</p></div><div id="MySignature"></div><br><div class="clear"></div><br><div id="blog_post_info_block"><br><div id="BlogPostCategory"></div><br><div></div></div></div></div></div></div></div></div></div><script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/kesyoban.model.json"},"display":{"position":"right","width":130,"height":260},"mobile":{"show":false},"log":false,"tagMode":false});</script></body></html>
]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>机器学习，神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>House Prices Advanced Regression Techniques</title>
    <url>/2019/05/28/House_Prices_Advanced_Regression_Techniques/</url>
    <content><![CDATA[<p>Kaggle Competition 的练习</p>
<p><a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques" target="_blank" rel="noopener">房价预测</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据分析库</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 机器学习库</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC, LinearSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Perceptron</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">pd.options.display.max_rows = <span class="number">10</span>  <span class="comment"># 最大显示行数</span></span><br><span class="line">pd.options.display.float_format = <span class="string">'&#123;:.5f&#125;'</span>.format  <span class="comment"># 精确度 保留一位小数</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df = pd.read_csv(<span class="string">'/train.csv'</span>)</span><br><span class="line">test_df = pd.read_csv(<span class="string">'/test.csv'</span>)</span><br><span class="line">train_df.shape, test_df.shape</span><br></pre></td></tr></table></figure>
<pre><code>((1460, 81), (1459, 80))
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Id</th>
      <th>MSSubClass</th>
      <th>MSZoning</th>
      <th>LotFrontage</th>
      <th>LotArea</th>
      <th>Street</th>
      <th>Alley</th>
      <th>LotShape</th>
      <th>LandContour</th>
      <th>Utilities</th>
      <th>LotConfig</th>
      <th>LandSlope</th>
      <th>Neighborhood</th>
      <th>Condition1</th>
      <th>Condition2</th>
      <th>BldgType</th>
      <th>HouseStyle</th>
      <th>OverallQual</th>
      <th>OverallCond</th>
      <th>YearBuilt</th>
      <th>YearRemodAdd</th>
      <th>RoofStyle</th>
      <th>RoofMatl</th>
      <th>Exterior1st</th>
      <th>Exterior2nd</th>
      <th>MasVnrType</th>
      <th>MasVnrArea</th>
      <th>ExterQual</th>
      <th>ExterCond</th>
      <th>Foundation</th>
      <th>BsmtQual</th>
      <th>BsmtCond</th>
      <th>BsmtExposure</th>
      <th>BsmtFinType1</th>
      <th>BsmtFinSF1</th>
      <th>BsmtFinType2</th>
      <th>BsmtFinSF2</th>
      <th>BsmtUnfSF</th>
      <th>TotalBsmtSF</th>
      <th>Heating</th>
      <th>...</th>
      <th>CentralAir</th>
      <th>Electrical</th>
      <th>1stFlrSF</th>
      <th>2ndFlrSF</th>
      <th>LowQualFinSF</th>
      <th>GrLivArea</th>
      <th>BsmtFullBath</th>
      <th>BsmtHalfBath</th>
      <th>FullBath</th>
      <th>HalfBath</th>
      <th>BedroomAbvGr</th>
      <th>KitchenAbvGr</th>
      <th>KitchenQual</th>
      <th>TotRmsAbvGrd</th>
      <th>Functional</th>
      <th>Fireplaces</th>
      <th>FireplaceQu</th>
      <th>GarageType</th>
      <th>GarageYrBlt</th>
      <th>GarageFinish</th>
      <th>GarageCars</th>
      <th>GarageArea</th>
      <th>GarageQual</th>
      <th>GarageCond</th>
      <th>PavedDrive</th>
      <th>WoodDeckSF</th>
      <th>OpenPorchSF</th>
      <th>EnclosedPorch</th>
      <th>3SsnPorch</th>
      <th>ScreenPorch</th>
      <th>PoolArea</th>
      <th>PoolQC</th>
      <th>Fence</th>
      <th>MiscFeature</th>
      <th>MiscVal</th>
      <th>MoSold</th>
      <th>YrSold</th>
      <th>SaleType</th>
      <th>SaleCondition</th>
      <th>SalePrice</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>60</td>
      <td>RL</td>
      <td>65.00000</td>
      <td>8450</td>
      <td>Pave</td>
      <td>NaN</td>
      <td>Reg</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>Inside</td>
      <td>Gtl</td>
      <td>CollgCr</td>
      <td>Norm</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>2Story</td>
      <td>7</td>
      <td>5</td>
      <td>2003</td>
      <td>2003</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>VinylSd</td>
      <td>VinylSd</td>
      <td>BrkFace</td>
      <td>196.00000</td>
      <td>Gd</td>
      <td>TA</td>
      <td>PConc</td>
      <td>Gd</td>
      <td>TA</td>
      <td>No</td>
      <td>GLQ</td>
      <td>706</td>
      <td>Unf</td>
      <td>0</td>
      <td>150</td>
      <td>856</td>
      <td>GasA</td>
      <td>...</td>
      <td>Y</td>
      <td>SBrkr</td>
      <td>856</td>
      <td>854</td>
      <td>0</td>
      <td>1710</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>Gd</td>
      <td>8</td>
      <td>Typ</td>
      <td>0</td>
      <td>NaN</td>
      <td>Attchd</td>
      <td>2003.00000</td>
      <td>RFn</td>
      <td>2</td>
      <td>548</td>
      <td>TA</td>
      <td>TA</td>
      <td>Y</td>
      <td>0</td>
      <td>61</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0</td>
      <td>2</td>
      <td>2008</td>
      <td>WD</td>
      <td>Normal</td>
      <td>208500</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>20</td>
      <td>RL</td>
      <td>80.00000</td>
      <td>9600</td>
      <td>Pave</td>
      <td>NaN</td>
      <td>Reg</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>FR2</td>
      <td>Gtl</td>
      <td>Veenker</td>
      <td>Feedr</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>1Story</td>
      <td>6</td>
      <td>8</td>
      <td>1976</td>
      <td>1976</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>MetalSd</td>
      <td>MetalSd</td>
      <td>None</td>
      <td>0.00000</td>
      <td>TA</td>
      <td>TA</td>
      <td>CBlock</td>
      <td>Gd</td>
      <td>TA</td>
      <td>Gd</td>
      <td>ALQ</td>
      <td>978</td>
      <td>Unf</td>
      <td>0</td>
      <td>284</td>
      <td>1262</td>
      <td>GasA</td>
      <td>...</td>
      <td>Y</td>
      <td>SBrkr</td>
      <td>1262</td>
      <td>0</td>
      <td>0</td>
      <td>1262</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>TA</td>
      <td>6</td>
      <td>Typ</td>
      <td>1</td>
      <td>TA</td>
      <td>Attchd</td>
      <td>1976.00000</td>
      <td>RFn</td>
      <td>2</td>
      <td>460</td>
      <td>TA</td>
      <td>TA</td>
      <td>Y</td>
      <td>298</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0</td>
      <td>5</td>
      <td>2007</td>
      <td>WD</td>
      <td>Normal</td>
      <td>181500</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>60</td>
      <td>RL</td>
      <td>68.00000</td>
      <td>11250</td>
      <td>Pave</td>
      <td>NaN</td>
      <td>IR1</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>Inside</td>
      <td>Gtl</td>
      <td>CollgCr</td>
      <td>Norm</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>2Story</td>
      <td>7</td>
      <td>5</td>
      <td>2001</td>
      <td>2002</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>VinylSd</td>
      <td>VinylSd</td>
      <td>BrkFace</td>
      <td>162.00000</td>
      <td>Gd</td>
      <td>TA</td>
      <td>PConc</td>
      <td>Gd</td>
      <td>TA</td>
      <td>Mn</td>
      <td>GLQ</td>
      <td>486</td>
      <td>Unf</td>
      <td>0</td>
      <td>434</td>
      <td>920</td>
      <td>GasA</td>
      <td>...</td>
      <td>Y</td>
      <td>SBrkr</td>
      <td>920</td>
      <td>866</td>
      <td>0</td>
      <td>1786</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>Gd</td>
      <td>6</td>
      <td>Typ</td>
      <td>1</td>
      <td>TA</td>
      <td>Attchd</td>
      <td>2001.00000</td>
      <td>RFn</td>
      <td>2</td>
      <td>608</td>
      <td>TA</td>
      <td>TA</td>
      <td>Y</td>
      <td>0</td>
      <td>42</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0</td>
      <td>9</td>
      <td>2008</td>
      <td>WD</td>
      <td>Normal</td>
      <td>223500</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>70</td>
      <td>RL</td>
      <td>60.00000</td>
      <td>9550</td>
      <td>Pave</td>
      <td>NaN</td>
      <td>IR1</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>Corner</td>
      <td>Gtl</td>
      <td>Crawfor</td>
      <td>Norm</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>2Story</td>
      <td>7</td>
      <td>5</td>
      <td>1915</td>
      <td>1970</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>Wd Sdng</td>
      <td>Wd Shng</td>
      <td>None</td>
      <td>0.00000</td>
      <td>TA</td>
      <td>TA</td>
      <td>BrkTil</td>
      <td>TA</td>
      <td>Gd</td>
      <td>No</td>
      <td>ALQ</td>
      <td>216</td>
      <td>Unf</td>
      <td>0</td>
      <td>540</td>
      <td>756</td>
      <td>GasA</td>
      <td>...</td>
      <td>Y</td>
      <td>SBrkr</td>
      <td>961</td>
      <td>756</td>
      <td>0</td>
      <td>1717</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>Gd</td>
      <td>7</td>
      <td>Typ</td>
      <td>1</td>
      <td>Gd</td>
      <td>Detchd</td>
      <td>1998.00000</td>
      <td>Unf</td>
      <td>3</td>
      <td>642</td>
      <td>TA</td>
      <td>TA</td>
      <td>Y</td>
      <td>0</td>
      <td>35</td>
      <td>272</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0</td>
      <td>2</td>
      <td>2006</td>
      <td>WD</td>
      <td>Abnorml</td>
      <td>140000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>60</td>
      <td>RL</td>
      <td>84.00000</td>
      <td>14260</td>
      <td>Pave</td>
      <td>NaN</td>
      <td>IR1</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>FR2</td>
      <td>Gtl</td>
      <td>NoRidge</td>
      <td>Norm</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>2Story</td>
      <td>8</td>
      <td>5</td>
      <td>2000</td>
      <td>2000</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>VinylSd</td>
      <td>VinylSd</td>
      <td>BrkFace</td>
      <td>350.00000</td>
      <td>Gd</td>
      <td>TA</td>
      <td>PConc</td>
      <td>Gd</td>
      <td>TA</td>
      <td>Av</td>
      <td>GLQ</td>
      <td>655</td>
      <td>Unf</td>
      <td>0</td>
      <td>490</td>
      <td>1145</td>
      <td>GasA</td>
      <td>...</td>
      <td>Y</td>
      <td>SBrkr</td>
      <td>1145</td>
      <td>1053</td>
      <td>0</td>
      <td>2198</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>Gd</td>
      <td>9</td>
      <td>Typ</td>
      <td>1</td>
      <td>TA</td>
      <td>Attchd</td>
      <td>2000.00000</td>
      <td>RFn</td>
      <td>3</td>
      <td>836</td>
      <td>TA</td>
      <td>TA</td>
      <td>Y</td>
      <td>192</td>
      <td>84</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0</td>
      <td>12</td>
      <td>2008</td>
      <td>WD</td>
      <td>Normal</td>
      <td>250000</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 81 columns</p>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test_df.head()</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Id</th>
      <th>MSSubClass</th>
      <th>LotFrontage</th>
      <th>LotArea</th>
      <th>OverallQual</th>
      <th>OverallCond</th>
      <th>YearBuilt</th>
      <th>YearRemodAdd</th>
      <th>MasVnrArea</th>
      <th>BsmtFinSF1</th>
      <th>BsmtFinSF2</th>
      <th>BsmtUnfSF</th>
      <th>TotalBsmtSF</th>
      <th>1stFlrSF</th>
      <th>2ndFlrSF</th>
      <th>LowQualFinSF</th>
      <th>GrLivArea</th>
      <th>BsmtFullBath</th>
      <th>BsmtHalfBath</th>
      <th>FullBath</th>
      <th>HalfBath</th>
      <th>BedroomAbvGr</th>
      <th>KitchenAbvGr</th>
      <th>TotRmsAbvGrd</th>
      <th>Fireplaces</th>
      <th>GarageYrBlt</th>
      <th>GarageCars</th>
      <th>GarageArea</th>
      <th>WoodDeckSF</th>
      <th>OpenPorchSF</th>
      <th>EnclosedPorch</th>
      <th>3SsnPorch</th>
      <th>ScreenPorch</th>
      <th>PoolArea</th>
      <th>MiscVal</th>
      <th>MoSold</th>
      <th>YrSold</th>
      <th>SalePrice</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1201.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1452.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1379.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>730.50000</td>
      <td>56.89726</td>
      <td>70.04996</td>
      <td>10516.82808</td>
      <td>6.09932</td>
      <td>5.57534</td>
      <td>1971.26781</td>
      <td>1984.86575</td>
      <td>103.68526</td>
      <td>443.63973</td>
      <td>46.54932</td>
      <td>567.24041</td>
      <td>1057.42945</td>
      <td>1162.62671</td>
      <td>346.99247</td>
      <td>5.84452</td>
      <td>1515.46370</td>
      <td>0.42534</td>
      <td>0.05753</td>
      <td>1.56507</td>
      <td>0.38288</td>
      <td>2.86644</td>
      <td>1.04658</td>
      <td>6.51781</td>
      <td>0.61301</td>
      <td>1978.50616</td>
      <td>1.76712</td>
      <td>472.98014</td>
      <td>94.24452</td>
      <td>46.66027</td>
      <td>21.95411</td>
      <td>3.40959</td>
      <td>15.06096</td>
      <td>2.75890</td>
      <td>43.48904</td>
      <td>6.32192</td>
      <td>2007.81575</td>
      <td>180921.19589</td>
    </tr>
    <tr>
      <th>std</th>
      <td>421.61001</td>
      <td>42.30057</td>
      <td>24.28475</td>
      <td>9981.26493</td>
      <td>1.38300</td>
      <td>1.11280</td>
      <td>30.20290</td>
      <td>20.64541</td>
      <td>181.06621</td>
      <td>456.09809</td>
      <td>161.31927</td>
      <td>441.86696</td>
      <td>438.70532</td>
      <td>386.58774</td>
      <td>436.52844</td>
      <td>48.62308</td>
      <td>525.48038</td>
      <td>0.51891</td>
      <td>0.23875</td>
      <td>0.55092</td>
      <td>0.50289</td>
      <td>0.81578</td>
      <td>0.22034</td>
      <td>1.62539</td>
      <td>0.64467</td>
      <td>24.68972</td>
      <td>0.74732</td>
      <td>213.80484</td>
      <td>125.33879</td>
      <td>66.25603</td>
      <td>61.11915</td>
      <td>29.31733</td>
      <td>55.75742</td>
      <td>40.17731</td>
      <td>496.12302</td>
      <td>2.70363</td>
      <td>1.32810</td>
      <td>79442.50288</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.00000</td>
      <td>20.00000</td>
      <td>21.00000</td>
      <td>1300.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1872.00000</td>
      <td>1950.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>334.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>334.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>2.00000</td>
      <td>0.00000</td>
      <td>1900.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>2006.00000</td>
      <td>34900.00000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>365.75000</td>
      <td>20.00000</td>
      <td>59.00000</td>
      <td>7553.50000</td>
      <td>5.00000</td>
      <td>5.00000</td>
      <td>1954.00000</td>
      <td>1967.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>223.00000</td>
      <td>795.75000</td>
      <td>882.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1129.50000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>2.00000</td>
      <td>1.00000</td>
      <td>5.00000</td>
      <td>0.00000</td>
      <td>1961.00000</td>
      <td>1.00000</td>
      <td>334.50000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>5.00000</td>
      <td>2007.00000</td>
      <td>129975.00000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>730.50000</td>
      <td>50.00000</td>
      <td>69.00000</td>
      <td>9478.50000</td>
      <td>6.00000</td>
      <td>5.00000</td>
      <td>1973.00000</td>
      <td>1994.00000</td>
      <td>0.00000</td>
      <td>383.50000</td>
      <td>0.00000</td>
      <td>477.50000</td>
      <td>991.50000</td>
      <td>1087.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1464.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>2.00000</td>
      <td>0.00000</td>
      <td>3.00000</td>
      <td>1.00000</td>
      <td>6.00000</td>
      <td>1.00000</td>
      <td>1980.00000</td>
      <td>2.00000</td>
      <td>480.00000</td>
      <td>0.00000</td>
      <td>25.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>6.00000</td>
      <td>2008.00000</td>
      <td>163000.00000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1095.25000</td>
      <td>70.00000</td>
      <td>80.00000</td>
      <td>11601.50000</td>
      <td>7.00000</td>
      <td>6.00000</td>
      <td>2000.00000</td>
      <td>2004.00000</td>
      <td>166.00000</td>
      <td>712.25000</td>
      <td>0.00000</td>
      <td>808.00000</td>
      <td>1298.25000</td>
      <td>1391.25000</td>
      <td>728.00000</td>
      <td>0.00000</td>
      <td>1776.75000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>2.00000</td>
      <td>1.00000</td>
      <td>3.00000</td>
      <td>1.00000</td>
      <td>7.00000</td>
      <td>1.00000</td>
      <td>2002.00000</td>
      <td>2.00000</td>
      <td>576.00000</td>
      <td>168.00000</td>
      <td>68.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>8.00000</td>
      <td>2009.00000</td>
      <td>214000.00000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1460.00000</td>
      <td>190.00000</td>
      <td>313.00000</td>
      <td>215245.00000</td>
      <td>10.00000</td>
      <td>9.00000</td>
      <td>2010.00000</td>
      <td>2010.00000</td>
      <td>1600.00000</td>
      <td>5644.00000</td>
      <td>1474.00000</td>
      <td>2336.00000</td>
      <td>6110.00000</td>
      <td>4692.00000</td>
      <td>2065.00000</td>
      <td>572.00000</td>
      <td>5642.00000</td>
      <td>3.00000</td>
      <td>2.00000</td>
      <td>3.00000</td>
      <td>2.00000</td>
      <td>8.00000</td>
      <td>3.00000</td>
      <td>14.00000</td>
      <td>3.00000</td>
      <td>2010.00000</td>
      <td>4.00000</td>
      <td>1418.00000</td>
      <td>857.00000</td>
      <td>547.00000</td>
      <td>552.00000</td>
      <td>508.00000</td>
      <td>480.00000</td>
      <td>738.00000</td>
      <td>15500.00000</td>
      <td>12.00000</td>
      <td>2010.00000</td>
      <td>755000.00000</td>
    </tr>
  </tbody>
</table>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># test_df.describe()</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.info()</span><br><span class="line">print(<span class="string">'_'</span> * <span class="number">50</span>)</span><br><span class="line">test_df.info()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;
RangeIndex: 1460 entries, 0 to 1459
Data columns (total 81 columns):
Id               1460 non-null int64
MSSubClass       1460 non-null int64
MSZoning         1460 non-null object
LotFrontage      1201 non-null float64
LotArea          1460 non-null int64
Street           1460 non-null object
Alley            91 non-null object
LotShape         1460 non-null object
LandContour      1460 non-null object
Utilities        1460 non-null object
LotConfig        1460 non-null object
LandSlope        1460 non-null object
Neighborhood     1460 non-null object
Condition1       1460 non-null object
Condition2       1460 non-null object
BldgType         1460 non-null object
HouseStyle       1460 non-null object
OverallQual      1460 non-null int64
OverallCond      1460 non-null int64
YearBuilt        1460 non-null int64
YearRemodAdd     1460 non-null int64
RoofStyle        1460 non-null object
RoofMatl         1460 non-null object
Exterior1st      1460 non-null object
Exterior2nd      1460 non-null object
MasVnrType       1452 non-null object
MasVnrArea       1452 non-null float64
ExterQual        1460 non-null object
ExterCond        1460 non-null object
Foundation       1460 non-null object
BsmtQual         1423 non-null object
BsmtCond         1423 non-null object
BsmtExposure     1422 non-null object
BsmtFinType1     1423 non-null object
BsmtFinSF1       1460 non-null int64
BsmtFinType2     1422 non-null object
BsmtFinSF2       1460 non-null int64
BsmtUnfSF        1460 non-null int64
TotalBsmtSF      1460 non-null int64
Heating          1460 non-null object
HeatingQC        1460 non-null object
CentralAir       1460 non-null object
Electrical       1459 non-null object
1stFlrSF         1460 non-null int64
2ndFlrSF         1460 non-null int64
LowQualFinSF     1460 non-null int64
GrLivArea        1460 non-null int64
BsmtFullBath     1460 non-null int64
BsmtHalfBath     1460 non-null int64
FullBath         1460 non-null int64
HalfBath         1460 non-null int64
BedroomAbvGr     1460 non-null int64
KitchenAbvGr     1460 non-null int64
KitchenQual      1460 non-null object
TotRmsAbvGrd     1460 non-null int64
Functional       1460 non-null object
Fireplaces       1460 non-null int64
FireplaceQu      770 non-null object
GarageType       1379 non-null object
GarageYrBlt      1379 non-null float64
GarageFinish     1379 non-null object
GarageCars       1460 non-null int64
GarageArea       1460 non-null int64
GarageQual       1379 non-null object
GarageCond       1379 non-null object
PavedDrive       1460 non-null object
WoodDeckSF       1460 non-null int64
OpenPorchSF      1460 non-null int64
EnclosedPorch    1460 non-null int64
3SsnPorch        1460 non-null int64
ScreenPorch      1460 non-null int64
PoolArea         1460 non-null int64
PoolQC           7 non-null object
Fence            281 non-null object
MiscFeature      54 non-null object
MiscVal          1460 non-null int64
MoSold           1460 non-null int64
YrSold           1460 non-null int64
SaleType         1460 non-null object
SaleCondition    1460 non-null object
SalePrice        1460 non-null int64
dtypes: float64(3), int64(35), object(43)
memory usage: 924.0+ KB
__________________________________________________
&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;
RangeIndex: 1459 entries, 0 to 1458
Data columns (total 80 columns):
Id               1459 non-null int64
MSSubClass       1459 non-null int64
MSZoning         1455 non-null object
LotFrontage      1232 non-null float64
LotArea          1459 non-null int64
Street           1459 non-null object
Alley            107 non-null object
LotShape         1459 non-null object
LandContour      1459 non-null object
Utilities        1457 non-null object
LotConfig        1459 non-null object
LandSlope        1459 non-null object
Neighborhood     1459 non-null object
Condition1       1459 non-null object
Condition2       1459 non-null object
BldgType         1459 non-null object
HouseStyle       1459 non-null object
OverallQual      1459 non-null int64
OverallCond      1459 non-null int64
YearBuilt        1459 non-null int64
YearRemodAdd     1459 non-null int64
RoofStyle        1459 non-null object
RoofMatl         1459 non-null object
Exterior1st      1458 non-null object
Exterior2nd      1458 non-null object
MasVnrType       1443 non-null object
MasVnrArea       1444 non-null float64
ExterQual        1459 non-null object
ExterCond        1459 non-null object
Foundation       1459 non-null object
BsmtQual         1415 non-null object
BsmtCond         1414 non-null object
BsmtExposure     1415 non-null object
BsmtFinType1     1417 non-null object
BsmtFinSF1       1458 non-null float64
BsmtFinType2     1417 non-null object
BsmtFinSF2       1458 non-null float64
BsmtUnfSF        1458 non-null float64
TotalBsmtSF      1458 non-null float64
Heating          1459 non-null object
HeatingQC        1459 non-null object
CentralAir       1459 non-null object
Electrical       1459 non-null object
1stFlrSF         1459 non-null int64
2ndFlrSF         1459 non-null int64
LowQualFinSF     1459 non-null int64
GrLivArea        1459 non-null int64
BsmtFullBath     1457 non-null float64
BsmtHalfBath     1457 non-null float64
FullBath         1459 non-null int64
HalfBath         1459 non-null int64
BedroomAbvGr     1459 non-null int64
KitchenAbvGr     1459 non-null int64
KitchenQual      1458 non-null object
TotRmsAbvGrd     1459 non-null int64
Functional       1457 non-null object
Fireplaces       1459 non-null int64
FireplaceQu      729 non-null object
GarageType       1383 non-null object
GarageYrBlt      1381 non-null float64
GarageFinish     1381 non-null object
GarageCars       1458 non-null float64
GarageArea       1458 non-null float64
GarageQual       1381 non-null object
GarageCond       1381 non-null object
PavedDrive       1459 non-null object
WoodDeckSF       1459 non-null int64
OpenPorchSF      1459 non-null int64
EnclosedPorch    1459 non-null int64
3SsnPorch        1459 non-null int64
ScreenPorch      1459 non-null int64
PoolArea         1459 non-null int64
PoolQC           3 non-null object
Fence            290 non-null object
MiscFeature      51 non-null object
MiscVal          1459 non-null int64
MoSold           1459 non-null int64
YrSold           1459 non-null int64
SaleType         1458 non-null object
SaleCondition    1459 non-null object
dtypes: float64(11), int64(26), object(43)
memory usage: 912.0+ KB
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.describe(include=<span class="string">"O"</span>)</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MSZoning</th>
      <th>Street</th>
      <th>Alley</th>
      <th>LotShape</th>
      <th>LandContour</th>
      <th>Utilities</th>
      <th>LotConfig</th>
      <th>LandSlope</th>
      <th>Neighborhood</th>
      <th>Condition1</th>
      <th>Condition2</th>
      <th>BldgType</th>
      <th>HouseStyle</th>
      <th>RoofStyle</th>
      <th>RoofMatl</th>
      <th>Exterior1st</th>
      <th>Exterior2nd</th>
      <th>MasVnrType</th>
      <th>ExterQual</th>
      <th>ExterCond</th>
      <th>Foundation</th>
      <th>BsmtQual</th>
      <th>BsmtCond</th>
      <th>BsmtExposure</th>
      <th>BsmtFinType1</th>
      <th>BsmtFinType2</th>
      <th>Heating</th>
      <th>HeatingQC</th>
      <th>CentralAir</th>
      <th>Electrical</th>
      <th>KitchenQual</th>
      <th>Functional</th>
      <th>FireplaceQu</th>
      <th>GarageType</th>
      <th>GarageFinish</th>
      <th>GarageQual</th>
      <th>GarageCond</th>
      <th>PavedDrive</th>
      <th>PoolQC</th>
      <th>Fence</th>
      <th>MiscFeature</th>
      <th>SaleType</th>
      <th>SaleCondition</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1460</td>
      <td>1460</td>
      <td>91</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1452</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1423</td>
      <td>1423</td>
      <td>1422</td>
      <td>1423</td>
      <td>1422</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1459</td>
      <td>1460</td>
      <td>1460</td>
      <td>770</td>
      <td>1379</td>
      <td>1379</td>
      <td>1379</td>
      <td>1379</td>
      <td>1460</td>
      <td>7</td>
      <td>281</td>
      <td>54</td>
      <td>1460</td>
      <td>1460</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>5</td>
      <td>2</td>
      <td>2</td>
      <td>4</td>
      <td>4</td>
      <td>2</td>
      <td>5</td>
      <td>3</td>
      <td>25</td>
      <td>9</td>
      <td>8</td>
      <td>5</td>
      <td>8</td>
      <td>6</td>
      <td>8</td>
      <td>15</td>
      <td>16</td>
      <td>4</td>
      <td>4</td>
      <td>5</td>
      <td>6</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
      <td>6</td>
      <td>6</td>
      <td>6</td>
      <td>5</td>
      <td>2</td>
      <td>5</td>
      <td>4</td>
      <td>7</td>
      <td>5</td>
      <td>6</td>
      <td>3</td>
      <td>5</td>
      <td>5</td>
      <td>3</td>
      <td>3</td>
      <td>4</td>
      <td>4</td>
      <td>9</td>
      <td>6</td>
    </tr>
    <tr>
      <th>top</th>
      <td>RL</td>
      <td>Pave</td>
      <td>Grvl</td>
      <td>Reg</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>Inside</td>
      <td>Gtl</td>
      <td>NAmes</td>
      <td>Norm</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>1Story</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>VinylSd</td>
      <td>VinylSd</td>
      <td>None</td>
      <td>TA</td>
      <td>TA</td>
      <td>PConc</td>
      <td>TA</td>
      <td>TA</td>
      <td>No</td>
      <td>Unf</td>
      <td>Unf</td>
      <td>GasA</td>
      <td>Ex</td>
      <td>Y</td>
      <td>SBrkr</td>
      <td>TA</td>
      <td>Typ</td>
      <td>Gd</td>
      <td>Attchd</td>
      <td>Unf</td>
      <td>TA</td>
      <td>TA</td>
      <td>Y</td>
      <td>Gd</td>
      <td>MnPrv</td>
      <td>Shed</td>
      <td>WD</td>
      <td>Normal</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>1151</td>
      <td>1454</td>
      <td>50</td>
      <td>925</td>
      <td>1311</td>
      <td>1459</td>
      <td>1052</td>
      <td>1382</td>
      <td>225</td>
      <td>1260</td>
      <td>1445</td>
      <td>1220</td>
      <td>726</td>
      <td>1141</td>
      <td>1434</td>
      <td>515</td>
      <td>504</td>
      <td>864</td>
      <td>906</td>
      <td>1282</td>
      <td>647</td>
      <td>649</td>
      <td>1311</td>
      <td>953</td>
      <td>430</td>
      <td>1256</td>
      <td>1428</td>
      <td>741</td>
      <td>1365</td>
      <td>1334</td>
      <td>735</td>
      <td>1360</td>
      <td>380</td>
      <td>870</td>
      <td>605</td>
      <td>1311</td>
      <td>1326</td>
      <td>1340</td>
      <td>3</td>
      <td>157</td>
      <td>49</td>
      <td>1267</td>
      <td>1198</td>
    </tr>
  </tbody>
</table>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="分析概要"><a href="#分析概要" class="headerlink" title="分析概要"></a>分析概要</h1><table>
<thead>
<tr>
<th style="text-align:center">Feature</th>
<th style="text-align:center">Status</th>
<th style="text-align:center">Dispose </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Alley</td>
<td style="text-align:center">缺失比较多</td>
<td style="text-align:center">删除 </td>
</tr>
<tr>
<td style="text-align:center">PoolQC</td>
<td style="text-align:center">只有七家有游泳池并且和 PoolArea 相关</td>
<td style="text-align:center">先不填充 删除 </td>
</tr>
<tr>
<td style="text-align:center">Fence</td>
<td style="text-align:center">栏杆质量只有20%的有</td>
<td style="text-align:center">缺失的填充为没有</td>
</tr>
<tr>
<td style="text-align:center">MiscFeature</td>
<td style="text-align:center">其他项目也只有好少的房子有</td>
<td style="text-align:center">先不填充 删除 </td>
</tr>
<tr>
<td style="text-align:center">FireplaceQu</td>
<td style="text-align:center">有一半家没有壁炉</td>
<td style="text-align:center">填 0</td>
</tr>
<tr>
<td style="text-align:center">Garagetype</td>
<td style="text-align:center">空代表没有</td>
<td style="text-align:center">填 0</td>
</tr>
<tr>
<td style="text-align:center">Garagefinish</td>
<td style="text-align:center">空代表没有</td>
<td style="text-align:center">填 0</td>
</tr>
<tr>
<td style="text-align:center">Garagequal</td>
<td style="text-align:center">空代表没有</td>
<td style="text-align:center">填 0</td>
</tr>
<tr>
<td style="text-align:center">Garagecond</td>
<td style="text-align:center">空代表没有</td>
<td style="text-align:center">填 0</td>
</tr>
<tr>
<td style="text-align:center">LotFrontage</td>
<td style="text-align:center">和物业相连的街道有1/3缺失</td>
<td style="text-align:center">没想到太好的填充方法 删除 </td>
</tr>
</tbody>
</table>
<h1 id="整理-description-文件"><a href="#整理-description-文件" class="headerlink" title="整理 description 文件"></a>整理 description 文件</h1><p>数据描述文件记录了所有特征所代表的含义，其中许多特征是字符串，现在我们要整理为个字典，便于我们查询。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">description_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'/data_description.txt'</span>,<span class="string">'r'</span>) <span class="keyword">as</span> description:</span><br><span class="line">    description_data = description.read()</span><br><span class="line">    description.close()</span><br><span class="line">    </span><br><span class="line">description_data = description_data.split(<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> description_data:</span><br><span class="line">    <span class="keyword">if</span> <span class="string">':'</span> <span class="keyword">in</span> i:</span><br><span class="line">        key = i.split(<span class="string">':'</span>)[<span class="number">0</span>]</span><br><span class="line">        description_dict[key] = []</span><br><span class="line">    <span class="keyword">elif</span> i.split() <span class="keyword">and</span> <span class="string">'       '</span> <span class="keyword">in</span> i:</span><br><span class="line">        value = i.split()[<span class="number">0</span>]</span><br><span class="line">        description_dict[key].append(value)</span><br><span class="line"></span><br><span class="line">print(description_dict)</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;MSSubClass&apos;: [&apos;20&apos;, &apos;30&apos;, &apos;40&apos;, &apos;45&apos;, &apos;50&apos;, &apos;60&apos;, &apos;70&apos;, &apos;75&apos;, &apos;80&apos;, &apos;85&apos;, &apos;90&apos;, &apos;120&apos;, &apos;150&apos;, &apos;160&apos;, &apos;180&apos;, &apos;190&apos;], &apos;MSZoning&apos;: [&apos;A&apos;, &apos;C&apos;, &apos;FV&apos;, &apos;I&apos;, &apos;RH&apos;, &apos;RL&apos;, &apos;RP&apos;, &apos;RM&apos;], &apos;LotFrontage&apos;: [], &apos;LotArea&apos;: [], &apos;Street&apos;: [&apos;Grvl&apos;, &apos;Pave&apos;], &apos;Alley&apos;: [&apos;Grvl&apos;, &apos;Pave&apos;, &apos;NA&apos;], &apos;LotShape&apos;: [&apos;Reg&apos;, &apos;IR1&apos;, &apos;IR2&apos;, &apos;IR3&apos;], &apos;LandContour&apos;: [&apos;Lvl&apos;, &apos;Bnk&apos;, &apos;HLS&apos;, &apos;Low&apos;], &apos;Utilities&apos;: [&apos;AllPub&apos;, &apos;NoSewr&apos;, &apos;NoSeWa&apos;, &apos;ELO&apos;], &apos;LotConfig&apos;: [&apos;Inside&apos;, &apos;Corner&apos;, &apos;CulDSac&apos;, &apos;FR2&apos;, &apos;FR3&apos;], &apos;LandSlope&apos;: [&apos;Gtl&apos;, &apos;Mod&apos;, &apos;Sev&apos;], &apos;Neighborhood&apos;: [&apos;Blmngtn&apos;, &apos;Blueste&apos;, &apos;BrDale&apos;, &apos;BrkSide&apos;, &apos;ClearCr&apos;, &apos;CollgCr&apos;, &apos;Crawfor&apos;, &apos;Edwards&apos;, &apos;Gilbert&apos;, &apos;IDOTRR&apos;, &apos;MeadowV&apos;, &apos;Mitchel&apos;, &apos;Names&apos;, &apos;NoRidge&apos;, &apos;NPkVill&apos;, &apos;NridgHt&apos;, &apos;NWAmes&apos;, &apos;OldTown&apos;, &apos;SWISU&apos;, &apos;Sawyer&apos;, &apos;SawyerW&apos;, &apos;Somerst&apos;, &apos;StoneBr&apos;, &apos;Timber&apos;, &apos;Veenker&apos;], &apos;Condition1&apos;: [&apos;Artery&apos;, &apos;Feedr&apos;, &apos;Norm&apos;, &apos;RRNn&apos;, &apos;RRAn&apos;, &apos;PosN&apos;, &apos;PosA&apos;, &apos;RRNe&apos;, &apos;RRAe&apos;], &apos;Condition2&apos;: [&apos;Artery&apos;, &apos;Feedr&apos;, &apos;Norm&apos;, &apos;RRNn&apos;, &apos;RRAn&apos;, &apos;PosN&apos;, &apos;PosA&apos;, &apos;RRNe&apos;, &apos;RRAe&apos;], &apos;BldgType&apos;: [&apos;1Fam&apos;, &apos;2FmCon&apos;, &apos;Duplx&apos;, &apos;TwnhsE&apos;, &apos;TwnhsI&apos;], &apos;HouseStyle&apos;: [&apos;1Story&apos;], &apos;       1.5Fin\tOne and one-half story&apos;: [], &apos;       1.5Unf\tOne and one-half story&apos;: [&apos;2Story&apos;], &apos;       2.5Fin\tTwo and one-half story&apos;: [], &apos;       2.5Unf\tTwo and one-half story&apos;: [&apos;SFoyer&apos;, &apos;SLvl&apos;], &apos;OverallQual&apos;: [&apos;10&apos;, &apos;9&apos;, &apos;8&apos;, &apos;7&apos;, &apos;6&apos;, &apos;5&apos;, &apos;4&apos;, &apos;3&apos;, &apos;2&apos;, &apos;1&apos;], &apos;OverallCond&apos;: [&apos;10&apos;, &apos;9&apos;, &apos;8&apos;, &apos;7&apos;, &apos;6&apos;, &apos;5&apos;, &apos;4&apos;, &apos;3&apos;, &apos;2&apos;, &apos;1&apos;], &apos;YearBuilt&apos;: [], &apos;YearRemodAdd&apos;: [], &apos;RoofStyle&apos;: [&apos;Flat&apos;, &apos;Gable&apos;, &apos;Gambrel&apos;, &apos;Hip&apos;, &apos;Mansard&apos;, &apos;Shed&apos;], &apos;RoofMatl&apos;: [&apos;ClyTile&apos;, &apos;CompShg&apos;, &apos;Membran&apos;, &apos;Metal&apos;, &apos;Roll&apos;, &apos;Tar&amp;Grv&apos;, &apos;WdShake&apos;, &apos;WdShngl&apos;], &apos;Exterior1st&apos;: [&apos;AsbShng&apos;, &apos;AsphShn&apos;, &apos;BrkComm&apos;, &apos;BrkFace&apos;, &apos;CBlock&apos;, &apos;CemntBd&apos;, &apos;HdBoard&apos;, &apos;ImStucc&apos;, &apos;MetalSd&apos;, &apos;Other&apos;, &apos;Plywood&apos;, &apos;PreCast&apos;, &apos;Stone&apos;, &apos;Stucco&apos;, &apos;VinylSd&apos;, &apos;Wd&apos;, &apos;WdShing&apos;], &apos;Exterior2nd&apos;: [&apos;AsbShng&apos;, &apos;AsphShn&apos;, &apos;BrkComm&apos;, &apos;BrkFace&apos;, &apos;CBlock&apos;, &apos;CemntBd&apos;, &apos;HdBoard&apos;, &apos;ImStucc&apos;, &apos;MetalSd&apos;, &apos;Other&apos;, &apos;Plywood&apos;, &apos;PreCast&apos;, &apos;Stone&apos;, &apos;Stucco&apos;, &apos;VinylSd&apos;, &apos;Wd&apos;, &apos;WdShing&apos;], &apos;MasVnrType&apos;: [&apos;BrkCmn&apos;, &apos;BrkFace&apos;, &apos;CBlock&apos;, &apos;None&apos;, &apos;Stone&apos;], &apos;MasVnrArea&apos;: [], &apos;ExterQual&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;], &apos;ExterCond&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;], &apos;Foundation&apos;: [&apos;BrkTil&apos;, &apos;CBlock&apos;, &apos;PConc&apos;, &apos;Slab&apos;, &apos;Stone&apos;, &apos;Wood&apos;], &apos;BsmtQual&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;, &apos;NA&apos;], &apos;BsmtCond&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;, &apos;NA&apos;], &apos;BsmtExposure&apos;: [&apos;Gd&apos;, &apos;Av&apos;, &apos;Mn&apos;, &apos;No&apos;, &apos;NA&apos;], &apos;BsmtFinType1&apos;: [&apos;GLQ&apos;, &apos;ALQ&apos;, &apos;BLQ&apos;, &apos;Rec&apos;, &apos;LwQ&apos;, &apos;Unf&apos;, &apos;NA&apos;], &apos;BsmtFinSF1&apos;: [], &apos;BsmtFinType2&apos;: [&apos;GLQ&apos;, &apos;ALQ&apos;, &apos;BLQ&apos;, &apos;Rec&apos;, &apos;LwQ&apos;, &apos;Unf&apos;, &apos;NA&apos;], &apos;BsmtFinSF2&apos;: [], &apos;BsmtUnfSF&apos;: [], &apos;TotalBsmtSF&apos;: [], &apos;Heating&apos;: [&apos;Floor&apos;, &apos;GasA&apos;, &apos;GasW&apos;, &apos;Grav&apos;, &apos;OthW&apos;, &apos;Wall&apos;], &apos;HeatingQC&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;], &apos;CentralAir&apos;: [&apos;N&apos;, &apos;Y&apos;], &apos;Electrical&apos;: [&apos;SBrkr&apos;, &apos;FuseA&apos;, &apos;FuseF&apos;, &apos;FuseP&apos;, &apos;Mix&apos;], &apos;1stFlrSF&apos;: [], &apos;2ndFlrSF&apos;: [], &apos;LowQualFinSF&apos;: [], &apos;GrLivArea&apos;: [], &apos;BsmtFullBath&apos;: [], &apos;BsmtHalfBath&apos;: [], &apos;FullBath&apos;: [], &apos;HalfBath&apos;: [], &apos;Bedroom&apos;: [], &apos;Kitchen&apos;: [], &apos;KitchenQual&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;], &apos;TotRmsAbvGrd&apos;: [], &apos;Functional&apos;: [&apos;Typ&apos;, &apos;Min1&apos;, &apos;Min2&apos;, &apos;Mod&apos;, &apos;Maj1&apos;, &apos;Maj2&apos;, &apos;Sev&apos;, &apos;Sal&apos;], &apos;Fireplaces&apos;: [], &apos;FireplaceQu&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;, &apos;NA&apos;], &apos;GarageType&apos;: [&apos;2Types&apos;, &apos;Attchd&apos;, &apos;Basment&apos;, &apos;BuiltIn&apos;, &apos;CarPort&apos;, &apos;Detchd&apos;, &apos;NA&apos;], &apos;GarageYrBlt&apos;: [], &apos;GarageFinish&apos;: [&apos;Fin&apos;, &apos;RFn&apos;, &apos;Unf&apos;, &apos;NA&apos;], &apos;GarageCars&apos;: [], &apos;GarageArea&apos;: [], &apos;GarageQual&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;, &apos;NA&apos;], &apos;GarageCond&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;, &apos;NA&apos;], &apos;PavedDrive&apos;: [&apos;Y&apos;, &apos;P&apos;, &apos;N&apos;], &apos;WoodDeckSF&apos;: [], &apos;OpenPorchSF&apos;: [], &apos;EnclosedPorch&apos;: [], &apos;3SsnPorch&apos;: [], &apos;ScreenPorch&apos;: [], &apos;PoolArea&apos;: [], &apos;PoolQC&apos;: [&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;NA&apos;], &apos;Fence&apos;: [&apos;GdPrv&apos;, &apos;MnPrv&apos;, &apos;GdWo&apos;, &apos;MnWw&apos;, &apos;NA&apos;], &apos;MiscFeature&apos;: [&apos;Elev&apos;, &apos;Gar2&apos;, &apos;Othr&apos;, &apos;Shed&apos;, &apos;TenC&apos;, &apos;NA&apos;], &apos;MiscVal&apos;: [], &apos;MoSold&apos;: [], &apos;YrSold&apos;: [], &apos;SaleType&apos;: [&apos;WD&apos;, &apos;CWD&apos;, &apos;VWD&apos;, &apos;New&apos;, &apos;COD&apos;, &apos;Con&apos;, &apos;ConLw&apos;, &apos;ConLI&apos;, &apos;ConLD&apos;, &apos;Oth&apos;], &apos;SaleCondition&apos;: [&apos;Normal&apos;, &apos;Abnorml&apos;, &apos;AdjLand&apos;, &apos;Alloca&apos;, &apos;Family&apos;, &apos;Partial&apos;]}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">description_dict[<span class="string">'FireplaceQu'</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;Ex&apos;, &apos;Gd&apos;, &apos;TA&apos;, &apos;Fa&apos;, &apos;Po&apos;, &apos;NA&apos;]
</code></pre><h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><p>首先先删除一些确实较多和不太好填充的feature。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 删除</span></span><br><span class="line">train_df = train_df.drop([<span class="string">'Alley'</span>, <span class="string">'PoolQC'</span>, <span class="string">'MiscFeature'</span>, <span class="string">'LotFrontage'</span>], axis=<span class="number">1</span>)</span><br><span class="line">test_df = test_df.drop([<span class="string">'Alley'</span>, <span class="string">'PoolQC'</span>, <span class="string">'MiscFeature'</span>, <span class="string">'LotFrontage'</span>], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="处理-GarageYrBlt-Year-garage-was-built"><a href="#处理-GarageYrBlt-Year-garage-was-built" class="headerlink" title="处理 GarageYrBlt: Year garage was built"></a>处理 GarageYrBlt: Year garage was built</h2><p>车库的年代，没有填充 0，改为 1900 年开始。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_garage_year</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    dataset = dataset.fillna(<span class="number">1900</span>)</span><br><span class="line">    dataset -= <span class="number">1900</span></span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line">    </span><br><span class="line">train_df[<span class="string">'GarageYrBlt'</span>] = preprocess_garage_year(train_df[<span class="string">'GarageYrBlt'</span>])</span><br><span class="line">test_df[<span class="string">'GarageYrBlt'</span>] = preprocess_garage_year(test_df[<span class="string">'GarageYrBlt'</span>])</span><br></pre></td></tr></table></figure>
<h2 id="处理-Electrical"><a href="#处理-Electrical" class="headerlink" title="处理 Electrical"></a>处理 Electrical</h2><p>Electrical: Electrical system</p>
<pre><code>SBrkr    Standard Circuit Breakers &amp; Romex
FuseA    Fuse Box over 60 AMP and all Romex wiring (Average) 
FuseF    60 AMP Fuse Box and mostly Romex wiring (Fair)
FuseP    60 AMP Fuse Box and mostly knob &amp; tube wiring (poor)
Mix  Mixed
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">freq_port = train_df.Electrical.dropna().mode()[<span class="number">0</span>] <span class="comment"># 返回出现次数最多的值（众数）</span></span><br><span class="line">freq_port</span><br></pre></td></tr></table></figure>
<pre><code>&apos;SBrkr&apos;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_garage_year</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    dataset = dataset.fillna(freq_port)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line">    </span><br><span class="line">train_df[<span class="string">'Electrical'</span>] = preprocess_garage_year(train_df[<span class="string">'Electrical'</span>])</span><br><span class="line">test_df[<span class="string">'Electrical'</span>] = preprocess_garage_year(test_df[<span class="string">'Electrical'</span>])</span><br></pre></td></tr></table></figure>
<h2 id="处理-MasVnrArea-Masonry-veneer-area-in-square-feet"><a href="#处理-MasVnrArea-Masonry-veneer-area-in-square-feet" class="headerlink" title="处理 MasVnrArea: Masonry veneer area in square feet"></a>处理 MasVnrArea: Masonry veneer area in square feet</h2><p>砖石饰面面积:砖石饰面面积(平方英尺)</p>
<p>缺失的不是太多（148），mean 103，众数（75%以上）为 0，还没想到太好的填充，先填个0试试吧。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.MasVnrArea.describe()</span><br></pre></td></tr></table></figure>
<pre><code>count   1452.00000
mean     103.68526
std      181.06621
min        0.00000
25%        0.00000
50%        0.00000
75%      166.00000
max     1600.00000
Name: MasVnrArea, dtype: float64
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_masvararea</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    dataset = dataset.fillna(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line">    </span><br><span class="line">train_df[<span class="string">'MasVnrArea'</span>] = preprocess_masvararea(train_df[<span class="string">'MasVnrArea'</span>])</span><br><span class="line">test_df[<span class="string">'MasVnrArea'</span>] = preprocess_masvararea(test_df[<span class="string">'MasVnrArea'</span>])</span><br></pre></td></tr></table></figure>
<h2 id="处理-MasVnrType"><a href="#处理-MasVnrType" class="headerlink" title="处理 MasVnrType"></a>处理 MasVnrType</h2><p>MasVnrType: Masonry veneer type</p>
<pre><code>BrkCmn   Brick Common
BrkFace  Brick Face
CBlock   Cinder Block
None None
Stone    Stone
</code></pre><p>这个值很奇怪，不太明白这是什么，是没有好呢还是 Stone 好呢？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.MasVnrType.dropna().mode()[<span class="number">0</span>] <span class="comment"># 返回出现次数最多的值（众数）</span></span><br></pre></td></tr></table></figure>
<pre><code>&apos;None&apos;
</code></pre><p>大多数都没有，那就把缺失值填为没有吧。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_masvnrtype</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    dataset = dataset.fillna(<span class="string">'None'</span>)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line">    </span><br><span class="line">train_df[<span class="string">'MasVnrType'</span>] = preprocess_masvnrtype(train_df[<span class="string">'MasVnrType'</span>])</span><br><span class="line">test_df[<span class="string">'MasVnrType'</span>] = preprocess_masvnrtype(test_df[<span class="string">'MasVnrType'</span>])</span><br></pre></td></tr></table></figure>
<h2 id="处理其他缺失-feature"><a href="#处理其他缺失-feature" class="headerlink" title="处理其他缺失 feature"></a>处理其他缺失 feature</h2><p>需要填充缺失值和重编码。</p>
<p>根据 data description 把 字符串类型的 feature 重编码。</p>
<p>构造 feature 对应的 map</p>
<p>观察发现以下这些缺失我们可以填充，顺便重编码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">missing_value = [<span class="string">'Fence'</span>,</span><br><span class="line">    <span class="string">'FireplaceQu'</span>,</span><br><span class="line">    <span class="string">'GarageType'</span>,</span><br><span class="line">    <span class="string">'GarageFinish'</span>,</span><br><span class="line">    <span class="string">'GarageQual'</span>,</span><br><span class="line">    <span class="string">'GarageCond'</span>,</span><br><span class="line">    <span class="string">'BsmtQual'</span>,</span><br><span class="line">    <span class="string">'BsmtCond'</span>,</span><br><span class="line">    <span class="string">'BsmtExposure'</span>,</span><br><span class="line">    <span class="string">'BsmtFinType1'</span>,</span><br><span class="line">    <span class="string">'BsmtFinType2'</span>,]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_map</span><span class="params">(map_list, end_index=<span class="number">1</span>)</span>:</span></span><br><span class="line">    d = &#123;&#125;</span><br><span class="line">    j = len(map_list) - end_index</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> map_list:</span><br><span class="line">        d[i] = j</span><br><span class="line">        j -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">missing_map_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> feature <span class="keyword">in</span> missing_value:</span><br><span class="line">    missing_map_dict[feature] = generate_map(description_dict[feature])</span><br><span class="line"></span><br><span class="line">missing_map_dict</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;BsmtCond&apos;: {&apos;Ex&apos;: 5, &apos;Fa&apos;: 2, &apos;Gd&apos;: 4, &apos;NA&apos;: 0, &apos;Po&apos;: 1, &apos;TA&apos;: 3},
 &apos;BsmtExposure&apos;: {&apos;Av&apos;: 3, &apos;Gd&apos;: 4, &apos;Mn&apos;: 2, &apos;NA&apos;: 0, &apos;No&apos;: 1},
 &apos;BsmtFinType1&apos;: {&apos;ALQ&apos;: 5,
  &apos;BLQ&apos;: 4,
  &apos;GLQ&apos;: 6,
  &apos;LwQ&apos;: 2,
  &apos;NA&apos;: 0,
  &apos;Rec&apos;: 3,
  &apos;Unf&apos;: 1},
 &apos;BsmtFinType2&apos;: {&apos;ALQ&apos;: 5,
  &apos;BLQ&apos;: 4,
  &apos;GLQ&apos;: 6,
  &apos;LwQ&apos;: 2,
  &apos;NA&apos;: 0,
  &apos;Rec&apos;: 3,
  &apos;Unf&apos;: 1},
 &apos;BsmtQual&apos;: {&apos;Ex&apos;: 5, &apos;Fa&apos;: 2, &apos;Gd&apos;: 4, &apos;NA&apos;: 0, &apos;Po&apos;: 1, &apos;TA&apos;: 3},
 &apos;Fence&apos;: {&apos;GdPrv&apos;: 4, &apos;GdWo&apos;: 2, &apos;MnPrv&apos;: 3, &apos;MnWw&apos;: 1, &apos;NA&apos;: 0},
 &apos;FireplaceQu&apos;: {&apos;Ex&apos;: 5, &apos;Fa&apos;: 2, &apos;Gd&apos;: 4, &apos;NA&apos;: 0, &apos;Po&apos;: 1, &apos;TA&apos;: 3},
 &apos;GarageCond&apos;: {&apos;Ex&apos;: 5, &apos;Fa&apos;: 2, &apos;Gd&apos;: 4, &apos;NA&apos;: 0, &apos;Po&apos;: 1, &apos;TA&apos;: 3},
 &apos;GarageFinish&apos;: {&apos;Fin&apos;: 3, &apos;NA&apos;: 0, &apos;RFn&apos;: 2, &apos;Unf&apos;: 1},
 &apos;GarageQual&apos;: {&apos;Ex&apos;: 5, &apos;Fa&apos;: 2, &apos;Gd&apos;: 4, &apos;NA&apos;: 0, &apos;Po&apos;: 1, &apos;TA&apos;: 3},
 &apos;GarageType&apos;: {&apos;2Types&apos;: 6,
  &apos;Attchd&apos;: 5,
  &apos;Basment&apos;: 4,
  &apos;BuiltIn&apos;: 3,
  &apos;CarPort&apos;: 2,
  &apos;Detchd&apos;: 1,
  &apos;NA&apos;: 0}}
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 预处理 feature 把 str 转换为序列 </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_feature_strtoint</span><span class="params">(feature_df, feature_mapping, default=<span class="number">0</span>)</span>:</span></span><br><span class="line">    feature_df = feature_df.map(feature_mapping)</span><br><span class="line">    feature_df = feature_df.fillna(default)</span><br><span class="line">    <span class="keyword">return</span> feature_df</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_feature</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> missing_value:</span><br><span class="line">        dataset[feature] = preprocess_feature_strtoint(dataset[feature], missing_map_dict[feature])</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df = preprocess_feature(train_df)</span><br><span class="line">test_df = preprocess_feature(test_df)</span><br></pre></td></tr></table></figure>
<p>检查训练集缺失值，已经没有了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df[train_df.isnull().values==<span class="keyword">True</span>]</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Id</th>
      <th>MSSubClass</th>
      <th>MSZoning</th>
      <th>LotArea</th>
      <th>Street</th>
      <th>LotShape</th>
      <th>LandContour</th>
      <th>Utilities</th>
      <th>LotConfig</th>
      <th>LandSlope</th>
      <th>Neighborhood</th>
      <th>Condition1</th>
      <th>Condition2</th>
      <th>BldgType</th>
      <th>HouseStyle</th>
      <th>OverallQual</th>
      <th>OverallCond</th>
      <th>YearBuilt</th>
      <th>YearRemodAdd</th>
      <th>RoofStyle</th>
      <th>RoofMatl</th>
      <th>Exterior1st</th>
      <th>Exterior2nd</th>
      <th>MasVnrType</th>
      <th>MasVnrArea</th>
      <th>ExterQual</th>
      <th>ExterCond</th>
      <th>Foundation</th>
      <th>BsmtQual</th>
      <th>BsmtCond</th>
      <th>BsmtExposure</th>
      <th>BsmtFinType1</th>
      <th>BsmtFinSF1</th>
      <th>BsmtFinType2</th>
      <th>BsmtFinSF2</th>
      <th>BsmtUnfSF</th>
      <th>TotalBsmtSF</th>
      <th>Heating</th>
      <th>HeatingQC</th>
      <th>CentralAir</th>
      <th>Electrical</th>
      <th>1stFlrSF</th>
      <th>2ndFlrSF</th>
      <th>LowQualFinSF</th>
      <th>GrLivArea</th>
      <th>BsmtFullBath</th>
      <th>BsmtHalfBath</th>
      <th>FullBath</th>
      <th>HalfBath</th>
      <th>BedroomAbvGr</th>
      <th>KitchenAbvGr</th>
      <th>KitchenQual</th>
      <th>TotRmsAbvGrd</th>
      <th>Functional</th>
      <th>Fireplaces</th>
      <th>FireplaceQu</th>
      <th>GarageType</th>
      <th>GarageYrBlt</th>
      <th>GarageFinish</th>
      <th>GarageCars</th>
      <th>GarageArea</th>
      <th>GarageQual</th>
      <th>GarageCond</th>
      <th>PavedDrive</th>
      <th>WoodDeckSF</th>
      <th>OpenPorchSF</th>
      <th>EnclosedPorch</th>
      <th>3SsnPorch</th>
      <th>ScreenPorch</th>
      <th>PoolArea</th>
      <th>Fence</th>
      <th>MiscVal</th>
      <th>MoSold</th>
      <th>YrSold</th>
      <th>SaleType</th>
      <th>SaleCondition</th>
      <th>SalePrice</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table>
</div>

<h2 id="填充测试集"><a href="#填充测试集" class="headerlink" title="填充测试集"></a>填充测试集</h2><p>测试集还有许多缺失，先决定用众数填充。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_df[test_df.isnull().values==<span class="keyword">True</span>]</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Id</th>
      <th>MSSubClass</th>
      <th>MSZoning</th>
      <th>LotArea</th>
      <th>Street</th>
      <th>LotShape</th>
      <th>LandContour</th>
      <th>Utilities</th>
      <th>LotConfig</th>
      <th>LandSlope</th>
      <th>Neighborhood</th>
      <th>Condition1</th>
      <th>Condition2</th>
      <th>BldgType</th>
      <th>HouseStyle</th>
      <th>OverallQual</th>
      <th>OverallCond</th>
      <th>YearBuilt</th>
      <th>YearRemodAdd</th>
      <th>RoofStyle</th>
      <th>RoofMatl</th>
      <th>Exterior1st</th>
      <th>Exterior2nd</th>
      <th>MasVnrType</th>
      <th>MasVnrArea</th>
      <th>ExterQual</th>
      <th>ExterCond</th>
      <th>Foundation</th>
      <th>BsmtQual</th>
      <th>BsmtCond</th>
      <th>BsmtExposure</th>
      <th>BsmtFinType1</th>
      <th>BsmtFinSF1</th>
      <th>BsmtFinType2</th>
      <th>BsmtFinSF2</th>
      <th>BsmtUnfSF</th>
      <th>TotalBsmtSF</th>
      <th>Heating</th>
      <th>HeatingQC</th>
      <th>CentralAir</th>
      <th>Electrical</th>
      <th>1stFlrSF</th>
      <th>2ndFlrSF</th>
      <th>LowQualFinSF</th>
      <th>GrLivArea</th>
      <th>BsmtFullBath</th>
      <th>BsmtHalfBath</th>
      <th>FullBath</th>
      <th>HalfBath</th>
      <th>BedroomAbvGr</th>
      <th>KitchenAbvGr</th>
      <th>KitchenQual</th>
      <th>TotRmsAbvGrd</th>
      <th>Functional</th>
      <th>Fireplaces</th>
      <th>FireplaceQu</th>
      <th>GarageType</th>
      <th>GarageYrBlt</th>
      <th>GarageFinish</th>
      <th>GarageCars</th>
      <th>GarageArea</th>
      <th>GarageQual</th>
      <th>GarageCond</th>
      <th>PavedDrive</th>
      <th>WoodDeckSF</th>
      <th>OpenPorchSF</th>
      <th>EnclosedPorch</th>
      <th>3SsnPorch</th>
      <th>ScreenPorch</th>
      <th>PoolArea</th>
      <th>Fence</th>
      <th>MiscVal</th>
      <th>MoSold</th>
      <th>YrSold</th>
      <th>SaleType</th>
      <th>SaleCondition</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>95</th>
      <td>1556</td>
      <td>50</td>
      <td>RL</td>
      <td>10632</td>
      <td>Pave</td>
      <td>IR1</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>Inside</td>
      <td>Gtl</td>
      <td>ClearCr</td>
      <td>Norm</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>1.5Fin</td>
      <td>5</td>
      <td>3</td>
      <td>1917</td>
      <td>1950</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>Wd Sdng</td>
      <td>Wd Sdng</td>
      <td>None</td>
      <td>0.00000</td>
      <td>TA</td>
      <td>TA</td>
      <td>BrkTil</td>
      <td>4.00000</td>
      <td>2.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>689.00000</td>
      <td>689.00000</td>
      <td>GasA</td>
      <td>Gd</td>
      <td>N</td>
      <td>SBrkr</td>
      <td>725</td>
      <td>499</td>
      <td>0</td>
      <td>1224</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>NaN</td>
      <td>6</td>
      <td>Mod</td>
      <td>0</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>17.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>180.00000</td>
      <td>2.00000</td>
      <td>2.00000</td>
      <td>N</td>
      <td>0</td>
      <td>0</td>
      <td>248</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.00000</td>
      <td>0</td>
      <td>1</td>
      <td>2010</td>
      <td>COD</td>
      <td>Normal</td>
    </tr>
    <tr>
      <th>455</th>
      <td>1916</td>
      <td>30</td>
      <td>NaN</td>
      <td>21780</td>
      <td>Grvl</td>
      <td>Reg</td>
      <td>Lvl</td>
      <td>NaN</td>
      <td>Inside</td>
      <td>Gtl</td>
      <td>IDOTRR</td>
      <td>Norm</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>1Story</td>
      <td>2</td>
      <td>4</td>
      <td>1910</td>
      <td>1950</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>Wd Sdng</td>
      <td>Wd Sdng</td>
      <td>None</td>
      <td>0.00000</td>
      <td>Fa</td>
      <td>Fa</td>
      <td>CBlock</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>GasA</td>
      <td>TA</td>
      <td>N</td>
      <td>FuseA</td>
      <td>810</td>
      <td>0</td>
      <td>0</td>
      <td>810</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>TA</td>
      <td>4</td>
      <td>Min1</td>
      <td>0</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>75.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>280.00000</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>N</td>
      <td>119</td>
      <td>24</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.00000</td>
      <td>0</td>
      <td>3</td>
      <td>2009</td>
      <td>ConLD</td>
      <td>Normal</td>
    </tr>
    <tr>
      <th>455</th>
      <td>1916</td>
      <td>30</td>
      <td>NaN</td>
      <td>21780</td>
      <td>Grvl</td>
      <td>Reg</td>
      <td>Lvl</td>
      <td>NaN</td>
      <td>Inside</td>
      <td>Gtl</td>
      <td>IDOTRR</td>
      <td>Norm</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>1Story</td>
      <td>2</td>
      <td>4</td>
      <td>1910</td>
      <td>1950</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>Wd Sdng</td>
      <td>Wd Sdng</td>
      <td>None</td>
      <td>0.00000</td>
      <td>Fa</td>
      <td>Fa</td>
      <td>CBlock</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>GasA</td>
      <td>TA</td>
      <td>N</td>
      <td>FuseA</td>
      <td>810</td>
      <td>0</td>
      <td>0</td>
      <td>810</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>TA</td>
      <td>4</td>
      <td>Min1</td>
      <td>0</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>75.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>280.00000</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>N</td>
      <td>119</td>
      <td>24</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.00000</td>
      <td>0</td>
      <td>3</td>
      <td>2009</td>
      <td>ConLD</td>
      <td>Normal</td>
    </tr>
    <tr>
      <th>485</th>
      <td>1946</td>
      <td>20</td>
      <td>RL</td>
      <td>31220</td>
      <td>Pave</td>
      <td>IR1</td>
      <td>Bnk</td>
      <td>NaN</td>
      <td>FR2</td>
      <td>Gtl</td>
      <td>Gilbert</td>
      <td>Feedr</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>1Story</td>
      <td>6</td>
      <td>2</td>
      <td>1952</td>
      <td>1952</td>
      <td>Hip</td>
      <td>CompShg</td>
      <td>BrkFace</td>
      <td>BrkFace</td>
      <td>None</td>
      <td>0.00000</td>
      <td>TA</td>
      <td>TA</td>
      <td>CBlock</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>1632.00000</td>
      <td>1632.00000</td>
      <td>GasA</td>
      <td>TA</td>
      <td>Y</td>
      <td>FuseA</td>
      <td>1474</td>
      <td>0</td>
      <td>0</td>
      <td>1474</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>TA</td>
      <td>7</td>
      <td>Min2</td>
      <td>2</td>
      <td>4.00000</td>
      <td>5.00000</td>
      <td>52.00000</td>
      <td>1.00000</td>
      <td>2.00000</td>
      <td>495.00000</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>Y</td>
      <td>0</td>
      <td>0</td>
      <td>144</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.00000</td>
      <td>750</td>
      <td>5</td>
      <td>2008</td>
      <td>WD</td>
      <td>Normal</td>
    </tr>
    <tr>
      <th>660</th>
      <td>2121</td>
      <td>20</td>
      <td>RM</td>
      <td>5940</td>
      <td>Pave</td>
      <td>IR1</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>FR3</td>
      <td>Gtl</td>
      <td>BrkSide</td>
      <td>Feedr</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>1Story</td>
      <td>4</td>
      <td>7</td>
      <td>1946</td>
      <td>1950</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>MetalSd</td>
      <td>CBlock</td>
      <td>None</td>
      <td>0.00000</td>
      <td>TA</td>
      <td>TA</td>
      <td>PConc</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>nan</td>
      <td>0.00000</td>
      <td>nan</td>
      <td>nan</td>
      <td>nan</td>
      <td>GasA</td>
      <td>TA</td>
      <td>Y</td>
      <td>FuseA</td>
      <td>896</td>
      <td>0</td>
      <td>0</td>
      <td>896</td>
      <td>nan</td>
      <td>nan</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>TA</td>
      <td>4</td>
      <td>Typ</td>
      <td>0</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>46.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>280.00000</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>Y</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3.00000</td>
      <td>0</td>
      <td>4</td>
      <td>2008</td>
      <td>ConLD</td>
      <td>Abnorml</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1013</th>
      <td>2474</td>
      <td>50</td>
      <td>RM</td>
      <td>10320</td>
      <td>Pave</td>
      <td>Reg</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>Corner</td>
      <td>Gtl</td>
      <td>IDOTRR</td>
      <td>Artery</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>1.5Fin</td>
      <td>4</td>
      <td>1</td>
      <td>1910</td>
      <td>1950</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>Wd Sdng</td>
      <td>Wd Sdng</td>
      <td>None</td>
      <td>0.00000</td>
      <td>Fa</td>
      <td>Fa</td>
      <td>CBlock</td>
      <td>3.00000</td>
      <td>2.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>771.00000</td>
      <td>771.00000</td>
      <td>GasA</td>
      <td>Fa</td>
      <td>Y</td>
      <td>SBrkr</td>
      <td>866</td>
      <td>504</td>
      <td>114</td>
      <td>1484</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>TA</td>
      <td>6</td>
      <td>NaN</td>
      <td>0</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>10.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>264.00000</td>
      <td>3.00000</td>
      <td>2.00000</td>
      <td>N</td>
      <td>14</td>
      <td>211</td>
      <td>0</td>
      <td>0</td>
      <td>84</td>
      <td>0</td>
      <td>0.00000</td>
      <td>0</td>
      <td>9</td>
      <td>2007</td>
      <td>COD</td>
      <td>Abnorml</td>
    </tr>
    <tr>
      <th>1029</th>
      <td>2490</td>
      <td>20</td>
      <td>RL</td>
      <td>13770</td>
      <td>Pave</td>
      <td>Reg</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>Corner</td>
      <td>Gtl</td>
      <td>Sawyer</td>
      <td>Feedr</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>1Story</td>
      <td>5</td>
      <td>6</td>
      <td>1958</td>
      <td>1998</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>Plywood</td>
      <td>Plywood</td>
      <td>BrkFace</td>
      <td>340.00000</td>
      <td>TA</td>
      <td>TA</td>
      <td>CBlock</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>2.00000</td>
      <td>3.00000</td>
      <td>190.00000</td>
      <td>4.00000</td>
      <td>873.00000</td>
      <td>95.00000</td>
      <td>1158.00000</td>
      <td>GasA</td>
      <td>TA</td>
      <td>Y</td>
      <td>SBrkr</td>
      <td>1176</td>
      <td>0</td>
      <td>0</td>
      <td>1176</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>TA</td>
      <td>6</td>
      <td>Typ</td>
      <td>2</td>
      <td>4.00000</td>
      <td>5.00000</td>
      <td>58.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>303.00000</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>Y</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.00000</td>
      <td>0</td>
      <td>10</td>
      <td>2007</td>
      <td>NaN</td>
      <td>Normal</td>
    </tr>
    <tr>
      <th>1116</th>
      <td>2577</td>
      <td>70</td>
      <td>RM</td>
      <td>9060</td>
      <td>Pave</td>
      <td>Reg</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>Inside</td>
      <td>Gtl</td>
      <td>IDOTRR</td>
      <td>Norm</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>2Story</td>
      <td>5</td>
      <td>6</td>
      <td>1923</td>
      <td>1999</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>Wd Sdng</td>
      <td>Plywood</td>
      <td>None</td>
      <td>0.00000</td>
      <td>TA</td>
      <td>TA</td>
      <td>BrkTil</td>
      <td>4.00000</td>
      <td>3.00000</td>
      <td>1.00000</td>
      <td>5.00000</td>
      <td>548.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>311.00000</td>
      <td>859.00000</td>
      <td>GasA</td>
      <td>Ex</td>
      <td>Y</td>
      <td>SBrkr</td>
      <td>942</td>
      <td>886</td>
      <td>0</td>
      <td>1828</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>Gd</td>
      <td>6</td>
      <td>Typ</td>
      <td>0</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>nan</td>
      <td>nan</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>Y</td>
      <td>174</td>
      <td>0</td>
      <td>212</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3.00000</td>
      <td>0</td>
      <td>3</td>
      <td>2007</td>
      <td>WD</td>
      <td>Alloca</td>
    </tr>
    <tr>
      <th>1116</th>
      <td>2577</td>
      <td>70</td>
      <td>RM</td>
      <td>9060</td>
      <td>Pave</td>
      <td>Reg</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>Inside</td>
      <td>Gtl</td>
      <td>IDOTRR</td>
      <td>Norm</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>2Story</td>
      <td>5</td>
      <td>6</td>
      <td>1923</td>
      <td>1999</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>Wd Sdng</td>
      <td>Plywood</td>
      <td>None</td>
      <td>0.00000</td>
      <td>TA</td>
      <td>TA</td>
      <td>BrkTil</td>
      <td>4.00000</td>
      <td>3.00000</td>
      <td>1.00000</td>
      <td>5.00000</td>
      <td>548.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>311.00000</td>
      <td>859.00000</td>
      <td>GasA</td>
      <td>Ex</td>
      <td>Y</td>
      <td>SBrkr</td>
      <td>942</td>
      <td>886</td>
      <td>0</td>
      <td>1828</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>Gd</td>
      <td>6</td>
      <td>Typ</td>
      <td>0</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>nan</td>
      <td>nan</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>Y</td>
      <td>174</td>
      <td>0</td>
      <td>212</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3.00000</td>
      <td>0</td>
      <td>3</td>
      <td>2007</td>
      <td>WD</td>
      <td>Alloca</td>
    </tr>
    <tr>
      <th>1444</th>
      <td>2905</td>
      <td>20</td>
      <td>NaN</td>
      <td>31250</td>
      <td>Pave</td>
      <td>Reg</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>Inside</td>
      <td>Gtl</td>
      <td>Mitchel</td>
      <td>Artery</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>1Story</td>
      <td>1</td>
      <td>3</td>
      <td>1951</td>
      <td>1951</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>CBlock</td>
      <td>VinylSd</td>
      <td>None</td>
      <td>0.00000</td>
      <td>TA</td>
      <td>Fa</td>
      <td>CBlock</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>GasA</td>
      <td>TA</td>
      <td>Y</td>
      <td>FuseA</td>
      <td>1600</td>
      <td>0</td>
      <td>0</td>
      <td>1600</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>TA</td>
      <td>6</td>
      <td>Mod</td>
      <td>0</td>
      <td>0.00000</td>
      <td>5.00000</td>
      <td>51.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>270.00000</td>
      <td>2.00000</td>
      <td>3.00000</td>
      <td>N</td>
      <td>0</td>
      <td>0</td>
      <td>135</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.00000</td>
      <td>0</td>
      <td>5</td>
      <td>2006</td>
      <td>WD</td>
      <td>Normal</td>
    </tr>
  </tbody>
</table>
<p>22 rows × 76 columns</p>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> test_df.columns.values.tolist():</span><br><span class="line">    freq_port = test_df[i].dropna().mode()[<span class="number">0</span>] <span class="comment"># 返回出现次数最多的值（众数)</span></span><br><span class="line">    test_df[i] = test_df[i].fillna(freq_port)</span><br></pre></td></tr></table></figure>
<h2 id="字符串类型-feature-重编码"><a href="#字符串类型-feature-重编码" class="headerlink" title="字符串类型 feature 重编码"></a>字符串类型 feature 重编码</h2><p>处理完缺失值后观察下还有那些feature是字符串形式的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.describe(include=<span class="string">"O"</span>)</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MSZoning</th>
      <th>Street</th>
      <th>LotShape</th>
      <th>LandContour</th>
      <th>Utilities</th>
      <th>LotConfig</th>
      <th>LandSlope</th>
      <th>Neighborhood</th>
      <th>Condition1</th>
      <th>Condition2</th>
      <th>BldgType</th>
      <th>HouseStyle</th>
      <th>RoofStyle</th>
      <th>RoofMatl</th>
      <th>Exterior1st</th>
      <th>Exterior2nd</th>
      <th>MasVnrType</th>
      <th>ExterQual</th>
      <th>ExterCond</th>
      <th>Foundation</th>
      <th>Heating</th>
      <th>HeatingQC</th>
      <th>CentralAir</th>
      <th>Electrical</th>
      <th>KitchenQual</th>
      <th>Functional</th>
      <th>PavedDrive</th>
      <th>SaleType</th>
      <th>SaleCondition</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
      <td>1460</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>5</td>
      <td>2</td>
      <td>4</td>
      <td>4</td>
      <td>2</td>
      <td>5</td>
      <td>3</td>
      <td>25</td>
      <td>9</td>
      <td>8</td>
      <td>5</td>
      <td>8</td>
      <td>6</td>
      <td>8</td>
      <td>15</td>
      <td>16</td>
      <td>4</td>
      <td>4</td>
      <td>5</td>
      <td>6</td>
      <td>6</td>
      <td>5</td>
      <td>2</td>
      <td>5</td>
      <td>4</td>
      <td>7</td>
      <td>3</td>
      <td>9</td>
      <td>6</td>
    </tr>
    <tr>
      <th>top</th>
      <td>RL</td>
      <td>Pave</td>
      <td>Reg</td>
      <td>Lvl</td>
      <td>AllPub</td>
      <td>Inside</td>
      <td>Gtl</td>
      <td>NAmes</td>
      <td>Norm</td>
      <td>Norm</td>
      <td>1Fam</td>
      <td>1Story</td>
      <td>Gable</td>
      <td>CompShg</td>
      <td>VinylSd</td>
      <td>VinylSd</td>
      <td>None</td>
      <td>TA</td>
      <td>TA</td>
      <td>PConc</td>
      <td>GasA</td>
      <td>Ex</td>
      <td>Y</td>
      <td>SBrkr</td>
      <td>TA</td>
      <td>Typ</td>
      <td>Y</td>
      <td>WD</td>
      <td>Normal</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>1151</td>
      <td>1454</td>
      <td>925</td>
      <td>1311</td>
      <td>1459</td>
      <td>1052</td>
      <td>1382</td>
      <td>225</td>
      <td>1260</td>
      <td>1445</td>
      <td>1220</td>
      <td>726</td>
      <td>1141</td>
      <td>1434</td>
      <td>515</td>
      <td>504</td>
      <td>872</td>
      <td>906</td>
      <td>1282</td>
      <td>647</td>
      <td>1428</td>
      <td>741</td>
      <td>1365</td>
      <td>1335</td>
      <td>735</td>
      <td>1360</td>
      <td>1340</td>
      <td>1267</td>
      <td>1198</td>
    </tr>
  </tbody>
</table>
</div>

<p>目前还有以下的 feature 需要编码</p>
<p>[‘MSZoning’, ‘Street’, ‘LotShape’, ‘LandContour’, ‘Utilities’, ‘LotConfig’, ‘LandSlope’, ‘Neighborhood’, ‘Condition1’, ‘Condition2’, ‘BldgType’, ‘HouseStyle’, ‘RoofStyle’, ‘RoofMatl’, ‘Exterior1st’, ‘Exterior2nd’, ‘MasVnrType’, ‘ExterQual’, ‘ExterCond’, ‘Foundation’, ‘Heating’, ‘HeatingQC’, ‘CentralAir’, ‘Electrical’, ‘KitchenQual’, ‘Functional’, ‘PavedDrive’, ‘SaleType’, ‘SaleCondition’]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature = [<span class="string">'MSZoning'</span>, <span class="string">'Street'</span>, <span class="string">'LotShape'</span>, <span class="string">'LandContour'</span>, <span class="string">'Utilities'</span>, <span class="string">'LotConfig'</span>, <span class="string">'LandSlope'</span>, <span class="string">'Neighborhood'</span>, <span class="string">'Condition1'</span>, <span class="string">'Condition2'</span>, <span class="string">'BldgType'</span>, <span class="string">'HouseStyle'</span>, <span class="string">'RoofStyle'</span>, <span class="string">'RoofMatl'</span>, <span class="string">'Exterior1st'</span>, <span class="string">'Exterior2nd'</span>, <span class="string">'MasVnrType'</span>, <span class="string">'ExterQual'</span>, <span class="string">'ExterCond'</span>, <span class="string">'Foundation'</span>, <span class="string">'Heating'</span>, <span class="string">'HeatingQC'</span>, <span class="string">'CentralAir'</span>, <span class="string">'Electrical'</span>, <span class="string">'KitchenQual'</span>, <span class="string">'Functional'</span>, <span class="string">'PavedDrive'</span>, <span class="string">'SaleType'</span>, <span class="string">'SaleCondition'</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> feature:</span><br><span class="line">    print(set(train_df[i]))</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;RL&apos;, &apos;FV&apos;, &apos;C (all)&apos;, &apos;RM&apos;, &apos;RH&apos;}
{&apos;Grvl&apos;, &apos;Pave&apos;}
{&apos;IR2&apos;, &apos;Reg&apos;, &apos;IR3&apos;, &apos;IR1&apos;}
{&apos;Lvl&apos;, &apos;Low&apos;, &apos;HLS&apos;, &apos;Bnk&apos;}
{&apos;AllPub&apos;, &apos;NoSeWa&apos;}
{&apos;CulDSac&apos;, &apos;FR2&apos;, &apos;Corner&apos;, &apos;FR3&apos;, &apos;Inside&apos;}
{&apos;Gtl&apos;, &apos;Mod&apos;, &apos;Sev&apos;}
{&apos;BrDale&apos;, &apos;NoRidge&apos;, &apos;Blmngtn&apos;, &apos;Sawyer&apos;, &apos;NPkVill&apos;, &apos;NridgHt&apos;, &apos;SawyerW&apos;, &apos;Mitchel&apos;, &apos;OldTown&apos;, &apos;NWAmes&apos;, &apos;NAmes&apos;, &apos;Somerst&apos;, &apos;Veenker&apos;, &apos;SWISU&apos;, &apos;CollgCr&apos;, &apos;BrkSide&apos;, &apos;ClearCr&apos;, &apos;IDOTRR&apos;, &apos;Crawfor&apos;, &apos;StoneBr&apos;, &apos;Timber&apos;, &apos;Gilbert&apos;, &apos;Blueste&apos;, &apos;MeadowV&apos;, &apos;Edwards&apos;}
{&apos;Artery&apos;, &apos;PosN&apos;, &apos;PosA&apos;, &apos;Norm&apos;, &apos;RRNn&apos;, &apos;RRAe&apos;, &apos;RRNe&apos;, &apos;Feedr&apos;, &apos;RRAn&apos;}
{&apos;Artery&apos;, &apos;PosN&apos;, &apos;PosA&apos;, &apos;Norm&apos;, &apos;RRNn&apos;, &apos;RRAe&apos;, &apos;Feedr&apos;, &apos;RRAn&apos;}
{&apos;2fmCon&apos;, &apos;Duplex&apos;, &apos;Twnhs&apos;, &apos;TwnhsE&apos;, &apos;1Fam&apos;}
{&apos;SFoyer&apos;, &apos;1.5Fin&apos;, &apos;2Story&apos;, &apos;1.5Unf&apos;, &apos;2.5Fin&apos;, &apos;2.5Unf&apos;, &apos;1Story&apos;, &apos;SLvl&apos;}
{&apos;Mansard&apos;, &apos;Shed&apos;, &apos;Gable&apos;, &apos;Flat&apos;, &apos;Gambrel&apos;, &apos;Hip&apos;}
{&apos;Roll&apos;, &apos;Metal&apos;, &apos;ClyTile&apos;, &apos;WdShngl&apos;, &apos;CompShg&apos;, &apos;Tar&amp;Grv&apos;, &apos;Membran&apos;, &apos;WdShake&apos;}
{&apos;BrkFace&apos;, &apos;Plywood&apos;, &apos;MetalSd&apos;, &apos;Stucco&apos;, &apos;WdShing&apos;, &apos;CBlock&apos;, &apos;AsphShn&apos;, &apos;Stone&apos;, &apos;ImStucc&apos;, &apos;CemntBd&apos;, &apos;Wd Sdng&apos;, &apos;VinylSd&apos;, &apos;BrkComm&apos;, &apos;AsbShng&apos;, &apos;HdBoard&apos;}
{&apos;BrkFace&apos;, &apos;Plywood&apos;, &apos;Wd Shng&apos;, &apos;MetalSd&apos;, &apos;CmentBd&apos;, &apos;Stucco&apos;, &apos;Other&apos;, &apos;CBlock&apos;, &apos;AsphShn&apos;, &apos;ImStucc&apos;, &apos;Stone&apos;, &apos;Wd Sdng&apos;, &apos;VinylSd&apos;, &apos;AsbShng&apos;, &apos;Brk Cmn&apos;, &apos;HdBoard&apos;}
{&apos;BrkFace&apos;, &apos;None&apos;, &apos;BrkCmn&apos;, &apos;Stone&apos;}
{&apos;Ex&apos;, &apos;Gd&apos;, &apos;Fa&apos;, &apos;TA&apos;}
{&apos;Fa&apos;, &apos;Gd&apos;, &apos;Po&apos;, &apos;TA&apos;, &apos;Ex&apos;}
{&apos;BrkTil&apos;, &apos;PConc&apos;, &apos;CBlock&apos;, &apos;Stone&apos;, &apos;Slab&apos;, &apos;Wood&apos;}
{&apos;GasA&apos;, &apos;GasW&apos;, &apos;Wall&apos;, &apos;Floor&apos;, &apos;Grav&apos;, &apos;OthW&apos;}
{&apos;Fa&apos;, &apos;Gd&apos;, &apos;Po&apos;, &apos;TA&apos;, &apos;Ex&apos;}
{&apos;Y&apos;, &apos;N&apos;}
{&apos;SBrkr&apos;, &apos;Mix&apos;, &apos;FuseA&apos;, &apos;FuseP&apos;, &apos;FuseF&apos;}
{&apos;Ex&apos;, &apos;Gd&apos;, &apos;Fa&apos;, &apos;TA&apos;}
{&apos;Mod&apos;, &apos;Min1&apos;, &apos;Maj1&apos;, &apos;Min2&apos;, &apos;Maj2&apos;, &apos;Typ&apos;, &apos;Sev&apos;}
{&apos;Y&apos;, &apos;N&apos;, &apos;P&apos;}
{&apos;Con&apos;, &apos;ConLD&apos;, &apos;ConLw&apos;, &apos;CWD&apos;, &apos;WD&apos;, &apos;New&apos;, &apos;ConLI&apos;, &apos;Oth&apos;, &apos;COD&apos;}
{&apos;Abnorml&apos;, &apos;Alloca&apos;, &apos;Normal&apos;, &apos;AdjLand&apos;, &apos;Partial&apos;, &apos;Family&apos;}
</code></pre><p>观察到这些 feature 即有优劣等级划分的，也有没有等级的，既然如此，把有等级区分的编码为序列，没有等级的使用独热码。</p>
<p>有等级的：</p>
<pre><code>MSZoning
ExterQual
ExterCond
HeatingQC
KitchenQual
</code></pre><p>其余的使用onehot编码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">order_feature_set =&#123;<span class="string">'MSZoning'</span>,</span><br><span class="line"><span class="string">'ExterQual'</span>,</span><br><span class="line"><span class="string">'ExterCond'</span>,</span><br><span class="line"><span class="string">'HeatingQC'</span>,</span><br><span class="line"><span class="string">'KitchenQual'</span>,&#125;</span><br><span class="line"></span><br><span class="line">onehot_feature_set = set(feature) - order_feature_set</span><br></pre></td></tr></table></figure>
<h3 id="Order-编码"><a href="#Order-编码" class="headerlink" title="Order 编码"></a>Order 编码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_order_feature</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> order_feature_set:</span><br><span class="line">        order_map = generate_map(description_dict[i], <span class="number">0</span>)</span><br><span class="line">        dataset[i] = dataset[i].map(order_map)</span><br><span class="line">        dataset[i] = dataset[i].fillna(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df = preprocess_order_feature(train_df)</span><br><span class="line">test_df = preprocess_order_feature(test_df)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Id</th>
      <th>MSSubClass</th>
      <th>MSZoning</th>
      <th>LotArea</th>
      <th>OverallQual</th>
      <th>OverallCond</th>
      <th>YearBuilt</th>
      <th>YearRemodAdd</th>
      <th>MasVnrArea</th>
      <th>ExterQual</th>
      <th>ExterCond</th>
      <th>BsmtQual</th>
      <th>BsmtCond</th>
      <th>BsmtExposure</th>
      <th>BsmtFinType1</th>
      <th>BsmtFinSF1</th>
      <th>BsmtFinType2</th>
      <th>BsmtFinSF2</th>
      <th>BsmtUnfSF</th>
      <th>TotalBsmtSF</th>
      <th>HeatingQC</th>
      <th>1stFlrSF</th>
      <th>2ndFlrSF</th>
      <th>LowQualFinSF</th>
      <th>GrLivArea</th>
      <th>BsmtFullBath</th>
      <th>BsmtHalfBath</th>
      <th>FullBath</th>
      <th>HalfBath</th>
      <th>BedroomAbvGr</th>
      <th>KitchenAbvGr</th>
      <th>KitchenQual</th>
      <th>TotRmsAbvGrd</th>
      <th>Fireplaces</th>
      <th>FireplaceQu</th>
      <th>GarageType</th>
      <th>GarageYrBlt</th>
      <th>GarageFinish</th>
      <th>GarageCars</th>
      <th>GarageArea</th>
      <th>GarageQual</th>
      <th>GarageCond</th>
      <th>WoodDeckSF</th>
      <th>OpenPorchSF</th>
      <th>EnclosedPorch</th>
      <th>3SsnPorch</th>
      <th>ScreenPorch</th>
      <th>PoolArea</th>
      <th>Fence</th>
      <th>MiscVal</th>
      <th>MoSold</th>
      <th>YrSold</th>
      <th>SalePrice</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>730.50000</td>
      <td>56.89726</td>
      <td>2.82534</td>
      <td>10516.82808</td>
      <td>6.09932</td>
      <td>5.57534</td>
      <td>1971.26781</td>
      <td>1984.86575</td>
      <td>103.11712</td>
      <td>3.39589</td>
      <td>3.08356</td>
      <td>3.48904</td>
      <td>2.93493</td>
      <td>1.63014</td>
      <td>3.54589</td>
      <td>443.63973</td>
      <td>1.24726</td>
      <td>46.54932</td>
      <td>567.24041</td>
      <td>1057.42945</td>
      <td>4.14521</td>
      <td>1162.62671</td>
      <td>346.99247</td>
      <td>5.84452</td>
      <td>1515.46370</td>
      <td>0.42534</td>
      <td>0.05753</td>
      <td>1.56507</td>
      <td>0.38288</td>
      <td>2.86644</td>
      <td>1.04658</td>
      <td>3.51164</td>
      <td>6.51781</td>
      <td>0.61301</td>
      <td>1.82534</td>
      <td>3.51438</td>
      <td>74.15068</td>
      <td>1.71575</td>
      <td>1.76712</td>
      <td>472.98014</td>
      <td>2.81027</td>
      <td>2.80890</td>
      <td>94.24452</td>
      <td>46.66027</td>
      <td>21.95411</td>
      <td>3.40959</td>
      <td>15.06096</td>
      <td>2.75890</td>
      <td>0.56575</td>
      <td>43.48904</td>
      <td>6.32192</td>
      <td>2007.81575</td>
      <td>180921.19589</td>
    </tr>
    <tr>
      <th>std</th>
      <td>421.61001</td>
      <td>42.30057</td>
      <td>1.02017</td>
      <td>9981.26493</td>
      <td>1.38300</td>
      <td>1.11280</td>
      <td>30.20290</td>
      <td>20.64541</td>
      <td>180.73137</td>
      <td>0.57428</td>
      <td>0.35105</td>
      <td>0.87648</td>
      <td>0.55216</td>
      <td>1.06739</td>
      <td>2.10778</td>
      <td>456.09809</td>
      <td>0.89233</td>
      <td>161.31927</td>
      <td>441.86696</td>
      <td>438.70532</td>
      <td>0.95950</td>
      <td>386.58774</td>
      <td>436.52844</td>
      <td>48.62308</td>
      <td>525.48038</td>
      <td>0.51891</td>
      <td>0.23875</td>
      <td>0.55092</td>
      <td>0.50289</td>
      <td>0.81578</td>
      <td>0.22034</td>
      <td>0.66376</td>
      <td>1.62539</td>
      <td>0.64467</td>
      <td>1.81088</td>
      <td>1.93321</td>
      <td>29.98205</td>
      <td>0.89283</td>
      <td>0.74732</td>
      <td>213.80484</td>
      <td>0.72290</td>
      <td>0.71969</td>
      <td>125.33879</td>
      <td>66.25603</td>
      <td>61.11915</td>
      <td>29.31733</td>
      <td>55.75742</td>
      <td>40.17731</td>
      <td>1.20448</td>
      <td>496.12302</td>
      <td>2.70363</td>
      <td>1.32810</td>
      <td>79442.50288</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.00000</td>
      <td>20.00000</td>
      <td>0.00000</td>
      <td>1300.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1872.00000</td>
      <td>1950.00000</td>
      <td>0.00000</td>
      <td>2.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>334.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>334.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>2.00000</td>
      <td>2.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>2006.00000</td>
      <td>34900.00000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>365.75000</td>
      <td>20.00000</td>
      <td>3.00000</td>
      <td>7553.50000</td>
      <td>5.00000</td>
      <td>5.00000</td>
      <td>1954.00000</td>
      <td>1967.00000</td>
      <td>0.00000</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>223.00000</td>
      <td>795.75000</td>
      <td>3.00000</td>
      <td>882.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1129.50000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>2.00000</td>
      <td>1.00000</td>
      <td>3.00000</td>
      <td>5.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>58.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>334.50000</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>5.00000</td>
      <td>2007.00000</td>
      <td>129975.00000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>730.50000</td>
      <td>50.00000</td>
      <td>3.00000</td>
      <td>9478.50000</td>
      <td>6.00000</td>
      <td>5.00000</td>
      <td>1973.00000</td>
      <td>1994.00000</td>
      <td>0.00000</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>4.00000</td>
      <td>3.00000</td>
      <td>1.00000</td>
      <td>4.00000</td>
      <td>383.50000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>477.50000</td>
      <td>991.50000</td>
      <td>5.00000</td>
      <td>1087.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1464.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>2.00000</td>
      <td>0.00000</td>
      <td>3.00000</td>
      <td>1.00000</td>
      <td>3.00000</td>
      <td>6.00000</td>
      <td>1.00000</td>
      <td>2.00000</td>
      <td>5.00000</td>
      <td>77.00000</td>
      <td>2.00000</td>
      <td>2.00000</td>
      <td>480.00000</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>0.00000</td>
      <td>25.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>6.00000</td>
      <td>2008.00000</td>
      <td>163000.00000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1095.25000</td>
      <td>70.00000</td>
      <td>3.00000</td>
      <td>11601.50000</td>
      <td>7.00000</td>
      <td>6.00000</td>
      <td>2000.00000</td>
      <td>2004.00000</td>
      <td>164.25000</td>
      <td>4.00000</td>
      <td>3.00000</td>
      <td>4.00000</td>
      <td>3.00000</td>
      <td>2.00000</td>
      <td>6.00000</td>
      <td>712.25000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>808.00000</td>
      <td>1298.25000</td>
      <td>5.00000</td>
      <td>1391.25000</td>
      <td>728.00000</td>
      <td>0.00000</td>
      <td>1776.75000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>2.00000</td>
      <td>1.00000</td>
      <td>3.00000</td>
      <td>1.00000</td>
      <td>4.00000</td>
      <td>7.00000</td>
      <td>1.00000</td>
      <td>4.00000</td>
      <td>5.00000</td>
      <td>101.00000</td>
      <td>2.00000</td>
      <td>2.00000</td>
      <td>576.00000</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>168.00000</td>
      <td>68.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>8.00000</td>
      <td>2009.00000</td>
      <td>214000.00000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1460.00000</td>
      <td>190.00000</td>
      <td>6.00000</td>
      <td>215245.00000</td>
      <td>10.00000</td>
      <td>9.00000</td>
      <td>2010.00000</td>
      <td>2010.00000</td>
      <td>1600.00000</td>
      <td>5.00000</td>
      <td>5.00000</td>
      <td>5.00000</td>
      <td>4.00000</td>
      <td>4.00000</td>
      <td>6.00000</td>
      <td>5644.00000</td>
      <td>6.00000</td>
      <td>1474.00000</td>
      <td>2336.00000</td>
      <td>6110.00000</td>
      <td>5.00000</td>
      <td>4692.00000</td>
      <td>2065.00000</td>
      <td>572.00000</td>
      <td>5642.00000</td>
      <td>3.00000</td>
      <td>2.00000</td>
      <td>3.00000</td>
      <td>2.00000</td>
      <td>8.00000</td>
      <td>3.00000</td>
      <td>5.00000</td>
      <td>14.00000</td>
      <td>3.00000</td>
      <td>5.00000</td>
      <td>6.00000</td>
      <td>110.00000</td>
      <td>3.00000</td>
      <td>4.00000</td>
      <td>1418.00000</td>
      <td>5.00000</td>
      <td>5.00000</td>
      <td>857.00000</td>
      <td>547.00000</td>
      <td>552.00000</td>
      <td>508.00000</td>
      <td>480.00000</td>
      <td>738.00000</td>
      <td>4.00000</td>
      <td>15500.00000</td>
      <td>12.00000</td>
      <td>2010.00000</td>
      <td>755000.00000</td>
    </tr>
  </tbody>
</table>
</div>

<h3 id="One-hot-编码"><a href="#One-hot-编码" class="headerlink" title="One hot 编码"></a>One hot 编码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.shape, test_df.shape</span><br></pre></td></tr></table></figure>
<pre><code>((1460, 77), (1459, 76))
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_onehot_feature</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    dataset_len = len(dataset)</span><br><span class="line">    result = pd.DataFrame()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> onehot_feature_set:</span><br><span class="line">        tmp_pd = pd.get_dummies(dataset[i], prefix=i, dtype=float)</span><br><span class="line">        result = pd.concat([result, tmp_pd], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_onehot_df = preprocess_onehot_feature(train_df)</span><br><span class="line">test_onehot_df = preprocess_onehot_feature(test_df)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(train_onehot_df.shape, test_onehot_df.shape)</span><br><span class="line">test_missing = set(train_onehot_df.columns.values.tolist()) - set(test_onehot_df.columns.values.tolist())</span><br><span class="line">test_missing</span><br></pre></td></tr></table></figure>
<pre><code>(1460, 168) (1459, 153)





{&apos;Condition2_RRAe&apos;,
 &apos;Condition2_RRAn&apos;,
 &apos;Condition2_RRNn&apos;,
 &apos;Electrical_Mix&apos;,
 &apos;Exterior1st_ImStucc&apos;,
 &apos;Exterior1st_Stone&apos;,
 &apos;Exterior2nd_Other&apos;,
 &apos;Heating_Floor&apos;,
 &apos;Heating_OthW&apos;,
 &apos;HouseStyle_2.5Fin&apos;,
 &apos;RoofMatl_ClyTile&apos;,
 &apos;RoofMatl_Membran&apos;,
 &apos;RoofMatl_Metal&apos;,
 &apos;RoofMatl_Roll&apos;,
 &apos;Utilities_NoSeWa&apos;}
</code></pre><p>又遇到个坑，测试集比训练集的情况少。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成一个缺失的 dagafarm</span></span><br><span class="line">test_missing_zeros = pd.Series(np.zeros(<span class="number">1459</span>))</span><br><span class="line">test_missing_fd = pd.DataFrame(&#123;k: test_missing_zeros <span class="keyword">for</span> k <span class="keyword">in</span> test_missing&#125; )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把缺失的填进去</span></span><br><span class="line">test_onehot_df = pd.concat([test_onehot_df, test_missing_fd], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(train_onehot_df.shape, test_onehot_df.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(1460, 168) (1459, 168)
</code></pre><p>把 新 one hot 编码的 feature 添加到 原集合中，并删除原来的 feature</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 把one hot的feature添加到训练集和测试集</span></span><br><span class="line">train_df = pd.concat([train_df, train_onehot_df], axis=<span class="number">1</span>)</span><br><span class="line">test_df = pd.concat([test_df, test_onehot_df], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步删除训练集和测试集中的原feature</span></span><br><span class="line">train_df = train_df.drop(onehot_feature_set, axis=<span class="number">1</span>)</span><br><span class="line">test_df = test_df.drop(onehot_feature_set, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Id</th>
      <th>MSSubClass</th>
      <th>MSZoning</th>
      <th>LotArea</th>
      <th>OverallQual</th>
      <th>OverallCond</th>
      <th>YearBuilt</th>
      <th>YearRemodAdd</th>
      <th>MasVnrArea</th>
      <th>ExterQual</th>
      <th>ExterCond</th>
      <th>BsmtQual</th>
      <th>BsmtCond</th>
      <th>BsmtExposure</th>
      <th>BsmtFinType1</th>
      <th>BsmtFinSF1</th>
      <th>BsmtFinType2</th>
      <th>BsmtFinSF2</th>
      <th>BsmtUnfSF</th>
      <th>TotalBsmtSF</th>
      <th>HeatingQC</th>
      <th>1stFlrSF</th>
      <th>2ndFlrSF</th>
      <th>LowQualFinSF</th>
      <th>GrLivArea</th>
      <th>BsmtFullBath</th>
      <th>BsmtHalfBath</th>
      <th>FullBath</th>
      <th>HalfBath</th>
      <th>BedroomAbvGr</th>
      <th>KitchenAbvGr</th>
      <th>KitchenQual</th>
      <th>TotRmsAbvGrd</th>
      <th>Fireplaces</th>
      <th>FireplaceQu</th>
      <th>GarageType</th>
      <th>GarageYrBlt</th>
      <th>GarageFinish</th>
      <th>GarageCars</th>
      <th>GarageArea</th>
      <th>...</th>
      <th>Exterior1st_BrkFace</th>
      <th>Exterior1st_CBlock</th>
      <th>Exterior1st_CemntBd</th>
      <th>Exterior1st_HdBoard</th>
      <th>Exterior1st_ImStucc</th>
      <th>Exterior1st_MetalSd</th>
      <th>Exterior1st_Plywood</th>
      <th>Exterior1st_Stone</th>
      <th>Exterior1st_Stucco</th>
      <th>Exterior1st_VinylSd</th>
      <th>Exterior1st_Wd Sdng</th>
      <th>Exterior1st_WdShing</th>
      <th>RoofMatl_ClyTile</th>
      <th>RoofMatl_CompShg</th>
      <th>RoofMatl_Membran</th>
      <th>RoofMatl_Metal</th>
      <th>RoofMatl_Roll</th>
      <th>RoofMatl_Tar&amp;Grv</th>
      <th>RoofMatl_WdShake</th>
      <th>RoofMatl_WdShngl</th>
      <th>Electrical_FuseA</th>
      <th>Electrical_FuseF</th>
      <th>Electrical_FuseP</th>
      <th>Electrical_Mix</th>
      <th>Electrical_SBrkr</th>
      <th>SaleCondition_Abnorml</th>
      <th>SaleCondition_AdjLand</th>
      <th>SaleCondition_Alloca</th>
      <th>SaleCondition_Family</th>
      <th>SaleCondition_Normal</th>
      <th>SaleCondition_Partial</th>
      <th>Condition1_Artery</th>
      <th>Condition1_Feedr</th>
      <th>Condition1_Norm</th>
      <th>Condition1_PosA</th>
      <th>Condition1_PosN</th>
      <th>Condition1_RRAe</th>
      <th>Condition1_RRAn</th>
      <th>Condition1_RRNe</th>
      <th>Condition1_RRNn</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>...</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
      <td>1460.00000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>730.50000</td>
      <td>56.89726</td>
      <td>2.82534</td>
      <td>10516.82808</td>
      <td>6.09932</td>
      <td>5.57534</td>
      <td>1971.26781</td>
      <td>1984.86575</td>
      <td>103.11712</td>
      <td>3.39589</td>
      <td>3.08356</td>
      <td>3.48904</td>
      <td>2.93493</td>
      <td>1.63014</td>
      <td>3.54589</td>
      <td>443.63973</td>
      <td>1.24726</td>
      <td>46.54932</td>
      <td>567.24041</td>
      <td>1057.42945</td>
      <td>4.14521</td>
      <td>1162.62671</td>
      <td>346.99247</td>
      <td>5.84452</td>
      <td>1515.46370</td>
      <td>0.42534</td>
      <td>0.05753</td>
      <td>1.56507</td>
      <td>0.38288</td>
      <td>2.86644</td>
      <td>1.04658</td>
      <td>3.51164</td>
      <td>6.51781</td>
      <td>0.61301</td>
      <td>1.82534</td>
      <td>3.51438</td>
      <td>74.15068</td>
      <td>1.71575</td>
      <td>1.76712</td>
      <td>472.98014</td>
      <td>...</td>
      <td>0.03425</td>
      <td>0.00068</td>
      <td>0.04178</td>
      <td>0.15205</td>
      <td>0.00068</td>
      <td>0.15068</td>
      <td>0.07397</td>
      <td>0.00137</td>
      <td>0.01712</td>
      <td>0.35274</td>
      <td>0.14110</td>
      <td>0.01781</td>
      <td>0.00068</td>
      <td>0.98219</td>
      <td>0.00068</td>
      <td>0.00068</td>
      <td>0.00068</td>
      <td>0.00753</td>
      <td>0.00342</td>
      <td>0.00411</td>
      <td>0.06438</td>
      <td>0.01849</td>
      <td>0.00205</td>
      <td>0.00068</td>
      <td>0.91438</td>
      <td>0.06918</td>
      <td>0.00274</td>
      <td>0.00822</td>
      <td>0.01370</td>
      <td>0.82055</td>
      <td>0.08562</td>
      <td>0.03288</td>
      <td>0.05548</td>
      <td>0.86301</td>
      <td>0.00548</td>
      <td>0.01301</td>
      <td>0.00753</td>
      <td>0.01781</td>
      <td>0.00137</td>
      <td>0.00342</td>
    </tr>
    <tr>
      <th>std</th>
      <td>421.61001</td>
      <td>42.30057</td>
      <td>1.02017</td>
      <td>9981.26493</td>
      <td>1.38300</td>
      <td>1.11280</td>
      <td>30.20290</td>
      <td>20.64541</td>
      <td>180.73137</td>
      <td>0.57428</td>
      <td>0.35105</td>
      <td>0.87648</td>
      <td>0.55216</td>
      <td>1.06739</td>
      <td>2.10778</td>
      <td>456.09809</td>
      <td>0.89233</td>
      <td>161.31927</td>
      <td>441.86696</td>
      <td>438.70532</td>
      <td>0.95950</td>
      <td>386.58774</td>
      <td>436.52844</td>
      <td>48.62308</td>
      <td>525.48038</td>
      <td>0.51891</td>
      <td>0.23875</td>
      <td>0.55092</td>
      <td>0.50289</td>
      <td>0.81578</td>
      <td>0.22034</td>
      <td>0.66376</td>
      <td>1.62539</td>
      <td>0.64467</td>
      <td>1.81088</td>
      <td>1.93321</td>
      <td>29.98205</td>
      <td>0.89283</td>
      <td>0.74732</td>
      <td>213.80484</td>
      <td>...</td>
      <td>0.18192</td>
      <td>0.02617</td>
      <td>0.20016</td>
      <td>0.35920</td>
      <td>0.02617</td>
      <td>0.35786</td>
      <td>0.26182</td>
      <td>0.03700</td>
      <td>0.12978</td>
      <td>0.47799</td>
      <td>0.34824</td>
      <td>0.13230</td>
      <td>0.02617</td>
      <td>0.13230</td>
      <td>0.02617</td>
      <td>0.02617</td>
      <td>0.02617</td>
      <td>0.08650</td>
      <td>0.05844</td>
      <td>0.06400</td>
      <td>0.24552</td>
      <td>0.13477</td>
      <td>0.04530</td>
      <td>0.02617</td>
      <td>0.27989</td>
      <td>0.25384</td>
      <td>0.05229</td>
      <td>0.09032</td>
      <td>0.11628</td>
      <td>0.38386</td>
      <td>0.27989</td>
      <td>0.17837</td>
      <td>0.22899</td>
      <td>0.34395</td>
      <td>0.07385</td>
      <td>0.11337</td>
      <td>0.08650</td>
      <td>0.13230</td>
      <td>0.03700</td>
      <td>0.05844</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.00000</td>
      <td>20.00000</td>
      <td>0.00000</td>
      <td>1300.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1872.00000</td>
      <td>1950.00000</td>
      <td>0.00000</td>
      <td>2.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>334.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>334.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>2.00000</td>
      <td>2.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>...</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>365.75000</td>
      <td>20.00000</td>
      <td>3.00000</td>
      <td>7553.50000</td>
      <td>5.00000</td>
      <td>5.00000</td>
      <td>1954.00000</td>
      <td>1967.00000</td>
      <td>0.00000</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>223.00000</td>
      <td>795.75000</td>
      <td>3.00000</td>
      <td>882.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1129.50000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>2.00000</td>
      <td>1.00000</td>
      <td>3.00000</td>
      <td>5.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>58.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>334.50000</td>
      <td>...</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>730.50000</td>
      <td>50.00000</td>
      <td>3.00000</td>
      <td>9478.50000</td>
      <td>6.00000</td>
      <td>5.00000</td>
      <td>1973.00000</td>
      <td>1994.00000</td>
      <td>0.00000</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>4.00000</td>
      <td>3.00000</td>
      <td>1.00000</td>
      <td>4.00000</td>
      <td>383.50000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>477.50000</td>
      <td>991.50000</td>
      <td>5.00000</td>
      <td>1087.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1464.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>2.00000</td>
      <td>0.00000</td>
      <td>3.00000</td>
      <td>1.00000</td>
      <td>3.00000</td>
      <td>6.00000</td>
      <td>1.00000</td>
      <td>2.00000</td>
      <td>5.00000</td>
      <td>77.00000</td>
      <td>2.00000</td>
      <td>2.00000</td>
      <td>480.00000</td>
      <td>...</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1095.25000</td>
      <td>70.00000</td>
      <td>3.00000</td>
      <td>11601.50000</td>
      <td>7.00000</td>
      <td>6.00000</td>
      <td>2000.00000</td>
      <td>2004.00000</td>
      <td>164.25000</td>
      <td>4.00000</td>
      <td>3.00000</td>
      <td>4.00000</td>
      <td>3.00000</td>
      <td>2.00000</td>
      <td>6.00000</td>
      <td>712.25000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>808.00000</td>
      <td>1298.25000</td>
      <td>5.00000</td>
      <td>1391.25000</td>
      <td>728.00000</td>
      <td>0.00000</td>
      <td>1776.75000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>2.00000</td>
      <td>1.00000</td>
      <td>3.00000</td>
      <td>1.00000</td>
      <td>4.00000</td>
      <td>7.00000</td>
      <td>1.00000</td>
      <td>4.00000</td>
      <td>5.00000</td>
      <td>101.00000</td>
      <td>2.00000</td>
      <td>2.00000</td>
      <td>576.00000</td>
      <td>...</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1460.00000</td>
      <td>190.00000</td>
      <td>6.00000</td>
      <td>215245.00000</td>
      <td>10.00000</td>
      <td>9.00000</td>
      <td>2010.00000</td>
      <td>2010.00000</td>
      <td>1600.00000</td>
      <td>5.00000</td>
      <td>5.00000</td>
      <td>5.00000</td>
      <td>4.00000</td>
      <td>4.00000</td>
      <td>6.00000</td>
      <td>5644.00000</td>
      <td>6.00000</td>
      <td>1474.00000</td>
      <td>2336.00000</td>
      <td>6110.00000</td>
      <td>5.00000</td>
      <td>4692.00000</td>
      <td>2065.00000</td>
      <td>572.00000</td>
      <td>5642.00000</td>
      <td>3.00000</td>
      <td>2.00000</td>
      <td>3.00000</td>
      <td>2.00000</td>
      <td>8.00000</td>
      <td>3.00000</td>
      <td>5.00000</td>
      <td>14.00000</td>
      <td>3.00000</td>
      <td>5.00000</td>
      <td>6.00000</td>
      <td>110.00000</td>
      <td>3.00000</td>
      <td>4.00000</td>
      <td>1418.00000</td>
      <td>...</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 221 columns</p>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.info()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;
RangeIndex: 1460 entries, 0 to 1459
Columns: 221 entries, Id to Condition1_RRNn
dtypes: float64(182), int64(39)
memory usage: 2.5 MB
</code></pre><p>终于处理完了，已经没有 object 类型的数据了。接下来就能建模了。</p>
<h1 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_df.sample(frac=<span class="number">1</span>)</span><br><span class="line">X_train = train_df.drop([<span class="string">'SalePrice'</span>, <span class="string">'Id'</span>], axis=<span class="number">1</span>)</span><br><span class="line">Y_train = train_df[<span class="string">'SalePrice'</span>]</span><br><span class="line">X_test = test_df.drop(<span class="string">'Id'</span>, axis=<span class="number">1</span>).copy()</span><br><span class="line">X_train.shape, Y_train.shape, X_test.shape</span><br></pre></td></tr></table></figure>
<pre><code>((1460, 219), (1460,), (1459, 219))
</code></pre><p>先用逻辑回归看看</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Id</th>
      <th>MSSubClass</th>
      <th>MSZoning</th>
      <th>LotArea</th>
      <th>OverallQual</th>
      <th>OverallCond</th>
      <th>YearBuilt</th>
      <th>YearRemodAdd</th>
      <th>MasVnrArea</th>
      <th>ExterQual</th>
      <th>ExterCond</th>
      <th>BsmtQual</th>
      <th>BsmtCond</th>
      <th>BsmtExposure</th>
      <th>BsmtFinType1</th>
      <th>BsmtFinSF1</th>
      <th>BsmtFinType2</th>
      <th>BsmtFinSF2</th>
      <th>BsmtUnfSF</th>
      <th>TotalBsmtSF</th>
      <th>HeatingQC</th>
      <th>1stFlrSF</th>
      <th>2ndFlrSF</th>
      <th>LowQualFinSF</th>
      <th>GrLivArea</th>
      <th>BsmtFullBath</th>
      <th>BsmtHalfBath</th>
      <th>FullBath</th>
      <th>HalfBath</th>
      <th>BedroomAbvGr</th>
      <th>KitchenAbvGr</th>
      <th>KitchenQual</th>
      <th>TotRmsAbvGrd</th>
      <th>Fireplaces</th>
      <th>FireplaceQu</th>
      <th>GarageType</th>
      <th>GarageYrBlt</th>
      <th>GarageFinish</th>
      <th>GarageCars</th>
      <th>GarageArea</th>
      <th>...</th>
      <th>Exterior1st_BrkFace</th>
      <th>Exterior1st_CBlock</th>
      <th>Exterior1st_CemntBd</th>
      <th>Exterior1st_HdBoard</th>
      <th>Exterior1st_ImStucc</th>
      <th>Exterior1st_MetalSd</th>
      <th>Exterior1st_Plywood</th>
      <th>Exterior1st_Stone</th>
      <th>Exterior1st_Stucco</th>
      <th>Exterior1st_VinylSd</th>
      <th>Exterior1st_Wd Sdng</th>
      <th>Exterior1st_WdShing</th>
      <th>RoofMatl_ClyTile</th>
      <th>RoofMatl_CompShg</th>
      <th>RoofMatl_Membran</th>
      <th>RoofMatl_Metal</th>
      <th>RoofMatl_Roll</th>
      <th>RoofMatl_Tar&amp;Grv</th>
      <th>RoofMatl_WdShake</th>
      <th>RoofMatl_WdShngl</th>
      <th>Electrical_FuseA</th>
      <th>Electrical_FuseF</th>
      <th>Electrical_FuseP</th>
      <th>Electrical_Mix</th>
      <th>Electrical_SBrkr</th>
      <th>SaleCondition_Abnorml</th>
      <th>SaleCondition_AdjLand</th>
      <th>SaleCondition_Alloca</th>
      <th>SaleCondition_Family</th>
      <th>SaleCondition_Normal</th>
      <th>SaleCondition_Partial</th>
      <th>Condition1_Artery</th>
      <th>Condition1_Feedr</th>
      <th>Condition1_Norm</th>
      <th>Condition1_PosA</th>
      <th>Condition1_PosN</th>
      <th>Condition1_RRAe</th>
      <th>Condition1_RRAn</th>
      <th>Condition1_RRNe</th>
      <th>Condition1_RRNn</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>60</td>
      <td>3.00000</td>
      <td>8450</td>
      <td>7</td>
      <td>5</td>
      <td>2003</td>
      <td>2003</td>
      <td>196.00000</td>
      <td>4</td>
      <td>3</td>
      <td>4.00000</td>
      <td>3.00000</td>
      <td>1.00000</td>
      <td>6.00000</td>
      <td>706</td>
      <td>1.00000</td>
      <td>0</td>
      <td>150</td>
      <td>856</td>
      <td>5</td>
      <td>856</td>
      <td>854</td>
      <td>0</td>
      <td>1710</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>4</td>
      <td>8</td>
      <td>0</td>
      <td>0.00000</td>
      <td>5.00000</td>
      <td>103.00000</td>
      <td>2.00000</td>
      <td>2</td>
      <td>548</td>
      <td>...</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>20</td>
      <td>3.00000</td>
      <td>9600</td>
      <td>6</td>
      <td>8</td>
      <td>1976</td>
      <td>1976</td>
      <td>0.00000</td>
      <td>3</td>
      <td>3</td>
      <td>4.00000</td>
      <td>3.00000</td>
      <td>4.00000</td>
      <td>5.00000</td>
      <td>978</td>
      <td>1.00000</td>
      <td>0</td>
      <td>284</td>
      <td>1262</td>
      <td>5</td>
      <td>1262</td>
      <td>0</td>
      <td>0</td>
      <td>1262</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>6</td>
      <td>1</td>
      <td>3.00000</td>
      <td>5.00000</td>
      <td>76.00000</td>
      <td>2.00000</td>
      <td>2</td>
      <td>460</td>
      <td>...</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>60</td>
      <td>3.00000</td>
      <td>11250</td>
      <td>7</td>
      <td>5</td>
      <td>2001</td>
      <td>2002</td>
      <td>162.00000</td>
      <td>4</td>
      <td>3</td>
      <td>4.00000</td>
      <td>3.00000</td>
      <td>2.00000</td>
      <td>6.00000</td>
      <td>486</td>
      <td>1.00000</td>
      <td>0</td>
      <td>434</td>
      <td>920</td>
      <td>5</td>
      <td>920</td>
      <td>866</td>
      <td>0</td>
      <td>1786</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>4</td>
      <td>6</td>
      <td>1</td>
      <td>3.00000</td>
      <td>5.00000</td>
      <td>101.00000</td>
      <td>2.00000</td>
      <td>2</td>
      <td>608</td>
      <td>...</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>70</td>
      <td>3.00000</td>
      <td>9550</td>
      <td>7</td>
      <td>5</td>
      <td>1915</td>
      <td>1970</td>
      <td>0.00000</td>
      <td>3</td>
      <td>3</td>
      <td>3.00000</td>
      <td>4.00000</td>
      <td>1.00000</td>
      <td>5.00000</td>
      <td>216</td>
      <td>1.00000</td>
      <td>0</td>
      <td>540</td>
      <td>756</td>
      <td>4</td>
      <td>961</td>
      <td>756</td>
      <td>0</td>
      <td>1717</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>4</td>
      <td>7</td>
      <td>1</td>
      <td>4.00000</td>
      <td>1.00000</td>
      <td>98.00000</td>
      <td>1.00000</td>
      <td>3</td>
      <td>642</td>
      <td>...</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>60</td>
      <td>3.00000</td>
      <td>14260</td>
      <td>8</td>
      <td>5</td>
      <td>2000</td>
      <td>2000</td>
      <td>350.00000</td>
      <td>4</td>
      <td>3</td>
      <td>4.00000</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>6.00000</td>
      <td>655</td>
      <td>1.00000</td>
      <td>0</td>
      <td>490</td>
      <td>1145</td>
      <td>5</td>
      <td>1145</td>
      <td>1053</td>
      <td>0</td>
      <td>2198</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>4</td>
      <td>9</td>
      <td>1</td>
      <td>3.00000</td>
      <td>5.00000</td>
      <td>100.00000</td>
      <td>2.00000</td>
      <td>3</td>
      <td>836</td>
      <td>...</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1455</th>
      <td>1456</td>
      <td>60</td>
      <td>3.00000</td>
      <td>7917</td>
      <td>6</td>
      <td>5</td>
      <td>1999</td>
      <td>2000</td>
      <td>0.00000</td>
      <td>3</td>
      <td>3</td>
      <td>4.00000</td>
      <td>3.00000</td>
      <td>1.00000</td>
      <td>1.00000</td>
      <td>0</td>
      <td>1.00000</td>
      <td>0</td>
      <td>953</td>
      <td>953</td>
      <td>5</td>
      <td>953</td>
      <td>694</td>
      <td>0</td>
      <td>1647</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>7</td>
      <td>1</td>
      <td>3.00000</td>
      <td>5.00000</td>
      <td>99.00000</td>
      <td>2.00000</td>
      <td>2</td>
      <td>460</td>
      <td>...</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>1456</th>
      <td>1457</td>
      <td>20</td>
      <td>3.00000</td>
      <td>13175</td>
      <td>6</td>
      <td>6</td>
      <td>1978</td>
      <td>1988</td>
      <td>119.00000</td>
      <td>3</td>
      <td>3</td>
      <td>4.00000</td>
      <td>3.00000</td>
      <td>1.00000</td>
      <td>5.00000</td>
      <td>790</td>
      <td>3.00000</td>
      <td>163</td>
      <td>589</td>
      <td>1542</td>
      <td>3</td>
      <td>2073</td>
      <td>0</td>
      <td>0</td>
      <td>2073</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>7</td>
      <td>2</td>
      <td>3.00000</td>
      <td>5.00000</td>
      <td>78.00000</td>
      <td>1.00000</td>
      <td>2</td>
      <td>500</td>
      <td>...</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>1457</th>
      <td>1458</td>
      <td>70</td>
      <td>3.00000</td>
      <td>9042</td>
      <td>7</td>
      <td>9</td>
      <td>1941</td>
      <td>2006</td>
      <td>0.00000</td>
      <td>5</td>
      <td>4</td>
      <td>3.00000</td>
      <td>4.00000</td>
      <td>1.00000</td>
      <td>6.00000</td>
      <td>275</td>
      <td>1.00000</td>
      <td>0</td>
      <td>877</td>
      <td>1152</td>
      <td>5</td>
      <td>1188</td>
      <td>1152</td>
      <td>0</td>
      <td>2340</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>4</td>
      <td>1</td>
      <td>4</td>
      <td>9</td>
      <td>2</td>
      <td>4.00000</td>
      <td>5.00000</td>
      <td>41.00000</td>
      <td>2.00000</td>
      <td>1</td>
      <td>252</td>
      <td>...</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>1458</th>
      <td>1459</td>
      <td>20</td>
      <td>3.00000</td>
      <td>9717</td>
      <td>5</td>
      <td>6</td>
      <td>1950</td>
      <td>1996</td>
      <td>0.00000</td>
      <td>3</td>
      <td>3</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>2.00000</td>
      <td>6.00000</td>
      <td>49</td>
      <td>3.00000</td>
      <td>1029</td>
      <td>0</td>
      <td>1078</td>
      <td>4</td>
      <td>1078</td>
      <td>0</td>
      <td>0</td>
      <td>1078</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>4</td>
      <td>5</td>
      <td>0</td>
      <td>0.00000</td>
      <td>5.00000</td>
      <td>50.00000</td>
      <td>1.00000</td>
      <td>1</td>
      <td>240</td>
      <td>...</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>1459</th>
      <td>1460</td>
      <td>20</td>
      <td>3.00000</td>
      <td>9937</td>
      <td>5</td>
      <td>6</td>
      <td>1965</td>
      <td>1965</td>
      <td>0.00000</td>
      <td>4</td>
      <td>3</td>
      <td>3.00000</td>
      <td>3.00000</td>
      <td>1.00000</td>
      <td>4.00000</td>
      <td>830</td>
      <td>2.00000</td>
      <td>290</td>
      <td>136</td>
      <td>1256</td>
      <td>4</td>
      <td>1256</td>
      <td>0</td>
      <td>0</td>
      <td>1256</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>6</td>
      <td>0</td>
      <td>0.00000</td>
      <td>5.00000</td>
      <td>65.00000</td>
      <td>3.00000</td>
      <td>1</td>
      <td>276</td>
      <td>...</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>1.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
      <td>0.00000</td>
    </tr>
  </tbody>
</table>
<p>1460 rows × 220 columns</p>
</div>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 逻辑回归</span></span><br><span class="line"></span><br><span class="line">logreg = LogisticRegression()</span><br><span class="line">logreg.fit(X_train, Y_train)</span><br><span class="line">Y_pred = logreg.predict(X_test)</span><br><span class="line">acc_log = round(logreg.score(X_train, Y_train) * <span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">acc_log</span><br></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &apos;lbfgs&apos; in 0.22. Specify a solver to silence this warning.
  FutureWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to &apos;auto&apos; in 0.22. Specify the multi_class option to silence this warning.
  &quot;this warning.&quot;, FutureWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  &quot;the number of iterations.&quot;, ConvergenceWarning)





90.55
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Random Forest</span></span><br><span class="line"></span><br><span class="line">random_forest = RandomForestClassifier(n_estimators=<span class="number">100</span>)</span><br><span class="line">random_forest.fit(X_train, Y_train)</span><br><span class="line">Y_pred = random_forest.predict(X_test)</span><br><span class="line">random_forest.score(X_train, Y_train)</span><br><span class="line">acc_random_forest = round(random_forest.score(X_train, Y_train) * <span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">acc_random_forest</span><br></pre></td></tr></table></figure>
<pre><code>100.0
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">X = X_train.loc[: <span class="number">1000</span>]</span><br><span class="line">Y = Y_train.loc[: <span class="number">1000</span>]</span><br><span class="line"></span><br><span class="line">X_t = X_train.loc[<span class="number">1000</span>: ]</span><br><span class="line">Y_t = Y_train.loc[<span class="number">1000</span>: ]</span><br><span class="line"></span><br><span class="line">logreg = RandomForestClassifier(n_estimators=<span class="number">100</span>)</span><br><span class="line">logreg.fit(X, Y)</span><br><span class="line">pred = logreg.predict(X_t)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">30</span>, <span class="number">5</span>))</span><br><span class="line">x = np.array(range(len(pred)))</span><br><span class="line">plt.scatter(x, pred)</span><br><span class="line">plt.scatter(x, Y_t)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="House_Prices_Advanced_Regression_Techniques_75_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">30</span>, <span class="number">5</span>))</span><br><span class="line">plt.plot(pred - Y_t)</span><br><span class="line">plt.show()</span><br><span class="line">np.var(pred - Y_t)</span><br></pre></td></tr></table></figure>
<p><img src="House_Prices_Advanced_Regression_Techniques_76_0.png" alt="png"></p>
<pre><code>2231563230.032489
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.array([range(len(pred))]*<span class="number">10</span>).T</span><br></pre></td></tr></table></figure>
<pre><code>array([[  0,   0,   0, ...,   0,   0,   0],
       [  1,   1,   1, ...,   1,   1,   1],
       [  2,   2,   2, ...,   2,   2,   2],
       ...,
       [457, 457, 457, ..., 457, 457, 457],
       [458, 458, 458, ..., 458, 458, 458],
       [459, 459, 459, ..., 459, 459, 459]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Support Vector Machines</span></span><br><span class="line"></span><br><span class="line">svm = SVC()</span><br><span class="line">svm.fit(X_train, Y_train)</span><br><span class="line">Y_pred = svm.predict(X_test)</span><br><span class="line">acc_svm = round(svm.score(X_train, Y_train) * <span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">acc_svm</span><br></pre></td></tr></table></figure>
<pre><code>/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from &apos;auto&apos; to &apos;scale&apos; in version 0.22 to account better for unscaled features. Set gamma explicitly to &apos;auto&apos; or &apos;scale&apos; to avoid this warning.
  &quot;avoid this warning.&quot;, FutureWarning)





99.52
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存结果</span></span><br><span class="line"></span><br><span class="line">submission = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">"Id"</span>: test_df[<span class="string">"Id"</span>],</span><br><span class="line">    <span class="string">"SalePrice"</span>: Y_pred</span><br><span class="line">    &#125;)</span><br><span class="line">submission.to_csv(<span class="string">"/submission.csv"</span>, index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Kaggle</category>
      </categories>
      <tags>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习术语表</title>
    <url>/2019/01/19/MachineLearningGlossary/</url>
    <content><![CDATA[
<script src="https://developers.google.com/_static/3aadba8ea3/js/managed/mathjax/MathJax.js?config=TeX-AMS-MML_SVG&hl=zh-cn"></script>

机器学习术语表


<p>本术语表中列出了一般的机器学习术语和 TensorFlow 专用术语的定义。</p>

<h2 class="glossary">A</h2>

<p><a name="AB_testing"></a>
</p><h2 class="hide-from-toc">A/B 测试 (A/B testing)</h2><p></p>
<p>一种统计方法，用于将两种或多种技术进行比较，通常是将当前采用的技术与新技术进行比较。A/B 测试不仅旨在确定哪种技术的效果更好，而且还有助于了解相应差异是否具有显著的统计意义。A/B 测试通常是采用一种衡量方式对两种技术进行比较，但也适用于任意有限数量的技术和衡量方式。</p>
<p><a name="accuracy"></a>
</p><h2 class="hide-from-toc">准确率 (accuracy)</h2><p></p>
<p><a href="#classification_model"><strong>分类模型</strong></a>的正确预测所占的比例。在<a href="#multi-class"><strong>多类别分类</strong></a>中，准确率的定义如下：</p>
<div>
$$\text{准确率} = \frac{\text{正确的预测数}} {\text{样本总数}}$$
</div>

<p>在<a href="#binary_classification"><strong>二元分类</strong></a>中，准确率的定义如下：</p>
<div>
$$\text{准确率} = \frac{\text{正例数} + \text{负例数}} {\text{样本总数}}$$
</div>

<p>请参阅<a href="#TP"><strong>正例</strong></a>和<a href="#TN"><strong>负例</strong></a>。</p>
<p><a name="activation_function"></a>
</p><h2 class="hide-from-toc">激活函数 (activation function)</h2><p></p>
<p>一种函数（例如 <a href="#ReLU"><strong>ReLU</strong></a> 或 <a href="#sigmoid_function"><strong>S 型函数</strong></a>），用于对上一层的所有输入求加权和，然后生成一个输出值（通常为非线性值），并将其传递给下一层。</p>
<p><a name="AdaGrad"></a>
</p><h2 class="hide-from-toc">AdaGrad</h2><p></p>
<p>一种先进的梯度下降法，用于重新调整每个参数的梯度，以便有效地为每个参数指定独立的<a href="#learning_rate"><strong>学习速率</strong></a>。如需查看完整的解释，请参阅<a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" target="_blank" rel="noopener">这篇论文</a>。</p>
<p><a name="AUC"></a>
</p><h2 class="hide-from-toc">ROC 曲线下面积 (AUC, Area under the ROC Curve)</h2><p></p>
<p>一种会考虑所有可能<a href="#classification_threshold"><strong>分类阈值</strong></a>的评估指标。</p>
<p><a href="#ROC">ROC 曲线</a>下面积是，对于随机选择的正类别样本确实为正类别，以及随机选择的负类别样本为正类别，分类器更确信前者的概率。</p>
<h2 class="glossary">B</h2>

<p><a name="backpropagation"></a>
</p><h2 class="hide-from-toc">反向传播算法 (backpropagation)</h2><p></p>
<p>在<a href="#neural_network"><strong>神经网络</strong></a>上执行<a href="#gradient_descent"><strong>梯度下降法</strong></a>的主要算法。该算法会先按前向传播方式计算（并缓存）每个节点的输出值，然后再按反向传播遍历图的方式计算损失函数值相对于每个参数的<a href="https://en.wikipedia.org/wiki/Partial_derivative" target="_blank" rel="noopener">偏导数</a>。</p>
<p><a name="baseline"></a>
</p><h2 class="hide-from-toc">基准 (baseline)</h2><p></p>
<p>一种简单的<a href="#model"><strong>模型</strong></a>或启发法，用作比较模型效果时的参考点。基准有助于模型开发者针对特定问题量化最低预期效果。</p>
<p><a name="batch"></a>
</p><h2 class="hide-from-toc">批次 (batch)</h2><p></p>
<p><a href="#model_training"><strong>模型训练</strong></a>的一次<a href="#iteration"><strong>迭代</strong></a>（即一次<a href="#gradient"><strong>梯度</strong></a>更新）中使用的样本集。</p>
<p>另请参阅<a href="#batch_size"><strong>批次大小</strong></a>。</p>
<p><a name="batch_size"></a>
</p><h2 class="hide-from-toc">批次大小 (batch size)</h2><p></p>
<p>一个<a href="#batch"><strong>批次</strong></a>中的样本数。例如，<a href="#SGD"><strong>SGD</strong></a> 的批次大小为 1，而<a href="#mini-batch"><strong>小批次</strong></a>的大小通常介于 10 到 1000 之间。批次大小在训练和推断期间通常是固定的；不过，TensorFlow 允许使用动态批次大小。</p>
<p><a name="bias"></a>
</p><h2 class="hide-from-toc">偏差 (bias)</h2><p></p>
<p>距离原点的截距或偏移。偏差（也称为<strong>偏差项</strong>）在机器学习模型中用 b 或 w<sub>0</sub> 表示。<em></em><i></i>例如，在下面的公式中，偏差为 b：<em></em></p>
<div>
$$y' = b + w_1x_1 + w_2x_2 + … w_nx_n$$
</div>

<p>请勿与<a href="#prediction_bias"><strong>预测偏差</strong></a>混淆。</p>
<p><a name="binary_classification"></a>
</p><h2 class="hide-from-toc">二元分类 (binary classification)</h2><p></p>
<p>一种分类任务，可输出两种互斥类别之一。例如，对电子邮件进行评估并输出“垃圾邮件”或“非垃圾邮件”的机器学习模型就是一个二元分类器。</p>
<p><a name="binning"></a>
</p><h2 class="hide-from-toc">分箱 (binning)</h2><p></p>
<p>请参阅<a href="#bucketing"><strong>分桶</strong></a>。</p>

<p></p><p><a name="bucketing"></a>
</p><h2 class="hide-from-toc">分桶 (bucketing)</h2><p></p>
<p>将一个特征（通常是<a href="#continuous_feature"><strong>连续</strong></a>特征）转换成多个二元特征（称为桶或箱），通常根据值区间进行转换。例如，您可以将温度区间分割为离散分箱，而不是将温度表示成单个连续的浮点特征。假设温度数据可精确到小数点后一位，则可以将介于 0.0 到 15.0 度之间的所有温度都归入一个分箱，将介于 15.1 到 30.0 度之间的所有温度归入第二个分箱，并将介于 30.1 到 50.0 度之间的所有温度归入第三个分箱。</p>
<h2 class="glossary">C</h2>

<p><a name="calibration_layer"></a>
</p><h2 class="hide-from-toc">校准层 (calibration layer)</h2><p></p>
<p>一种预测后调整，通常是为了降低<a href="#prediction_bias"><strong>预测偏差</strong></a>的影响。调整后的预测和概率应与观察到的标签集的分布一致。</p>
<p><a name="candidate_sampling"></a>
</p><h2 class="hide-from-toc">候选采样 (candidate sampling)</h2><p></p>
<p>一种训练时进行的优化，会使用某种函数（例如 softmax）针对所有正类别标签计算概率，但对于负类别标签，则仅针对其随机样本计算概率。例如，如果某个样本的标签为“小猎犬”和“狗”，则候选采样将针对“小猎犬”和“狗”类别输出以及其他类别（猫、棒棒糖、栅栏）的随机子集计算预测概率和相应的损失项。<em></em><em></em><em></em><em></em><em></em><em></em><em></em>这种采样基于的想法是，只要<a href="#positive_class"><strong>正类别</strong></a>始终得到适当的正增强，<a href="#negative_class"><strong>负类别</strong></a>就可以从频率较低的负增强中进行学习，这确实是在实际中观察到的情况。候选采样的目的是，通过不针对所有负类别计算预测结果来提高计算效率。</p>
<p><a name="categorical_data"></a>
</p><h2 class="hide-from-toc">分类数据 (categorical data)</h2><p></p>
<p>一种<a href="#feature"><strong>特征</strong></a>，拥有一组离散的可能值。以某个名为 <code>house style</code> 的分类特征为例，该特征拥有一组离散的可能值（共三个），即 <code>Tudor, ranch, colonial</code>。通过将 <code>house style</code> 表示成分类数据，相应模型可以学习 <code>Tudor</code>、<code>ranch</code> 和 <code>colonial</code> 分别对房价的影响。</p>
<p>有时，离散集中的值是互斥的，只能将其中一个值应用于指定样本。例如，<code>car maker</code> 分类特征可能只允许一个样本有一个值 (<code>Toyota</code>)。在其他情况下，则可以应用多个值。一辆车可能会被喷涂多种不同的颜色，因此，<code>car color</code> 分类特征可能会允许单个样本具有多个值（例如 <code>red</code> 和 <code>white</code>）。</p>
<p>分类特征有时称为<a href="#discrete_feature"><strong>离散特征</strong></a>。</p>
<p>与<a href="#numerical_data"><strong>数值数据</strong></a>相对。</p>
<p><a name="centroid"></a>
</p><h2 class="hide-from-toc">形心 (centroid)</h2><p></p>
<p>聚类的中心，由 <a href="#k-means"><strong>k-means</strong></a> 或 <a href="#k-median"><strong>k-median</strong></a> 算法决定。例如，如果 k 为 3，则 k-means 或 k-median 算法会找出 3 个形心。</p>
<p><a name="checkpoint"></a>
</p><h2 class="hide-from-toc">检查点 (checkpoint)</h2><p></p>
<p>一种数据，用于捕获模型变量在特定时间的状态。借助检查点，可以导出模型<a href="#weight"><strong>权重</strong></a>，跨多个会话执行训练，以及使训练在发生错误之后得以继续（例如作业抢占）。请注意，<a href="#graph"><strong>图</strong></a>本身不包含在检查点中。</p>
<p><a name="class"></a>
</p><h2 class="hide-from-toc">类别 (class)</h2><p></p>
<p>为标签枚举的一组目标值中的一个。例如，在检测垃圾邮件的<a href="#binary_classification"><strong>二元分类</strong></a>模型中，两种类别分别是“垃圾邮件”和“非垃圾邮件”。<em></em><em></em>在识别狗品种的<a href="#multi_class_classification"><strong>多类别分类</strong></a>模型中，类别可以是“贵宾犬”、“小猎犬”、“哈巴犬”等等。<em></em><em></em><em></em></p>
<p><a name="class_imbalanced_data_set"></a>
</p><h2 class="hide-from-toc">分类不平衡的数据集 (class-imbalanced data set)</h2><p></p>
<p>一种<a href="#binary_classification"><strong>二元分类</strong></a>问题，在此类问题中，两种类别的<a href="#label"><strong>标签</strong></a>在出现频率方面具有很大的差距。例如，在某个疾病数据集中，0.0001 的样本具有正类别标签，0.9999 的样本具有负类别标签，这就属于分类不平衡问题；但在某个足球比赛预测器中，0.51 的样本的标签为其中一个球队赢，0.49 的样本的标签为另一个球队赢，这就不属于分类不平衡问题。<em></em></p>
<p><a name="classification_model"></a>
</p><h2 class="hide-from-toc">分类模型 (classification model)</h2><p></p>
<p>一种机器学习模型，用于区分两种或多种离散类别。例如，某个自然语言处理分类模型可以确定输入的句子是法语、西班牙语还是意大利语。请与<a href="#regression_model"><strong>回归模型</strong></a>进行比较。</p>
<p><a name="classification_threshold"></a>
</p><h2 class="hide-from-toc">分类阈值 (classification threshold)</h2><p></p>
<p>一种标量值条件，应用于模型预测的得分，旨在将<a href="#positive_class"><strong>正类别</strong></a>与<a href="#negative_class"><strong>负类别</strong></a>区分开。将<a href="#logistic_regression"><strong>逻辑回归</strong></a>结果映射到<a href="#binary_classification"><strong>二元分类</strong></a>时使用。以某个逻辑回归模型为例，该模型用于确定指定电子邮件是垃圾邮件的概率。如果分类阈值为 0.9，那么逻辑回归值高于 0.9 的电子邮件将被归类为“垃圾邮件”，低于 0.9 的则被归类为“非垃圾邮件”。<em></em><em></em></p>
<p><a name="clustering"></a>
</p><h2 class="hide-from-toc">聚类 (clustering)</h2><p></p>
<p>将关联的<a href="#example"><strong>样本</strong></a>分成一组，一般用于<a href="#unsupervised_machine_learning"><strong>非监督式学习</strong></a>。在所有样本均分组完毕后，相关人员便可选择性地为每个聚类赋予含义。</p>
<p>聚类算法有很多。例如，<a href="#k-means"><strong>k-means</strong></a> 算法会基于样本与<a href="#centroid"><strong>形心</strong></a>的接近程度聚类样本，如下图所示：</p>
<p>
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:cc="http://creativecommons.org/ns#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" xmlns="http://www.w3.org/2000/svg" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" height="300px" version="1.1" viewbox="0.0 0.0 444.33858267716533 337.4278215223097" fill="none" stroke="none" stroke-linecap="square" stroke-miterlimit="10" id="svg417" sodipodi:docname="zh-cn-clustering-image.svg" inkscape:version="0.92.2 5c3e80d, 2017-08-06">
  <metadata id="metadata423">
    <rdf:rdf>
      <cc:work rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type rdf:resource="http://purl.org/dc/dcmitype/StillImage">
        <dc:title>
      </dc:title></dc:type></cc:work>
    </rdf:rdf>
  </metadata>
  <defs id="defs421">
  <sodipodi:namedview pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" objecttolerance="10" gridtolerance="10" guidetolerance="10" inkscape:pageopacity="0" inkscape:pageshadow="2" inkscape:window-width="1028" inkscape:window-height="681" id="namedview419" showgrid="false" inkscape:zoom="0.98911345" inkscape:cx="222.1693" inkscape:cy="168.71391" inkscape:window-x="0" inkscape:window-y="0" inkscape:window-maximized="0" inkscape:current-layer="svg417">
  <desc id="desc2">50 or so examples clustered into two groups.</desc>
  <clippath id="p.0">
    <path d="m0 0l444.3386 0l0 337.42783l-444.3386 0l0 -337.42783z" clip-rule="nonzero" id="path4"/>
  </clippath>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path7" d="M 0,0 H 444.3386 V 337.42783 H 0 Z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path9" d="M 76.13614,8.670278 V 298.37106"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path11" d="M 76.13614,8.670278 V 298.37106"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path13" d="M 76.13614,298.3742 H 435.06528"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path15" d="M 76.13614,298.3742 H 435.06528"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path17" d="m 241.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path19" d="m 241.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path21" d="m 177.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path23" d="m 177.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path25" d="m 185.39021,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path27" d="m 185.39021,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path29" d="m 185.39021,194.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path31" d="m 185.39021,194.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path33" d="m 201.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path35" d="m 201.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path37" d="m 193.39021,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path39" d="m 193.39021,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path41" d="m 170.4952,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path43" d="m 170.4952,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path45" d="m 225.39021,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path47" d="m 225.39021,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path49" d="m 233.39021,146.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path51" d="m 233.39021,146.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path53" d="m 209.39021,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path55" d="m 209.39021,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path57" d="m 193.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path59" d="m 193.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path61" d="m 201.39021,218.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path63" d="m 201.39021,218.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path65" d="m 217.39021,218.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path67" d="m 217.39021,218.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path69" d="m 217.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path71" d="m 217.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path73" d="m 225.39021,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path75" d="m 225.39021,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path77" d="m 241.39021,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path79" d="m 241.39021,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path81" d="m 233.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path83" d="m 233.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path85" d="m 217.39021,174.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path87" d="m 217.39021,174.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path89" d="m 161.39021,151.37369 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path91" d="m 161.39021,151.37369 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path93" d="m 197.53194,202.19783 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path95" d="m 197.53194,202.19783 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path97" d="m 297.3902,74.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path99" d="m 297.3902,74.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path101" d="m 305.3902,114.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path103" d="m 305.3902,114.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path105" d="m 297.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path107" d="m 297.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path109" d="m 305.3902,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path111" d="m 305.3902,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path113" d="m 297.3902,50.01936 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path115" d="m 297.3902,50.01936 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path117" d="m 321.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path119" d="m 321.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path121" d="m 329.3902,66.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path123" d="m 329.3902,66.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path125" d="m 313.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path127" d="m 313.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path129" d="m 313.3902,98.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path131" d="m 313.3902,98.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path133" d="m 329.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path135" d="m 329.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path137" d="m 329.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path139" d="m 329.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path141" d="m 337.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path143" d="m 337.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path145" d="m 353.3902,98.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path147" d="m 353.3902,98.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path149" d="m 361.3902,74.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path151" d="m 361.3902,74.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path153" d="M 201.92647,298.54874 H 313.39105 V 326.2338 H 201.92647 Z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path157" d="m 14.842442,124.66404 h 61.007877 v 53.35433 H 14.842442 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path163" d="m 369.3902,114.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path165" d="m 369.3902,114.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path167" d="m 265.3902,66.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path169" d="m 265.3902,66.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path171" d="m 281.3902,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path173" d="m 281.3902,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path175" d="m 289.3902,114.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path177" d="m 289.3902,114.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path179" d="m 249.39021,154.01936 h 8.53542 v 10.64566 h -8.53542 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path181" d="m 249.39021,154.01936 h 8.53542 v 10.64566 h -8.53542 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path183" d="m 241.39021,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path185" d="m 241.39021,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path187" d="m 353.3902,247.37369 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path189" d="m 353.3902,247.37369 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path191" d="m 353.3902,274.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path193" d="m 353.3902,274.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path195" d="m 358.13217,242.38574 h 89.19684 v 14.07873 h -89.19684 z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path199" d="m 358.13217,267.9925 h 89.19684 v 14.07877 h -89.19684 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path203" d="m 217.39021,130.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path205" d="m 217.39021,130.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path207" d="m 193.39021,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path209" d="m 193.39021,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path211" d="m 209.39021,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path213" d="m 209.39021,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path215" d="m 177.39021,154.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path217" d="m 177.39021,154.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path219" d="m 233.39021,146.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path221" d="m 233.39021,146.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path223" d="m 225.39021,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path225" d="m 225.39021,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path227" d="m 209.39021,142.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path229" d="m 209.39021,142.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path231" d="m 241.39021,130.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path233" d="m 241.39021,130.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path235" d="m 233.39021,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path237" d="m 233.39021,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path239" d="m 273.3902,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path241" d="m 273.3902,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path243" d="m 265.3902,128.53508 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path245" d="m 265.3902,128.53508 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path247" d="m 273.3902,114.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path249" d="m 273.3902,114.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path251" d="m 281.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path253" d="m 281.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path255" d="m 281.3902,146.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path257" d="m 281.3902,146.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path259" d="m 281.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path261" d="m 281.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path263" d="m 297.3902,146.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path265" d="m 297.3902,146.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path267" d="m 297.3902,130.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path269" d="m 297.3902,130.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path271" d="m 305.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path273" d="m 305.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path275" d="m 249.39021,114.01936 h 8.53542 v 10.64567 h -8.53542 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path277" d="m 249.39021,114.01936 h 8.53542 v 10.64567 h -8.53542 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path279" d="m 257.3902,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path281" d="m 257.3902,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path283" d="m 185.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path285" d="m 185.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path287" d="m 273.3902,34.01936 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path289" d="m 273.3902,34.01936 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path291" d="m 257.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path293" d="m 257.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path295" d="m 265.3902,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path297" d="m 265.3902,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path299" d="m 249.39021,194.01936 h 8.53542 v 10.64566 h -8.53542 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path301" d="m 249.39021,194.01936 h 8.53542 v 10.64566 h -8.53542 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path303" d="m 281.3902,130.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path305" d="m 281.3902,130.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path307" d="m 217.39021,146.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path309" d="m 217.39021,146.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path311" d="m 177.39021,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path313" d="m 177.39021,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path315" d="m 153.39021,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path317" d="m 153.39021,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path319" d="m 169.39021,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path321" d="m 169.39021,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path323" d="m 169.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path325" d="m 169.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path327" d="m 193.39021,154.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path329" d="m 193.39021,154.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path331" d="m 217.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path333" d="m 217.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path335" d="m 180.44258,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path337" d="m 180.44258,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path339" d="m 265.3902,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path341" d="m 265.3902,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path343" d="m 273.3902,98.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path345" d="m 273.3902,98.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path347" d="m 161.39021,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path349" d="m 161.39021,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path351" d="m 153.39021,194.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path353" d="m 153.39021,194.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path355" d="m 313.3902,42.01936 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path357" d="m 313.3902,42.01936 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path359" d="m 178.4952,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path361" d="m 178.4952,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path363" d="m 169.39021,194.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path365" d="m 169.39021,194.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path367" d="m 183.98425,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path369" d="m 183.98425,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path371" d="m 185.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path373" d="m 185.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path375" d="m 177.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path377" d="m 177.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path379" d="m 169.39021,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path381" d="m 169.39021,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path383" d="m 265.3902,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path385" d="m 265.3902,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#ff0000;fill-rule:evenodd" inkscape:connector-curvature="0" id="path387" d="m 200.3517,180.38058 v 0 c 0,-2.93973 2.1434,-5.32285 4.78741,-5.32285 v 0 c 1.26969,0 2.48739,0.56081 3.3852,1.55903 0.89781,0.99823 1.4022,2.35211 1.4022,3.76382 v 0 c 0,2.93971 -2.1434,5.32283 -4.7874,5.32283 v 0 c -2.64401,0 -4.78741,-2.38312 -4.78741,-5.32283 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path389" d="m 200.3517,180.38058 v 0 c 0,-2.93973 2.1434,-5.32285 4.78741,-5.32285 v 0 c 1.26969,0 2.48739,0.56081 3.3852,1.55903 0.89781,0.99823 1.4022,2.35211 1.4022,3.76382 v 0 c 0,2.93971 -2.1434,5.32283 -4.7874,5.32283 v 0 c -2.64401,0 -4.78741,-2.38312 -4.78741,-5.32283 z"/>
  <path style="fill:#ff0000;fill-rule:evenodd" inkscape:connector-curvature="0" id="path391" d="m 304.3517,100.38058 v 0 c 0,-2.93972 2.14337,-5.322838 4.78738,-5.322838 v 0 c 1.26972,0 2.4874,0.560799 3.38523,1.559029 0.89779,0.998222 1.40219,2.352104 1.40219,3.763809 v 0 c 0,2.93972 -2.1434,5.32284 -4.78742,5.32284 v 0 c -2.64401,0 -4.78738,-2.38312 -4.78738,-5.32284 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path393" d="m 304.3517,100.38058 v 0 c 0,-2.93972 2.14337,-5.322838 4.78738,-5.322838 v 0 c 1.26972,0 2.4874,0.560799 3.38523,1.559029 0.89779,0.998222 1.40219,2.352104 1.40219,3.763809 v 0 c 0,2.93972 -2.1434,5.32284 -4.78742,5.32284 v 0 c -2.64401,0 -4.78738,-2.38312 -4.78738,-5.32284 z"/>
  <path style="fill:#ff0000;fill-rule:evenodd" inkscape:connector-curvature="0" id="path395" d="m 352.3517,228.38058 v 0 c 0,-2.93973 2.14337,-5.32285 4.78738,-5.32285 v 0 c 1.26972,0 2.4874,0.56081 3.38523,1.55903 0.89779,0.99823 1.40219,2.35211 1.40219,3.76382 v 0 c 0,2.93971 -2.1434,5.32283 -4.78742,5.32283 v 0 c -2.64401,0 -4.78738,-2.38312 -4.78738,-5.32283 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path397" d="m 352.3517,228.38058 v 0 c 0,-2.93973 2.14337,-5.32285 4.78738,-5.32285 v 0 c 1.26972,0 2.4874,0.56081 3.38523,1.55903 0.89779,0.99823 1.40219,2.35211 1.40219,3.76382 v 0 c 0,2.93971 -2.1434,5.32283 -4.78742,5.32283 v 0 c -2.64401,0 -4.78738,-2.38312 -4.78738,-5.32283 z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path399" d="m 358.13217,216.9798 h 89.19684 v 14.07873 h -89.19684 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path403" d="m 233.39021,234.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path405" d="m 233.39021,234.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path407" d="m 145.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path409" d="m 145.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path411" d="m 161.39021,242.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path413" d="m 161.39021,242.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <text xml:space="preserve" style="font-style:normal;font-weight:normal;font-size:40px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1" x="18.34626" y="141.43674" id="text1436"><tspan sodipodi:role="line" id="tspan1434" x="18.34626" y="141.43674" style="font-size:16px">树</tspan></text>
  <text id="text1440" y="162.25957" x="17.209122" style="font-style:normal;font-weight:normal;font-size:40px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1" xml:space="preserve"><tspan style="font-size:16px" y="162.25957" x="17.209122" id="tspan1438" sodipodi:role="line">高度</tspan></text>
  <text id="text1444" y="317.6904" x="208.99216" style="font-style:normal;font-weight:normal;font-size:40px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1" xml:space="preserve"><tspan style="font-size:16px" y="317.6904" x="208.99216" id="tspan1442" sodipodi:role="line">树宽度</tspan></text>
  <text xml:space="preserve" style="font-style:normal;font-weight:normal;font-size:40px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1" x="365.96228" y="232.28316" id="text1448"><tspan sodipodi:role="line" id="tspan1446" x="365.96228" y="232.28316" style="font-size:13.33333302px">形心</tspan></text>
  <text id="text1498" y="256.28314" x="365.96228" style="font-style:normal;font-weight:normal;font-size:40px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1" xml:space="preserve"><tspan style="font-size:13.33333302px" y="256.28314" x="365.96228" id="tspan1496" sodipodi:role="line">聚类 1</tspan></text>
  <text xml:space="preserve" style="font-style:normal;font-weight:normal;font-size:40px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1" x="365.96228" y="284.28314" id="text1502"><tspan sodipodi:role="line" id="tspan1500" x="365.96228" y="284.28314" style="font-size:13.33333302px">聚类 2</tspan></text>
</sodipodi:namedview></defs></svg>

</p>

<p>之后，研究人员便可查看这些聚类并进行其他操作，例如，将聚类 1 标记为“矮型树”，将聚类 2 标记为“全尺寸树”。</p>
<p>再举一个例子，例如基于样本与中心点距离的聚类算法，如下所示：</p>
<p>
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:cc="http://creativecommons.org/ns#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" xmlns="http://www.w3.org/2000/svg" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" version="1.1" viewbox="0.0 0.0 353.6010498687664 238.3254593175853" fill="none" stroke="none" stroke-linecap="square" stroke-miterlimit="10" id="svg539" height="300px" sodipodi:docname="RingCluster.svg" inkscape:version="0.92.2 5c3e80d, 2017-08-06">
  <metadata id="metadata545">
    <rdf:rdf>
      <cc:work rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type rdf:resource="http://purl.org/dc/dcmitype/StillImage">
        <dc:title/>
      </dc:type></cc:work>
    </rdf:rdf>
  </metadata>
  <defs id="defs543">
  <sodipodi:namedview pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" objecttolerance="10" gridtolerance="10" guidetolerance="10" inkscape:pageopacity="0" inkscape:pageshadow="2" inkscape:window-width="1106" inkscape:window-height="741" id="namedview541" showgrid="false" inkscape:zoom="0.91628689" inkscape:cx="176.80052" inkscape:cy="119.16273" inkscape:window-x="0" inkscape:window-y="0" inkscape:window-maximized="0" inkscape:current-layer="svg539">
  <desc id="desc2">Three sets of examples, each somewhat further from the center.</desc>
  <clippath id="p.0">
    <path d="m0 0l353.60104 0l0 238.32545l-353.60104 0l0 -238.32545z" clip-rule="nonzero" id="path4"/>
  </clippath>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path7" d="M 0,0 H 353.60104 V 238.32545 H 0 Z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path9" d="m 281.3902,186.1856 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path11" d="m 281.3902,186.1856 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path13" d="m 281.3902,212.83125 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path15" d="m 281.3902,212.83125 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path17" d="m 286.13217,179.7917 h 89.19684 v 14.07873 h -89.19684 z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path21" d="m 286.13217,206.8044 h 89.19684 v 14.07875 h -89.19684 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path25" d="m 97.390205,43.245956 h 8.535435 v 10.645668 h -8.535435 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path27" d="m 97.390205,43.245956 h 8.535435 v 10.645668 h -8.535435 z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path29" d="m 286.13217,155.7917 h 89.19684 v 14.07873 h -89.19684 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path33" d="m 281.3902,162.1856 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path35" d="m 281.3902,162.1856 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path37" d="m 84.46938,74.452866 h 8.535439 V 85.098534 H 84.46938 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path39" d="m 84.46938,74.452866 h 8.535439 V 85.098534 H 84.46938 Z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path41" d="m 105.3902,106.18559 h 8.53544 v 10.64567 h -8.53544 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path43" d="m 105.3902,106.18559 h 8.53544 v 10.64567 h -8.53544 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path45" d="m 121.3902,114.18559 h 8.53544 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path47" d="m 121.3902,114.18559 h 8.53544 v 10.64567 h -8.53543 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path49" d="m 137.39021,130.1856 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path51" d="m 137.39021,130.1856 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path53" d="m 121.3902,82.185585 h 8.53544 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path55" d="m 121.3902,82.185585 h 8.53544 v 10.645676 h -8.53543 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path57" d="m 137.39021,106.18559 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path59" d="m 137.39021,106.18559 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path61" d="m 81.390205,122.18559 h 8.535431 v 10.64566 h -8.535431 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path63" d="m 81.390205,122.18559 h 8.535431 v 10.64566 h -8.535431 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path65" d="m 97.390205,138.1856 h 8.535435 v 10.64566 h -8.535435 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path67" d="m 97.390205,138.1856 h 8.535435 v 10.64566 h -8.535435 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path69" d="m 105.3902,122.18559 h 8.53544 v 10.64566 h -8.53544 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path71" d="m 105.3902,122.18559 h 8.53544 v 10.64566 h -8.53544 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path73" d="m 121.3902,98.185585 h 8.53544 v 10.645675 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path75" d="m 121.3902,98.185585 h 8.53544 v 10.645675 h -8.53543 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path77" d="m 121.3902,138.1856 h 8.53544 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path79" d="m 121.3902,138.1856 h 8.53544 v 10.64566 h -8.53543 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path81" d="m 153.39021,122.18559 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path83" d="m 153.39021,122.18559 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path85" d="m 145.39021,90.185585 h 8.53543 v 10.645675 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path87" d="m 145.39021,90.185585 h 8.53543 v 10.645675 h -8.53543 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path89" d="m 161.39021,106.18559 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path91" d="m 161.39021,106.18559 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path93" d="m 81.390205,103.10641 h 8.535431 v 10.64567 h -8.535431 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path95" d="m 81.390205,103.10641 h 8.535431 v 10.64567 h -8.535431 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path97" d="m 121.3902,51.245956 h 8.53544 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path99" d="m 121.3902,51.245956 h 8.53544 v 10.645668 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path101" d="m 145.39021,51.245956 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path103" d="m 145.39021,51.245956 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path105" d="m 161.39021,67.24596 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path107" d="m 161.39021,67.24596 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path109" d="m 177.39021,83.24596 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path111" d="m 177.39021,83.24596 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path113" d="m 177.39021,99.24596 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path115" d="m 177.39021,99.24596 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path117" d="m 185.39021,115.24596 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path119" d="m 185.39021,115.24596 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path121" d="m 169.39021,131.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path123" d="m 169.39021,131.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path125" d="m 185.39021,123.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path127" d="m 185.39021,123.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path129" d="m 177.39021,139.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path131" d="m 177.39021,139.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path133" d="m 169.39021,155.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path135" d="m 169.39021,155.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path137" d="m 145.39021,171.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path139" d="m 145.39021,171.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path141" d="m 121.3902,171.24596 h 8.53544 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path143" d="m 121.3902,171.24596 h 8.53544 v 10.64568 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path145" d="m 137.39021,163.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path147" d="m 137.39021,163.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path149" d="m 97.390205,171.24596 h 8.535435 v 10.64568 h -8.535435 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path151" d="m 97.390205,171.24596 h 8.535435 v 10.64568 h -8.535435 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path153" d="m 113.3902,176.86975 h 8.53544 v 10.64568 h -8.53544 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path155" d="m 113.3902,176.86975 h 8.53544 v 10.64568 h -8.53544 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path157" d="m 81.390205,160.86975 h 8.535431 v 10.64568 h -8.535431 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path159" d="m 81.390205,160.86975 h 8.535431 v 10.64568 h -8.535431 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path161" d="m 81.390205,176.86975 h 8.535431 v 10.64568 h -8.535431 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path163" d="m 81.390205,176.86975 h 8.535431 v 10.64568 h -8.535431 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path165" d="m 73.390205,152.86975 h 8.535431 v 10.64568 h -8.535431 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path167" d="m 73.390205,152.86975 h 8.535431 v 10.64568 h -8.535431 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path169" d="m 65.390205,168.86975 h 8.535431 v 10.64568 h -8.535431 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path171" d="m 65.390205,168.86975 h 8.535431 v 10.64568 h -8.535431 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path173" d="m 57.390205,128.86975 h 8.535431 v 10.64568 h -8.535431 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path175" d="m 57.390205,128.86975 h 8.535431 v 10.64568 h -8.535431 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path177" d="m 57.390205,144.86975 h 8.535431 v 10.64568 h -8.535431 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path179" d="m 57.390205,144.86975 h 8.535431 v 10.64568 h -8.535431 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path181" d="m 41.390205,120.86975 h 8.535435 v 10.64568 h -8.535435 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path183" d="m 41.390205,120.86975 h 8.535435 v 10.64568 h -8.535435 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path185" d="m 57.390205,104.86975 h 8.535431 v 10.64567 h -8.535431 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path187" d="m 57.390205,104.86975 h 8.535431 v 10.64567 h -8.535431 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path189" d="m 49.390205,120.86975 h 8.535435 v 10.64568 h -8.535435 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path191" d="m 49.390205,120.86975 h 8.535435 v 10.64568 h -8.535435 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path193" d="M 49.390205,80.86975 H 57.92564 V 91.515418 H 49.390205 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path195" d="M 49.390205,80.86975 H 57.92564 V 91.515418 H 49.390205 Z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path197" d="m 65.390205,64.86975 h 8.535431 v 10.645668 h -8.535431 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path199" d="m 65.390205,64.86975 h 8.535431 v 10.645668 h -8.535431 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path201" d="m 81.390205,56.86975 h 8.535431 v 10.645668 h -8.535431 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path203" d="m 81.390205,56.86975 h 8.535431 v 10.645668 h -8.535431 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path205" d="m 97.390205,56.86975 h 8.535435 v 10.645668 h -8.535435 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path207" d="m 97.390205,56.86975 h 8.535435 v 10.645668 h -8.535435 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path209" d="m 41.390205,104.86975 h 8.535435 v 10.64567 h -8.535435 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path211" d="m 41.390205,104.86975 h 8.535435 v 10.64567 h -8.535435 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path213" d="m 161.39021,160.86975 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path215" d="m 161.39021,160.86975 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path217" d="m 145.39021,144.86975 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path219" d="m 145.39021,144.86975 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path221" d="m 105.3902,66.185585 h 8.53544 v 10.645676 h -8.53544 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path223" d="m 105.3902,66.185585 h 8.53544 v 10.645676 h -8.53544 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path225" d="m 121.3902,66.185585 h 8.53544 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path227" d="m 121.3902,66.185585 h 8.53544 v 10.645676 h -8.53543 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path229" d="m 137.39021,66.185585 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path231" d="m 137.39021,66.185585 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path233" d="m 97.390205,82.185585 h 8.535435 v 10.645676 h -8.535435 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path235" d="m 97.390205,82.185585 h 8.535435 v 10.645676 h -8.535435 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path237" d="m 89.390205,98.185585 h 8.535431 v 10.645675 h -8.535431 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path239" d="m 89.390205,98.185585 h 8.535431 v 10.645675 h -8.535431 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path241" d="m 129.39021,146.1856 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path243" d="m 129.39021,146.1856 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path245" d="m 105.3902,146.1856 h 8.53544 v 10.64566 h -8.53544 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path247" d="m 105.3902,146.1856 h 8.53544 v 10.64566 h -8.53544 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path249" d="m 153.39021,90.185585 h 8.53543 v 10.645675 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path251" d="m 153.39021,90.185585 h 8.53543 v 10.645675 h -8.53543 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path253" d="m 137.39021,82.185585 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path255" d="m 137.39021,82.185585 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path257" d="m 153.39021,98.185585 h 8.53543 v 10.645675 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path259" d="m 153.39021,98.185585 h 8.53543 v 10.645675 h -8.53543 z"/>
  <path style="fill:#f4cccc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path261" d="m 65.390205,119.10641 h 8.535431 v 10.64567 h -8.535431 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path263" d="m 65.390205,119.10641 h 8.535431 v 10.64567 h -8.535431 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path265" d="m 150.00438,11.245955 h 8.53543 v 10.645669 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path267" d="m 150.00438,11.245955 h 8.53543 v 10.645669 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path269" d="m 166.00438,27.245955 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path271" d="m 166.00438,27.245955 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path273" d="m 126.00438,15.998363 h 8.53543 v 10.645669 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path275" d="m 126.00438,15.998363 h 8.53543 v 10.645669 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path277" d="m 109.60805,35.245956 h 8.53544 v 10.645668 h -8.53544 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path279" d="m 109.60805,35.245956 h 8.53544 v 10.645668 h -8.53544 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path281" d="m 78.00438,11.245955 h 8.535431 V 21.891624 H 78.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path283" d="m 78.00438,11.245955 h 8.535431 V 21.891624 H 78.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path285" d="m 94.00438,11.245955 h 8.53543 v 10.645669 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path287" d="m 94.00438,11.245955 h 8.53543 v 10.645669 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path289" d="m 62.00438,23.731083 h 8.535431 V 34.376751 H 62.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path291" d="m 62.00438,23.731083 h 8.535431 V 34.376751 H 62.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path293" d="m 182.00438,43.245956 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path295" d="m 182.00438,43.245956 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path297" d="m 182.00438,43.245956 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path299" d="m 182.00438,43.245956 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path301" d="m 198.00438,59.245956 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path303" d="m 198.00438,59.245956 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path305" d="m 198.00438,59.245956 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path307" d="m 198.00438,59.245956 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path309" d="m 214.00438,75.24596 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path311" d="m 214.00438,75.24596 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path313" d="m 192.1133,75.24596 h 8.53545 v 10.645668 h -8.53545 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path315" d="m 192.1133,75.24596 h 8.53545 v 10.645668 h -8.53545 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path317" d="m 206.00438,91.24596 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path319" d="m 206.00438,91.24596 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path321" d="m 222.00438,107.24596 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path323" d="m 222.00438,107.24596 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path325" d="m 206.00438,123.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path327" d="m 206.00438,123.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path329" d="m 206.00438,107.24596 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path331" d="m 206.00438,107.24596 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path333" d="m 222.00438,123.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path335" d="m 222.00438,123.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path337" d="m 214.00438,139.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path339" d="m 214.00438,139.24596 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path341" d="m 198.00438,148.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path343" d="m 198.00438,148.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path345" d="m 198.00438,164.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path347" d="m 198.00438,164.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path349" d="m 182.00438,164.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path351" d="m 182.00438,164.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path353" d="m 182.00438,180.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path355" d="m 182.00438,180.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path357" d="m 166.00438,188.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path359" d="m 166.00438,188.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path361" d="m 142.00438,188.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path363" d="m 142.00438,188.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path365" d="m 134.00438,204.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path367" d="m 134.00438,204.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path369" d="m 118.00438,196.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path371" d="m 118.00438,196.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path373" d="m 102.00438,204.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path375" d="m 102.00438,204.91919 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path377" d="m 86.00438,196.91919 h 8.535431 v 10.64566 H 86.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path379" d="m 86.00438,196.91919 h 8.535431 v 10.64566 H 86.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path381" d="m 70.00438,188.91919 h 8.535431 v 10.64566 H 70.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path383" d="m 70.00438,188.91919 h 8.535431 v 10.64566 H 70.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path385" d="m 46.00438,180.91919 h 8.535435 v 10.64566 H 46.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path387" d="m 46.00438,180.91919 h 8.535435 v 10.64566 H 46.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path389" d="m 62.00438,180.91919 h 8.535431 v 10.64566 H 62.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path391" d="m 62.00438,180.91919 h 8.535431 v 10.64566 H 62.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path393" d="m 46.00438,164.91919 h 8.535435 v 10.64566 H 46.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path395" d="m 46.00438,164.91919 h 8.535435 v 10.64566 H 46.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path397" d="m 38.00438,172.91919 h 8.535435 v 10.64566 H 38.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path399" d="m 38.00438,172.91919 h 8.535435 v 10.64566 H 38.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path401" d="m 30.00438,156.91919 h 8.535435 v 10.64566 H 30.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path403" d="m 30.00438,156.91919 h 8.535435 v 10.64566 H 30.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path405" d="m 14.00438,140.91919 h 8.535434 v 10.64566 H 14.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path407" d="m 14.00438,140.91919 h 8.535434 v 10.64566 H 14.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path409" d="m 30.00438,140.91919 h 8.535435 v 10.64566 H 30.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path411" d="m 30.00438,140.91919 h 8.535435 v 10.64566 H 30.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path413" d="m 22.00438,124.91918 h 8.535435 v 10.64567 H 22.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path415" d="m 22.00438,124.91918 h 8.535435 v 10.64567 H 22.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path417" d="m 6.00438,108.91918 h 8.535433 v 10.64568 H 6.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path419" d="m 6.00438,108.91918 h 8.535433 v 10.64568 H 6.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path421" d="m 22.00438,108.91918 h 8.535435 v 10.64568 H 22.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path423" d="m 22.00438,108.91918 h 8.535435 v 10.64568 H 22.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path425" d="m 22.00438,84.91918 h 8.535435 V 95.564856 H 22.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path427" d="m 22.00438,84.91918 h 8.535435 V 95.564856 H 22.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path429" d="m 14.00438,92.91918 h 8.535434 v 10.64568 H 14.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path431" d="m 14.00438,92.91918 h 8.535434 v 10.64568 H 14.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path433" d="m 30.00438,76.91918 h 8.535435 V 87.564856 H 30.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path435" d="m 30.00438,76.91918 h 8.535435 V 87.564856 H 30.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path437" d="m 22.00438,60.919186 h 8.535435 V 71.564858 H 22.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path439" d="m 22.00438,60.919186 h 8.535435 V 71.564858 H 22.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path441" d="m 14.00438,76.91918 h 8.535434 V 87.564856 H 14.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path443" d="m 14.00438,76.91918 h 8.535434 V 87.564856 H 14.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path445" d="m 22.00438,41.404312 h 8.535435 V 52.04998 H 22.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path447" d="m 22.00438,41.404312 h 8.535435 V 52.04998 H 22.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path449" d="m 46.00438,44.919186 h 8.535435 V 55.564854 H 46.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path451" d="m 46.00438,44.919186 h 8.535435 V 55.564854 H 46.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path453" d="m 54.00438,36.919186 h 8.535435 V 47.564854 H 54.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path455" d="m 54.00438,36.919186 h 8.535435 V 47.564854 H 54.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path457" d="m 70.00438,28.919184 h 8.535431 v 10.64567 H 70.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path459" d="m 70.00438,28.919184 h 8.535431 v 10.64567 H 70.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path461" d="m 46.00438,28.919184 h 8.535435 v 10.64567 H 46.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path463" d="m 46.00438,28.919184 h 8.535435 v 10.64567 H 46.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path465" d="m 30.00438,44.919186 h 8.535435 V 55.564854 H 30.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path467" d="m 30.00438,44.919186 h 8.535435 V 55.564854 H 30.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path469" d="m 86.00438,25.404312 h 8.535431 V 36.04998 H 86.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path471" d="m 86.00438,25.404312 h 8.535431 V 36.04998 H 86.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path473" d="m 102.00438,17.404312 h 8.53543 V 28.04998 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path475" d="m 102.00438,17.404312 h 8.53543 V 28.04998 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path477" d="m 118.00438,9.404312 h 8.53543 V 20.04998 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path479" d="m 118.00438,9.404312 h 8.53543 V 20.04998 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path481" d="m 134.00438,17.404312 h 8.53543 V 28.04998 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path483" d="m 134.00438,17.404312 h 8.53543 V 28.04998 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path485" d="m 118.00438,25.404312 h 8.53543 V 36.04998 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path487" d="m 118.00438,25.404312 h 8.53543 V 36.04998 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path489" d="m 169.51926,41.404312 h 8.53543 V 52.04998 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path491" d="m 169.51926,41.404312 h 8.53543 V 52.04998 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path493" d="m 177.39021,83.24596 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path495" d="m 177.39021,83.24596 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path497" d="m 161.39021,59.245956 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path499" d="m 161.39021,59.245956 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path501" d="m 63.281284,84.38462 h 8.535435 v 10.645676 h -8.535435 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path503" d="m 63.281284,84.38462 h 8.535435 v 10.645676 h -8.535435 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path505" d="M 49.390205,64.86975 H 57.92564 V 75.515418 H 49.390205 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path507" d="M 49.390205,64.86975 H 57.92564 V 75.515418 H 49.390205 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path509" d="m 38.00438,57.404312 h 8.535435 V 68.04998 H 38.00438 Z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path511" d="m 38.00438,57.404312 h 8.535435 V 68.04998 H 38.00438 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path513" d="m 201.51926,73.40431 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path515" d="m 201.51926,73.40431 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path517" d="m 201.51926,73.40431 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path519" d="m 201.51926,73.40431 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path521" d="m 142.00438,31.998363 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path523" d="m 142.00438,31.998363 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path525" d="m 134.57831,43.245956 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path527" d="m 134.57831,43.245956 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path529" d="m 153.39021,59.245956 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path531" d="m 153.39021,59.245956 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path533" d="m 169.39021,75.24596 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path535" d="m 169.39021,75.24596 h 8.53543 v 10.645668 h -8.53543 z"/>
  <text xml:space="preserve" style="font-style:normal;font-weight:normal;font-size:40px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1" x="294.4848" y="171.75241" id="text1354"><tspan sodipodi:role="line" id="tspan1352" x="294.4848" y="171.75241" style="font-size:13.33333302px">聚类 1</tspan></text>
  <text id="text1623" y="195.75241" x="294.4848" style="font-style:normal;font-weight:normal;font-size:40px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1" xml:space="preserve"><tspan style="font-size:13.33333302px" y="195.75241" x="294.4848" id="tspan1621" sodipodi:role="line">聚类 2</tspan></text>
  <text xml:space="preserve" style="font-style:normal;font-weight:normal;font-size:40px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1" x="294.4848" y="221.75241" id="text1627"><tspan sodipodi:role="line" id="tspan1625" x="294.4848" y="221.75241" style="font-size:13.33333302px">聚类 3</tspan></text>
</sodipodi:namedview></defs></svg>

</p>

<p><a name="collaborative_filtering"></a>
</p><h2 class="hide-from-toc">协同过滤 (collaborative filtering)</h2><p></p>
<p>根据很多其他用户的兴趣来预测某位用户的兴趣。协同过滤通常用在推荐系统中。</p>
<p><a name="confusion_matrix"></a>
</p><h2 class="hide-from-toc">混淆矩阵 (confusion matrix)</h2><p></p>
<p>一种 NxN 表格，用于总结<a href="#classification_model"><strong>分类模型</strong></a>的预测效果；即标签和模型预测的分类之间的关联。在混淆矩阵中，一个轴表示模型预测的标签，另一个轴表示实际标签。N 表示类别个数。在<a href="#binary_classification"><strong>二元分类</strong></a>问题中，N=2。例如，下面显示了一个二元分类问题的混淆矩阵示例：</p>
<table>
<thead>
<tr>
<th></th>
<th>肿瘤（预测的标签）</th>
<th>非肿瘤（预测的标签）</th>
</tr>
</thead>
<tbody>
<tr>
<td>肿瘤（实际标签）</td>
<td>18</td>
<td>1</td>
</tr>
<tr>
<td>非肿瘤（实际标签）</td>
<td>6</td>
<td>452</td>
</tr>
</tbody>
</table>
<p>上面的混淆矩阵显示，在 19 个实际有肿瘤的样本中，该模型正确地将 18 个归类为有肿瘤（18 个正例），错误地将 1 个归类为没有肿瘤（1 个假负例）。同样，在 458 个实际没有肿瘤的样本中，模型归类正确的有 452 个（452 个负例），归类错误的有 6 个（6 个假正例）。</p>
<p>多类别分类问题的混淆矩阵有助于确定出错模式。例如，某个混淆矩阵可以揭示，某个经过训练以识别手写数字的模型往往会将 4 错误地预测为 9，将 7 错误地预测为 1。</p>
<p>混淆矩阵包含计算各种效果指标（包括<a href="#precision"><strong>精确率</strong></a>和<a href="#recall"><strong>召回率</strong></a>）所需的充足信息。</p>
<p><a name="continuous_feature"></a>
</p><h2 class="hide-from-toc">连续特征 (continuous feature)</h2><p></p>
<p>一种浮点特征，可能值的区间不受限制。与<a href="#discrete_feature"><strong>离散特征</strong></a>相对。</p>
<p><a name="convergence"></a>
</p><h2 class="hide-from-toc">收敛 (convergence)</h2><p></p>
<p>通俗来说，收敛通常是指在训练期间达到的一种状态，即经过一定次数的迭代之后，训练<a href="#loss"><strong>损失</strong></a>和验证损失在每次迭代中的变化都非常小或根本没有变化。也就是说，如果采用当前数据进行额外的训练将无法改进模型，模型即达到收敛状态。在深度学习中，损失值有时会在最终下降之前的多次迭代中保持不变或几乎保持不变，暂时形成收敛的假象。</p>
<p>另请参阅<a href="#early_stopping"><strong>早停法</strong></a>。</p>
<p>另请参阅 Boyd 和 Vandenberghe 合著的 <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf" target="_blank" rel="noopener">Convex Optimization</a>（《凸优化》）。</p>
<p><a name="convex_function"></a>
</p><h2 class="hide-from-toc">凸函数 (convex function)</h2><p></p>
<p>一种函数，函数图像以上的区域为<a href="#convex_set"><strong>凸集</strong></a>。典型凸函数的形状类似于字母 <strong>U</strong>。例如，以下都是凸函数：</p>
<p>
<img src="https://developers.google.com/machine-learning/glossary/images/convex_functions.png?hl=zh-cn" height="300" alt="典型凸函数的形状类似于字母 U。">
</p>

<p>相反，以下函数则不是凸函数。请注意图像上方的区域如何不是凸集：</p>
<p>
<svg xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:cc="http://creativecommons.org/ns#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 379.11578 299.19577" height="299.19577" width="379.11578" xml:space="preserve" id="svg2" version="1.1">
  <title>非凸函数</title>
  <desc>非凸函数。</desc>
  <defs id="defs6"><clippath id="clipPath20" clippathunits="userSpaceOnUse"><path id="path18" d="M 0,0 H 365760 V 274320 H 0 Z"/></clippath></defs><g transform="matrix(1.3333333,0,0,-1.3333333,-345.28663,532.80533)" id="g10"><path id="path28" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 0,540 H 719.99856 V 0.00108 H 0 Z"/><path id="path36" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="m 278.33212,392.51998 c 12.48685,-27.90513 53.26204,-155.55219 74.92111,-167.43077 21.65907,-11.87858 37.99042,86.40436 55.03336,96.15926 17.04293,9.7549 33.05274,-49.79059 47.22431,-37.62985 14.17157,12.16073 31.50421,92.1619 37.80504,110.59427"/><path id="path38" style="fill:none;stroke:#4285f4;stroke-width:1.49999702;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="m 278.33212,392.51998 c 12.48685,-27.90513 53.26204,-155.55219 74.92111,-167.43077 21.65907,-11.87858 37.99042,86.40436 55.03336,96.15926 17.04293,9.7549 33.05274,-49.79059 47.22431,-37.62985 14.17157,12.16073 31.50421,92.1619 37.80504,110.59427"/><path id="path46" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="m 482.16811,278.04383 h 61.13372 v -37.91331 h -61.13372 z"/><text y="-255.73386" x="488.91809" id="text56" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)">局部最低点</text>
<path id="path70" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="m 452.82959,279.16588 29.33852,-20.0787"/><path id="path72" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="m 459.19298,274.8109 22.97513,-15.72372"/><path id="path74" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1" d="m 459.19298,274.8109 3.51718,0.65906 -7.16613,1.8382 4.308,-6.01443 z"/><path id="path82" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="m 264.28807,179.58871 v 220.0153"/><path id="path84" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 264.28807,179.58871 V 391.89309"/><path id="path86" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1" d="m 264.28807,391.89309 2.53033,-2.53033 -2.53033,6.95198 -2.53033,-6.95198 z"/><path id="path94" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 263.27841,180.53025 H 543.2936"/><path id="path96" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 263.27841,180.53025 H 535.58265"/><path id="path98" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1" d="m 535.58265,180.53025 -2.53033,-2.53031 6.95195,2.53031 -6.95195,2.53031 z"/><path id="path106" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 382.99136,219.87466 H 444.1251 V 181.96135 H 382.99136 Z"/><text y="-202.56468" x="389.74133" id="text116" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)">局部最低点</text>
<path id="path130" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 358.85873,221.41495 388.19726,205.6827"/><path id="path132" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 365.65429,217.77094 388.19726,205.6827"/><path id="path134" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1" d="m 365.65429,217.77094 3.42568,1.03417 -7.32245,1.05539 4.93094,-5.51524 z"/><path id="path142" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 280.99156,219.87466 H 342.1253 V 181.96135 H 280.99156 Z"/><text y="-202.56468" x="269.74155" id="text152" style="font-variant:normal;font-weight:normal;font-size:10.99997807px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)">全局最低点</text>
<path id="path166" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 354.92356,221.42398 324.80551,206.63661"/><path id="path168" style="fill:none;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 348.00192,218.02558 324.80551,206.63661"/><path id="path170" style="fill:#424242;fill-opacity:1;fill-rule:evenodd;stroke:#424242;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:11.47371292;stroke-dasharray:none;stroke-opacity:1" d="m 348.00192,218.02558 -1.15616,-3.38646 5.12521,5.33518 -7.35556,-0.79256 z"/></g></svg>

</p>

<p><strong>严格凸函数</strong>只有一个局部最低点，该点也是全局最低点。经典的 U 形函数都是严格凸函数。不过，有些凸函数（例如直线）则不是这样。</p>
<p>很多常见的<a href="#loss_functions"><strong>损失函数</strong></a>（包括下列函数）都是凸函数：</p>
<ul>
<li><a href="#L2_loss"><strong>L<sub>2</sub> 损失函数</strong></a></li>
<li><a href="#Log_Loss"><strong>对数损失函数</strong></a></li>
<li><a href="#L1_regularization"><strong>L<sub>1</sub> 正则化</strong></a></li>
<li><a href="#L2_regularization"><strong>L<sub>2</sub> 正则化</strong></a></li>
</ul>
<p><a href="#gradient_descent"><strong>梯度下降法</strong></a>的很多变体都一定能找到一个接近严格凸函数最小值的点。同样，<a href="#SGD"><strong>随机梯度下降法</strong></a>的很多变体都有很高的可能性能够找到接近严格凸函数最小值的点（但并非一定能找到）。</p>
<p>两个凸函数的和（例如 L<sub>2</sub> 损失函数 + L<sub>1</sub> 正则化）也是凸函数。</p>
<p><a href="#deep_model"><strong>深度模型</strong></a>绝不会是凸函数。值得注意的是，专门针对<a href="#convex_optimization"><strong>凸优化</strong></a>设计的算法往往总能在深度网络上找到非常好的解决方案，虽然这些解决方案并不一定对应于全局最小值。</p>
<p><a name="convex_optimization"></a>
</p><h2 class="hide-from-toc">凸优化 (convex optimization)</h2><p></p>
<p>使用数学方法（例如<a href="#gradient_descent"><strong>梯度下降法</strong></a>）寻找<a href="#convex_function"><strong>凸函数</strong></a>最小值的过程。机器学习方面的大量研究都是专注于如何通过公式将各种问题表示成凸优化问题，以及如何更高效地解决这些问题。</p>
<p>如需完整的详细信息，请参阅 Boyd 和 Vandenberghe 合著的 <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf" target="_blank" rel="noopener">Convex Optimization</a>（《凸优化》）。</p>
<p><a name="convex_set"></a>
</p><h2 class="hide-from-toc">凸集 (convex set)</h2><p></p>
<p>欧几里得空间的一个子集，其中任意两点之间的连线仍完全落在该子集内。例如，下面的两个图形都是凸集：</p>
<p>
<img src="https://developers.google.com/machine-learning/glossary/images/convex_set.png?hl=zh-cn" alt="矩形和半椭圆形都是凸集。">
</p>

<p>相反，下面的两个图形都不是凸集：</p>
<p>
<img src="https://developers.google.com/machine-learning/glossary/images/nonconvex_set.png?hl=zh-cn" alt="缺少一块的饼图以及烟花图都是非凸集。">
</p>

<p><a name="convolution"></a>
</p><h2 class="hide-from-toc">卷积 (convolution)</h2><p></p>
<p>简单来说，卷积在数学中指两个函数的组合。在机器学习中，卷积结合使用卷积过滤器和输入矩阵来训练权重。</p>
<p>机器学习中的“卷积”一词通常是<a href="#convolutional_operation"><strong>卷积运算</strong></a>或<a href="#convolutional_layer"><strong>卷积层</strong></a>的简称。</p>
<p>如果没有卷积，机器学习算法就需要学习大张量中每个单元格各自的权重。例如，用 2K x 2K 图像训练的机器学习算法将被迫找出 400 万个单独的权重。而使用卷积，机器学习算法只需在<a href="#convolutional_filter"><strong>卷积过滤器</strong></a>中找出每个单元格的权重，大大减少了训练模型所需的内存。在应用卷积过滤器后，它只需跨单元格进行复制，每个单元格都会与过滤器相乘。</p>
<p><a name="convolutional_filter"></a>
</p><h2 class="hide-from-toc">卷积过滤器 (convolutional filter)</h2><p></p>
<p><a href="#convolutional_operation"><strong>卷积运算</strong></a>中的两个参与方之一。（另一个参与方是输入矩阵切片。）卷积过滤器是一种矩阵，其<a href="#rank"><strong>等级</strong></a>与输入矩阵相同，但形状小一些。以 28×28 的输入矩阵为例，过滤器可以是小于 28×28 的任何二维矩阵。</p>
<p>在图形操作中，卷积过滤器中的所有单元格通常按照固定模式设置为 1 和 0。在机器学习中，卷积过滤器通常先选择随机数字，然后由网络训练出理想值。</p>
<p><a name="convolutional_layer"></a>
</p><h2 class="hide-from-toc">卷积层 (convolutional layer)</h2><p></p>
<p>深度神经网络的一个层，<a href="#convolutional_filter"><strong>卷积过滤器</strong></a>会在其中传递输入矩阵。以下面的 3x3 <a href="#convolutional_filter"><strong>卷积过滤器</strong></a>为例：</p>
<p>

</p>

<p>下面的动画显示了一个由 9 个卷积运算（涉及 5x5 输入矩阵）组成的卷积层。请注意，每个卷积运算都涉及一个不同的 3x3 输入矩阵切片。由此产生的 3×3 矩阵（右侧）就包含 9 个卷积运算的结果：</p>
<p>
<img src="https://developers.google.com/machine-learning/glossary/images/AnimatedConvolution.gif?hl=zh-cn">
</p>

<p><a name="convolutional_neural_network"></a>
</p><h2 class="hide-from-toc">卷积神经网络 (convolutional neural network)</h2><p></p>
<p>一种神经网络，其中至少有一层为<a href="#convolutional_layer"><strong>卷积层</strong></a>。典型的卷积神经网络包含以下几层的组合：</p>
<ul>
<li>卷积层</li>
<li>池化层</li>
<li>密集层</li>
</ul>
<p>卷积神经网络在解决某些类型的问题（如图像识别）上取得了巨大成功。</p>
<p><a name="convolutional_operation"></a>
</p><h2 class="hide-from-toc">卷积运算 (convolutional operation)</h2><p></p>
<p>如下所示的两步数学运算：</p>
<ol>
<li>对<a href="#convolutional_filter"><strong>卷积过滤器</strong></a>和输入矩阵切片执行元素级乘法。（输入矩阵切片与卷积过滤器具有相同的等级和大小。）</li>
<li>对生成的积矩阵中的所有值求和。</li>
</ol>
<p>以下面的 5x5 输入矩阵为例：</p>
<p>

</p>

<p>现在，以下面这个 2x2 卷积过滤器为例：</p>
<p>

</p>

<p>每个卷积运算都涉及一个 2x2 输入矩阵切片。例如，假设我们使用输入矩阵左上角的 2x2 切片。这样一来，对此切片进行卷积运算将如下所示：</p>
<p>

</p>

<p><a href="#convolutional_layer"><strong>卷积层</strong></a>由一系列卷积运算组成，每个卷积运算都针对不同的输入矩阵切片。</p>
<p><a name="cost"></a>
</p><h2 class="hide-from-toc">成本 (cost)</h2><p></p>
<p>与<a href="#loss"><strong>损失</strong></a>的含义相同。</p>
<p><a name="cross-entropy"></a>
</p><h2 class="hide-from-toc">交叉熵 (cross-entropy)</h2><p></p>
<p><a href="#Log_Loss"><strong>对数损失函数</strong></a>向<a href="#multi-class"><strong>多类别分类问题</strong></a>的一种泛化。交叉熵可以量化两种概率分布之间的差异。另请参阅<a href="#perplexity"><strong>困惑度</strong></a>。</p>
<p><a name="custom_estimator"></a>
</p><h2 class="hide-from-toc">自定义 Estimator (custom Estimator)</h2><p></p>
<p>您按照<a href="https://www.tensorflow.org/extend/estimators?hl=zh-cn" target="_blank" rel="noopener">这些说明</a>自行编写的 <a href="#Estimators"><strong>Estimator</strong></a>。</p>
<p>与<a href="#pre-made_Estimator"><strong>预创建的 Estimator</strong></a> 相对。</p>
<h2 class="glossary">D</h2>

<p><a name="data_analysis"></a>
</p><h2 class="hide-from-toc">数据分析 (data analysis)</h2><p></p>
<p>根据样本、测量结果和可视化内容来理解数据。数据分析在首次收到数据集、构建第一个模型之前特别有用。此外，数据分析在理解实验和调试系统问题方面也至关重要。</p>
<p><a name="DataFrame"></a>
</p><h2 class="hide-from-toc">DataFrame</h2><p></p>
<p>一种热门的数据类型，用于表示 Pandas 中的数据集。DataFrame 类似于表格。DataFrame 的每一列都有一个名称（标题），每一行都由一个数字标识。</p>
<p><a name="data_set"></a>
</p><h2 class="hide-from-toc">数据集 (data set)</h2><p></p>
<p>一组<a href="#example"><strong>样本</strong></a>的集合。</p>
<p><a name="dataset_API"></a>
</p><h2 class="hide-from-toc">Dataset API (tf.data)</h2><p></p>
<p>一种高级别的 TensorFlow API，用于读取数据并将其转换为机器学习算法所需的格式。<code>tf.data.Dataset</code> 对象表示一系列元素，其中每个元素都包含一个或多个<a href="#tensor"><strong>张量</strong></a>。<code>tf.data.Iterator</code> 对象可获取 <code>Dataset</code> 中的元素。</p>
<p>如需详细了解 Dataset API，请参阅《TensorFlow 编程人员指南》中的<a href="https://www.tensorflow.org/programmers_guide/datasets?hl=zh-cn" target="_blank" rel="noopener">导入数据</a>。</p>
<p><a name="decision_boundary"></a>
</p><h2 class="hide-from-toc">决策边界 (decision boundary)</h2><p></p>
<p>在<a href="#binary_classification"><strong>二元分类</strong></a>或<a href="#multi-class"><strong>多类别分类问题</strong></a>中，模型学到的类别之间的分界线。例如，在以下表示某个二元分类问题的图片中，决策边界是橙色类别和蓝色类别之间的分界线：</p>
<p>
<img src="https://developers.google.com/machine-learning/glossary/images/decision_boundary.png?hl=zh-cn" alt="两种类别之间明确定义的边界。">
</p>

<p><a name="dense_layer"></a>
</p><h2 class="hide-from-toc">密集层 (dense layer)</h2><p></p>
<p>与<a href="#fully_connected_layer"><strong>全连接层</strong></a>的含义相同。</p>
<p><a name="deep_model"></a>
</p><h2 class="hide-from-toc">深度模型 (deep model)</h2><p></p>
<p>一种<a href="#neural_network"><strong>神经网络</strong></a>，其中包含多个<a href="#hidden_layer"><strong>隐藏层</strong></a>。深度模型依赖于可训练的非线性关系。</p>
<p>与<a href="#wide_model"><strong>宽度模型</strong></a>相对。</p>

<p></p><p><a name="dense_feature"></a>
</p><h2 class="hide-from-toc">密集特征 (dense feature)</h2><p></p>
<p>一种大部分值是非零值的<a href="#feature"><strong>特征</strong></a>，通常是浮点值<a href="#tensor"><strong>张量</strong></a>。与<a href="#sparse_features"><strong>稀疏特征</strong></a>相对。</p>
<p><a name="device"></a>
</p><h2 class="hide-from-toc">设备 (device)</h2><p></p>
<p>一类可运行 TensorFlow 会话的硬件，包括 CPU、GPU 和 TPU。</p>
<p><a name="discrete_feature"></a>
</p><h2 class="hide-from-toc">离散特征 (discrete feature)</h2><p></p>
<p>一种<a href="#feature"><strong>特征</strong></a>，包含有限个可能值。例如，某个值只能是“动物”、“蔬菜”或“矿物”的特征便是一个离散特征（或分类特征）。<em></em><em></em><em></em>与<a href="#continuous_feature"><strong>连续特征</strong></a>相对。</p>
<p><a name="dropout_regularization"></a>
</p><h2 class="hide-from-toc">丢弃正则化 (dropout regularization)</h2><p></p>
<p><a href="#regularization"><strong>正则化</strong></a>的一种形式，在训练<a href="#neural_network"><strong>神经网络</strong></a>方面非常有用。丢弃正则化的运作机制是，在一个梯度步长中移除从神经网络层中随机选择的固定数量的单元。丢弃的单元越多，正则化效果就越强。这类似于训练神经网络以模拟较小网络的指数级规模集成学习。如需完整的详细信息，请参阅 <a href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" target="_blank" rel="noopener">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>（《丢弃：一种防止神经网络过拟合的简单方法》）。</p>
<p><a name="dynamic_model"></a>
</p><h2 class="hide-from-toc">动态模型 (dynamic model)</h2><p></p>
<p>一种<a href="#model"><strong>模型</strong></a>，以持续更新的方式在线接受训练。也就是说，数据会源源不断地进入这种模型。</p>
<h2 class="glossary">E</h2>

<p><a name="early_stopping"></a>
</p><h2 class="hide-from-toc">早停法 (early stopping)</h2><p></p>
<p>一种<a href="#regularization"><strong>正则化</strong></a>方法，是指在训练损失仍可以继续降低之前结束模型训练。<em></em>使用早停法时，您会在<a href="#validation_set"><strong>验证数据集</strong></a>的损失开始增大（也就是<a href="#generalization"><strong>泛化</strong></a>效果变差）时结束模型训练。</p>
<p><a name="embeddings"></a>
</p><h2 class="hide-from-toc">嵌套 (embeddings)</h2><p></p>
<p>一种分类特征，以连续值特征表示。通常，嵌套是指将高维度向量映射到低维度的空间。例如，您可以采用以下两种方式之一来表示英文句子中的单词：</p>
<ul>
<li>表示成包含百万个元素（高维度）的<a href="#sparse_features"><strong>稀疏向量</strong></a>，其中所有元素都是整数。向量中的每个单元格都表示一个单独的英文单词，单元格中的值表示相应单词在句子中出现的次数。由于单个英文句子包含的单词不太可能超过 50 个，因此向量中几乎每个单元格都包含 0。少数非 0 的单元格中将包含一个非常小的整数（通常为 1），该整数表示相应单词在句子中出现的次数。</li>
<li>表示成包含数百个元素（低维度）的<a href="#dense_feature"><strong>密集向量</strong></a>，其中每个元素都存储一个介于 0 到 1 之间的浮点值。这就是一种嵌套。</li>
</ul>
<p>在 TensorFlow 中，会按<a href="#backpropagation"><strong>反向传播</strong></a><a href="#loss"><strong>损失</strong></a>训练嵌套，和训练<a href="#neural_network"><strong>神经网络</strong></a>中的任何其他参数一样。</p>
<p><a name="ERM"></a>
</p><h2 class="hide-from-toc">经验风险最小化 (ERM, empirical risk minimization)</h2><p></p>
<p>用于选择可以将基于训练集的损失降至最低的函数。与<a href="#SRM"><strong>结构风险最小化</strong></a>相对。</p>
<p><a name="ensemble"></a>
</p><h2 class="hide-from-toc">集成学习 (ensemble)</h2><p></p>
<p>多个<a href="#model"><strong>模型</strong></a>的预测结果的并集。您可以通过以下一项或多项来创建集成学习：</p>
<ul>
<li>不同的初始化</li>
<li>不同的<a href="#hyperparameter"><strong>超参数</strong></a></li>
<li>不同的整体结构</li>
</ul>
<p><a href="https://www.tensorflow.org/tutorials/wide_and_deep?hl=zh-cn" target="_blank" rel="noopener">深度模型和宽度模型</a>属于一种集成学习。

</p><p><a name="epoch"></a>
</p><h2 class="hide-from-toc">周期 (epoch)</h2><p></p>
<p>在训练时，整个数据集的一次完整遍历，以便不漏掉任何一个样本。因此，一个周期表示（<code>N</code>/<a href="#batch_size"><strong>批次大小</strong></a>）次训练<a href="#iteration"><strong>迭代</strong></a>，其中 <code>N</code> 是样本总数。</p>
<p><a name="Estimators"></a>
</p><h2 class="hide-from-toc">Estimator</h2><p></p>
<p><code>tf.Estimator</code> 类的一个实例，用于封装负责构建 TensorFlow 图并运行 TensorFlow 会话的逻辑。您可以创建<a href="#custom_estimator"><strong>自定义 Estimator</strong></a>（如需相关介绍，请<a href="https://www.tensorflow.org/extend/estimators?hl=zh-cn" target="_blank" rel="noopener">点击此处</a>），也可以实例化其他人<a href="#pre-made_Estimator"><strong>预创建的 Estimator</strong></a>。</p>
<p><a name="example"></a>
</p><h2 class="hide-from-toc">样本 (example)</h2><p></p>
<p>数据集的一行。一个样本包含一个或多个<a href="#feature"><strong>特征</strong></a>，此外还可能包含一个<a href="#label"><strong>标签</strong></a>。另请参阅<a href="#labeled_example"><strong>有标签样本</strong></a>和<a href="#unlabeled_example"><strong>无标签样本</strong></a>。</p>
<h2 class="glossary">F</h2>

<p><a name="FN"></a>
</p><h2 class="hide-from-toc">假负例 (FN, false negative)</h2><p></p>
<p>被模型错误地预测为<a href="#negative_class"><strong>负类别</strong></a>的样本。例如，模型推断出某封电子邮件不是垃圾邮件（负类别），但该电子邮件其实是垃圾邮件。</p>
<p><a name="false_positive"></a>
</p><h2 class="hide-from-toc">假正例 (FP, false positive)</h2><p></p>
<p>被模型错误地预测为<a href="#positive_class"><strong>正类别</strong></a>的样本。例如，模型推断出某封电子邮件是垃圾邮件（正类别），但该电子邮件其实不是垃圾邮件。</p>
<p><a name="FP_rate"></a>
</p><h2 class="hide-from-toc">假正例率（false positive rate, 简称 FP 率）</h2><p></p>
<p><a href="#ROC"><strong>ROC 曲线</strong></a>中的 x 轴。FP 率的定义如下：</p>
<div>
$$\text{假正例率} = \frac{\text{假正例数}}{\text{假正例数} + \text{负例数}}$$
</div>

<p><a name="feature"></a>
</p><h2 class="hide-from-toc">特征 (feature)</h2><p></p>
<p>在进行<a href="#prediction"><strong>预测</strong></a>时使用的输入变量。</p>
<p><a name="feature_columns"></a>
</p><h2 class="hide-from-toc">特征列 (tf.feature_column)</h2><p></p>
<p>指定模型应该如何解读特定特征的一种函数。此类函数的输出结果是所有 <a href="#Estimators"><strong>Estimators</strong></a> 构造函数的必需参数。</p>
<p>借助 <code>tf.feature_column</code> 函数，模型可对输入特征的不同表示法轻松进行实验。有关详情，请参阅《TensorFlow 编程人员指南》中的<a href="https://www.tensorflow.org/get_started/feature_columns?hl=zh-cn" target="_blank" rel="noopener">特征列</a>一章。</p>
<p>“特征列”是 Google 专用的术语。特征列在 Yahoo/Microsoft 使用的 <a href="https://en.wikipedia.org/wiki/Vowpal_Wabbit" target="_blank" rel="noopener">VW</a> 系统中称为“命名空间”，也称为<a href="https://www.csie.ntu.edu.tw/~cjlin/libffm/" target="_blank" rel="noopener">场</a>。</p>
<p><a name="feature_cross"></a>
</p><h2 class="hide-from-toc">特征组合 (feature cross)</h2><p></p>
<p>通过将单独的特征进行组合（求笛卡尔积）而形成的<a href="#synthetic_feature"><strong>合成特征</strong></a>。特征组合有助于表达非线性关系。</p>
<p><a name="feature_engineering"></a>
</p><h2 class="hide-from-toc">特征工程 (feature engineering)</h2><p></p>
<p>指以下过程：确定哪些<a href="#feature"><strong>特征</strong></a>可能在训练模型方面非常有用，然后将日志文件及其他来源的原始数据转换为所需的特征。在 TensorFlow 中，特征工程通常是指将原始日志文件条目转换为 <a href="#tf.Example"><strong>tf.Example</strong></a> 协议缓冲区。另请参阅 <a href="https://github.com/tensorflow/transform" target="_blank" rel="noopener">tf.Transform</a>。</p>
<p>特征工程有时称为<strong>特征提取</strong>。</p>
<p><a name="feature_set"></a>
</p><h2 class="hide-from-toc">特征集 (feature set)</h2><p></p>
<p>训练机器学习模型时采用的一组<a href="#feature"><strong>特征</strong></a>。例如，对于某个用于预测房价的模型，邮政编码、房屋面积以及房屋状况可以组成一个简单的特征集。</p>
<p><a name="feature_spec"></a>
</p><h2 class="hide-from-toc">特征规范 (feature spec)</h2><p></p>
<p>用于描述如何从 <a href="#tf.Example"><strong>tf.Example</strong></a> 协议缓冲区提取<a href="#feature"><strong>特征</strong></a>数据。由于 tf.Example 协议缓冲区只是一个数据容器，因此您必须指定以下内容：</p>
<ul>
<li>要提取的数据（即特征的键）</li>
<li>数据类型（例如 float 或 int）</li>
<li>长度（固定或可变）</li>
</ul>
<p><a href="#Estimators"><strong>Estimator API</strong></a> 提供了一些可用来根据给定 <a href="#feature_columns"><strong>FeatureColumns</strong></a> 列表生成特征规范的工具。</p>
<p><a name="few-shot_learning"></a>
</p><h2 class="hide-from-toc">少量样本学习 (few-shot learning)</h2><p></p>
<p>一种机器学习方法（通常用于对象分类），旨在仅通过少量训练样本学习有效的分类器。</p>
<p>另请参阅<a href="#one-shot_learning"><strong>单样本学习</strong></a>。</p>
<p><a name="full_softmax"></a>
</p><h2 class="hide-from-toc">完整 softmax (full softmax)</h2><p></p>
<p>请参阅 <a href="#softmax"><strong>softmax</strong></a>。与<a href="#candidate_sampling"><strong>候选采样</strong></a>相对。</p>
<p><a name="fully_connected_layer"></a>
</p><h2 class="hide-from-toc">全连接层 (fully connected layer)</h2><p></p>
<p>一种<a href="#hidden_layer"><strong>隐藏层</strong></a>，其中的每个<a href="#node"><strong>节点</strong></a>均与下一个隐藏层中的每个节点相连。<em></em></p>
<p>全连接层又称为<a href="#dense_layer"><strong>密集层</strong></a>。</p>
<h2 class="glossary">G</h2>


<p></p><p><a name="generalization"></a>
</p><h2 class="hide-from-toc">泛化 (generalization)</h2><p></p>
<p>指的是模型依据训练时采用的数据，针对以前未见过的新数据做出正确预测的能力。</p>
<p><a name="generalized_linear_model"></a>
</p><h2 class="hide-from-toc">广义线性模型 (generalized linear model)</h2><p></p>
<p><a href="#least_squares_regression"><strong>最小二乘回归</strong></a>模型（基于<a href="https://en.wikipedia.org/wiki/Gaussian_noise" target="_blank" rel="noopener">高斯噪声</a>）向其他类型的模型（基于其他类型的噪声，例如<a href="https://en.wikipedia.org/wiki/Shot_noise" target="_blank" rel="noopener">泊松噪声</a>或分类噪声）进行的一种泛化。广义线性模型的示例包括：</p>
<ul>
<li><a href="#logistic_regression"><strong>逻辑回归</strong></a></li>
<li>多类别回归</li>
<li>最小二乘回归</li>
</ul>
<p>可以通过<a href="https://en.wikipedia.org/wiki/Convex_optimization" target="_blank" rel="noopener">凸优化</a>找到广义线性模型的参数。</p>
<p>广义线性模型具有以下特性：</p>
<ul>
<li>最优的最小二乘回归模型的平均预测结果等于训练数据的平均标签。</li>
<li>最优的逻辑回归模型预测的平均概率等于训练数据的平均标签。</li>
</ul>
<p>广义线性模型的功能受其特征的限制。与深度模型不同，广义线性模型无法“学习新特征”。</p>
<p><a name="gradient"></a>
</p><h2 class="hide-from-toc">梯度 (gradient)</h2><p></p>
<p><a href="#partial_derivative"><strong>偏导数</strong></a>相对于所有自变量的向量。在机器学习中，梯度是模型函数偏导数的向量。梯度指向最高速上升的方向。</p>
<p><a name="gradient_clipping"></a>
</p><h2 class="hide-from-toc">梯度裁剪 (gradient clipping)</h2><p></p>
<p>在应用<a href="#gradient"><strong>梯度</strong></a>值之前先设置其上限。梯度裁剪有助于确保数值稳定性以及防止<a href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf" target="_blank" rel="noopener">梯度爆炸</a>。</p>
<p><a name="gradient_descent"></a>
</p><h2 class="hide-from-toc">梯度下降法 (gradient descent)</h2><p></p>
<p>一种通过计算并且减小梯度将<a href="#loss"><strong>损失</strong></a>降至最低的技术，它以训练数据为条件，来计算损失相对于模型参数的梯度。通俗来说，梯度下降法以迭代方式调整参数，逐渐找到<a href="#weight"><strong>权重</strong></a>和偏差的最佳组合，从而将损失降至最低。</p>
<p><a name="graph"></a>
</p><h2 class="hide-from-toc">图 (graph)</h2><p></p>
<p>TensorFlow 中的一种计算规范。图中的节点表示操作。边缘具有方向，表示将某项操作的结果（一个<a href="https://www.tensorflow.org/api_docs/python/tf/Tensor?hl=zh-cn" target="_blank" rel="noopener">张量</a>）作为一个操作数传递给另一项操作。可以使用 <a href="#TensorBoard"><strong>TensorBoard</strong></a> 直观呈现图。</p>
<h2 class="glossary">H</h2>

<p><a name="heuristic"></a>
</p><h2 class="hide-from-toc">启发法 (heuristic)</h2><p></p>
<p>一种非最优但实用的问题解决方案，足以用于进行改进或从中学习。</p>
<p><a name="hidden_layer"></a>
</p><h2 class="hide-from-toc">隐藏层 (hidden layer)</h2><p></p>
<p><a href="#neural_network"><strong>神经网络</strong></a>中的合成层，介于<a href="#input_layer"><strong>输入层</strong></a>（即特征）和<a href="#output_layer"><strong>输出层</strong></a>（即预测）之间。神经网络包含一个或多个隐藏层。</p>
<p><a name="hinge-loss"></a>
</p><h2 class="hide-from-toc">合页损失函数 (hinge loss)</h2><p></p>
<p>一系列用于<a href="#classification_model"><strong>分类</strong></a>的<a href="#loss"><strong>损失</strong></a>函数，旨在找到距离每个训练样本都尽可能远的<a href="#decision_boundary"><strong>决策边界</strong></a>，从而使样本和边界之间的裕度最大化。
<a href="#KSVMs"><strong>KSVM</strong></a> 使用合页损失函数（或相关函数，例如平方合页损失函数）。对于二元分类，合页损失函数的定义如下：</p>
<div>
$$\text{loss} = \text{max}(0, 1 - (y' * y))$$
</div>

<p>其中“y'”表示分类器模型的原始输出：<em></em></p>
<div>
$$y' = b + w_1x_1 + w_2x_2 + … w_nx_n$$
</div>

<p>“y”表示真标签，值为 -1 或 +1。<em></em></p>
<p>因此，合页损失与 (y * y') 的关系图如下所示：</p>
<p>
<svg xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:cc="http://creativecommons.org/ns#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 659.45264 566.00629" height="566.00629" width="659.45264" xml:space="preserve" id="svg2" version="1.1">
  <title>合页损失函数 与 (y * y')</title>
  <desc>合页损失函数与原始分类器得分的关系图在坐标 (1,0) 处显示明显的合页。</desc>
  <defs id="defs6"><clippath id="clipPath20" clippathunits="userSpaceOnUse"><path id="path18" d="M 0,0 H 365760 V 274320 H 0 Z"/></clippath></defs><g transform="matrix(1.3333333,0,0,-1.3333333,-22.422343,676.03376)" id="g10"><path id="path28" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 0,540 H 719.99856 V 0.00108 H 0 Z"/><path id="path36" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 95.419101,498.0867 V 157.52832"/><path id="path38" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 95.419101,498.0867 V 157.52832"/><path id="path46" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 95.419101,157.76328 H 503.46553"/><path id="path48" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 95.419101,157.76328 H 503.46553"/><path id="path56" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 294.60847,173.49483 V 142.03032"/><path id="path58" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 294.60847,173.49483 V 142.03032"/><path id="path66" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 362.51227,173.49483 V 142.03032"/><path id="path68" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 362.51227,173.49483 V 142.03032"/><path id="path76" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 430.41607,173.49483 V 142.03032"/><path id="path78" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 430.41607,173.49483 V 142.03032"/><path id="path86" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 226.70466,173.49483 V 142.03032"/><path id="path88" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 226.70466,173.49483 V 142.03032"/><path id="path96" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 158.80086,173.49483 V 142.03032"/><path id="path98" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 158.80086,173.49483 V 142.03032"/><path id="path106" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 498.31987,173.49483 V 142.03032"/><path id="path108" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 498.31987,173.49483 V 142.03032"/><path id="path116" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="m 282.09479,138.47718 h 26.38577 v -31.4645 h -26.38577 z"/><text y="-122.12723" x="292.508" id="text126" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)"><tspan style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" id="tspan124" y="-122.12723" x="292.508">0</tspan></text>
<path id="path136" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="m 124.85634,138.47718 h 66.992 v -31.4645 h -66.992 z"/><text y="-122.12723" x="153.90829" id="text146" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)"><tspan style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" id="tspan144" y="-122.12723" x="153.90829 157.23828">-2</tspan></text>
<path id="path156" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="m 198.28295,138.47718 h 56.50382 v -31.4645 h -56.50382 z"/><text y="-122.12723" x="222.09081" id="text166" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)"><tspan style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" id="tspan164" y="-122.12723" x="222.09081 225.42079">-1</tspan></text>
<path id="path176" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="m 350.12473,138.47718 h 26.38578 v -31.4645 h -26.38578 z"/><text y="-122.12723" x="360.53796" id="text186" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)"><tspan style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" id="tspan184" y="-122.12723" x="360.53796">1</tspan></text>
<path id="path196" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="m 417.22358,138.47718 h 26.38577 v -31.4645 h -26.38577 z"/><text y="-122.12723" x="427.63678" id="text206" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)"><tspan style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" id="tspan204" y="-122.12723" x="427.63678">2</tspan></text>
<path id="path216" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="m 485.02047,138.47718 h 26.38577 v -31.4645 h -26.38577 z"/><text y="-122.12723" x="495.43369" id="text226" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)"><tspan style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" id="tspan224" y="-122.12723" x="495.43369">3</tspan></text>
<path id="path236" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 111.32608,415.76206 H 79.861573"/><path id="path238" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 111.32608,415.76206 H 79.861573"/><path id="path246" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 111.32608,347.85826 H 79.861573"/><path id="path248" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 111.32608,347.85826 H 79.861573"/><path id="path256" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 111.32608,279.95446 H 79.861573"/><path id="path258" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 111.32608,279.95446 H 79.861573"/><path id="path266" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 111.32608,483.66586 H 79.861573"/><path id="path268" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 111.32608,483.66586 H 79.861573"/><path id="path276" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 50.757773,302.93551 H 77.143547 V 255.21907 H 50.757773 Z"/><text y="-275.47729" x="61.170982" id="text286" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)"><tspan style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" id="tspan284" y="-275.47729" x="61.170982">1</tspan></text>
<path id="path296" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 53.622428,371.98048 H 80.008206 V 324.26403 H 53.622428 Z"/><text y="-344.52228" x="64.035637" id="text306" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)"><tspan style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" id="tspan304" y="-344.52228" x="64.035637">2</tspan></text>
<path id="path316" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 53.622428,438.99539 H 80.008206 V 391.27895 H 53.622428 Z"/><text y="-411.53717" x="64.035637" id="text326" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)"><tspan style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" id="tspan324" y="-411.53717" x="64.035637">3</tspan></text>
<path id="path336" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 53.622428,507.02533 H 80.008206 V 459.30889 H 53.622428 Z"/><text y="-479.56711" x="64.035637" id="text346" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)"><tspan style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" id="tspan344" y="-479.56711" x="64.035637">4</tspan></text>
<path id="path356" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 95.053282,483.16711 360.44646,217.77393"/><path id="path358" style="fill:none;stroke:#ff0000;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 95.053282,483.16711 360.44646,217.77393"/><path id="path366" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 359.32928,218.08535 H 501.43923"/><path id="path368" style="fill:none;stroke:#ff0000;stroke-width:2.24999547;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 359.32928,218.08535 H 501.43923"/><path id="path376" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 50.757773,241.9206 H 77.143547 V 194.20416 H 50.757773 Z"/><text y="-214.46239" x="61.170982" id="text386" style="font-variant:normal;font-weight:normal;font-size:9.99997997px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)"><tspan style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" id="tspan384" y="-214.46239" x="61.170982">0</tspan></text>
<path id="path396" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 111.32608,217.92452 H 79.861573"/><path id="path398" style="fill:none;stroke:#000000;stroke-width:0.74999851;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:14.3355875;stroke-dasharray:none;stroke-opacity:1" d="M 111.32608,217.92452 H 79.861573"/><path id="path406" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 9.299194,394.88415 H 62.614048 V 344.4748 H 9.299194 Z"/><text y="-376.6142" x="16.049181" id="text416" style="font-variant:normal;font-weight:normal;font-size:11.99997616px;font-family:Arial;-inkscape-font-specification:ArialMT;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)">合页损失函数</text>
<path id="path430" style="fill:#000000;fill-opacity:0;fill-rule:evenodd;stroke:none;stroke-width:0.0019685" d="M 95.419101,110.41818 H 498.31593 V 82.5206 H 95.419101 Z"/><path id="path438" style="fill:#ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" d="m 266.48095,103.66819 h 60.77311 V 88.068225 h -60.77311 z"/><text y="-91.188217" x="270.09167" id="text442" style="font-variant:normal;font-weight:normal;font-size:12.99997425px;font-family:Consolas;-inkscape-font-specification:Consolas;writing-mode:lr-tb;fill:#222222;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0.74999851" transform="scale(1,-1)"><tspan style="stroke-width:0.74999851;-inkscape-font-specification:sans-serif;font-family:sans-serif;font-weight:normal;font-style:normal;font-stretch:normal;font-variant:normal" id="tspan440" y="-91.188217" x="270.09167 277.22867 284.36563 291.50262 298.63962 305.77661 312.91357 320.05057">(y * y')</tspan></text>
</g></svg>

</p>

<p><a name="holdout_data"></a>
</p><h2 class="hide-from-toc">维持数据 (holdout data)</h2><p></p>
<p>训练期间故意不使用（“维持”）的<a href="#example"><strong>样本</strong></a>。<a href="#validation_set"><strong>验证数据集</strong></a>和<a href="#test_set"><strong>测试数据集</strong></a>都属于维持数据。维持数据有助于评估模型向训练时所用数据之外的数据进行泛化的能力。与基于训练数据集的损失相比，基于维持数据集的损失有助于更好地估算基于未见过的数据集的损失。</p>
<p><a name="hyperparameter"></a>
</p><h2 class="hide-from-toc">超参数 (hyperparameter)</h2><p></p>
<p>在模型训练的连续过程中，您调节的“旋钮”。例如，<a href="#learning_rate"><strong>学习速率</strong></a>就是一种超参数。</p>
<p>与<a href="#parameter"><strong>参数</strong></a>相对。</p>
<p><a name="hyperplane"></a>
</p><h2 class="hide-from-toc">超平面 (hyperplane)</h2><p></p>
<p>将一个空间划分为两个子空间的边界。例如，在二维空间中，直线就是一个超平面，在三维空间中，平面则是一个超平面。在机器学习中更典型的是：超平面是分隔高维度空间的边界。<a href="#KSVMs"><strong>核支持向量机</strong></a>利用超平面将正类别和负类别区分开来（通常是在极高维度空间中）。</p>
<h2 class="glossary">I</h2>

<p><a name="iid"></a>
</p><h2 class="hide-from-toc">独立同等分布 (i.i.d, independently and identically distributed)</h2><p></p>
<p>从不会改变的分布中提取的数据，其中提取的每个值都不依赖于之前提取的值。i.i.d. 是机器学习的<a href="https://en.wikipedia.org/wiki/Ideal_gas" target="_blank" rel="noopener">理想气体</a> - 一种实用的数学结构，但在现实世界中几乎从未发现过。例如，某个网页的访问者在短时间内的分布可能为 i.i.d.，即分布在该短时间内没有变化，且一位用户的访问行为通常与另一位用户的访问行为无关。不过，如果将时间窗口扩大，网页访问者的分布可能呈现出季节性变化。</p>
<p><a name="inference"></a>
</p><h2 class="hide-from-toc">推断 (inference)</h2><p></p>
<p>在机器学习中，推断通常指以下过程：通过将训练过的模型应用于<a href="#unlabeled_example"><strong>无标签样本</strong></a>来做出预测。在统计学中，推断是指在某些观测数据条件下拟合分布参数的过程。（请参阅<a href="https://en.wikipedia.org/wiki/Statistical_inference" target="_blank" rel="noopener">维基百科中有关统计学推断的文章</a>。）</p>
<p><a name="input_function"></a>
</p><h2 class="hide-from-toc">输入函数 (input function)</h2><p></p>
<p>在 TensorFlow 中，用于将输入数据返回到 <a href="#Estimators"><strong>Estimator</strong></a> 的训练、评估或预测方法的函数。例如，训练输入函数会返回<a href="#training_set"><strong>训练集</strong></a>中的<a href="#batch"><strong>一批</strong></a>特征和标签。</p>
<p><a name="input_layer"></a>
</p><h2 class="hide-from-toc">输入层 (input layer)</h2><p></p>
<p><a href="#neural_network"><strong>神经网络</strong></a>中的第一层（接收输入数据的层）。</p>
<p><a name="instance"></a>
</p><h2 class="hide-from-toc">实例 (instance)</h2><p></p>
<p>与<a href="#example"><strong>样本</strong></a>的含义相同。</p>
<p><a name="interpretability"></a>
</p><h2 class="hide-from-toc">可解释性 (interpretability)</h2><p></p>
<p>模型的预测可解释的难易程度。深度模型通常不可解释，也就是说，很难对深度模型的不同层进行解释。相比之下，线性回归模型和<a href="#wide_model"><strong>宽度模型</strong></a>的可解释性通常要好得多。</p>
<p><a name="inter-rater_agreement"></a>
</p><h2 class="hide-from-toc">评分者间一致性信度 (inter-rater agreement)</h2><p></p>
<p>一种衡量指标，用于衡量在执行某项任务时评分者达成一致的频率。如果评分者未达成一致，则可能需要改进任务说明。有时也称为<strong>注释者间一致性信度</strong>或<strong>评分者间可靠性信度</strong>。另请参阅 <a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa" target="_blank" rel="noopener">Cohen's kappa</a>（最热门的评分者间一致性信度衡量指标之一）。</p>
<p><a name="iteration"></a>
</p><h2 class="hide-from-toc">迭代 (iteration)</h2><p></p>
<p>模型的权重在训练期间的一次更新。迭代包含计算参数在单<a href="#batch"><strong>批次</strong></a>数据上的梯度损失。</p>
<h2 class="glossary">K</h2>

<p><a name="k-means"></a>
</p><h2 class="hide-from-toc">k-means</h2><p></p>
<p>一种热门的<a href="#clustering"><strong>聚类</strong></a>算法，用于对非监督式学习中的样本进行分组。k-means 算法基本上会执行以下操作：</p>
<ul>
<li>以迭代方式确定最佳的 k 中心点（称为<a href="#centroid"><strong>形心</strong></a>）。</li>
<li>将每个样本分配到最近的形心。与同一个形心距离最近的样本属于同一个组。</li>
</ul>
<p>k-means 算法会挑选形心位置，以最大限度地减小每个样本与其最接近形心之间的距离的累积平方。<em></em></p>
<p>以下面的小狗高度与小狗宽度的关系图为例：</p>
<p>
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:cc="http://creativecommons.org/ns#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" xmlns="http://www.w3.org/2000/svg" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" version="1.1" viewbox="0.0 0.0 444.33858267716533 337.4278215223097" fill="none" stroke="none" stroke-linecap="square" stroke-miterlimit="10" id="svg299" height="400px" sodipodi:docname="zh-cn-dogdim-image.svg" inkscape:version="0.92.2 5c3e80d, 2017-08-06">
  <metadata id="metadata305">
    <rdf:rdf>
      <cc:work rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type rdf:resource="http://purl.org/dc/dcmitype/StillImage">
        <dc:title>
      </dc:title></dc:type></cc:work>
    </rdf:rdf>
  </metadata>
  <defs id="defs303">
  <sodipodi:namedview pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" objecttolerance="10" gridtolerance="10" guidetolerance="10" inkscape:pageopacity="0" inkscape:pageshadow="2" inkscape:window-width="1044" inkscape:window-height="665" id="namedview301" showgrid="false" inkscape:zoom="0.98911345" inkscape:cx="229.10775" inkscape:cy="168.71391" inkscape:window-x="0" inkscape:window-y="0" inkscape:window-maximized="0" inkscape:current-layer="svg299">
  <desc id="desc2">50 or so examples along a two-dimensional graph.</desc>
  <clippath id="p.0">
    <path d="m0 0l444.3386 0l0 337.42783l-444.3386 0l0 -337.42783z" clip-rule="nonzero" id="path4"/>
  </clippath>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path7" d="M 0,0 H 444.3386 V 337.42783 H 0 Z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path9" d="M 76.13614,8.670278 V 298.37106"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path11" d="M 76.13614,8.670278 V 298.37106"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path13" d="M 76.13614,298.3742 H 435.06528"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path15" d="M 76.13614,298.3742 H 435.06528"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path17" d="m 129.39021,258.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path19" d="m 129.39021,258.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path21" d="m 145.39021,266.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path23" d="m 145.39021,266.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path25" d="m 153.39021,250.01936 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path27" d="m 153.39021,250.01936 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path29" d="m 153.39021,234.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path31" d="m 153.39021,234.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path33" d="m 169.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path35" d="m 169.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path37" d="m 161.39021,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path39" d="m 161.39021,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path41" d="m 137.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path43" d="m 137.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path45" d="m 193.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path47" d="m 193.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path49" d="m 201.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path51" d="m 201.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path53" d="m 177.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path55" d="m 177.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path57" d="m 225.39021,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path59" d="m 225.39021,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path61" d="m 161.39021,266.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path63" d="m 161.39021,266.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path65" d="m 177.39021,274.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path67" d="m 177.39021,274.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path69" d="m 185.39021,258.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path71" d="m 185.39021,258.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path73" d="m 185.39021,242.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path75" d="m 185.39021,242.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path77" d="m 193.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path79" d="m 193.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path81" d="m 209.39021,218.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path83" d="m 209.39021,218.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path85" d="m 231.39359,188.01598 h 8.53545 v 10.64566 h -8.53545 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path87" d="m 231.39359,188.01598 h 8.53545 v 10.64566 h -8.53545 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path89" d="m 217.39021,242.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path91" d="m 217.39021,242.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path93" d="m 185.39021,214.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path95" d="m 185.39021,214.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path97" d="m 249.39021,186.01936 h 8.53542 v 10.64566 h -8.53542 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path99" d="m 249.39021,186.01936 h 8.53542 v 10.64566 h -8.53542 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path101" d="m 257.3902,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path103" d="m 257.3902,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path105" d="m 177.39021,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path107" d="m 177.39021,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path109" d="m 241.39021,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path111" d="m 241.39021,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path113" d="m 165.53194,242.19783 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path115" d="m 165.53194,242.19783 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path117" d="m 249.39021,138.01936 h 8.53542 v 10.64566 h -8.53542 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path119" d="m 249.39021,138.01936 h 8.53542 v 10.64566 h -8.53542 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path121" d="m 289.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path123" d="m 289.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path125" d="m 265.3902,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path127" d="m 265.3902,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path129" d="m 273.3902,98.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path131" d="m 273.3902,98.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path133" d="m 289.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path135" d="m 289.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path137" d="m 297.3902,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path139" d="m 297.3902,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path141" d="m 297.3902,66.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path143" d="m 297.3902,66.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path145" d="m 313.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path147" d="m 313.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path149" d="m 321.3902,66.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path151" d="m 321.3902,66.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path153" d="m 257.3902,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path155" d="m 257.3902,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path157" d="m 273.3902,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path159" d="m 273.3902,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path161" d="m 281.3902,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path163" d="m 281.3902,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path165" d="m 281.3902,146.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path167" d="m 281.3902,146.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path169" d="m 281.3902,130.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path171" d="m 281.3902,130.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path173" d="m 305.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path175" d="m 305.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path177" d="m 305.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path179" d="m 305.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path181" d="m 321.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path183" d="m 321.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path185" d="m 321.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path187" d="m 321.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path189" d="m 329.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path191" d="m 329.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path193" d="m 345.3902,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path195" d="m 345.3902,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path197" d="m 353.3902,74.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path199" d="m 353.3902,74.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path201" d="m 230.69205,307.41568 h 58.70865 v 27.68506 h -58.70865 z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path205" d="m 9.035876,131.41568 h 64.37795 v 27.68504 H 9.035876 Z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path209" d="m 209.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path211" d="m 209.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path213" d="m 201.39021,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path215" d="m 201.39021,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path217" d="m 201.39021,149.29642 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path219" d="m 201.39021,149.29642 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path221" d="m 233.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path223" d="m 233.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path225" d="m 241.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path227" d="m 241.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path229" d="m 217.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path231" d="m 217.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path233" d="m 233.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path235" d="m 233.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path237" d="m 249.39021,218.01936 h 8.53542 v 10.64566 h -8.53542 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path239" d="m 249.39021,218.01936 h 8.53542 v 10.64566 h -8.53542 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path241" d="m 241.39021,227.42531 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path243" d="m 241.39021,227.42531 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path245" d="m 225.39021,214.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path247" d="m 225.39021,214.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path249" d="m 217.39021,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path251" d="m 217.39021,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path253" d="m 241.39021,154.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path255" d="m 241.39021,154.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path257" d="m 241.39021,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path259" d="m 241.39021,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path261" d="m 257.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path263" d="m 257.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path265" d="m 233.39021,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path267" d="m 233.39021,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path269" d="m 257.3902,154.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path271" d="m 257.3902,154.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path273" d="m 270.5783,170.72234 h 8.53543 V 181.368 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path275" d="m 270.5783,170.72234 h 8.53543 V 181.368 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path277" d="m 257.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path279" d="m 257.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path281" d="m 369.3902,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path283" d="m 369.3902,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path285" d="m 277.87534,111.20746 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path287" d="m 277.87534,111.20746 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path289" d="m 293.87534,127.20746 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path291" d="m 293.87534,127.20746 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path293" d="m 369.3902,50.01936 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path295" d="m 369.3902,50.01936 h 8.53543 v 10.645668 h -8.53543 z"/>
  <text xml:space="preserve" style="font-style:normal;font-weight:normal;font-size:40px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1" x="31.341196" y="169.55675" id="text1114"><tspan sodipodi:role="line" id="tspan1112" x="31.341196" y="169.55675" style="font-size:16px">高度</tspan></text>
  <text id="text1263" y="319.6904" x="229.47644" style="font-style:normal;font-weight:normal;font-size:40px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1" xml:space="preserve"><tspan style="font-size:16px" y="319.6904" x="229.47644" id="tspan1261" sodipodi:role="line">宽度</tspan></text>
</sodipodi:namedview></defs></svg>

</p>

<p>如果 k=3，则 k-means 算法会确定三个形心。每个样本都被分配到与其最接近的形心，最终产生三个组：</p>
<p>
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:cc="http://creativecommons.org/ns#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" xmlns="http://www.w3.org/2000/svg" xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape" version="1.1" viewbox="0.0 0.0 444.33858267716533 337.4278215223097" fill="none" stroke="none" stroke-linecap="square" stroke-miterlimit="10" id="svg343" height="400px" sodipodi:docname="zh-cn-dogdimkmeans-image.svg" inkscape:version="0.92.2 5c3e80d, 2017-08-06">
  <metadata id="metadata349">
    <rdf:rdf>
      <cc:work rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type rdf:resource="http://purl.org/dc/dcmitype/StillImage">
        <dc:title>
      </dc:title></dc:type></cc:work>
    </rdf:rdf>
  </metadata>
  <defs id="defs347">
  <sodipodi:namedview pagecolor="#ffffff" bordercolor="#666666" borderopacity="1" objecttolerance="10" gridtolerance="10" guidetolerance="10" inkscape:pageopacity="0" inkscape:pageshadow="2" inkscape:window-width="1093" inkscape:window-height="756" id="namedview345" showgrid="false" inkscape:zoom="0.98911345" inkscape:cx="222.1693" inkscape:cy="168.71391" inkscape:window-x="0" inkscape:window-y="0" inkscape:window-maximized="0" inkscape:current-layer="svg343">
  <desc id="desc2">The same 50 examples clustered into 3 groups.</desc>
  <clippath id="p.0">
    <path d="m0 0l444.3386 0l0 337.42783l-444.3386 0l0 -337.42783z" clip-rule="nonzero" id="path4"/>
  </clippath>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path7" d="M 0,0 H 444.3386 V 337.42783 H 0 Z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path9" d="M 76.13614,8.670278 V 298.37106"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path11" d="M 76.13614,8.670278 V 298.37106"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path13" d="M 76.13614,298.3742 H 435.06528"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path15" d="M 76.13614,298.3742 H 435.06528"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path17" d="m 129.39021,258.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path19" d="m 129.39021,258.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path21" d="m 145.39021,266.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path23" d="m 145.39021,266.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path25" d="m 153.39021,250.01936 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path27" d="m 153.39021,250.01936 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path29" d="m 153.39021,234.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path31" d="m 153.39021,234.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path33" d="m 169.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path35" d="m 169.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path37" d="m 161.39021,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path39" d="m 161.39021,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path41" d="m 137.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path43" d="m 137.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path45" d="m 193.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path47" d="m 193.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path49" d="m 201.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path51" d="m 201.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path53" d="m 177.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path55" d="m 177.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path57" d="m 225.39021,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path59" d="m 225.39021,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path61" d="m 161.39021,266.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path63" d="m 161.39021,266.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path65" d="m 177.39021,274.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path67" d="m 177.39021,274.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path69" d="m 185.39021,258.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path71" d="m 185.39021,258.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path73" d="m 185.39021,242.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path75" d="m 185.39021,242.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path77" d="m 193.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path79" d="m 193.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path81" d="m 209.39021,218.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path83" d="m 209.39021,218.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path85" d="m 231.39359,188.01598 h 8.53545 v 10.64566 h -8.53545 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path87" d="m 231.39359,188.01598 h 8.53545 v 10.64566 h -8.53545 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path89" d="m 217.39021,242.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path91" d="m 217.39021,242.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path93" d="m 185.39021,214.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path95" d="m 185.39021,214.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path97" d="m 249.39021,186.01936 h 8.53542 v 10.64566 h -8.53542 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path99" d="m 249.39021,186.01936 h 8.53542 v 10.64566 h -8.53542 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path101" d="m 257.3902,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path103" d="m 257.3902,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path105" d="m 177.39021,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path107" d="m 177.39021,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path109" d="m 241.39021,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path111" d="m 241.39021,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path113" d="m 165.53194,242.19783 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path115" d="m 165.53194,242.19783 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path117" d="m 249.39021,138.01936 h 8.53542 v 10.64566 h -8.53542 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path119" d="m 249.39021,138.01936 h 8.53542 v 10.64566 h -8.53542 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path121" d="m 289.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path123" d="m 289.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path125" d="m 265.3902,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path127" d="m 265.3902,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path129" d="m 273.3902,98.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path131" d="m 273.3902,98.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path133" d="m 289.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path135" d="m 289.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path137" d="m 297.3902,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path139" d="m 297.3902,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path141" d="m 297.3902,66.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path143" d="m 297.3902,66.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path145" d="m 313.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path147" d="m 313.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path149" d="m 321.3902,66.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path151" d="m 321.3902,66.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path153" d="m 257.3902,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path155" d="m 257.3902,170.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path157" d="m 273.3902,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path159" d="m 273.3902,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path161" d="m 281.3902,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path163" d="m 281.3902,162.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path165" d="m 281.3902,146.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path167" d="m 281.3902,146.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path169" d="m 279.98425,130.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path171" d="m 279.98425,130.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path173" d="m 305.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path175" d="m 305.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path177" d="m 305.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path179" d="m 305.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path181" d="m 321.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path183" d="m 321.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path185" d="m 321.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path187" d="m 321.3902,106.01936 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path189" d="m 329.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path191" d="m 329.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path193" d="m 345.3902,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path195" d="m 345.3902,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path197" d="m 353.3902,74.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path199" d="m 353.3902,74.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path201" d="m 230.69205,307.41568 h 58.70865 v 27.68506 h -58.70865 z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path205" d="m 9.035876,131.41568 h 64.37795 v 27.68504 H 9.035876 Z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path209" d="m 209.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path211" d="m 209.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path213" d="m 201.39021,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path215" d="m 201.39021,210.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path217" d="m 201.39021,149.29642 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path219" d="m 201.39021,149.29642 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path221" d="m 233.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path223" d="m 233.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path225" d="m 241.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path227" d="m 241.39021,186.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path229" d="m 217.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path231" d="m 217.39021,202.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path233" d="m 233.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path235" d="m 233.39021,226.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path237" d="m 249.39021,214.50449 h 8.53542 v 10.64568 h -8.53542 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path239" d="m 249.39021,214.50449 h 8.53542 v 10.64568 h -8.53542 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path241" d="m 241.39021,227.42531 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path243" d="m 241.39021,227.42531 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path245" d="m 225.39021,214.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path247" d="m 225.39021,214.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path249" d="m 217.39021,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path251" d="m 217.39021,178.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path253" d="m 241.39021,154.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path255" d="m 241.39021,154.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path257" d="m 257.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path259" d="m 257.3902,82.019356 h 8.53543 v 10.645676 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path261" d="m 248.25154,109.53423 h 8.53542 v 10.64567 h -8.53542 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path263" d="m 248.25154,109.53423 h 8.53542 v 10.64567 h -8.53542 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path265" d="m 233.39021,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path267" d="m 233.39021,138.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path269" d="m 257.3902,154.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path271" d="m 257.3902,154.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path273" d="m 270.5783,170.72234 h 8.53543 V 181.368 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path275" d="m 270.5783,170.72234 h 8.53543 V 181.368 h -8.53543 z"/>
  <path style="fill:#cfe2f3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path277" d="m 257.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path279" d="m 257.3902,122.01936 h 8.53543 v 10.64566 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path281" d="m 369.3902,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path283" d="m 369.3902,90.019356 h 8.53543 v 10.645674 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path285" d="m 277.87534,111.20746 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path287" d="m 277.87534,111.20746 h 8.53543 v 10.64567 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path289" d="m 293.87534,127.20746 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path291" d="m 293.87534,127.20746 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path293" d="m 369.3902,50.01936 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path295" d="m 369.3902,50.01936 h 8.53543 v 10.645668 h -8.53543 z"/>
  <path style="fill:#ff0000;fill-rule:evenodd" inkscape:connector-curvature="0" id="path297" d="m 309.83728,100.98425 v 0 c 0,-4.235634 3.46185,-7.669289 7.73227,-7.669289 v 0 c 2.05072,0 4.01746,0.808007 5.46756,2.246277 1.45007,1.438271 2.26474,3.388985 2.26474,5.423012 v 0 c 0,4.23563 -3.46188,7.66929 -7.7323,7.66929 v 0 c -4.27042,0 -7.73227,-3.43366 -7.73227,-7.66929 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path299" d="m 309.83728,100.98425 v 0 c 0,-4.235634 3.46185,-7.669289 7.73227,-7.669289 v 0 c 2.05072,0 4.01746,0.808007 5.46756,2.246277 1.45007,1.438271 2.26474,3.388985 2.26474,5.423012 v 0 c 0,4.23563 -3.46188,7.66929 -7.7323,7.66929 v 0 c -4.27042,0 -7.73227,-3.43366 -7.73227,-7.66929 z"/>
  <path style="fill:#ff0000;fill-rule:evenodd" inkscape:connector-curvature="0" id="path301" d="m 245.83727,164.98425 v 0 c 0,-4.23563 3.46187,-7.6693 7.73228,-7.6693 v 0 c 2.05074,0 4.01746,0.80802 5.46756,2.2463 1.45008,1.43826 2.26474,3.38897 2.26474,5.423 v 0 c 0,4.23563 -3.46188,7.6693 -7.7323,7.6693 v 0 c -4.27041,0 -7.73228,-3.43367 -7.73228,-7.6693 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path303" d="m 245.83727,164.98425 v 0 c 0,-4.23563 3.46187,-7.6693 7.73228,-7.6693 v 0 c 2.05074,0 4.01746,0.80802 5.46756,2.2463 1.45008,1.43826 2.26474,3.38897 2.26474,5.423 v 0 c 0,4.23563 -3.46188,7.6693 -7.7323,7.6693 v 0 c -4.27041,0 -7.73228,-3.43367 -7.73228,-7.6693 z"/>
  <path style="fill:#ff0000;fill-rule:evenodd" inkscape:connector-curvature="0" id="path305" d="m 181.83727,228.98425 v 0 c 0,-4.23563 3.46187,-7.6693 7.73228,-7.6693 v 0 c 2.05074,0 4.01748,0.80802 5.46756,2.2463 1.45008,1.43826 2.26473,3.38897 2.26473,5.423 v 0 c 0,4.23563 -3.46185,7.6693 -7.73229,7.6693 v 0 c -4.27041,0 -7.73228,-3.43367 -7.73228,-7.6693 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path307" d="m 181.83727,228.98425 v 0 c 0,-4.23563 3.46187,-7.6693 7.73228,-7.6693 v 0 c 2.05074,0 4.01748,0.80802 5.46756,2.2463 1.45008,1.43826 2.26473,3.38897 2.26473,5.423 v 0 c 0,4.23563 -3.46185,7.6693 -7.73229,7.6693 v 0 c -4.27041,0 -7.73228,-3.43367 -7.73228,-7.6693 z"/>
  <path style="fill:#fff2cc;fill-rule:evenodd" inkscape:connector-curvature="0" id="path309" d="m 345.3902,223.34503 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path311" d="m 345.3902,223.34503 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path313" d="m 345.3902,244.68219 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path315" d="m 345.3902,244.68219 h 8.53543 v 10.64568 h -8.53543 z"/>
  <path style="fill:#d9ead3;fill-rule:evenodd" inkscape:connector-curvature="0" id="path317" d="m 345.3902,266.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path319" d="m 345.3902,266.01935 h 8.53543 v 10.64569 h -8.53543 z"/>
  <path style="fill:#ff0000;fill-rule:evenodd" inkscape:connector-curvature="0" id="path321" d="m 341.83728,204.98425 v 0 c 0,-4.23563 3.46185,-7.6693 7.73227,-7.6693 v 0 c 2.05072,0 4.01746,0.80802 5.46756,2.2463 1.45007,1.43826 2.26474,3.38897 2.26474,5.423 v 0 c 0,4.23563 -3.46188,7.6693 -7.7323,7.6693 v 0 c -4.27042,0 -7.73227,-3.43367 -7.73227,-7.6693 z"/>
  <path style="fill-rule:evenodd;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:round" inkscape:connector-curvature="0" id="path323" d="m 341.83728,204.98425 v 0 c 0,-4.23563 3.46185,-7.6693 7.73227,-7.6693 v 0 c 2.05072,0 4.01746,0.80802 5.46756,2.2463 1.45007,1.43826 2.26474,3.38897 2.26474,5.423 v 0 c 0,4.23563 -3.46188,7.6693 -7.7323,7.6693 v 0 c -4.27042,0 -7.73227,-3.43367 -7.73227,-7.6693 z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path325" d="m 352.08487,197.31496 h 76.44095 v 15.33859 h -76.44095 z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path329" d="m 352.08487,219.67468 h 76.44095 v 15.33859 h -76.44095 z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path333" d="m 352.08487,242.03441 h 76.44095 v 15.33858 h -76.44095 z"/>
  <path style="fill:#000000;fill-opacity:0;fill-rule:evenodd" inkscape:connector-curvature="0" id="path337" d="m 352.08487,264.39413 h 76.44095 v 15.33859 h -76.44095 z"/>
  <text xml:space="preserve" style="font-style:normal;font-weight:normal;font-size:40px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1" x="29.319185" y="166.41365" id="text1158"><tspan sodipodi:role="line" id="tspan1156" x="29.319185" y="166.41365" style="font-size:16px">高度</tspan></text>
  <text id="text4913" y="317.84448" x="225.38838" style="font-style:normal;font-weight:normal;font-size:40px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1" xml:space="preserve"><tspan style="font-size:16px" y="317.84448" x="225.38838" id="tspan4911" sodipodi:role="line">宽度</tspan></text>
  <text xml:space="preserve" style="font-style:normal;font-weight:normal;font-size:40px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1" x="361.94028" y="209.03001" id="text4917"><tspan sodipodi:role="line" id="tspan4915" x="361.94028" y="209.03001" style="font-size:13.33333302px">形心</tspan></text>
  <text xml:space="preserve" style="font-style:normal;font-weight:normal;font-size:40px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1" x="361.94034" y="232.28316" id="text4921"><tspan sodipodi:role="line" id="tspan4919" x="361.94034" y="232.28316" style="font-size:13.33333302px">聚类 1</tspan></text>
  <text id="text4925" y="254.28314" x="361.94034" style="font-style:normal;font-weight:normal;font-size:40px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1" xml:space="preserve"><tspan style="font-size:13.33333302px" y="254.28314" x="361.94034" id="tspan4923" sodipodi:role="line">聚类 2</tspan></text>
  <text xml:space="preserve" style="font-style:normal;font-weight:normal;font-size:40px;line-height:125%;font-family:sans-serif;letter-spacing:0px;word-spacing:0px;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1" x="361.94034" y="276.28314" id="text4929"><tspan sodipodi:role="line" id="tspan4927" x="361.94034" y="276.28314" style="font-size:13.33333302px">聚类 3</tspan></text>
</sodipodi:namedview></defs></svg>

</p>

<p>假设制造商想要确定小、中和大号狗毛衣的理想尺寸。在该聚类中，三个形心用于标识每只狗的平均高度和平均宽度。因此，制造商可能应该根据这三个形心确定毛衣尺寸。请注意，聚类的形心通常不是聚类中的样本。<em></em></p>
<p>上图显示了 k-means 应用于仅具有两个特征（高度和宽度）的样本。请注意，k-means 可以跨多个特征为样本分组。</p>
<p><a name="k-median"></a>
</p><h2 class="hide-from-toc">k-median</h2><p></p>
<p>与 <a href="#k-means"><strong>k-means</strong></a> 紧密相关的聚类算法。两者的实际区别如下：</p>
<ul>
<li>对于 k-means，确定形心的方法是，最大限度地减小候选形心与它的每个样本之间的距离平方和。<em></em></li>
<li>对于 k-median，确定形心的方法是，最大限度地减小候选形心与它的每个样本之间的距离总和。</li>
</ul>
<p>请注意，距离的定义也有所不同：</p>
<ul>
<li>k-means 采用从形心到样本的<a href="https://en.wikipedia.org/wiki/Euclidean_distance" target="_blank" rel="noopener">欧几里得距离</a>。（在二维空间中，欧几里得距离即使用勾股定理来计算斜边。）例如，(2,2) 与 (5,-2) 之间的 k-means 距离为：</li>
</ul>
<div>
$${\text{欧几里德距离}} = {\sqrt {(2-5)^2 + (2--2)^2}} = 5$$
</div>

<ul>
<li>k-median 采用从形心到样本的<a href="https://en.wikipedia.org/wiki/Taxicab_geometry" target="_blank" rel="noopener">曼哈顿距离</a>。这个距离是每个维度中绝对差异值的总和。例如，(2,2) 与 (5,-2) 之间的 k-median 距离为：</li>
</ul>
<div>
$${\text{曼哈顿距离}} = \lvert 2-5 \rvert + \lvert 2--2 \rvert = 7$$
</div>

<p><a name="Keras"></a>
</p><h2 class="hide-from-toc">Keras</h2><p></p>
<p>一种热门的 Python 机器学习 API。<a href="https://keras.io" target="_blank" rel="noopener">Keras</a> 能够在多种深度学习框架上运行，其中包括 TensorFlow（在该框架上，Keras 作为 <a href="https://www.tensorflow.org/api_docs/python/tf/keras?hl=zh-cn" target="_blank" rel="noopener"><strong>tf.keras</strong></a> 提供）。</p>
<p><a name="KSVMs"></a>
</p><h2 class="hide-from-toc">核支持向量机 (KSVM, Kernel Support Vector Machines)</h2><p></p>
<p>一种分类算法，旨在通过将输入数据向量映射到更高维度的空间，来最大化<a href="#positive_class"><strong>正类别</strong></a>和<a href="#negative_class"><strong>负类别</strong></a>之间的裕度。以某个输入数据集包含一百个特征的分类问题为例。为了最大化正类别和负类别之间的裕度，KSVM 可以在内部将这些特征映射到百万维度的空间。KSVM 使用<a href="#hinge-loss">合页损失函数</a>。</p>
<h2 class="glossary">L</h2>

<p><a name="L1_loss"></a>
</p><h2 class="hide-from-toc">L<sub>1</sub> 损失函数 (L₁ loss)</h2><p></p>
<p>一种<a href="#loss"><strong>损失</strong></a>函数，基于模型预测的值与<a href="#label"><strong>标签</strong></a>的实际值之差的绝对值。与 <a href="#squared_loss"><strong>L<sub>2</sub> 损失函数</strong></a>相比，L<sub>1</sub> 损失函数对离群值的敏感性弱一些。</p>
<p><a name="L1_regularization"></a>
</p><h2 class="hide-from-toc">L<sub>1</sub> 正则化 (L₁ regularization)</h2><p></p>
<p>一种<a href="#regularization"><strong>正则化</strong></a>，根据权重的绝对值的总和来惩罚权重。在依赖<a href="#sparse_features"><strong>稀疏特征</strong></a>的模型中，L<sub>1</sub> 正则化有助于使不相关或几乎不相关的特征的权重正好为 0，从而将这些特征从模型中移除。与 <a href="#L2_regularization"><strong>L<sub>2</sub> 正则化</strong></a>相对。</p>
<p><a name="L2_loss"></a>
</p><h2 class="hide-from-toc">L<sub>2</sub> 损失函数 (L₂ loss)</h2><p></p>
<p>请参阅<a href="#squared_loss"><strong>平方损失函数</strong></a>。</p>
<p><a name="L2_regularization"></a>
</p><h2 class="hide-from-toc">L<sub>2</sub> 正则化 (L₂ regularization)</h2><p></p>
<p>一种<a href="#regularization"><strong>正则化</strong></a>，根据权重的平方和来惩罚权重。<em></em>L<sub>2</sub> 正则化有助于使离群值（具有较大正值或较小负值）权重接近于 0，但又不正好为 0。（与 <a href="#L1_regularization"><strong>L1 正则化</strong></a>相对。）在线性模型中，L<sub>2</sub> 正则化始终可以改进泛化。</p>
<p><a name="label"></a>
</p><h2 class="hide-from-toc">标签 (label)</h2><p></p>
<p>在监督式学习中，标签指<a href="#example"><strong>样本</strong></a>的“答案”或“结果”部分。有标签数据集中的每个样本都包含一个或多个特征以及一个标签。例如，在房屋数据集中，特征可能包括卧室数、卫生间数以及房龄，而标签则可能是房价。在垃圾邮件检测数据集中，特征可能包括主题行、发件人以及电子邮件本身，而标签则可能是“垃圾邮件”或“非垃圾邮件”。</p>
<p><a name="labeled_example"></a>
</p><h2 class="hide-from-toc">有标签样本 (labeled example)</h2><p></p>
<p>包含<a href="#feature"><strong>特征</strong></a>和<a href="#label"><strong>标签</strong></a>的样本。在监督式训练中，模型从有标签样本中学习规律。</p>
<p><a name="lambda"></a>
</p><h2 class="hide-from-toc">lambda</h2><p></p>
<p>与<a href="#regularization_rate"><strong>正则化率</strong></a>的含义相同。</p>
<p>（多含义术语，我们在此关注的是该术语在<a href="#regularization"><strong>正则化</strong></a>中的定义。）</p>
<p><a name="layer"></a>
</p><h2 class="hide-from-toc">层 (layer)</h2><p></p>
<p><a href="#neural_network"><strong>神经网络</strong></a>中的一组<a href="#neuron"><strong>神经元</strong></a>，负责处理一组输入特征，或一组神经元的输出。</p>
<p>此外还指 TensorFlow 中的抽象层。层是 Python 函数，以<a href="#tensor"><strong>张量</strong></a>和配置选项作为输入，然后生成其他张量作为输出。当必要的张量组合起来后，用户便可以通过<a href="#model_function"><strong>模型函数</strong></a>将结果转换为 <a href="#Estimators"><strong>Estimator</strong></a>。</p>
<p><a name="layers_API"></a>
</p><h2 class="hide-from-toc">Layers API (tf.layers)</h2><p></p>
<p>一种 TensorFlow API，用于以层组合的方式构建<a href="#deep_model"><strong>深度</strong></a>神经网络。通过 Layers API，您可以构建不同类型的<a href="#layer"><strong>层</strong></a>，例如：</p>
<ul>
<li>通过 <code>tf.layers.Dense</code> 构建<a href="#fully_connected_layer"><strong>全连接层</strong></a>。</li>
<li>通过 <code>tf.layers.Conv2D</code> 构建卷积层。</li>
</ul>
<p>在编写<a href="#custom_estimator"><strong>自定义 Estimator</strong></a> 时，您可以编写“层”对象来定义所有<a href="#hidden_layers"><strong>隐藏层</strong></a>的特征。</p>
<p>Layers API 遵循 <a href="#Keras"><strong>Keras</strong></a> layers API 规范。也就是说，除了前缀不同以外，Layers API 中的所有函数均与 Keras layers API 中的对应函数具有相同的名称和签名。</p>
<p><a name="learning_rate"></a>
</p><h2 class="hide-from-toc">学习速率 (learning rate)</h2><p></p>
<p>在训练模型时用于梯度下降的一个标量。在每次迭代期间，<a href="#gradient_descent"><strong>梯度下降法</strong></a>都会将学习速率与梯度相乘。得出的乘积称为<strong>梯度步长</strong>。</p>
<p>学习速率是一个重要的<a href="#hyperparameter"><strong>超参数</strong></a>。</p>
<p><a name="least_squares_regression"></a>
</p><h2 class="hide-from-toc">最小二乘回归 (least squares regression)</h2><p></p>
<p>一种通过最小化 <a href="#L2_loss"><strong>L<sub>2</sub> 损失</strong></a>训练出的线性回归模型。</p>
<p><a name="linear_regression"></a>
</p><h2 class="hide-from-toc">线性回归 (linear regression)</h2><p></p>
<p>一种<a href="#regression_model"><strong>回归模型</strong></a>，通过将输入特征进行线性组合输出连续值。</p>
<p><a name="logistic_regression"></a>
</p><h2 class="hide-from-toc">逻辑回归 (logistic regression)</h2><p></p>
<p>一种模型，通过将 <a href="#sigmoid_function"><strong>S 型函数</strong></a>应用于线性预测，生成分类问题中每个可能的离散标签值的概率。虽然逻辑回归经常用于<a href="#binary_classification"><strong>二元分类</strong></a>问题，但也可用于<a href="#multi-class"><strong>多类别</strong></a>分类问题（其叫法变为<strong>多类别逻辑回归</strong>或<strong>多项回归</strong>）。</p>
<p><a name="logits"></a>
</p><h2 class="hide-from-toc">对数 (logits)</h2><p></p>
<p>分类模型生成的原始（非标准化）预测向量，通常会传递给标准化函数。如果模型要解决多类别分类问题，则对数通常变成 <a href="https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2?hl=zh-cn" target="_blank" rel="noopener">softmax 函数</a>的输入。之后，softmax 函数会生成一个（标准化）概率向量，对应于每个可能的类别。</p>
<p>此外，对数有时也称为 <a href="#sigmoid_function"><strong>S 型函数</strong></a>的元素级反函数。如需了解详细信息，请参阅 <a href="https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits?hl=zh-cn" target="_blank" rel="noopener">tf.nn.sigmoid_cross_entropy_with_logits</a>。</p>
<p><a name="Log_Loss"></a>
</p><h2 class="hide-from-toc">对数损失函数 (Log Loss)</h2><p></p>
<p>二元<a href="#logistic_regression"><strong>逻辑回归</strong></a>中使用的<a href="#loss"><strong>损失</strong></a>函数。</p>
<p><a name="log-odds"></a>
</p><h2 class="hide-from-toc">对数几率 (log-odds)</h2><p></p>
<p>某个事件几率的对数。</p>
<p>如果事件涉及二元概率，则<strong>几率</strong>指的是成功概率 (p) 与失败概率 (1-p) 之比。例如，假设某个给定事件的成功概率为 90％，失败概率为 10％。在这种情况下，几率的计算公式如下：</p>
<div>
$${\text{几率}} = \frac{\text{p}} {\text{(1-p)}} = \frac{.9} {.1} = {\text{9}}$$
</div>

<p>简单来说，对数几率即几率的对数。按照惯例，“对数”指自然对数，但对数的基数其实可以是任何大于 1 的数。若遵循惯例，上述示例的对数几率应为：</p>
<div>
$${\text{对数几率}} = ln(9) ~= 2.2$$
</div>

<p>对数几率是 <a href="#sigmoid_function"><strong>S 型函数</strong></a>的反函数。</p>
<p><a name="loss"></a>
</p><h2 class="hide-from-toc">损失 (Loss)</h2><p></p>
<p>一种衡量指标，用于衡量模型的<a href="#prediction"><strong>预测</strong></a>偏离其<a href="#label"><strong>标签</strong></a>的程度。或者更悲观地说是衡量模型有多差。要确定此值，模型必须定义损失函数。例如，线性回归模型通常将<a href="#MSE"><strong>均方误差</strong></a>用作损失函数，而逻辑回归模型则使用<a href="#Log_Loss"><strong>对数损失函数</strong></a>。</p>
<h2 class="glossary">M</h2>

<p><a name="machine_learning"></a>
</p><h2 class="hide-from-toc">机器学习 (machine learning)</h2><p></p>
<p>一种程序或系统，用于根据输入数据构建（训练）预测模型。这种系统会利用学到的模型根据从分布（训练该模型时使用的同一分布）中提取的新数据（以前从未见过的数据）进行实用的预测。机器学习还指与这些程序或系统相关的研究领域。</p>
<p><a name="MSE"></a>
</p><h2 class="hide-from-toc">均方误差 (MSE, Mean Squared Error)</h2><p></p>
<p>每个样本的平均平方损失。MSE 的计算方法是<a href="#squared_loss"><strong>平方损失</strong></a>除以<a href="#example"><strong>样本</strong></a>数。<a href="#TensorFlow_Playground"><strong>TensorFlow Playground</strong></a> 显示的“训练损失”值和“测试损失”值都是 MSE。</p>
<p><a name="metric"></a>
</p><h2 class="hide-from-toc">指标 (metric)</h2><p></p>
<p>您关心的一个数值。可能可以也可能不可以直接在机器学习系统中得到优化。您的系统尝试优化的指标称为<a href="#objective"><strong>目标</strong></a>。</p>
<p><a name="metrics_API"></a>
</p><h2 class="hide-from-toc">Metrics API (tf.metrics)</h2><p></p>
<p>一种用于评估模型的 TensorFlow API。例如，<code>tf.metrics.accuracy</code> 用于确定模型的预测与标签匹配的频率。在编写<a href="#custom_estimator"><strong>自定义 Estimator</strong></a> 时，您可以调用 Metrics API 函数来指定应如何评估您的模型。</p>
<p><a name="mini-batch"></a>
</p><h2 class="hide-from-toc">小批次 (mini-batch)</h2><p></p>
<p>从整批<a href="#example"><strong>样本</strong></a>内随机选择并在训练或推断过程的一次迭代中一起运行的一小部分样本。小批次的<a href="#batch_size"><strong>批次大小</strong></a>通常介于 10 到 1000 之间。与基于完整的训练数据计算损失相比，基于小批次数据计算损失要高效得多。</p>
<p><a name="mini-batch_SGD"></a>
</p><h2 class="hide-from-toc">小批次随机梯度下降法 (SGD, mini-batch stochastic gradient descent)</h2><p></p>
<p>一种采用<a href="#mini-batch"><strong>小批次</strong></a>样本的<a href="#gradient_descent"><strong>梯度下降法</strong></a>。也就是说，小批次 SGD 会根据一小部分训练数据来估算梯度。<a href="#SGD"><strong>Vanilla SGD</strong></a> 使用的小批次的大小为 1。</p>
<p><a name="ML"></a>
</p><h2 class="hide-from-toc">ML</h2><p></p>
<p><a href="#machine_learning"><strong>机器学习</strong></a>的缩写。</p>
<p><a name="model"></a>
</p><h2 class="hide-from-toc">模型 (model)</h2><p></p>
<p>机器学习系统从训练数据学到的内容的表示形式。多含义术语，可以理解为下列两种相关含义之一：</p>
<ul>
<li>一种 <a href="#TensorFlow"><strong>TensorFlow</strong></a> 图，用于表示预测的计算结构。</li>
<li>该 TensorFlow 图的特定权重和偏差，通过<a href="#model_training"><strong>训练</strong></a>决定。</li>
</ul>
<p><a name="model_function"></a>
</p><h2 class="hide-from-toc">模型函数 (model function)</h2><p></p>
<p><a href="#Estimators"><strong>Estimator</strong></a> 中的函数，用于实现机器学习训练、评估和推断。例如，模型函数的训练部分可以处理以下任务：定义深度神经网络的拓扑并确定其<a href="#optimizer"><strong>优化器</strong></a>函数。如果使用<a href="#pre-made_Estimator"><strong>预创建的 Estimator</strong></a>，则有人已为您编写了模型函数。如果使用<a href="#custom_estimator"><strong>自定义 Estimator</strong></a>，则必须自行编写模型函数。</p>
<p>有关编写模型函数的详细信息，请参阅<a href="https://www.tensorflow.org/get_started/custom_estimators?hl=zh-cn" target="_blank" rel="noopener">创建自定义 Estimator</a>。</p>
<p><a name="model_training"></a>
</p><h2 class="hide-from-toc">模型训练 (model training)</h2><p></p>
<p>确定最佳<a href="#model"><strong>模型</strong></a>的过程。</p>
<p><a name="Momentum"></a>
</p><h2 class="hide-from-toc">动量 (Momentum)</h2><p></p>
<p>一种先进的梯度下降法，其中学习步长不仅取决于当前步长的导数，还取决于之前一步或多步的步长的导数。动量涉及计算梯度随时间而变化的指数级加权移动平均值，与物理学中的动量类似。动量有时可以防止学习过程被卡在局部最小的情况。</p>
<p><a name="multi-class"></a>
</p><h2 class="hide-from-toc">多类别分类 (multi-class classification)</h2><p></p>
<p>区分两种以上类别的分类问题。例如，枫树大约有 128 种，因此，确定枫树种类的模型就属于多类别模型。反之，仅将电子邮件分为两类（“垃圾邮件”和“非垃圾邮件”）的模型属于<a href="#binary_classification"><strong>二元分类模型</strong></a>。<em></em><em></em></p>
<p><a name="multinomial_classification"></a>
</p><h2 class="hide-from-toc">多项分类 (multinomial classification)</h2><p></p>
<p>与<a href="#multi-class"><strong>多类别分类</strong></a>的含义相同。</p>
<h2 class="glossary">N</h2>

<p><a name="NaN_trap"></a>
</p><h2 class="hide-from-toc">NaN 陷阱 (NaN trap)</h2><p></p>
<p>模型中的一个数字在训练期间变成 <a href="https://en.wikipedia.org/wiki/NaN" target="_blank" rel="noopener">NaN</a>，这会导致模型中的很多或所有其他数字最终也会变成 NaN。</p>
<p>NaN 是“非数字”的缩写。</p>
<p><a name="negative_class"></a>
</p><h2 class="hide-from-toc">负类别 (negative class)</h2><p></p>
<p>在<a href="#binary_classification"><strong>二元分类</strong></a>中，一种类别称为正类别，另一种类别称为负类别。正类别是我们要寻找的类别，负类别则是另一种可能性。例如，在医学检查中，负类别可以是“非肿瘤”。在电子邮件分类器中，负类别可以是“非垃圾邮件”。另请参阅<a href="#positive_class"><strong>正类别</strong></a>。</p>
<p><a name="neural_network"></a>
</p><h2 class="hide-from-toc">神经网络 (neural network)</h2><p></p>
<p>一种模型，灵感来源于脑部结构，由多个层构成（至少有一个是<a href="#hidden_layer"><strong>隐藏层</strong></a>），每个层都包含简单相连的单元或<a href="#neuron"><strong>神经元</strong></a>（具有非线性关系）。</p>
<p><a name="neuron"></a>
</p><h2 class="hide-from-toc">神经元 (neuron)</h2><p></p>
<p><a href="#neural_network"><strong>神经网络</strong></a>中的节点，通常会接收多个输入值并生成一个输出值。神经元通过将<a href="#activation_function"><strong>激活函数</strong></a>（非线性转换）应用于输入值的加权和来计算输出值。</p>
<p><a name="node"></a>
</p><h2 class="hide-from-toc">节点 (node)</h2><p></p>
<p>多含义术语，可以理解为下列两种含义之一：</p>
<ul>
<li><a href="#hidden_layer"><strong>隐藏层</strong></a>中的神经元。</li>
<li>TensorFlow <a href="#graph"><strong>图</strong></a>中的操作。</li>
</ul>
<p><a name="normalization"></a>
</p><h2 class="hide-from-toc">标准化 (normalization)</h2><p></p>
<p>将实际的值区间转换为标准的值区间（通常为 -1 到 +1 或 0 到 1）的过程。例如，假设某个特征的自然区间是 800 到 6000。通过减法和除法运算，您可以将这些值标准化为位于 -1 到 +1 区间内。</p>
<p>另请参阅<a href="#scaling"><strong>缩放</strong></a>。</p>
<p><a name="numerical_data"></a>
</p><h2 class="hide-from-toc">数值数据 (numerical data)</h2><p></p>
<p>用整数或实数表示的<a href="#feature"><strong>特征</strong></a>。例如，在房地产模型中，您可能会用数值数据表示房子大小（以平方英尺或平方米为单位）。如果用数值数据表示特征，则可以表明特征的值相互之间具有数学关系，并且与标签可能也有数学关系。<em></em>例如，如果用数值数据表示房子大小，则可以表明面积为 200 平方米的房子是面积为 100 平方米的房子的两倍。此外，房子面积的平方米数可能与房价存在一定的数学关系。</p>
<p>并非所有整数数据都应表示成数值数据。例如，世界上某些地区的邮政编码是整数，但在模型中，不应将整数邮政编码表示成数值数据。这是因为邮政编码 <code>20000</code> 在效力上并不是邮政编码 10000 的两倍（或一半）。此外，虽然不同的邮政编码确实与不同的房地产价值有关，但我们也不能假设邮政编码为 20000 的房地产在价值上是邮政编码为 10000 的房地产的两倍。<em></em>邮政编码应表示成<a href="#categorical_data"><strong>分类数据</strong></a>。</p>
<p>数值特征有时称为<a href="#continuous_feature"><strong>连续特征</strong></a>。</p>
<p><a name="numpy"></a>
</p><h2 class="hide-from-toc">Numpy</h2><p></p>
<p>一个<a href="http://www.numpy.org/" target="_blank" rel="noopener">开放源代码数学库</a>，在 Python 中提供高效的数组操作。<a href="#pandas"><strong>Pandas</strong></a> 建立在 Numpy 之上。</p>
<h2 class="glossary">O</h2>

<p><a name="objective"></a>
</p><h2 class="hide-from-toc">目标 (objective)</h2><p></p>
<p>算法尝试优化的指标。</p>
<p><a name="offline_inference"></a>
</p><h2 class="hide-from-toc">离线推断 (offline inference)</h2><p></p>
<p>生成一组<a href="#prediction"><strong>预测</strong></a>，存储这些预测，然后根据需求检索这些预测。与<a href="#online_inference"><strong>在线推断</strong></a>相对。</p>
<p><a name="one-hot_encoding"></a>
</p><h2 class="hide-from-toc">独热编码 (one-hot encoding)</h2><p></p>
<p>一种稀疏向量，其中：</p>
<ul>
<li>一个元素设为 1。</li>
<li>所有其他元素均设为 0。</li>
</ul>
<p>独热编码常用于表示拥有有限个可能值的字符串或标识符。例如，假设某个指定的植物学数据集记录了 15000 个不同的物种，其中每个物种都用独一无二的字符串标识符来表示。在特征工程过程中，您可能需要将这些字符串标识符编码为独热向量，向量的大小为 15000。</p>
<p><a name="one-shot_learning"></a>
</p><h2 class="hide-from-toc">单样本学习（one-shot learning，通常用于对象分类）</h2><p></p>
<p>一种机器学习方法，通常用于对象分类，旨在通过单个训练样本学习有效的分类器。</p>
<p>另请参阅<a href="#few-shot_learning"><strong>少量样本学习</strong></a>。</p>
<p><a name="one-vs.-all"></a>
</p><h2 class="hide-from-toc">一对多 (one-vs.-all)</h2><p></p>
<p>假设某个分类问题有 N 种可能的解决方案，一对多解决方案将包含 N 个单独的<a href="#binary_classification"><strong>二元分类器</strong></a> - 一个二元分类器对应一种可能的结果。例如，假设某个模型用于区分样本属于动物、蔬菜还是矿物，一对多解决方案将提供下列三个单独的二元分类器：</p>
<ul>
<li>动物和非动物</li>
<li>蔬菜和非蔬菜</li>
<li>矿物和非矿物</li>
</ul>
<p><a name="online_inference"></a>
</p><h2 class="hide-from-toc">在线推断 (online inference)</h2><p></p>
<p>根据需求生成<a href="#prediction"><strong>预测</strong></a>。与<a href="#offline_inference"><strong>离线推断</strong></a>相对。</p>
<p><a name="Operation"></a>
</p><h2 class="hide-from-toc">操作 (op, Operation)</h2><p></p>
<p>TensorFlow 图中的节点。在 TensorFlow 中，任何创建、操纵或销毁<a href="#tensor"><strong>张量</strong></a>的过程都属于操作。例如，矩阵相乘就是一种操作，该操作以两个张量作为输入，并生成一个张量作为输出。</p>
<p><a name="optimizer"></a>
</p><h2 class="hide-from-toc">优化器 (optimizer)</h2><p></p>
<p><a href="#gradient_descent"><strong>梯度下降法</strong></a>的一种具体实现。TensorFlow 的优化器基类是 <a href="https://www.tensorflow.org/api_docs/python/tf/train/Optimizer?hl=zh-cn" target="_blank" rel="noopener">tf.train.Optimizer</a>。不同的优化器可能会利用以下一个或多个概念来增强梯度下降法在指定<a href="#training_set"><strong>训练集</strong></a>中的效果：</p>
<ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer?hl=zh-cn" target="_blank" rel="noopener">动量</a> (Momentum)</li>
<li>更新频率（<a href="https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer?hl=zh-cn" target="_blank" rel="noopener">AdaGrad</a> = ADAptive GRADient descent；<a href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer?hl=zh-cn" target="_blank" rel="noopener">Adam</a> = ADAptive with Momentum；RMSProp）</li>
<li>稀疏性/正则化 (<a href="https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer?hl=zh-cn" target="_blank" rel="noopener">Ftrl</a>)</li>
<li>更复杂的数学方法（<a href="https://www.tensorflow.org/api_docs/python/tf/train/ProximalGradientDescentOptimizer?hl=zh-cn" target="_blank" rel="noopener">Proximal</a>，等等）</li>
</ul>
<p>甚至还包括 <a href="https://arxiv.org/abs/1606.04474" target="_blank" rel="noopener">NN 驱动的优化器</a>。</p>
<p><a name="outliers"></a>
</p><h2 class="hide-from-toc">离群值 (outlier)</h2><p></p>
<p>与大多数其他值差别很大的值。在机器学习中，下列所有值都是离群值。</p>
<ul>
<li>绝对值很高的<a href="#weight"><strong>权重</strong></a>。</li>
<li>与实际值相差很大的预测值。</li>
<li>值比平均值高大约 3 个标准偏差的输入数据。</li>
</ul>
<p>离群值常常会导致模型训练出现问题。</p>
<p><a name="output_layer"></a>
</p><h2 class="hide-from-toc">输出层 (output layer)</h2><p></p>
<p>神经网络的“最后”一层，也是包含答案的层。</p>
<p><a name="overfitting"></a>
</p><h2 class="hide-from-toc">过拟合 (overfitting)</h2><p></p>
<p>创建的模型与<a href="#training_set"><strong>训练数据</strong></a>过于匹配，以致于模型无法根据新数据做出正确的预测。</p>
<h2 class="glossary">P</h2>

<p><a name="pandas"></a>
</p><h2 class="hide-from-toc">Pandas</h2><p></p>
<p>面向列的数据分析 API。很多机器学习框架（包括 TensorFlow）都支持将 Pandas 数据结构作为输入。请参阅 <a href="http://pandas.pydata.org/" target="_blank" rel="noopener">Pandas 文档</a>。</p>
<p><a name="parameter"></a>
</p><h2 class="hide-from-toc">参数 (parameter)</h2><p></p>
<p>机器学习系统自行训练的模型的变量。例如，<a href="#weight"><strong>权重</strong></a>就是一种参数，它们的值是机器学习系统通过连续的训练迭代逐渐学习到的。与<a href="#hyperparameter"><strong>超参数</strong></a>相对。</p>
<p><a name="Parameter_Server"></a>
</p><h2 class="hide-from-toc">参数服务器 (PS, Parameter Server)</h2><p></p>
<p>一种作业，负责在分布式设置中跟踪模型<a href="#parameter"><strong>参数</strong></a>。</p>
<p><a name="parameter_update"></a>
</p><h2 class="hide-from-toc">参数更新 (parameter update)</h2><p></p>
<p>在训练期间（通常是在<a href="#gradient_descent"><strong>梯度下降法</strong></a>的单次迭代中）调整模型<a href="#parameter"><strong>参数</strong></a>的操作。</p>
<p><a name="partial_derivative"></a>
</p><h2 class="hide-from-toc">偏导数 (partial derivative)</h2><p></p>
<p>一种导数，除一个变量之外的所有变量都被视为常量。例如，f(x, y) 对 x 的偏导数就是 f(x) 的导数（即，使 y 保持恒定）。<em></em><em></em><em></em><em></em><em></em>f 对 x 的偏导数仅关注 x 如何变化，而忽略公式中的所有其他变量。<em></em><em></em><em></em></p>
<p><a name="partitioning_strategy"></a>
</p><h2 class="hide-from-toc">划分策略 (partitioning strategy)</h2><p></p>
<p>在<a href="#Parameter_Server"><strong>参数服务器</strong></a>间分割变量的算法。</p>

<p></p><p><a name="performance"></a>
</p><h2 class="hide-from-toc">性能 (performance)</h2><p></p>
<p>多含义术语，具有以下含义：</p>
<ul>
<li>在软件工程中的传统含义。即：相应软件的运行速度有多快（或有多高效）？</li>
<li>在机器学习中的含义。在机器学习领域，性能旨在回答以下问题：相应<a href="#model"><strong>模型</strong></a>的准确度有多高？即模型在预测方面的表现有多好？</li>
</ul>
<p><a name="perplexity"></a>
</p><h2 class="hide-from-toc">困惑度 (perplexity)</h2><p></p>
<p>一种衡量指标，用于衡量<a href="#model"><strong>模型</strong></a>能够多好地完成任务。例如，假设任务是读取用户使用智能手机键盘输入字词时输入的前几个字母，然后列出一组可能的完整字词。此任务的困惑度 (P) 是：为了使列出的字词中包含用户尝试输入的实际字词，您需要提供的猜测项的个数。</p>
<p>困惑度与<a href="#cross-entropy"><strong>交叉熵</strong></a>的关系如下：</p>
<div>
$$P= 2^{-\text{cross entropy}}$$
</div>

<p><a name="pipeline"></a>
</p><h2 class="hide-from-toc">流水线 (pipeline)</h2><p></p>
<p>机器学习算法的基础架构。流水线包括收集数据、将数据放入训练数据文件、训练一个或多个模型，以及将模型导出到生产环境。</p>
<p><a name="pooling"></a>
</p><h2 class="hide-from-toc">池化 (pooling)</h2><p></p>
<p>将一个或多个由前趋的<a href="#convolutional_layer"><strong>卷积层</strong></a>创建的矩阵压缩为较小的矩阵。池化通常是取整个池化区域的最大值或平均值。以下面的 3x3 矩阵为例：</p>
<p>

</p>

<p>池化运算与卷积运算类似：将矩阵分割为多个切片，然后按<a href="#stride"><strong>步长</strong></a>逐个运行卷积运算。例如，假设池化运算按 1x1 步长将卷积矩阵分割为 2x2 个切片。如下图所示，进行了四个池化运算。假设每个池化运算都选择该切片中四个值的最大值：</p>
<p>

</p>

<p>池化有助于在输入矩阵中实现<a href="#translational_invariance"><strong>平移不变性</strong></a>。</p>
<p>对于视觉应用来说，池化的更正式名称为<strong>空间池化</strong>。时间序列应用通常将池化称为<strong>时序池化</strong>。按照不太正式的说法，池化通常称为<strong>下采样</strong>或<strong>降采样</strong>。</p>
<p><a name="positive_class"></a>
</p><h2 class="hide-from-toc">正类别 (positive class)</h2><p></p>
<p>在<a href="#binary_classification"><strong>二元分类</strong></a>中，两种可能的类别分别被标记为正类别和负类别。正类别结果是我们要测试的对象。（不可否认的是，我们会同时测试这两种结果，但只关注正类别结果。）例如，在医学检查中，正类别可以是“肿瘤”。在电子邮件分类器中，正类别可以是“垃圾邮件”。</p>
<p>与<a href="#negative_class"><strong>负类别</strong></a>相对。</p>
<p><a name="precision"></a>
</p><h2 class="hide-from-toc">精确率 (precision)</h2><p></p>
<p>一种<a href="#classification_model"><strong>分类模型</strong></a>指标。精确率指模型正确预测<a href="#positive_class"><strong>正类别</strong></a>的频率，即：</p>
<div>
$$\text{精确率} = \frac{\text{正例数}} {\text{正例数} + \text{假正例数}}$$
</div>

<p><a name="prediction"></a>
</p><h2 class="hide-from-toc">预测 (prediction)</h2><p></p>
<p>模型在收到输入<a href="#example"><strong>样本</strong></a>后的输出。</p>
<p><a name="prediction_bias"></a>
</p><h2 class="hide-from-toc">预测偏差 (prediction bias)</h2><p></p>
<p>一种值，用于表明<a href="#prediction"><strong>预测</strong></a>平均值与数据集中<a href="#label"><strong>标签</strong></a>的平均值相差有多大。</p>
<p><a name="pre-made_Estimator"></a>
</p><h2 class="hide-from-toc">预创建的 Estimator (pre-made Estimator)</h2><p></p>
<p>其他人已建好的 <a href="#Estimator"><strong>Estimator</strong></a>。TensorFlow 提供了一些预创建的 Estimator，包括 <code>DNNClassifier</code>、<code>DNNRegressor</code> 和 <code>LinearClassifier</code>。您可以按照<a href="https://www.tensorflow.org/extend/estimators?hl=zh-cn" target="_blank" rel="noopener">这些说明</a>构建自己预创建的 Estimator。</p>
<p><a name="pre-trained_model"></a>
</p><h2 class="hide-from-toc">预训练模型 (pre-trained model)</h2><p></p>
<p>已经过训练的模型或模型组件（例如<a href="#embeddings"><strong>嵌套</strong></a>）。有时，您需要将预训练的嵌套馈送到<a href="#neural_network"><strong>神经网络</strong></a>。在其他时候，您的模型将自行训练嵌套，而不依赖于预训练的嵌套。</p>
<p><a name="prior_belief"></a>
</p><h2 class="hide-from-toc">先验信念 (prior belief)</h2><p></p>
<p>在开始采用相应数据进行训练之前，您对这些数据抱有的信念。例如，<a href="#L2_regularization"><strong>L<sub>2</sub> 正则化</strong></a>依赖的先验信念是<a href="#weight"><strong>权重</strong></a>应该很小且应以 0 为中心呈正态分布。</p>
<h2 class="glossary">Q</h2>

<p><a name="queue"></a>
</p><h2 class="hide-from-toc">队列 (queue)</h2><p></p>
<p>一种 TensorFlow <a href="#Operation"><strong>操作</strong></a>，用于实现队列数据结构。通常用于 I/O 中。</p>
<h2 class="glossary">R</h2>

<p><a name="rank"></a>
</p><h2 class="hide-from-toc">等级 (rank)</h2><p></p>
<p>机器学习中的一个多含义术语，可以理解为下列含义之一：</p>
<ul>
<li><a href="#tensor"><strong>张量</strong></a>中的维数。例如，标量等级为 0，向量等级为 1，矩阵等级为 2。</li>
<li>在将类别从最高到最低进行排序的机器学习问题中，类别的顺序位置。例如，行为排序系统可以将狗狗的奖励从最高（牛排）到最低（枯萎的羽衣甘蓝）进行排序。</li>
</ul>
<p><a name="rater"></a>
</p><h2 class="hide-from-toc">评分者 (rater)</h2><p></p>
<p>为<a href="#example"><strong>样本</strong></a>提供<a href="#label"><strong>标签</strong></a>的人。有时称为“注释者”。</p>
<p><a name="recall"></a>
</p><h2 class="hide-from-toc">召回率 (recall)</h2><p></p>
<p>一种<a href="#classification_model"><strong>分类模型</strong></a>指标，用于回答以下问题：在所有可能的正类别标签中，模型正确地识别出了多少个？即：</p>
<p>$$\text{召回率} = \frac{\text{正例数}} {\text{正例数} + \text{假负例数}}$$</p>
<p><a name="ReLU"></a>
</p><h2 class="hide-from-toc">修正线性单元 (ReLU, Rectified Linear Unit)</h2><p></p>
<p>一种<a href="#activation_function"><strong>激活函数</strong></a>，其规则如下：</p>
<ul>
<li>如果输入为负数或 0，则输出 0。</li>
<li>如果输入为正数，则输出等于输入。</li>
</ul>
<p><a name="regression_model"></a>
</p><h2 class="hide-from-toc">回归模型 (regression model)</h2><p></p>
<p>一种模型，能够输出连续的值（通常为浮点值）。请与<a href="#classification_model"><strong>分类模型</strong></a>进行比较，分类模型会输出离散值，例如“黄花菜”或“虎皮百合”。</p>
<p><a name="regularization"></a>
</p><h2 class="hide-from-toc">正则化 (regularization)</h2><p></p>
<p>对模型复杂度的惩罚。正则化有助于防止出现<a href="#overfitting"><strong>过拟合</strong></a>，包含以下类型：</p>
<ul>
<li><a href="#L1_regularization"><strong>L<sub>1</sub> 正则化</strong></a></li>
<li><a href="#L2_regularization"><strong>L<sub>2</sub> 正则化</strong></a></li>
<li><a href="#dropout_regularization"><strong>丢弃正则化</strong></a></li>
<li><a href="#early_stopping"><strong>早停法</strong></a>（这不是正式的正则化方法，但可以有效限制过拟合）</li>
</ul>
<p><a name="regularization_rate"></a>
</p><h2 class="hide-from-toc">正则化率 (regularization rate)</h2><p></p>
<p>一种标量值，以 lambda 表示，用于指定正则化函数的相对重要性。从下面简化的<a href="#loss"><strong>损失</strong></a>公式中可以看出正则化率的影响：</p>
<div>
$$\text{最小化(损失方程 + }\lambda\text{(正则化方程))}$$
</div>

<p>提高正则化率可以减少<a href="#overfitting"><strong>过拟合</strong></a>，但可能会使模型的<a href="#accuracy"><strong>准确率</strong></a>降低。</p>
<p><a name="representation"></a>
</p><h2 class="hide-from-toc">表示法 (representation)</h2><p></p>
<p>将数据映射到实用<a href="#feature"><strong>特征</strong></a>的过程。</p>
<p><a name="ROC"></a>
</p><h2 class="hide-from-toc">受试者工作特征曲线（receiver operating characteristic，简称 ROC 曲线）</h2><p></p>
<p>不同<a href="#classification_threshold"><strong>分类阈值</strong></a>下的<a href="#TP_rate"><strong>正例率</strong></a>和<a href="#FP_rate"><strong>假正例率</strong></a>构成的曲线。另请参阅<a href="#AUC"><strong>曲线下面积</strong></a>。</p>
<p><a name="root_directory"></a>
</p><h2 class="hide-from-toc">根目录 (root directory)</h2><p></p>
<p>您指定的目录，用于托管多个模型的 TensorFlow 检查点和事件文件的子目录。</p>
<p><a name="RMSE"></a>
</p><h2 class="hide-from-toc">均方根误差 (RMSE, Root Mean Squared Error)</h2><p></p>
<p><a href="#MSE"><strong>均方误差</strong></a>的平方根。</p>
<p><a name="rotational_invariance"></a>
</p><h2 class="hide-from-toc">旋转不变性 (rotational invariance)</h2><p></p>
<p>在图像分类问题中，即使图像的方向发生变化，算法也能成功地对图像进行分类。例如，无论网球拍朝上、侧向还是朝下放置，该算法仍然可以识别它。请注意，并非总是希望旋转不变；例如，倒置的“9”不应分类为“9”。</p>
<p>另请参阅<a href="#translational_invariance"><strong>平移不变性</strong></a>和<a href="#size_invariance"><strong>大小不变性</strong></a>。</p>
<h2 class="glossary">S</h2>

<p><a name="SavedModel"></a>
</p><h2 class="hide-from-toc">SavedModel</h2><p></p>
<p>保存和恢复 TensorFlow 模型时建议使用的格式。SavedModel 是一种独立于语言且可恢复的序列化格式，使较高级别的系统和工具可以创建、使用和转换 TensorFlow 模型。</p>
<p>如需完整的详细信息，请参阅《TensorFlow 编程人员指南》中的<a href="https://www.tensorflow.org/programmers_guide/saved_model?hl=zh-cn" target="_blank" rel="noopener">保存和恢复</a>。</p>
<p><a name="Saver"></a>
</p><h2 class="hide-from-toc">Saver</h2><p></p>
<p>一种 <a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver?hl=zh-cn" target="_blank" rel="noopener">TensorFlow 对象</a>，负责保存模型检查点。</p>
<p><a name="scaling"></a>
</p><h2 class="hide-from-toc">缩放 (scaling)</h2><p></p>
<p><a href="#feature_engineering"><strong>特征工程</strong></a>中的一种常用做法，是指对某个特征的值区间进行调整，使之与数据集中其他特征的值区间一致。例如，假设您希望数据集中所有浮点特征的值都位于 0 到 1 区间内，如果某个特征的值位于 0 到 500 区间内，您就可以通过将每个值除以 500 来缩放该特征。</p>
<p>另请参阅<a href="#normalization"><strong>标准化</strong></a>。</p>
<p><a name="scikit-learn"></a>
</p><h2 class="hide-from-toc">scikit-learn</h2><p></p>
<p>一个热门的开放源代码机器学习平台。请访问 <a href="http://www.scikit-learn.org/" target="_blank" rel="noopener">www.scikit-learn.org</a>。</p>
<p><a name="semi-supervised_learning"></a>
</p><h2 class="hide-from-toc">半监督式学习 (semi-supervised learning)</h2><p></p>
<p>训练模型时采用的数据中，某些训练样本有标签，而其他样本则没有标签。半监督式学习采用的一种技术是推断无标签样本的标签，然后使用推断出的标签进行训练，以创建新模型。如果获得有标签样本需要高昂的成本，而无标签样本则有很多，那么半监督式学习将非常有用。</p>
<p><a name="sequence_model"></a>
</p><h2 class="hide-from-toc">序列模型 (sequence model)</h2><p></p>
<p>一种模型，其输入具有序列依赖性。例如，根据之前观看过的一系列视频对观看的下一个视频进行预测。</p>


<p></p><p><a name="session"></a>
</p><h2 class="hide-from-toc">会话 (tf.session)</h2><p></p>
<p>封装了 TensorFlow 运行时状态的对象，用于运行全部或部分<a href="#graph"><strong>图</strong></a>。在使用底层 TensorFlow API 时，您可以直接创建并管理一个或多个 <code>tf.session</code> 对象。在使用 Estimator API 时，Estimator 会为您创建会话对象。</p>

<p></p><p><a name="sigmoid_function"></a>
</p><h2 class="hide-from-toc">S 型函数 (sigmoid function)</h2><p></p>
<p>一种函数，可将逻辑回归输出或多项回归输出（对数几率）映射到概率，以返回介于 0 到 1 之间的值。S 型函数的公式如下：</p>
<div>
$$y = \frac{1}{1 + e^{-\sigma}}$$
</div>

<p>在<a href="#logistic_regression"><strong>逻辑回归</strong></a>问题中，\(\sigma\) 非常简单：</p>
<div>
$$\sigma = b + w_1x_1 + w_2x_2 + … w_nx_n$$
</div>

<p>换句话说，S 型函数可将 \(\sigma\) 转换为介于 0 到 1 之间的概率。</p>
<p>在某些<a href="#neural_network"><strong>神经网络</strong></a>中，S 型函数可作为<a href="#activation_function"><strong>激活函数</strong></a>使用。</p>
<p><a name="size_invariance"></a>
</p><h2 class="hide-from-toc">大小不变性 (size invariance)</h2><p></p>
<p>在图像分类问题中，即使图像的大小发生变化，算法也能成功地对图像进行分类。例如，无论一只猫以 200 万像素还是 20 万像素呈现，该算法仍然可以识别它。请注意，即使是最好的图像分类算法，在大小不变性方面仍然会存在切实的限制。例如，对于仅以 20 像素呈现的猫图像，算法（或人）不可能正确对其进行分类。</p>
<p>另请参阅<a href="#translational_invariance"><strong>平移不变性</strong></a>和<a href="#rotational_invariance"><strong>旋转不变性</strong></a>。</p>
<p><a name="softmax"></a>
</p><h2 class="hide-from-toc">softmax</h2><p></p>
<p>一种函数，可提供<a href="#multi-class"><strong>多类别分类模型</strong></a>中每个可能类别的概率。这些概率的总和正好为 1.0。例如，softmax 可能会得出某个图像是狗、猫和马的概率分别是 0.9、0.08 和 0.02。（也称为<strong>完整 softmax</strong>。）</p>
<p>与<a href="#candidate_sampling"><strong>候选采样</strong></a>相对。</p>
<p><a name="sparse_features"></a>
</p><h2 class="hide-from-toc">稀疏特征 (sparse feature)</h2><p></p>
<p>一种<a href="#feature"><strong>特征</strong></a>向量，其中的大多数值都为 0 或为空。例如，某个向量包含一个为 1 的值和一百万个为 0 的值，则该向量就属于稀疏向量。再举一个例子，搜索查询中的单词也可能属于稀疏特征 - 在某种指定语言中有很多可能的单词，但在某个指定的查询中仅包含其中几个。</p>
<p>与<a href="#dense_feature"><strong>密集特征</strong></a>相对。</p>
<p><a name="sparse_representation"></a>
</p><h2 class="hide-from-toc">稀疏表示法 (sparse representation)</h2><p></p>
<p>一种张量<a href="#representation"><strong>表示法</strong></a>，仅存储非零元素。</p>
<p>例如，英语中包含约一百万个单词。表示一个英语句子中所用单词的数量，考虑以下两种方式：</p>
<ul>
<li>要采用<strong>密集表示法</strong>来表示此句子，则必须为所有一百万个单元格设置一个整数，然后在大部分单元格中放入 0，在少数单元格中放入一个非常小的整数。</li>
<li>要采用稀疏表示法来表示此句子，则仅存储象征句子中实际存在的单词的单元格。因此，如果句子只包含 20 个独一无二的单词，那么该句子的稀疏表示法将仅在 20 个单元格中存储一个整数。</li>
</ul>
<p>例如，假设以两种方式来表示句子“Dogs wag tails.”。如下表所示，密集表示法将使用约一百万个单元格；稀疏表示法则只使用 3 个单元格：</p>
<div id="sparse-dense-tables">
<table id="sparse-table">
<caption>密集表示法</caption>
<thead>
  <tr>
  <th>单元格编号</th>
  <th>单词</th>
  <th>出现次数</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>0</td>
    <td>a</td>
    <td>0</td>
  </tr>
  <tr>
    <td>1</td>
    <td>aardvark</td>
    <td>0</td>
  </tr>
  <tr>
    <td>2</td>
    <td>aargh</td>
    <td>0</td>
  </tr>
  <tr>
    <td>3</td>
    <td>aarti</td>
    <td>0</td>
  </tr>
  <tr class="elided-rows">
    <td colspan="3"><strong>… 出现次数为 0 的另外 140391 个单词</strong></td>
  </tr>
  <tr>
    <td>140395</td>
    <td>dogs</td>
    <td>1</td>
  </tr>
  <tr class="elided-rows">
    <td colspan="3"><strong>… 出现次数为 0 的 633062 个单词</strong></td>
  </tr>
  <tr>
    <td>773458</td>
    <td>tails</td>
    <td>1</td>
  </tr>
  <tr class="elided-rows">
    <td colspan="3"><strong>… 出现次数为 0 的 189136 个单词</strong></td>
  </tr>
  <tr>
    <td>962594</td>
    <td>wag</td>
    <td>1</td>
  </tr>
  <tr class="elided-rows">
    <td colspan="3"><strong>… 出现次数为 0 的很多其他单词</strong></td>
  </tr>
</tbody>
</table>

<table id="dense-table">
<caption>稀疏表示法</caption>
<thead>
  <tr>
  <th>单元格编号</th>
  <th>单词</th>
  <th>出现次数</th>
  </tr>
</thead>
<tbody>
<tr>
  <td>140395</td>
  <td>dogs</td>
  <td>1</td>
</tr>
<tr>
  <td>773458</td>
  <td>tails</td>
  <td>1</td>
</tr>
<tr>
  <td>962594</td>
  <td>wag</td>
  <td>1</td>
</tr>
</tbody>
</table>
</div>

<p><a name="sparsity"></a>
</p><h2 class="hide-from-toc">稀疏性 (sparsity)</h2><p></p>
<p>向量或矩阵中设置为 0（或空）的元素数除以该向量或矩阵中的条目总数。以一个 10x10 矩阵（其中 98 个单元格都包含 0）为例。稀疏性的计算方法如下：</p>
<div>
$${\text{稀疏性}} = \frac{\text{98}} {\text{100}} = {\text{0.98}}$$
</div>

<p><strong>特征稀疏性</strong>是指特征向量的稀疏性；<strong>模型稀疏性</strong>是指模型权重的稀疏性。</p>
<p><a name="spatial_pooling"></a>
</p><h2 class="hide-from-toc">空间池化 (spatial pooling)</h2><p></p>
<p>请参阅<a href="#pooling"><strong>池化</strong></a>。</p>
<p><a name="squared_hinge_loss"></a>
</p><h2 class="hide-from-toc">平方合页损失函数 (squared hinge loss)</h2><p></p>
<p><a href="#hinge-loss"><strong>合页损失函数</strong></a>的平方。与常规合页损失函数相比，平方合页损失函数对离群值的惩罚更严厉。</p>
<p><a name="squared_loss"></a>
</p><h2 class="hide-from-toc">平方损失函数 (squared loss)</h2><p></p>
<p>在<a href="#linear_regression"><strong>线性回归</strong></a>中使用的<a href="#loss"><strong>损失</strong></a>函数（也称为 <strong>L<sub>2</sub> 损失函数</strong>）。该函数可计算模型为有标签<a href="#example"><strong>样本</strong></a>预测的值和<a href="#label"><strong>标签</strong></a>的实际值之差的平方。由于取平方值，因此该损失函数会放大不佳预测的影响。也就是说，与 <a href="#L1_loss"><strong>L<sub>1</sub> 损失函数</strong></a>相比，平方损失函数对离群值的反应更强烈。</p>
<p><a name="static_model"></a>
</p><h2 class="hide-from-toc">静态模型 (static model)</h2><p></p>
<p>离线训练的一种模型。</p>
<p><a name="stationarity"></a>
</p><h2 class="hide-from-toc">平稳性 (stationarity)</h2><p></p>
<p>数据集中数据的一种属性，表示数据分布在一个或多个维度保持不变。这种维度最常见的是时间，即表明平稳性的数据不随时间而变化。例如，从 9 月到 12 月，表明平稳性的数据没有发生变化。</p>
<p><a name="step"></a>
</p><h2 class="hide-from-toc">步 (step)</h2><p></p>
<p>对一个<a href="#batch"><strong>批次</strong></a>的向前和向后评估。</p>
<p><a name="step_size"></a>
</p><h2 class="hide-from-toc">步长 (step size)</h2><p></p>
<p>与<a href="#learning_rate"><strong>学习速率</strong></a>的含义相同。</p>
<p><a name="SGD"></a>
</p><h2 class="hide-from-toc">随机梯度下降法 (SGD, stochastic gradient descent)</h2><p></p>
<p>批次大小为 1 的一种<a href="#gradient_descent"><strong>梯度下降法</strong></a>。换句话说，SGD 依赖于从数据集中随机均匀选择的单个样本来计算每步的梯度估算值。</p>
<p><a name="SRM"></a>
</p><h2 class="hide-from-toc">结构风险最小化 (SRM, structural risk minimization)</h2><p></p>
<p>一种算法，用于平衡以下两个目标：</p>
<ul>
<li>期望构建最具预测性的模型（例如损失最低）。</li>
<li>期望使模型尽可能简单（例如强大的正则化）。</li>
</ul>
<p>例如，旨在将基于训练集的损失和正则化降至最低的函数就是一种结构风险最小化算法。</p>
<p>如需更多信息，请参阅 <a href="http://www.svms.org/srm/" target="_blank" rel="noopener">http://www.svms.org/srm/</a>。</p>
<p>与<a href="#ERM"><strong>经验风险最小化</strong></a>相对。</p>
<p><a name="stride"></a>
</p><h2 class="hide-from-toc">步长 (stride)</h2><p></p>
<p>在卷积运算或池化中，下一个系列的输入切片的每个维度中的增量。例如，下面的动画演示了卷积运算过程中的一个 (1,1) 步长。因此，下一个输入切片是从上一个输入切片向右移动一个步长的位置开始。当运算到达右侧边缘时，下一个切片将回到最左边，但是下移一个位置。</p>
<p>
<img src="https://developers.google.com/machine-learning/glossary/images/AnimatedConvolution.gif?hl=zh-cn">
</p>

<p>前面的示例演示了一个二维步长。如果输入矩阵为三维，那么步长也将是三维。</p>
<p><a name="subsampling"></a>
</p><h2 class="hide-from-toc">下采样 (subsampling)</h2><p></p>
<p>请参阅<a href="#pooling"><strong>池化</strong></a>。</p>
<p><a name="summary"></a>
</p><h2 class="hide-from-toc">总结 (summary)</h2><p></p>
<p>在 TensorFlow 中的某一<a href="#step"><strong>步</strong></a>计算出的一个值或一组值，通常用于在训练期间跟踪模型指标。</p>
<p><a name="supervised_machine_learning"></a>
</p><h2 class="hide-from-toc">监督式机器学习 (supervised machine learning)</h2><p></p>
<p>根据输入数据及其对应的<a href="#label"><strong>标签</strong></a>来训练<a href="#model"><strong>模型</strong></a>。监督式机器学习类似于学生通过研究一系列问题及其对应的答案来学习某个主题。在掌握了问题和答案之间的对应关系后，学生便可以回答关于同一主题的新问题（以前从未见过的问题）。请与<a href="#unsupervised_machine_learning"><strong>非监督式机器学习</strong></a>进行比较。</p>
<p><a name="synthetic_feature"></a>
</p><h2 class="hide-from-toc">合成特征 (synthetic feature)</h2><p></p>
<p>一种<a href="#feature"><strong>特征</strong></a>，不在输入特征之列，而是从一个或多个输入特征衍生而来。合成特征包括以下类型：</p>
<ul>
<li>对连续特征进行<a href="#bucketing"><strong>分桶</strong></a>，以分为多个区间分箱。</li>
<li>将一个特征值与其他特征值或其本身相乘（或相除）。</li>
<li>创建一个<a href="#feature_cross"><strong>特征组合</strong></a>。</li>
</ul>
<p>仅通过<a href="#normalization"><strong>标准化</strong></a>或<a href="#scaling"><strong>缩放</strong></a>创建的特征不属于合成特征。</p>
<h2 class="glossary">T</h2>

<p><a name="target"></a>
</p><h2 class="hide-from-toc">目标 (target)</h2><p></p>
<p>与<a href="#label"><strong>标签</strong></a>的含义相同。</p>
<p><a name="temporal_data"></a>
</p><h2 class="hide-from-toc">时态数据 (temporal data)</h2><p></p>
<p>在不同时间点记录的数据。例如，记录的一年中每一天的冬外套销量就属于时态数据。</p>
<p><a name="tensor"></a>
</p><h2 class="hide-from-toc">张量 (Tensor)</h2><p></p>
<p>TensorFlow 程序中的主要数据结构。张量是 N 维（其中 N 可能非常大）数据结构，最常见的是标量、向量或矩阵。张量的元素可以包含整数值、浮点值或字符串值。</p>
<p><a name="TPU"></a>
</p><h2 class="hide-from-toc">张量处理单元 (TPU, Tensor Processing Unit)</h2><p></p>
<p>一种 ASIC（应用专用集成电路），用于优化 TensorFlow 程序的性能。</p>
<p><a name="tensor_rank"></a>
</p><h2 class="hide-from-toc">张量等级 (Tensor rank)</h2><p></p>
<p>请参阅<a href="#rank"><strong>等级</strong></a>。</p>
<p><a name="tensor_shape"></a>
</p><h2 class="hide-from-toc">张量形状 (Tensor shape)</h2><p></p>
<p><a href="#tensor"><strong>张量</strong></a>在各种维度中包含的元素数。例如，张量 [5, 10] 在一个维度中的形状为 5，在另一个维度中的形状为 10。</p>
<p><a name="tensor_size"></a>
</p><h2 class="hide-from-toc">张量大小 (Tensor size)</h2><p></p>
<p><a href="#tensor"><strong>张量</strong></a>包含的标量总数。例如，张量 [5, 10] 的大小为 50。</p>
<p><a name="TensorBoard"></a>
</p><h2 class="hide-from-toc">TensorBoard</h2><p></p>
<p>一个信息中心，用于显示在执行一个或多个 TensorFlow 程序期间保存的摘要信息。</p>
<p><a name="TensorFlow"></a>
</p><h2 class="hide-from-toc">TensorFlow</h2><p></p>
<p>一个大型的分布式机器学习平台。该术语还指 TensorFlow 堆栈中的基本 API 层，该层支持对数据流图进行一般计算。</p>
<p>虽然 TensorFlow 主要应用于机器学习领域，但也可用于需要使用数据流图进行数值计算的非机器学习任务。</p>
<p><a name="TensorFlow_Playground"></a>
</p><h2 class="hide-from-toc">TensorFlow Playground</h2><p></p>
<p>一款用于直观呈现不同的<a href="#hyperparameters"><strong>超参数</strong></a>对模型（主要是神经网络）训练的影响的程序。要试用 TensorFlow Playground，请前往 <a href="http://playground.tensorflow.org?hl=zh-cn" target="_blank" rel="noopener">http://playground.tensorflow.org</a>。</p>
<p><a name="TensorFlow_Serving"></a>
</p><h2 class="hide-from-toc">TensorFlow Serving</h2><p></p>
<p>一个平台，用于将训练过的模型部署到生产环境。</p>
<p><a name="test_set"></a>
</p><h2 class="hide-from-toc">测试集 (test set)</h2><p></p>
<p>数据集的子集，用于在<a href="#model"><strong>模型</strong></a>经由验证集的初步验证之后测试模型。

</p><p>与<a href="#training_set"><strong>训练集</strong></a>和<a href="#validation_set"><strong>验证集</strong></a>相对。</p>
<p><a name="tf.Example"></a>
</p><h2 class="hide-from-toc">tf.Example</h2><p></p>
<p>一种标准<a href="https://developers.google.com/protocol-buffers/?hl=zh-cn" target="_blank" rel="noopener">协议缓冲区</a>，旨在描述用于机器学习模型训练或推断的输入数据。</p>
<p><a name="time_series_analysis"></a>
</p><h2 class="hide-from-toc">时间序列分析 (time series analysis)</h2><p></p>
<p>机器学习和统计学的一个子领域，旨在分析<a href="#temporal_data"><strong>时态数据</strong></a>。很多类型的机器学习问题都需要时间序列分析，其中包括分类、聚类、预测和异常检测。例如，您可以利用时间序列分析根据历史销量数据预测未来每月的冬外套销量。</p>
<p><a name="training"></a>
</p><h2 class="hide-from-toc">训练 (training)</h2><p></p>
<p>确定构成模型的理想<a href="#parameter"><strong>参数</strong></a>的过程。</p>
<p><a name="training_set"></a>
</p><h2 class="hide-from-toc">训练集 (training set)</h2><p></p>
<p>数据集的子集，用于训练模型。</p>
<p>与<a href="#validation_set"><strong>验证集</strong></a>和<a href="#test_set"><strong>测试集</strong></a>相对。</p>
<p><a name="transfer_learning"></a>
</p><h2 class="hide-from-toc">迁移学习 (transfer learning)</h2><p></p>
<p>将信息从一个机器学习任务迁移到另一个机器学习任务。例如，在多任务学习中，一个模型可以完成多项任务，例如针对不同任务具有不同输出节点的<a href="#deep_model"><strong>深度模型</strong></a>。迁移学习可能涉及将知识从较简单任务的解决方案迁移到较复杂的任务，或者将知识从数据较多的任务迁移到数据较少的任务。</p>
<p>大多数机器学习系统都只能完成一项任务。<em></em>迁移学习是迈向人工智能的一小步；在人工智能中，单个程序可以完成多项任务。<em></em></p>
<p><a name="translational_invariance"></a>
</p><h2 class="hide-from-toc">平移不变性 (translational invariance)</h2><p></p>
<p>在图像分类问题中，即使图像中对象的位置发生变化，算法也能成功对图像进行分类。例如，无论一只狗位于画面正中央还是画面左侧，该算法仍然可以识别它。</p>
<p>另请参阅<a href="#size_invariance"><strong>大小不变性</strong></a>和<a href="#rotational_invariance"><strong>旋转不变性</strong></a>。</p>
<p><a name="TN"></a>
</p><h2 class="hide-from-toc">负例 (TN, true negative)</h2><p></p>
<p>被模型正确地预测为<a href="#negative_class"><strong>负类别</strong></a>的样本。<em></em>例如，模型推断出某封电子邮件不是垃圾邮件，而该电子邮件确实不是垃圾邮件。</p>
<p><a name="TP"></a>
</p><h2 class="hide-from-toc">正例 (TP, true positive)</h2><p></p>
<p>被模型正确地预测为<a href="#positive_class"><strong>正类别</strong></a>的样本。<em></em>例如，模型推断出某封电子邮件是垃圾邮件，而该电子邮件确实是垃圾邮件。</p>
<p><a name="TP_rate"></a>
</p><h2 class="hide-from-toc">正例率（true positive rate, 简称 TP 率）</h2><p></p>
<p>与<a href="#recall"><strong>召回率</strong></a>的含义相同，即：</p>
<div>
$$\text{正例率} = \frac{\text{正例数}} {\text{正例数} + \text{假负例数}}$$
</div>

<p>正例率是 <a href="#ROC"><strong>ROC 曲线</strong></a>的 y 轴。</p>
<h2 class="glossary">U</h2>

<p><a name="unlabeled_example"></a>
</p><h2 class="hide-from-toc">无标签样本 (unlabeled example)</h2><p></p>
<p>包含<a href="#feature"><strong>特征</strong></a>但没有<a href="#label"><strong>标签</strong></a>的样本。无标签样本是用于进行<a href="#inference"><strong>推断</strong></a>的输入内容。在<a href="#semi-supervised_learning"><strong>半监督式</strong></a>和<a href="#unsupervised_machine_learning"><strong>非监督式</strong></a>学习中，在训练期间会使用无标签样本。</p>
<p><a name="unsupervised_machine_learning"></a>
</p><h2 class="hide-from-toc">非监督式机器学习 (unsupervised machine learning)</h2><p></p>
<p>训练<a href="#model"><strong>模型</strong></a>，以找出数据集（通常是无标签数据集）中的规律。</p>
<p>非监督式机器学习最常见的用途是将数据分为不同的聚类，使相似的样本位于同一组中。例如，非监督式机器学习算法可以根据音乐的各种属性将歌曲分为不同的聚类。所得聚类可以作为其他机器学习算法（例如音乐推荐服务）的输入。在很难获取真标签的领域，聚类可能会非常有用。例如，在反滥用和反欺诈等领域，聚类有助于人们更好地了解相关数据。</p>
<p>非监督式机器学习的另一个例子是<a href="https://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank" rel="noopener"><strong>主成分分析 (PCA)</strong></a>。例如，通过对包含数百万购物车中物品的数据集进行主成分分析，可能会发现有柠檬的购物车中往往也有抗酸药。</p>
<p>请与<a href="#supervised_machine_learning"><strong>监督式机器学习</strong></a>进行比较。</p>
<h2 class="glossary">V</h2>

<p><a name="validation_set"></a>
</p><h2 class="hide-from-toc">验证集 (validation set)</h2><p></p>
<p>数据集的一个子集，从训练集分离而来，用于调整<a href="#hyperparameter"><strong>超参数</strong></a>。</p>
<p>与<a href="#training_set"><strong>训练集</strong></a>和<a href="#test_set"><strong>测试集</strong></a>相对。</p>

<p></p><h2 class="glossary">W</h2>

<p><a name="weight"></a>
</p><h2 class="hide-from-toc">权重 (weight)</h2><p></p>
<p>线性模型中<a href="#feature"><strong>特征</strong></a>的系数，或深度网络中的边。训练线性模型的目标是确定每个特征的理想权重。如果权重为 0，则相应的特征对模型来说没有任何贡献。</p>
<p><a name="wide_model"></a>
</p><h2 class="hide-from-toc">宽度模型 (wide model)</h2><p></p>
<p>一种线性模型，通常有很多<a href="#sparse_features"><strong>稀疏输入特征</strong></a>。我们之所以称之为“宽度模型”，是因为这是一种特殊类型的<a href="#neural_network"><strong>神经网络</strong></a>，其大量输入均直接与输出节点相连。与深度模型相比，宽度模型通常更易于调试和检查。虽然宽度模型无法通过<a href="#hidden_layer"><strong>隐藏层</strong></a>来表示非线性关系，但可以利用<a href="#feature_cross"><strong>特征组合</strong></a>、<a href="#bucketing"><strong>分桶</strong></a>等转换以不同的方式为非线性关系建模。</p>
<p>与<a href="#deep_model"><strong>深度模型</strong></a>相对。</p>




  
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>保存与加载Tensorflow模型</title>
    <url>/2020/08/01/%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BDTensorflow%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="保存与加载模型"><a href="#保存与加载模型" class="headerlink" title="保存与加载模型"></a>保存与加载模型</h1><p>安装tensorflow-datasets，导入依赖项：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%pip install tensorflow-datasets</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow_datasets <span class="keyword">as</span> tfds</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tfds.disable_progress_bar()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.__version__</span><br></pre></td></tr></table></figure>
<pre><code>2.3.0
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mirrored_strategy = tf.distribute.MirroredStrategy()</span><br></pre></td></tr></table></figure>
<p>创建一个分发变量和图形的策略</p>
<p>tf.distribute.MirroredStrategy 策略是如何运作的？</p>
<pre><code>所有变量和模型图都复制在副本上。
输入都均匀分布在副本中。
每个副本在收到输入后计算输入的损失和梯度。
通过求和，每一个副本上的梯度都能同步。
同步后，每个副本上的复制的变量都可以同样更新。
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">()</span>:</span></span><br><span class="line">  datasets, ds_info = tfds.load(name=<span class="string">'mnist'</span>, with_info=<span class="keyword">True</span>, as_supervised=<span class="keyword">True</span>)</span><br><span class="line">  mnist_train, mnist_test = datasets[<span class="string">'train'</span>], datasets[<span class="string">'test'</span>]</span><br><span class="line"></span><br><span class="line">  BUFFER_SIZE = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line">  BATCH_SIZE_PER_REPLICA = <span class="number">64</span></span><br><span class="line">  BATCH_SIZE = BATCH_SIZE_PER_REPLICA * mirrored_strategy.num_replicas_in_sync</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">scale</span><span class="params">(image, label)</span>:</span></span><br><span class="line">    image = tf.cast(image, tf.float32)</span><br><span class="line">    image /= <span class="number">255</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> image, label</span><br><span class="line"></span><br><span class="line">  train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)</span><br><span class="line">  eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> train_dataset, eval_dataset</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">with</span> mirrored_strategy.scope():</span><br><span class="line">        model = tf.keras.Sequential([</span><br><span class="line">            tf.keras.layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)),</span><br><span class="line">            tf.keras.layers.MaxPool2D(),</span><br><span class="line">            tf.keras.layers.Flatten(),</span><br><span class="line">            tf.keras.layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">            tf.keras.layers.Dense(<span class="number">10</span>)</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="keyword">True</span>),</span><br><span class="line">                     optimizer=tf.keras.optimizers.Adam(),</span><br><span class="line">                     metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">        <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = get_model()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_dataset, eval_dataset = get_data()</span><br></pre></td></tr></table></figure>
<pre><code>[1mDownloading and preparing dataset mnist/3.0.1 (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\Users\v-xujwan\tensorflow_datasets\mnist\3.0.1...[0m
Shuffling and writing examples to C:\Users\v-xujwan\tensorflow_datasets\mnist\3.0.1.incompleteI371SH\mnist-train.tfrecord
Shuffling and writing examples to C:\Users\v-xujwan\tensorflow_datasets\mnist\3.0.1.incompleteI371SH\mnist-test.tfrecord
[1mDataset mnist downloaded and prepared to C:\Users\v-xujwan\tensorflow_datasets\mnist\3.0.1. Subsequent calls will reuse this data.[0m
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.fit(train_dataset, epochs=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/2
WARNING:tensorflow:From D:\Anaconda3\envs\py36_tf2\lib\site-packages\tensorflow\python\data\ops\multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Iterator.get_next_as_optional()` instead.


WARNING:tensorflow:From D:\Anaconda3\envs\py36_tf2\lib\site-packages\tensorflow\python\data\ops\multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Iterator.get_next_as_optional()` instead.


938/938 [==============================] - 18s 19ms/step - loss: 0.2089 - accuracy: 0.9389
Epoch 2/2
938/938 [==============================] - 19s 20ms/step - loss: 0.0689 - accuracy: 0.97980s - los

&lt;tensorflow.python.keras.callbacks.History at 0x2b4c939e278&gt;
</code></pre><h1 id="保存并加载模型"><a href="#保存并加载模型" class="headerlink" title="保存并加载模型"></a>保存并加载模型</h1><p>现在有了一个简单的模型可以使用，让我们看一下保存/加载API。有两套可用的API：</p>
<pre><code>高级的keras model.save和tf.keras.models.load_model
低级的tf.saved_model.save和tf.saved_model.load
</code></pre><h2 id="使用keras-API"><a href="#使用keras-API" class="headerlink" title="使用keras API"></a>使用keras API</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras_model_path = <span class="string">"./tmp/keras_save"</span></span><br><span class="line">model.save(keras_model_path)</span><br></pre></td></tr></table></figure>
<pre><code>WARNING:tensorflow:From D:\Anaconda3\envs\py36_tf2\lib\site-packages\tensorflow\python\training\tracking\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.


WARNING:tensorflow:From D:\Anaconda3\envs\py36_tf2\lib\site-packages\tensorflow\python\training\tracking\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.


WARNING:tensorflow:From D:\Anaconda3\envs\py36_tf2\lib\site-packages\tensorflow\python\training\tracking\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.


WARNING:tensorflow:From D:\Anaconda3\envs\py36_tf2\lib\site-packages\tensorflow\python\training\tracking\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.


INFO:tensorflow:Assets written to: ./tmp/keras_save\assets


INFO:tensorflow:Assets written to: ./tmp/keras_save\assets
</code></pre><p>模型保存成功，看一下保存的文件</p>
<pre><code>└───tmp
    └───keras_save
        ├───assets
        └───variables
            └───variables.data-00000-of-00001
            └───variables.index
        └───saved_model.pb
</code></pre><p>接着还原模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">restored_keras_model = tf.keras.models.load_model(keras_model_path)</span><br></pre></td></tr></table></figure>
<p>还原后的模型可以继续训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">restored_keras_model.fit(train_dataset, epochs=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/2
938/938 [==============================] - 16s 17ms/step - loss: 0.0494 - accuracy: 0.09890s - loss: 0.0493 - accuracy: 0.09
Epoch 2/2
938/938 [==============================] - 16s 17ms/step - loss: 0.0353 - accuracy: 0.0989

&lt;tensorflow.python.keras.callbacks.History at 0x2b4c5b5a128&gt;
</code></pre><p>现在加载模型并使用进行训练tf.distribute.Strategy</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">another_strategy = tf.distribute.OneDeviceStrategy(<span class="string">"/cpu:0"</span>)</span><br><span class="line"><span class="keyword">with</span> another_strategy.scope():</span><br><span class="line">  restored_keras_model_ds = tf.keras.models.load_model(keras_model_path)</span><br><span class="line">  restored_keras_model_ds.fit(train_dataset, epochs=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/2
938/938 [==============================] - 16s 17ms/step - loss: 0.0501 - accuracy: 0.0990
Epoch 2/2
938/938 [==============================] - 16s 17ms/step - loss: 0.0354 - accuracy: 0.0989
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">restored_keras_model_ds.predict</span><br></pre></td></tr></table></figure>
<pre><code>&lt;tensorflow.python.keras.engine.sequential.Sequential at 0x2b4c75ea2b0&gt;
</code></pre><h2 id="使用tf-saved-model-API"><a href="#使用tf-saved-model-API" class="headerlink" title="使用tf.saved_model API"></a>使用tf.saved_model API</h2><p>现在使用低级的api，保存方法和keras类似</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = get_model()</span><br><span class="line">saved_model_path = <span class="string">"./tmp/tf_save"</span></span><br><span class="line">tf.saved_model.save(model, saved_model_path)</span><br></pre></td></tr></table></figure>
<pre><code>INFO:tensorflow:Assets written to: ./tmp/tf_save\assets


INFO:tensorflow:Assets written to: ./tmp/tf_save\assets
</code></pre><p>可以使用进行加载tf.saved_model.load()。但是，由于它是一个较低级别的API（因此具有更广泛的用例范围），因此它不会返回Keras模型。相反，它返回一个对象，该对象包含可用于进行推断的函数。例如：</p>
<p>还可以以分布式方式加载和进行推断：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">another_strategy = tf.distribute.MirroredStrategy()</span><br><span class="line"><span class="keyword">with</span> another_strategy.scope():</span><br><span class="line">  loaded = tf.saved_model.load(saved_model_path)</span><br><span class="line">  inference_func = loaded.signatures[DEFAULT_FUNCTION_KEY]</span><br><span class="line"></span><br><span class="line">  dist_predict_dataset = another_strategy.experimental_distribute_dataset(</span><br><span class="line">      predict_dataset)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Calling the function in a distributed manner</span></span><br><span class="line">  <span class="keyword">for</span> batch <span class="keyword">in</span> dist_predict_dataset:</span><br><span class="line">    another_strategy.run(inference_func,args=(batch,))</span><br></pre></td></tr></table></figure>
<pre><code>WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.


WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.


INFO:tensorflow:Using MirroredStrategy with devices (&apos;/job:localhost/replica:0/task:0/device:CPU:0&apos;,)


INFO:tensorflow:Using MirroredStrategy with devices (&apos;/job:localhost/replica:0/task:0/device:CPU:0&apos;,)


WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.


WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.


WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.


WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.


WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.


WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.


WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.


WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.


WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.


WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.
</code></pre><h1 id="保存检查点"><a href="#保存检查点" class="headerlink" title="保存检查点"></a>保存检查点</h1><p>检查站捕获所有参数（的精确值tf.Variable由模型中使用的对象）。检查点不包含由模型所定义的计算的任何描述，因此通常仅当将使用保存的参数值源代码可用有用。</p>
<p>在另一方面中SavedModel格式包括由除了参数值（检查点）模型中定义的计算的序列化描述。这种格式的模型是独立于创建模型的源代码。因此，它们适用于通过TensorFlow部署服务，TensorFlow精简版，TensorFlow.js，或在其他编程语言（的C，C ++，JAVA，围棋，防锈，C＃等TensorFlow API）的程序。</p>
<p>本指南涵盖API进行写入和读出检查站。</p>
<h2 id="建立"><a href="#建立" class="headerlink" title="建立"></a>建立</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(tf.keras.Model)</span>:</span></span><br><span class="line">    <span class="comment"># 一个简单的线性模型</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.l1 = tf.keras.layers.Dense(<span class="number">5</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.l1(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = Net()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.save_weights(<span class="string">'./tmp/easy_checkpoint'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="写检查站"><a href="#写检查站" class="headerlink" title="写检查站"></a>写检查站</h2><p>一个TensorFlow模型的持久状态被存储在tf.Variable对象。这些可以直接构造，但通常通过高级API等生成tf.keras.layers或tf.keras.Model 。</p>
<p>管理变量最简单的方法是将其安装到Python对象，然后引用这些对象。</p>
<p>的子类tf.train.Checkpoint ， tf.keras.layers.Layer和tf.keras.Model自动跟踪分配给它们的属性变量。下面的例子构造了一个简单的线性模型，然后写入其中包含所有模型的变量值的检查站。</p>
<p>您可以轻松地保存模型检查点与Model.save_weights</p>
<h1 id="手动检查点"><a href="#手动检查点" class="headerlink" title="手动检查点"></a>手动检查点</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toy_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    inputs = tf.range(<span class="number">10.</span>)[:, <span class="keyword">None</span>]</span><br><span class="line">    labels = inputs * <span class="number">5.</span> + tf.range(<span class="number">5.</span>)[<span class="keyword">None</span>, :]</span><br><span class="line">    <span class="keyword">return</span> tf.data.Dataset.from_tensor_slices(dict(x=inputs, y=labels)).repeat().batch(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(net, example, optimizer)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        output = net(example[<span class="string">'x'</span>])</span><br><span class="line">        loss = tf.reduce_mean(tf.abs(output - example[<span class="string">'y'</span>]))</span><br><span class="line">    variables = net.trainable_variables</span><br><span class="line">    gradients = tape.gradient(loss, variables)</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, variables))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<h2 id="创建检查点的对象"><a href="#创建检查点的对象" class="headerlink" title="创建检查点的对象"></a>创建检查点的对象</h2><p>手动进行检查点，您将需要一个tf.train.Checkpoint对象。凡检查点你想要的对象被设置为对象的属性。</p>
<p>一个tf.train.CheckpointManager也可用于管理多个检查点有帮助。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">opt = tf.keras.optimizers.Adam(<span class="number">0.1</span>)</span><br><span class="line">dataset = toy_dataset()</span><br><span class="line">iterator = iter(dataset)</span><br><span class="line">ckpt = tf.train.Checkpoint(step=tf.Variable(<span class="number">1</span>), optimizer=opt, net=net, iterator=iterator)</span><br><span class="line">manager = tf.train.CheckpointManager(ckpt, <span class="string">'./tmp/tf_ckpts'</span>, max_to_keep=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h2 id="训练和保存检查点模型"><a href="#训练和保存检查点模型" class="headerlink" title="训练和保存检查点模型"></a>训练和保存检查点模型</h2><p>下面的训练循环创建模型的实例和优化的，然后收集他们入tf.train.Checkpoint对象。它在循环中调用数据的每批训练步骤，并定期检查点写入到磁盘。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_and_checkpoint</span><span class="params">(net, manager)</span>:</span></span><br><span class="line">    ckpt.restore(manager.latest_checkpoint)</span><br><span class="line">    <span class="keyword">if</span> manager.latest_checkpoint:</span><br><span class="line">        print(<span class="string">"恢复点：&#123;&#125;"</span>.format(manager.latest_checkpoint))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"开始初始化"</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">        example = next(iterator)</span><br><span class="line">        loss = train_step(net, example, opt)</span><br><span class="line">        ckpt.step.assign_add(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> int(ckpt.step) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">          save_path = manager.save()</span><br><span class="line">          print(<span class="string">"保存检查点 &#123;&#125;: &#123;&#125;"</span>.format(int(ckpt.step), save_path))</span><br><span class="line">          print(<span class="string">"loss &#123;:1.2f&#125;"</span>.format(loss.numpy()))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_and_checkpoint(net, manager)</span><br></pre></td></tr></table></figure>
<pre><code>开始初始化
保存检查点 10: ./tmp/tf_ckpts\ckpt-1
loss 29.78
保存检查点 20: ./tmp/tf_ckpts\ckpt-2
loss 23.19
保存检查点 30: ./tmp/tf_ckpts\ckpt-3
loss 16.63
保存检查点 40: ./tmp/tf_ckpts\ckpt-4
loss 10.17
保存检查点 50: ./tmp/tf_ckpts\ckpt-5
loss 4.09
</code></pre><h1 id="恢复和继续训练"><a href="#恢复和继续训练" class="headerlink" title="恢复和继续训练"></a>恢复和继续训练</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">opt = tf.keras.optimizers.Adam(<span class="number">0.1</span>)</span><br><span class="line">dataset = toy_dataset()</span><br><span class="line">iterator = iter(dataset)</span><br><span class="line">ckpt = tf.train.Checkpoint(step=tf.Variable(<span class="number">1</span>), optimizer=opt, net=net, iterator=iterator)</span><br><span class="line">manager = tf.train.CheckpointManager(ckpt, <span class="string">'./tmp/tf_ckpts'</span>, max_to_keep=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_and_checkpoint(net, manager)</span><br></pre></td></tr></table></figure>
<pre><code>恢复点：./tmp/tf_ckpts\ckpt-10
保存检查点 110: ./tmp/tf_ckpts\ckpt-11
loss 0.27
保存检查点 120: ./tmp/tf_ckpts\ckpt-12
loss 0.20
保存检查点 130: ./tmp/tf_ckpts\ckpt-13
loss 0.16
保存检查点 140: ./tmp/tf_ckpts\ckpt-14
loss 0.21
保存检查点 150: ./tmp/tf_ckpts\ckpt-15
loss 0.20
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(manager.checkpoints)</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;./tmp/tf_ckpts\\ckpt-13&apos;, &apos;./tmp/tf_ckpts\\ckpt-14&apos;, &apos;./tmp/tf_ckpts\\ckpt-15&apos;]
</code></pre><p>这些路径，如’./tf_ckpts/ckpt-10’ ，不是磁盘上的文件。相反，它们是一个前缀index文件和包含可变值的一个或多个数据文件。这些前缀在单个组合在一起checkpoint文件（ ‘./tf_ckpts/checkpoint’ ），其中CheckpointManager保存其状态。</p>
<h1 id="手动检查"><a href="#手动检查" class="headerlink" title="手动检查"></a>手动检查</h1><p>tf.train.list_variables列出了检查点键和变量的形状在一个检查点。检查点键是显示在以上图上的路径。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.train.list_variables(tf.train.latest_checkpoint(<span class="string">'./tmp/tf_ckpts'</span>))</span><br></pre></td></tr></table></figure>
<pre><code>[(&apos;_CHECKPOINTABLE_OBJECT_GRAPH&apos;, []),
 (&apos;iterator/.ATTRIBUTES/ITERATOR_STATE&apos;, [1]),
 (&apos;net/l1/bias/.ATTRIBUTES/VARIABLE_VALUE&apos;, [5]),
 (&apos;net/l1/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE&apos;, [5]),
 (&apos;net/l1/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE&apos;, [5]),
 (&apos;net/l1/kernel/.ATTRIBUTES/VARIABLE_VALUE&apos;, [1, 5]),
 (&apos;net/l1/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE&apos;,
  [1, 5]),
 (&apos;net/l1/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE&apos;,
  [1, 5]),
 (&apos;optimizer/beta_1/.ATTRIBUTES/VARIABLE_VALUE&apos;, []),
 (&apos;optimizer/beta_2/.ATTRIBUTES/VARIABLE_VALUE&apos;, []),
 (&apos;optimizer/decay/.ATTRIBUTES/VARIABLE_VALUE&apos;, []),
 (&apos;optimizer/iter/.ATTRIBUTES/VARIABLE_VALUE&apos;, []),
 (&apos;optimizer/learning_rate/.ATTRIBUTES/VARIABLE_VALUE&apos;, []),
 (&apos;save_counter/.ATTRIBUTES/VARIABLE_VALUE&apos;, []),
 (&apos;step/.ATTRIBUTES/VARIABLE_VALUE&apos;, [])]
</code></pre><h1 id="保存与估计基于对象的检查站"><a href="#保存与估计基于对象的检查站" class="headerlink" title="保存与估计基于对象的检查站"></a>保存与估计基于对象的检查站</h1><p>通过默认保存变量名，而不是在前面的章节中描述的对象图检查点估计。 tf.train.Checkpoint将接受基于域名的检查点，但移动估计的模型以外的部位时，变量名可以更改model_fn 。保存基于对象的检查站，使得它更容易培养的估算内部模型，然后外面用它之一。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf_compat</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn</span><span class="params">(features, labels, mode)</span>:</span></span><br><span class="line">    net = Net()</span><br><span class="line">    opt = tf.keras.optimizers.Adam(<span class="number">0.1</span>)</span><br><span class="line">    ckpt = tf.train.Checkpoint(step=tf_compat.train.get_global_step(),</span><br><span class="line">                              optimizers=opt, net=net)</span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape()<span class="keyword">as</span> tape:</span><br><span class="line">        output = net(features[<span class="string">'x'</span>])</span><br><span class="line">        loss = tf.reduce_mean(tf.abs(output - features[<span class="string">'y'</span>]))</span><br><span class="line">    variables = net.trainable_variables</span><br><span class="line">    gradients = tape.gradient(loss, variables)</span><br><span class="line">    <span class="keyword">return</span> tf.estimator.EstimatorSpec(</span><br><span class="line">        mode,</span><br><span class="line">        loss=loss,</span><br><span class="line">        train_op=tf.group(opt.apply_gradients(zip(gradients, variables)),</span><br><span class="line">                          ckpt.step.assign_add(<span class="number">1</span>)),scaffold=tf_compat.train.Scaffold(saver=ckpt))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session()</span><br><span class="line">est = tf.estimator.Estimator(model_fn, <span class="string">'./tmp/tf_estimator_example/'</span>)</span><br><span class="line">est.train(toy_dataset, steps=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<pre><code>INFO:tensorflow:Using default config.


INFO:tensorflow:Using default config.


INFO:tensorflow:Using config: {&apos;_model_dir&apos;: &apos;./tmp/tf_estimator_example/&apos;, &apos;_tf_random_seed&apos;: None, &apos;_save_summary_steps&apos;: 100, &apos;_save_checkpoints_steps&apos;: None, &apos;_save_checkpoints_secs&apos;: 600, &apos;_session_config&apos;: allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, &apos;_keep_checkpoint_max&apos;: 5, &apos;_keep_checkpoint_every_n_hours&apos;: 10000, &apos;_log_step_count_steps&apos;: 100, &apos;_train_distribute&apos;: None, &apos;_device_fn&apos;: None, &apos;_protocol&apos;: None, &apos;_eval_distribute&apos;: None, &apos;_experimental_distribute&apos;: None, &apos;_experimental_max_worker_delay_secs&apos;: None, &apos;_session_creation_timeout_secs&apos;: 7200, &apos;_service&apos;: None, &apos;_cluster_spec&apos;: ClusterSpec({}), &apos;_task_type&apos;: &apos;worker&apos;, &apos;_task_id&apos;: 0, &apos;_global_id_in_cluster&apos;: 0, &apos;_master&apos;: &apos;&apos;, &apos;_evaluation_master&apos;: &apos;&apos;, &apos;_is_chief&apos;: True, &apos;_num_ps_replicas&apos;: 0, &apos;_num_worker_replicas&apos;: 1}


INFO:tensorflow:Using config: {&apos;_model_dir&apos;: &apos;./tmp/tf_estimator_example/&apos;, &apos;_tf_random_seed&apos;: None, &apos;_save_summary_steps&apos;: 100, &apos;_save_checkpoints_steps&apos;: None, &apos;_save_checkpoints_secs&apos;: 600, &apos;_session_config&apos;: allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, &apos;_keep_checkpoint_max&apos;: 5, &apos;_keep_checkpoint_every_n_hours&apos;: 10000, &apos;_log_step_count_steps&apos;: 100, &apos;_train_distribute&apos;: None, &apos;_device_fn&apos;: None, &apos;_protocol&apos;: None, &apos;_eval_distribute&apos;: None, &apos;_experimental_distribute&apos;: None, &apos;_experimental_max_worker_delay_secs&apos;: None, &apos;_session_creation_timeout_secs&apos;: 7200, &apos;_service&apos;: None, &apos;_cluster_spec&apos;: ClusterSpec({}), &apos;_task_type&apos;: &apos;worker&apos;, &apos;_task_id&apos;: 0, &apos;_global_id_in_cluster&apos;: 0, &apos;_master&apos;: &apos;&apos;, &apos;_evaluation_master&apos;: &apos;&apos;, &apos;_is_chief&apos;: True, &apos;_num_ps_replicas&apos;: 0, &apos;_num_worker_replicas&apos;: 1}


WARNING:tensorflow:From D:\Anaconda3\envs\py36_tf2\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.


WARNING:tensorflow:From D:\Anaconda3\envs\py36_tf2\lib\site-packages\tensorflow\python\training\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.


INFO:tensorflow:Calling model_fn.


INFO:tensorflow:Calling model_fn.


INFO:tensorflow:Done calling model_fn.


INFO:tensorflow:Done calling model_fn.


INFO:tensorflow:Create CheckpointSaverHook.


INFO:tensorflow:Create CheckpointSaverHook.


INFO:tensorflow:Graph was finalized.


INFO:tensorflow:Graph was finalized.


INFO:tensorflow:Running local_init_op.


INFO:tensorflow:Running local_init_op.


INFO:tensorflow:Done running local_init_op.


INFO:tensorflow:Done running local_init_op.


INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...


INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...


INFO:tensorflow:Saving checkpoints for 0 into ./tmp/tf_estimator_example/model.ckpt.


INFO:tensorflow:Saving checkpoints for 0 into ./tmp/tf_estimator_example/model.ckpt.


INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...


INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...


INFO:tensorflow:loss = 4.505075, step = 1


INFO:tensorflow:loss = 4.505075, step = 1


INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 10...


INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 10...


INFO:tensorflow:Saving checkpoints for 10 into ./tmp/tf_estimator_example/model.ckpt.


INFO:tensorflow:Saving checkpoints for 10 into ./tmp/tf_estimator_example/model.ckpt.


INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 10...


INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 10...


INFO:tensorflow:Loss for final step: 36.96539.


INFO:tensorflow:Loss for final step: 36.96539.


&lt;tensorflow_estimator.python.estimator.estimator.EstimatorV2 at 0x2b4ccb5f588&gt;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.train.latest_checkpoint(<span class="string">'./tmp/tf_estimator_example'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&apos;./tmp/tf_estimator_example\\model.ckpt-10&apos;
</code></pre><h1 id="tf-train-Checkpoint则可以从其加载估计的检查站model-dir-。"><a href="#tf-train-Checkpoint则可以从其加载估计的检查站model-dir-。" class="headerlink" title="tf.train.Checkpoint则可以从其加载估计的检查站model_dir 。"></a>tf.train.Checkpoint则可以从其加载估计的检查站model_dir 。</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">opt = tf.keras.optimizers.Adam(<span class="number">0.1</span>)</span><br><span class="line">net = Net()</span><br><span class="line">ckpt = tf.train.Checkpoint(step=tf.Variable(<span class="number">1</span>, dtype=tf.int64), optimizer=opt, net=net)</span><br><span class="line">ckpt.restore(tf.train.latest_checkpoint(<span class="string">'./tmp/tf_estimator_example/'</span>))</span><br><span class="line">ckpt.step.numpy()</span><br></pre></td></tr></table></figure>
<pre><code>10
</code></pre>]]></content>
      <categories>
        <category>tensorflow2</category>
      </categories>
  </entry>
</search>
