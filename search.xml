<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Django 部署(Apache)]]></title>
    <url>%2F2019%2F01%2F27%2FDjango%20%E9%83%A8%E7%BD%B2(Apache)%2F</url>
    <content type="text"><![CDATA[Django是一个，由Python写成的开放源代码的Web应用框架，在使用apache部署的时候走了好多坑这里记录下。 参考： Django教程 apache部署 前提条件 一个服务器，我使用的是阿里云服务器。 推荐使用ubuntu镜像，因为软件集成度高（就是简单，傻瓜也会玩）。 已经使用Django搭建好web服务，如何搭建看Django教程。 这里只记录部署apache的坑了，其他上面都有详细讲解，就略了。 安装apache2和mod_wsgi1234567sudo apt-get install apache2 # Python 2sudo apt-get install libapache2-mod-wsgi # Python 3sudo apt-get install libapache2-mod-wsgi-py3 看版本！！！（非常重要） 版本不同在配置上有区别，推荐使用比较新的版本，也就是2.4以上，如果是1，下面的配置是不一样的！！！1234apachectl -vServer version: Apache/2.4.18 (Ubuntu)Server built: 2018-06-07T19:43:03 先别急着配置，看看能不能正常启动 1sudo service apache2 restart 这时候正常情况会启动默认配置，使用浏览器访问你服务器的外网IP，如果正常会显示下图： 无法访问请检查阿里云的防火墙设置，看端口是否允许通过，浏览器默认是80，这里顺便加一个8080，供下面测试。 Apache2 Ubuntu Default Page 页面可以正常访问代表我们的apache安装成功，下面开始修改配置文件。 设置目录和文件的权限一般目录权限设置为 755，文件权限设置为 644 假如项目位置在 /home/user/WebService （在WebService 下面有一个 manage.py，WebService 是项目名称） 123cd /home/user/sudo chmod -R 644 WebServicesudo find WebService -type d | xargs chmod 755 Django 的 settings.py 要设置清楚media 文件夹一般用来存放用户上传文件，static 一般用来放自己网站的js，css，图片等，在settings.py中的相关设置 STATIC_URL 为静态文件的网址 STATIC_ROOT 为静态文件的根目录， MEDIA_URL 为用户上传文件夹的根目录，MEDIA_URL为对应的访问网址 需要media的 要给media目录单独设置写的权限 ALLOWED_HOSTS是为了限定请求中的host值，以防止黑客构造包来发送请求。只有在列表中的host才能访问。 ++注意：在这里本人强烈建议不要使用*通配符去配置，另外当DEBUG设置为False的时候必须配置这个配置。否则会抛出异常。++1ALLOWED_HOSTS = ['*'] 这里先写个*等全部调通了再改。。。 apache的配置文件12cd /etc/apache2/sites-availablesudo vim mysite.conf 在这里我们自己写个配置1234567891011121314151617181920&lt;VirtualHost *:8080&gt; ServerName www.yourdomain.com ServerAdmin youremail@mail.com ErrorLog $&#123;APACHE_LOG_DIR&#125;/error.log CustomLog $&#123;APACHE_LOG_DIR&#125;/access.log combined Alias /static/ /home/user//WebService/static/ &lt;Directory /home/user/WebService/static&gt; Options Indexes FollowSymLinks AllowOverride None Require all granted &lt;/Directory&gt; WSGIScriptAlias / /home/user/WebService/WebService/wsgi.py &lt;Directory /home/user/WebService/&gt; &lt;Files wsgi.py&gt; Options Indexes FollowSymLinks AllowOverride None Require all granted &lt;/Files&gt; &lt;/Directory&gt;&lt;/VirtualHost&gt; 根据自己的情况改改，要注意目录要对，并且也写下面这个配置，apache版本不同配置是不同的！！！ Options Indexes FollowSymLinks AllowOverride None Require all granted 因为我们刚配置里写的是8080的端口，所以要把它加到监听列表里12345678sudo vim /etc/apache2/ports.confListen 80加一句Listen 80Listen 8080 激活新配这里不用写路径 1sudo a2ensite mysite 或 sudo a2ensite mysite.conf 重启apach1sudo service apache2 restart 访问 你的网站，记得加端口号 0.0.0.0:8080 出错看log1cat /var/log/apache2/error.log]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 5]]></title>
    <url>%2F2019%2F01%2F09%2FTensorFlow_5%2F</url>
    <content type="text"><![CDATA[表示法（Representation）机器学习模型不能直接看到、听到或感知输入样本。必须创建数据表示，为模型提供有用的信号来了解数据的关键特性。也就是说，为了训练模型，必须选择最能代表数据的特征集。 特征工程传统编程的关注点是代码。在机器学习项目中，关注点变成了表示。也就是说，开发者通过添加和改善特征来调整模型。 将原始数据映射到特征下图左侧边是来自数据源的原始数据，右侧表示特征矢量，也就是组成数据集中样本的浮点值集。 特征工程指的是将原始数据转换为特征矢量。进行特征工程预计需要大量时间。 机器学习模型通常期望样本表示为实数矢量。这种矢量的构建方法如下：为每个字段衍生特征，然后将它们全部连接到一起 映射数值机器学习模型根据浮点值进行训练，因此整数和浮点原始数据不需要特殊编码。正如下图 所示，将原始整数值 6 转换为特征值 6.0 是没有意义的： 映射字符串值模型无法通过字符串值学习规律，因此您需要进行一些特征工程来将这些值转换为数字形式： 1.首先，为您要表示的所有特征的字符串值定义一个词汇表。对于 street_name 特征，该词汇表中将包含您知道的所有街道。 2.然后，使用该词汇表创建一个独热编码，用于将指定字符串值表示为二元矢量。在该矢量（与指定的字符串值对应）中： ·只有一个元素设为 1。 ·其他所有元素均设为 0。 ·该矢量的长度等于词汇表中的元素数。 下图显示了某条特定街道 (Shorebird Way) 的独热编码。在此二元矢量中，代表 Shorebird Way 的元素的值为 1，而代表所有其他街道的元素的值为 θ。 映射分类（枚举）值分类特征具有一组离散的可能值。例如，名为 Lowland Countries 的特征只包含 3 个可能值： {&apos;Netherlands&apos;, &apos;Belgium&apos;, &apos;Luxembourg&apos;} 您可能会将分类特征（如 Lowland Countries）编码为枚举类型或表示不同值的整数离散集。例如： 将荷兰表示为 0 将比利时表示为 1 将卢森堡表示为 2 不过，机器学习模型通常将每个分类特征表示为单独的布尔值。例如，Lowland Countries 在模型中可以表示为 3 个单独的布尔值特征： x1：是荷兰吗？ x2：是比利时吗？ x3：是卢森堡吗？ 采用这种方法编码还可以简化某个值可能属于多个分类这种情况（例如，“与法国接壤”对于比利时和卢森堡来说都是 True）。 良好特征的特点我们探索了将原始数据映射到合适特征矢量的方法，但这只是工作的一部分。现在，我们必须探索什么样的值才算这些特征矢量中良好的特征。 避免很少使用的离散特征值良好的特征值应该在数据集中出现大约 5 次以上。这样一来，模型就可以学习该特征值与标签是如何关联的。也就是说，大量离散值相同的样本可让模型有机会了解不同设置中的特征，从而判断何时可以对标签很好地做出预测。 例如，house_type 特征可能包含大量样本，其中它的值为 victorian： house_type: victorian 相反，如果某个特征的值仅出现一次或者很少出现，则模型就无法根据该特征进行预测。 例如，unique_house_id 就不适合作为特征，因为每个值只使用一次，模型无法从中学习任何规律： unique_house_id: 8SK982ZZ1242Z 最好具有清晰明确的含义每个特征对于项目中的任何人来说都应该具有清晰明确的含义。 例如，下面的房龄适合作为特征，可立即识别为年龄： house_age: 27 相反，对于下方特征值的含义，除了创建它的工程师，其他人恐怕辨识不出： house_age: 851472000 在某些情况下，混乱的数据（而不是糟糕的工程选择）会导致含义不清晰的值。 例如，以下 user_age 的来源没有检查值恰当与否： user_age: 277 不要将“神奇”的值与实际数据混为一谈良好的浮点特征不包含超出范围的异常断点或“神奇”的值。 例如，假设一个特征具有 0 到 1 之间的浮点值。那么，如下值是可以接受的： quality_rating: 0.82 quality_rating: 0.37 不过，如果用户没有输入 quality_rating，则数据集可能使用如下神奇值来表示不存在该值： quality_rating: -1 为解决神奇值的问题，需将该特征转换为两个特征： 一个特征只存储质量评分，不含神奇值。 一个特征存储布尔值，表示是否提供了 quality_rating。为该布尔值特征指定一个名称，例如 is_quality_rating_defined。 考虑上游不稳定性特征的定义不应随时间发生变化。 例如，下列值是有用的，因为城市名称一般不会改变。（注意，我们仍然需要将“br/sao_paulo”这样的字符串转换为独热矢量。） city_id: &quot;br/sao_paulo&quot; 但收集由其他模型推理的值会产生额外成本。可能值“219”目前代表圣保罗，但这种表示在未来运行其他模型时可能轻易发生变化： inferred_city_cluster: &quot;219&quot; 数据清理苹果树结出的果子有品相上乘的，也有虫蛀坏果。而高端便利店出售的苹果是 100% 完美的水果。从果园到水果店之间，专门有人花费大量时间将坏苹果剔除或给可以挽救的苹果涂上一层薄薄的蜡。作为一名机器学习工程师，您将花费大量的时间挑出坏样本并加工可以挽救的样本。即使是非常少量的“坏苹果”也会破坏掉一个大规模数据集。 缩放特征值缩放是指将浮点特征值从自然范围（例如 100 到 900）转换为标准范围（例如 0 到 1 或 -1 到 +1）。如果某个特征集只包含一个特征，则缩放可以提供的实际好处微乎其微或根本没有。不过，如果特征集包含多个特征，则缩放特征可以带来以下优势： ·帮助梯度下降法更快速地收敛。 ·帮助避免“NaN 陷阱”。在这种陷阱中，模型中的一个数值变成 NaN（例如，当某个值在训练期间超出浮点精确率限制时），并且模型中的所有其他数值最终也会因数学运算而变成 NaN。 ·帮助模型为每个特征确定合适的权重。如果没有进行特征缩放，则模型会对范围较大的特征投入过多精力。 您不需要对每个浮点特征进行完全相同的缩放。即使特征 A 的范围是 -1 到 +1，同时特征 B 的范围是 -3 到 +3，也不会产生什么恶劣的影响。不过，如果特征 B 的范围是 5000 到 100000，您的模型会出现糟糕的响应。 1234567891011121314151617要缩放数字数据，一种显而易见的方法是将 [最小值，最大值] 以线性方式映射到较小的范围，例如 [-1，+1]。另一种热门的缩放策略是计算每个值的 Z 得分。Z 得分与距离均值的标准偏差数相关。换而言之： scaled value = (value - mean) / stddev 例如，给定以下条件： ·均值 = 100 ·标准偏差 = 20 ·原始值 = 130 则： scaled_value = (130 - 100) / 20 scaled_value = 1.5使用 Z 得分进行缩放意味着，大多数缩放后的值将介于 -3 和 +3 之间，而少量值将略高于或低于该范围。 处理极端离群值分箱 清查截至目前，我们假定用于训练和测试的所有数据都是值得信赖的。在现实生活中，数据集中的很多样本是不可靠的，原因有以下一种或多种： 遗漏值。 例如，有人忘记为某个房屋的年龄输入值。 重复样本。 例如，服务器错误地将同一条记录上传了两次。 不良标签。 例如，有人错误地将一颗橡树的图片标记为枫树。 不良特征值。 例如，有人输入了多余的位数，或者温度计被遗落在太阳底下。 一旦检测到存在这些问题，您通常需要将相应样本从数据集中移除，从而“修正”不良样本。要检测遗漏值或重复样本，可以编写一个简单的程序。检测不良特征值或标签可能会比较棘手。 除了检测各个不良样本之外，还必须检测集合中的不良数据。直方图是一种用于可视化集合中数据的很好机制。此外，收集如下统计信息也会有所帮助： 最大值和最小值 均值和中间值 标准偏差 考虑生成离散特征的最常见值列表。例如，country:uk 的样本数是否符合您的预期？language:jp 是否真的应该作为您数据集中的最常用语言？ 了解数据 遵循以下规则： 记住预期的数据状态。 确认数据是否满足这些预期（或者您可以解释为何数据不满足预期）。 仔细检查训练数据是否与其他来源（例如信息中心）的数据一致。 像处理任何任务关键型代码一样谨慎处理您的数据。良好的机器学习依赖于良好的数据。 特征集编程练习创建一个包含极少特征但效果与更复杂的特征集一样出色的集合 设置和之前一样，我们先加载并准备加利福尼亚州住房数据。 12345678910111213141516171819202122from __future__ import print_functionimport mathfrom IPython import displayfrom matplotlib import cmfrom matplotlib import gridspecfrom matplotlib import pyplot as pltimport numpy as npimport pandas as pdfrom sklearn import metricsimport tensorflow as tffrom tensorflow.python.data import Datasettf.logging.set_verbosity(tf.logging.ERROR)pd.options.display.max_rows = 10pd.options.display.float_format = '&#123;:.1f&#125;'.formatcalifornia_housing_dataframe = pd.read_csv("https://download.mlcc.google.cn/mledu-datasets/california_housing_train.csv", sep=",")california_housing_dataframe = california_housing_dataframe.reindex( np.random.permutation(california_housing_dataframe.index)) 12345678910111213141516171819202122232425# 预处理def preprocess_features(california_housing_dataframe): selected_features = california_housing_dataframe[ ["latitude", "longitude", "housing_median_age", "total_rooms", "total_bedrooms", "population", "households", "median_income"]] processed_features = selected_features.copy() processed_features["rooms_per_person"] = ( california_housing_dataframe["total_rooms"]/ california_housing_dataframe["population"]) return processed_featuresdef preprocess_targets(california_housing_dataframe): output_targets = pd.DataFrame() # Scale the target to be in units of thousands of dollars. output_targets["median_house_value"] = ( california_housing_dataframe["median_house_value"] / 1000.0) return output_targets 123456789101112131415161718# 抽取前 12000 个数据作训练集training_examples = preprocess_features(california_housing_dataframe.head(12000))training_targets = preprocess_targets(california_housing_dataframe.head(12000))# 抽取最后 5000 个作验证集validation_examples = preprocess_features(california_housing_dataframe.tail(5000))validation_targets = preprocess_targets(california_housing_dataframe.tail(5000))# 检查一下我们的数据是否正常合理print("Training examples summary:")display.display(training_examples.describe())print("Validation examples summary:")display.display(validation_examples.describe())print("Training targets summary:")display.display(training_targets.describe())print("Validation targets summary:")display.display(validation_targets.describe()) Training examples summary: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } latitude longitude housing_median_age total_rooms total_bedrooms population households median_income rooms_per_person count 12000.0 12000.0 12000.0 12000.0 12000.0 12000.0 12000.0 12000.0 12000.0 mean 35.6 -119.6 28.6 2655.8 543.1 1433.2 504.6 3.9 2.0 std 2.1 2.0 12.6 2180.4 425.2 1122.9 387.0 1.9 1.1 min 32.5 -124.3 2.0 2.0 1.0 3.0 1.0 0.5 0.1 25% 33.9 -121.8 18.0 1467.0 298.0 793.0 283.0 2.6 1.5 50% 34.2 -118.5 29.0 2127.0 435.0 1170.0 410.0 3.5 1.9 75% 37.7 -118.0 37.0 3157.2 653.0 1726.0 608.0 4.8 2.3 max 42.0 -114.6 52.0 32627.0 6445.0 28566.0 6082.0 15.0 55.2 Validation examples summary: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } latitude longitude housing_median_age total_rooms total_bedrooms population households median_income rooms_per_person count 5000.0 5000.0 5000.0 5000.0 5000.0 5000.0 5000.0 5000.0 5000.0 mean 35.7 -119.6 28.6 2614.5 530.5 1420.8 493.0 3.9 2.0 std 2.1 2.0 12.6 2178.9 412.5 1205.6 378.5 2.0 1.3 min 32.5 -124.3 1.0 18.0 3.0 8.0 3.0 0.5 0.0 25% 33.9 -121.8 18.0 1445.0 294.0 780.0 276.8 2.6 1.5 50% 34.3 -118.5 29.0 2130.5 430.0 1159.5 406.0 3.5 1.9 75% 37.7 -118.0 37.0 3129.2 637.0 1700.8 595.0 4.7 2.3 max 41.8 -114.3 52.0 37937.0 5471.0 35682.0 5189.0 15.0 52.0 Training targets summary: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } median_house_value count 12000.0 mean 207.5 std 115.4 min 15.0 25% 119.9 50% 181.1 75% 265.6 max 500.0 Validation targets summary: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } median_house_value count 5000.0 mean 206.9 std 117.5 min 15.0 25% 118.8 50% 178.4 75% 263.1 max 500.0 构建良好的特征集如果只使用 2 个或 3 个特征，您可以获得的最佳效果是什么？ 相关矩阵展现了两两比较的相关性，既包括每个特征与目标特征之间的比较，也包括每个特征与其他特征之间的比较。 在这里，相关性被定义为皮尔逊相关系数。您不必理解具体数学原理也可完成本练习。 相关性值具有以下含义： -1.0：完全负相关 0.0：不相关 1.0：完全正相关 1234correlation_dataframe = training_examples.copy()correlation_dataframe["target"] = training_targets["median_house_value"]correlation_dataframe.corr() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } latitude longitude housing_median_age total_rooms total_bedrooms population households median_income rooms_per_person target latitude 1.0 -0.9 0.0 -0.0 -0.1 -0.1 -0.1 -0.1 0.1 -0.1 longitude -0.9 1.0 -0.1 0.0 0.1 0.1 0.1 -0.0 -0.1 -0.0 housing_median_age 0.0 -0.1 1.0 -0.4 -0.3 -0.3 -0.3 -0.1 -0.1 0.1 total_rooms -0.0 0.0 -0.4 1.0 0.9 0.9 0.9 0.2 0.1 0.1 total_bedrooms -0.1 0.1 -0.3 0.9 1.0 0.9 1.0 -0.0 0.1 0.0 population -0.1 0.1 -0.3 0.9 0.9 1.0 0.9 -0.0 -0.1 -0.0 households -0.1 0.1 -0.3 0.9 1.0 0.9 1.0 0.0 -0.0 0.1 median_income -0.1 -0.0 -0.1 0.2 -0.0 -0.0 0.0 1.0 0.3 0.7 rooms_per_person 0.1 -0.1 -0.1 0.1 0.1 -0.1 -0.0 0.3 1.0 0.2 target -0.1 -0.0 0.1 0.1 0.0 -0.0 0.1 0.7 0.2 1.0 理想情况下，我们希望具有与目标密切相关的特征。 此外，我们还希望有一些相互之间的相关性不太密切的特征，以便它们添加独立信息。 利用这些信息来尝试移除特征。您也可以尝试构建其他合成特征，例如两个原始特征的比例。 为方便起见，我们已经添加了前一个练习的训练代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136def construct_feature_columns(input_features): """Construct the TensorFlow Feature Columns. Args: input_features: The names of the numerical input features to use. Returns: A set of feature columns """ return set([tf.feature_column.numeric_column(my_feature) for my_feature in input_features])def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None): """Trains a linear regression model. Args: features: pandas DataFrame of features targets: pandas DataFrame of targets batch_size: Size of batches to be passed to the model shuffle: True or False. Whether to shuffle the data. num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely Returns: Tuple of (features, labels) for next data batch """ # Convert pandas data into a dict of np arrays. features = &#123;key:np.array(value) for key,value in dict(features).items()&#125; # Construct a dataset, and configure batching/repeating. ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit ds = ds.batch(batch_size).repeat(num_epochs) # Shuffle the data, if specified. if shuffle: ds = ds.shuffle(10000) # Return the next batch of data. features, labels = ds.make_one_shot_iterator().get_next() return features, labelsdef train_model( learning_rate, steps, batch_size, training_examples, training_targets, validation_examples, validation_targets): """Trains a linear regression model. In addition to training, this function also prints training progress information, as well as a plot of the training and validation loss over time. Args: learning_rate: A `float`, the learning rate. steps: A non-zero `int`, the total number of training steps. A training step consists of a forward and backward pass using a single batch. batch_size: A non-zero `int`, the batch size. training_examples: A `DataFrame` containing one or more columns from `california_housing_dataframe` to use as input features for training. training_targets: A `DataFrame` containing exactly one column from `california_housing_dataframe` to use as target for training. validation_examples: A `DataFrame` containing one or more columns from `california_housing_dataframe` to use as input features for validation. validation_targets: A `DataFrame` containing exactly one column from `california_housing_dataframe` to use as target for validation. Returns: A `LinearRegressor` object trained on the training data. """ periods = 10 steps_per_period = steps / periods # Create a linear regressor object. my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate) my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0) linear_regressor = tf.estimator.LinearRegressor( feature_columns=construct_feature_columns(training_examples), optimizer=my_optimizer ) # Create input functions. training_input_fn = lambda: my_input_fn(training_examples, training_targets["median_house_value"], batch_size=batch_size) predict_training_input_fn = lambda: my_input_fn(training_examples, training_targets["median_house_value"], num_epochs=1, shuffle=False) predict_validation_input_fn = lambda: my_input_fn(validation_examples, validation_targets["median_house_value"], num_epochs=1, shuffle=False) # Train the model, but do so inside a loop so that we can periodically assess # loss metrics. print("Training model...") print("RMSE (on training data):") training_rmse = [] validation_rmse = [] for period in range (0, periods): # Train the model, starting from the prior state. linear_regressor.train( input_fn=training_input_fn, steps=steps_per_period, ) # Take a break and compute predictions. training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn) training_predictions = np.array([item['predictions'][0] for item in training_predictions]) validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn) validation_predictions = np.array([item['predictions'][0] for item in validation_predictions]) # Compute training and validation loss. training_root_mean_squared_error = math.sqrt( metrics.mean_squared_error(training_predictions, training_targets)) validation_root_mean_squared_error = math.sqrt( metrics.mean_squared_error(validation_predictions, validation_targets)) # Occasionally print the current loss. print(" period %02d : %0.2f" % (period, training_root_mean_squared_error)) # Add the loss metrics from this period to our list. training_rmse.append(training_root_mean_squared_error) validation_rmse.append(validation_root_mean_squared_error) print("Model training finished.") # Output a graph of loss metrics over periods. plt.ylabel("RMSE") plt.xlabel("Periods") plt.title("Root Mean Squared Error vs. Periods") plt.tight_layout() plt.plot(training_rmse, label="training") plt.plot(validation_rmse, label="validation") plt.legend() return linear_regressor 搜索一组效果良好的特征和训练参数 12345678910111213141516171819202122232425262728293031323334353637minimal_features = [ "latitude", "longitude",]minimal_training_examples = training_examples[minimal_features]minimal_validation_examples = validation_examples[minimal_features]_ = train_model( learning_rate=0.01, steps=500, batch_size=5, training_examples=minimal_training_examples, training_targets=training_targets, validation_examples=minimal_validation_examples, validation_targets=validation_targets)plt.show()minimal_features = [ "median_income", "latitude",]minimal_training_examples = training_examples[minimal_features]minimal_validation_examples = validation_examples[minimal_features]_ = train_model( learning_rate=0.01, steps=500, batch_size=5, training_examples=minimal_training_examples, training_targets=training_targets, validation_examples=minimal_validation_examples, validation_targets=validation_targets)plt.show() Training model... RMSE (on training data): period 00 : 115.69 period 01 : 115.56 period 02 : 116.82 period 03 : 115.67 period 04 : 115.57 period 05 : 116.72 period 06 : 116.71 period 07 : 119.22 period 08 : 115.56 period 09 : 115.40 Model training finished. Training model... RMSE (on training data): period 00 : 165.32 period 01 : 125.38 period 02 : 116.53 period 03 : 115.99 period 04 : 115.63 period 05 : 114.81 period 06 : 114.06 period 07 : 113.58 period 08 : 114.35 period 09 : 112.88 Model training finished. 观察发现 经度 和 维度 貌似是没有什么相关度的，个人收入的中位数 和 维度 是一组比较好的特征组。 更好地利用纬度绘制 latitude 与 median_house_value 的图形后，表明两者确实不存在线性关系。 不过，有几个峰值与洛杉矶和旧金山大致相对应。 1plt.scatter(training_examples["latitude"], training_targets["median_house_value"]) &lt;matplotlib.collections.PathCollection at 0x7fb5a238afd0&gt; 尝试创建一些能够更好地利用纬度的合成特征。 例如，您可以创建某个特征，将 latitude 映射到值 |latitude - 38|，并将该特征命名为 distance_from_san_francisco。 或者，您可以将该空间分成 10 个不同的分桶（例如 latitude_32_to_33、latitude_33_to_34 等）：如果 latitude 位于相应分桶范围内，则显示值 1.0；如果不在范围内，则显示值 0.0。 使用相关矩阵来指导您构建合成特征；如果您发现效果还不错的合成特征，可以将其添加到您的模型中。 除了 latitude 之外，我们还会保留 median_income，以便与之前的结果进行比较。 我们决定对纬度进行分桶。在 Pandas 中使用 Series.apply 执行此操作相当简单。 12345678LATITUDE_RANGES = zip(range(32, 44), range(33, 45))def select_and_transform_features(source_df): selected_examples = pd.DataFrame() selected_examples["median_income"] = source_df["median_income"] for r in LATITUDE_RANGES: selected_examples["latitude_%d_to_%d" % r] = source_df["latitude"].apply( lambda l: 1.0 if l &gt;= r[0] and l &lt; r[1] else 0.0) return selected_examples 12selected_training_examples = select_and_transform_features(training_examples)selected_training_examples.head(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } median_income latitude_32_to_33 latitude_33_to_34 latitude_34_to_35 latitude_35_to_36 latitude_36_to_37 latitude_37_to_38 latitude_38_to_39 latitude_39_to_40 latitude_40_to_41 latitude_41_to_42 latitude_42_to_43 latitude_43_to_44 4828 3.4 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 6103 4.7 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 12selected_validation_examples = select_and_transform_features(validation_examples)selected_validation_examples.head(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } median_income latitude_32_to_33 latitude_33_to_34 latitude_34_to_35 latitude_35_to_36 latitude_36_to_37 latitude_37_to_38 latitude_38_to_39 latitude_39_to_40 latitude_40_to_41 latitude_41_to_42 latitude_42_to_43 latitude_43_to_44 8959 7.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 8330 2.6 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 12345678_ = train_model( learning_rate=0.01, steps=500, batch_size=5, training_examples=selected_training_examples, training_targets=training_targets, validation_examples=selected_validation_examples, validation_targets=validation_targets) Training model... RMSE (on training data): period 00 : 227.14 period 01 : 216.98 period 02 : 206.92 period 03 : 196.96 period 04 : 187.10 period 05 : 177.38 period 06 : 167.81 period 07 : 158.44 period 08 : 149.28 period 09 : 140.38 Model training finished.]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 4]]></title>
    <url>%2F2019%2F01%2F08%2FTensorFlow_4%2F</url>
    <content type="text"><![CDATA[验证通常我们会把数据分配为三分，训练集，交叉验证集和测试集，这样做的好处是为了避免过拟合，能够更好的泛化。 添加验证集后，我们的工作流程大概是这样的： 接下来的练习是尝试使用这个流程来训练 与在之前的练习中一样，我们将使用加利福尼亚州住房数据集，尝试根据 1990 年的人口普查数据在城市街区级别预测 median_house_value。 设置首先加载并准备数据。这一次使用多个特征，因此把逻辑模块化，以方便对特征进行预处理 12345678910111213141516171819from __future__ import print_functionimport mathfrom IPython import displayfrom matplotlib import cmfrom matplotlib import gridspecfrom matplotlib import pyplot as pltimport numpy as npimport pandas as pdfrom sklearn import metricsimport tensorflow as tffrom tensorflow.python.data import Datasettf.logging.set_verbosity(tf.logging.ERROR)pd.options.display.max_rows = 10pd.options.display.float_format = '&#123;:.1f&#125;'.formatcalifornia_housing_dataframe = pd.read_csv("https://download.mlcc.google.cn/mledu-datasets/california_housing_train.csv", sep=",") 12345678910111213141516171819202122232425262728293031323334353637def preprocess_features(california_housing_dataframe): """从加州住房数据集获取输入数据 参数: california_housing_dataframe:panda的 DataFrame 类型的数据集 返回: 用于模型feature的DataFrame """ selected_features = california_housing_dataframe[ ["latitude", "longitude", "housing_median_age", "total_rooms", "total_bedrooms", "population", "households", "median_income"]] processed_features = selected_features.copy() # Create a synthetic feature. processed_features["rooms_per_person"] = ( california_housing_dataframe["total_rooms"] / california_housing_dataframe["population"]) return processed_featuresdef preprocess_targets(california_housing_dataframe): """准备目标特性(即来自加州住房数据集。 参数: california_housing_dataframe:panda的 DataFrame 类型的数据集 来自加州住房数据集。 返回: 用于模型feature的DataFrame """ output_targets = pd.DataFrame() # 将targets的单位扩到到千为单位 output_targets["median_house_value"] = ( california_housing_dataframe["median_house_value"] / 1000.0) return output_targets 我们从 17000 个样本中选前 12000 个样本 作 训练集 12training_examples = preprocess_features(california_housing_dataframe.head(12000))training_examples.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } latitude longitude housing_median_age total_rooms total_bedrooms population households median_income rooms_per_person count 12000.0 12000.0 12000.0 12000.0 12000.0 12000.0 12000.0 12000.0 12000.0 mean 34.6 -118.5 27.5 2655.7 547.1 1476.0 505.4 3.8 1.9 std 1.6 1.2 12.1 2258.1 434.3 1174.3 391.7 1.9 1.3 min 32.5 -121.4 1.0 2.0 2.0 3.0 2.0 0.5 0.0 25% 33.8 -118.9 17.0 1451.8 299.0 815.0 283.0 2.5 1.4 50% 34.0 -118.2 28.0 2113.5 438.0 1207.0 411.0 3.5 1.9 75% 34.4 -117.8 36.0 3146.0 653.0 1777.0 606.0 4.6 2.3 max 41.8 -114.3 52.0 37937.0 5471.0 35682.0 5189.0 15.0 55.2 12training_targets = preprocess_targets(california_housing_dataframe.head(12000))training_targets.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } median_house_value count 12000.0 mean 198.0 std 111.9 min 15.0 25% 117.1 50% 170.5 75% 244.4 max 500.0 我们从 17000 个样本中选择后 5000 个为 验证集 12validation_examples = preprocess_features(california_housing_dataframe.tail(5000))validation_examples.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } latitude longitude housing_median_age total_rooms total_bedrooms population households median_income rooms_per_person count 5000.0 5000.0 5000.0 5000.0 5000.0 5000.0 5000.0 5000.0 5000.0 mean 38.1 -122.2 31.3 2614.8 521.1 1318.1 491.2 4.1 2.1 std 0.9 0.5 13.4 1979.6 388.5 1073.7 366.5 2.0 0.6 min 36.1 -124.3 1.0 8.0 1.0 8.0 1.0 0.5 0.1 25% 37.5 -122.4 20.0 1481.0 292.0 731.0 278.0 2.7 1.7 50% 37.8 -122.1 31.0 2164.0 424.0 1074.0 403.0 3.7 2.1 75% 38.4 -121.9 42.0 3161.2 635.0 1590.2 603.0 5.1 2.4 max 42.0 -121.4 52.0 32627.0 6445.0 28566.0 6082.0 15.0 18.3 12validation_targets = preprocess_targets(california_housing_dataframe.tail(5000))validation_targets.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } median_house_value count 5000.0 mean 229.5 std 122.5 min 15.0 25% 130.4 50% 213.0 75% 303.2 max 500.0 检查数据 我们根据基准预期情况检查一下我们的数据： 对于一些值（例如 median_house_value），我们可以检查这些值是否位于合理的范围内（请注意，这是 1990 年的数据，不是现在的！）。 对于 latitude 和 longitude 等其他值，我们可以通过 Google 进行快速搜索，并快速检查一下它们与预期值是否一致。 如果您仔细看，可能会发现下列异常情况： median_income 位于 3 到 15 的范围内。我们完全不清楚此范围究竟指的是什么，看起来可能是某对数尺度？无法找到相关记录；我们所能假设的只是，值越高，相应的收入越高。 median_house_value 的最大值是 500001。这看起来像是某种人为设定的上限。 rooms_per_person 特征通常在正常范围内，其中第 75 百分位数的值约为 2。但也有一些非常大的值（例如 18 或 55），这可能表明数据有一定程度的损坏。 绘制维度/经度与房屋价值中位数的曲线图 我们来详细了解一下 latitude 和 longitude 这两个特征。它们是相关城市街区的地理坐标。 利用这两个特征可以提供出色的可视化结果 - 我们来绘制 latitude 和 longitude 的曲线图，然后用颜色标注 median_house_value。 123456789101112131415161718192021222324252627def plot_scatter(training_examples, training_targets, validation_examples, validation_targets): plt.figure(figsize=(13, 8)) ax = plt.subplot(1, 2, 1) ax.set_title("Validation Data") ax.set_autoscaley_on(False) ax.set_ylim([32, 43]) ax.set_autoscalex_on(False) ax.set_xlim([-126, -112]) plt.scatter(validation_examples["longitude"], validation_examples["latitude"], cmap="coolwarm", c=validation_targets["median_house_value"] / validation_targets["median_house_value"].max()) ax = plt.subplot(1, 2, 2) ax.set_title("Training Date") ax.set_autoscaley_on(False) ax.set_ylim([32, 43]) ax.set_autoscalex_on(False) ax.set_xlim([-126, -112]) plt.scatter(training_examples["longitude"], training_examples["latitude"], cmap="coolwarm", c=training_targets["median_house_value"] / training_targets["median_house_value"].max()) _ = plt.plot() 1plot_scatter(training_examples, training_targets, validation_examples, validation_targets) 现在应该已经呈现出一幅不错的加利福尼亚州地图了，其中旧金山和洛杉矶等住房成本高昂的地区用红色表示。 根据训练集呈现的地图有几分像真正的地图，但根据验证集呈现的明显不像。 查看上面的摘要统计信息表格时，很容易产生想知道如何进行有用的数据检查的想法。每个街区 total_rooms 的第 75 百分位的正确值是什么？ 需要注意的关键一点是，对于任何指定特征或列，训练集和验证集之间的值的分布应该大致相同。 我们真正需要担心的是，真实情况并非这样，这一事实表明我们创建训练集和验证集的拆分方式很可能存在问题。 随机化处理数据我们需要在读入数据时，对数据进行随机化处理的。 如果我们在创建训练集和验证集之前，没有对数据进行正确的随机化处理，那么以某种特定顺序接收数据可能会导致出现问题（似乎就是此时的问题）。 发现并解决问题后，重新运行上面的 latitude/longitude 绘图单元格，并确认我们的健全性检查的结果看上去更好了。 顺便提一下，在这一步中，我们会学到一项重要经验。 机器学习中的调试通常是数据调试而不是代码调试。 如果数据有误，即使最高级的机器学习代码也挽救不了局面。 12345678910california_housing_dataframe = california_housing_dataframe.reindex( np.random.permutation(california_housing_dataframe.index))training_examples = preprocess_features(california_housing_dataframe.head(12000))training_targets = preprocess_targets(california_housing_dataframe.head(12000))validation_examples = preprocess_features(california_housing_dataframe.tail(5000))validation_targets = preprocess_targets(california_housing_dataframe.tail(5000))plot_scatter(training_examples, training_targets, validation_examples, validation_targets) 好的，这次的结果来看训练集和验证集都有相似的分布。 训练和评估模型尝试不同的超参数，获得最佳验证效果。 首先定义输入函数 1234567891011def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None): features = &#123;key: np.array(value) for key, value in dict(features).items()&#125; ds = Dataset.from_tensor_slices((features, targets)) ds = ds.batch(batch_size).repeat(num_epochs) if shuffle: ds = ds.shuffle(10000) features, labels = ds.make_one_shot_iterator().get_next() return features, labels 由于我们现在使用的是多个输入特征，因此需要把用于将特征列配置为独立函数的代码模块化。（目前此代码相当简单，因为我们的所有特征都是数值，但当我们在今后的练习中使用其他类型的特征时，会基于此代码进行构建。） 123456789def construct_feature_columns(input_features): """构造TensorFlow特征列 参数: input_features:要使用的数字输入特性的名称。 返回: 一个 feature columns 集合 """ return set([tf.feature_column.numeric_column(my_feature) for my_feature in input_features]) 接下来，继续完成 train_model() 代码，以设置输入函数和计算预测。 注意：可以参考以前的练习中的代码，但要确保针对相应数据集调用 predict()。 比较训练数据和验证数据的损失。使用一个原始特征时，我们得到的最佳均方根误差 (RMSE) 约为 180。 现在我们可以使用多个特征，不妨看一下可以获得多好的结果。 使用我们之前了解的一些方法检查数据。这些方法可能包括： 比较预测值和实际目标值的分布情况 绘制预测值和目标值的散点图 使用 latitude 和 longitude 绘制两个验证数据散点图： 一个散点图将颜色映射到实际目标 median_house_value 另一个散点图将颜色映射到预测的 median_house_value，并排进行比较。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798def train_model( learning_rate, strps, batch_size, training_examples, training_targets, validation_examples, validation_targets): """训练多元特征的线性回归模型 除训练外，此功能还打印训练进度信息， 以及随着时间的推移而失去的训练和验证。 参数: learning_rate:一个float，表示学习率 steps:一个非零的int，训练步骤的总数。训练步骤 由使用单个批处理的向前和向后传递组成。 batch_size:一个非零的int training_example: DataFrame 包含一个或多个列 ' california_housing_dataframe '作为训练的输入feature training_targets:一个' DataFrame '，它只包含一列 ' california_housing_dataframe '作为训练的目标。 validation_example: ' DataFrame '包含一个或多个列 ' california_housing_dataframe '作为验证的输入feature validation_targets: ' DataFrame '，仅包含来自其中的一列 ' california_housing_dataframe '作为验证的目标。 返回: 在训练数据上训练的“线性回归器”对象 """ periods = 10 steps_per_period = strps / periods # 创建一个线性回归对象 my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate) my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0) linear_regressor = tf.estimator.LinearRegressor( feature_columns=construct_feature_columns(training_examples), optimizer=my_optimizer ) # 创建输入函数 training_input_fn = lambda: my_input_fn( training_examples, training_targets["median_house_value"], batch_size=batch_size) predict_training_input_fn = lambda: my_input_fn( training_examples, training_targets["median_house_value"], num_epochs=1, shuffle=False) predict_validation_input_fn = lambda: my_input_fn( validation_examples, validation_targets["median_house_value"], num_epochs=1, shuffle=False) #训练模型，但要在循环中进行，这样我们才能定期评估 #损失指标 print("Training model...") print("RMSE (on training data):") training_rmse = [] validation_rmse = [] for period in range (0, periods): # Train the model, starting from the prior state. linear_regressor.train( input_fn=training_input_fn, steps=steps_per_period, ) # Take a break and compute predictions. training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn) training_predictions = np.array([item['predictions'][0] for item in training_predictions]) validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn) validation_predictions = np.array([item['predictions'][0] for item in validation_predictions]) # Compute training and validation loss. training_root_mean_squared_error = math.sqrt( metrics.mean_squared_error(training_predictions, training_targets)) validation_root_mean_squared_error = math.sqrt( metrics.mean_squared_error(validation_predictions, validation_targets)) # Occasionally print the current loss. print(" period %02d : %0.2f" % (period, training_root_mean_squared_error)) # Add the loss metrics from this period to our list. training_rmse.append(training_root_mean_squared_error) validation_rmse.append(validation_root_mean_squared_error) print("Model training finished.") # Output a graph of loss metrics over periods. plt.ylabel("RMSE") plt.xlabel("Periods") plt.title("Root Mean Squared Error vs. Periods") plt.tight_layout() plt.plot(training_rmse, label="training") plt.plot(validation_rmse, label="validation") plt.legend() return linear_regressor 12345678linear_regressor = train_model( learning_rate=0.00003, strps=500, batch_size=5, training_examples=training_examples, training_targets=training_targets, validation_examples=validation_examples, validation_targets=validation_targets) Training model... RMSE (on training data): period 00 : 217.57 period 01 : 200.14 period 02 : 185.74 period 03 : 175.52 period 04 : 170.71 period 05 : 167.06 period 06 : 165.72 period 07 : 165.65 period 08 : 166.77 period 09 : 168.40 Model training finished. 基于测试数据进行评估载入测试数据集并据此评估模型。 我们已对验证数据进行了大量迭代。接下来确保我们没有过拟合该特定样本集的特性。 测试数据集位于此处。 123456789101112131415161718california_housing_test_data = pd.read_csv("https://download.mlcc.google.cn/mledu-datasets/california_housing_test.csv", sep=",")test_examples = preprocess_features(california_housing_test_data)test_targets = preprocess_targets(california_housing_test_data)predict_test_input_fn = lambda: my_input_fn( test_examples, test_targets["median_house_value"], num_epochs=1, shuffle=False)test_predictions = linear_regressor.predict(input_fn=predict_test_input_fn)test_predictions = np.array([item['predictions'][0] for item in test_predictions])root_mean_squared_error = math.sqrt( metrics.mean_squared_error(test_predictions, test_targets))print("Final RMSE (on test data): %0.2f" % root_mean_squared_error) Final RMSE (on test data): 162.99]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 16. 3Sum Closest]]></title>
    <url>%2F2019%2F01%2F07%2FLeetCode%2016.%203Sum%20Closest%2F</url>
    <content type="text"><![CDATA[好久没写C的，写个C复习一下，写个简单的leetcode吧 3Sum ClosestGiven an array nums of n integers and an integer target, find three integers in nums such that the sum is closest to target. Return the sum of the three integers. You may assume that each input would have exactly one solution. Example: Given array nums = [-1, 2, 1, -4], and target = 1. The sum that is closest to the target is 2. (-1 + 2 + 1 = 2). 这个问题比较简单，我先写了个暴力解法1234567891011121314151617181920int threeSumClosest(int* nums, int numsSize, int target) &#123; int result=nums[0] + nums[1] + nums[2]; int i, j, k; int sum; for(i=0; i&lt;numsSize-2; i++) &#123; for(j=i+1; j&lt;numsSize-1; j++) &#123; for(k=j+1; k&lt;numsSize; k++) &#123; sum = nums[i] + nums[j] + nums[k]; if(abs(sum-target) &lt;= abs(result-target)) result = sum; &#125; &#125; &#125; return result;&#125; runtime为172 ms可想而知是很低的。 看了下别人的solution，先排序再算要快好多，于是我先尝试了 冒泡排序 12345678910void bubble_sort(int arr[], int len) &#123; int i, j, temp; for (i = 0; i &lt; len - 1; i++) for (j = 0; j &lt; len - 1 - i; j++) if (arr[j] &gt; arr[j + 1]) &#123; temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; &#125;&#125; Runtime 为8 ms，还是差点，仔细看了下，因该是我的排序算法慢了，看到别人的solution用c中的快排qsort()，可惜我之前没学过这个函数,所以没想到，赶紧恶补下。 原文地址 1234567891011121314151617181920212223242526272829303132333435void qsort( void *base, size_t nmemb, size_t size, int (*compar)(const void *, const void *) ); 函数功能：qsort()函数的功能是对数组进行排序，数组有nmemb个元素，每个元素大小为size。参数base - base指向数组的起始地址，通常该位置传入的是一个数组名参数nmemb - nmemb表示该数组的元素个数参数size - size表示该数组中每个元素的大小（字节数）参数(*compar)(const void *, const void *) - 此为指向比较函数的函数指针，决定了排序的顺序。函数返回值：无注意：如果两个元素的值是相同的，那么它们的前后顺序是不确定的。也就是说qsort()是一个不稳定的排序算法。compar参数compar参数指向一个比较两个元素的函数。比较函数的原型应该像下面这样。注意两个形参必须是const void *型，同时在调用compar 函数（compar实质为函数指针，这里称它所指向的函数也为compar）时，传入的实参也必须转换成const void *型。在compar函数内部会将const void *型转换成实际类型，见下文。int compar(const void *p1, const void *p2);如果compar返回值小于0（&lt; 0），那么p1所指向元素会被排在p2所指向元素的前面如果compar返回值等于0（= 0），那么p1所指向元素与p2所指向元素的顺序不确定如果compar返回值大于0（&gt; 0），那么p1所指向元素会被排在p2所指向元素的后面因此，如果想让qsort()进行从小到大（升序）排序，那么一个通用的compar函数可以写成这样：int compareMyType (const void * a, const void * b)&#123; if ( *(MyType*)a &lt; *(MyType*)b ) return -1; if ( *(MyType*)a == *(MyType*)b ) return 0; if ( *(MyType*)a &gt; *(MyType*)b ) return 1;&#125;注意：你要将MyType换成实际数组元素的类型。 使用快排后 Runtime: 4 ms, faster than 100.00% of C online submissions for 3Sum Closest. 123456789101112131415161718192021222324252627282930313233343536int comp(const void*a,const void*b)&#123; return *(int*)a-*(int*)b;&#125;int threeSumClosest(int* nums, int numsSize, int target) &#123; int result=nums[0] + nums[1] + nums[2]; if(numsSize &lt;= 3) return result; int i, j, k; int sum; qsort(nums, numsSize, sizeof(int), comp); for(i=0; i&lt;numsSize-2; i++) &#123; j = i + 1; k = numsSize - 1; while(j &lt; k) &#123; sum = nums[i] + nums[j] + nums[k]; if(abs(sum-target) &lt;= abs(result-target)) &#123; if(sum == target) return sum; result = sum; &#125; (sum &gt; target)? k--: j++; &#125; &#125; return result;&#125;]]></content>
      <categories>
        <category>算法练习</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>c</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 3]]></title>
    <url>%2F2018%2F12%2F29%2FTensorFlow_3%2F</url>
    <content type="text"><![CDATA[使用TensorFlow的基本步骤添加必要的库1234567891011121314151617from __future__ import print_functionimport mathfrom IPython import displayfrom matplotlib import cmfrom matplotlib import gridspecfrom matplotlib import pyplot as pltimport numpy as npimport pandas as pdfrom sklearn import metricsimport tensorflow as tffrom tensorflow.python.data import Datasettf.logging.set_verbosity(tf.logging.ERROR)pd.options.display.max_rows = 10 # 最大显示行数pd.options.display.float_format = '&#123;:.1f&#125;'.format # 精确度 保留一位小数 加载数据集加载的数据集，数据基于加利福尼亚州1990年的人口普查数据 1california_housing_dataframe = pd.read_csv("https://download.mlcc.google.cn/mledu-datasets/california_housing_train.csv", sep=",") 初始化数据集，对数据集进行随机化处理，以确保不会出现损害随机梯度下降的效果。此外，我们会将 median_house_value 调整为以千为单位，这样，模型就能够以常用范围内的学习速率较为轻松地学习这些数据。 1234california_housing_dataframe = california_housing_dataframe.reindex( np.random.permutation(california_housing_dataframe.index))california_housing_dataframe["median_house_value"] /= 1000.0california_housing_dataframe .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value 13098 -121.9 37.6 20.0 1309.0 184.0 514.0 172.0 11.0 475.8 6576 -118.3 34.1 52.0 1261.0 616.0 2309.0 581.0 1.6 225.0 12732 -121.8 37.7 17.0 3112.0 872.0 1392.0 680.0 3.0 172.5 6505 -118.3 34.0 34.0 1462.0 394.0 1310.0 351.0 1.2 90.1 339 -116.9 32.7 9.0 2652.0 393.0 1355.0 362.0 6.3 293.1 ... ... ... ... ... ... ... ... ... ... 6741 -118.3 33.9 41.0 896.0 198.0 605.0 168.0 2.3 128.1 496 -117.0 33.7 13.0 16148.0 3474.0 6159.0 3232.0 2.0 97.8 9140 -119.0 35.4 30.0 227.0 75.0 169.0 101.0 1.4 60.0 2610 -117.7 34.1 33.0 2081.0 409.0 1008.0 375.0 2.6 138.1 6827 -118.3 34.0 35.0 1090.0 345.0 1605.0 330.0 2.2 152.8 17000 rows × 9 columns 检查数据建议在使用数据之前，先对它有一个初步的了解。 输出关于各列的一些实用统计信息快速摘要：样本数，均值，标准偏差，最大值，最小值和各种分位数。 1california_housing_dataframe.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value count 17000.0 17000.0 17000.0 17000.0 17000.0 17000.0 17000.0 17000.0 17000.0 mean -119.6 35.6 28.6 2643.7 539.4 1429.6 501.2 3.9 207.3 std 2.0 2.1 12.6 2179.9 421.5 1147.9 384.5 1.9 116.0 min -124.3 32.5 1.0 2.0 1.0 3.0 1.0 0.5 15.0 25% -121.8 33.9 18.0 1462.0 297.0 790.0 282.0 2.6 119.4 50% -118.5 34.2 29.0 2127.0 434.0 1167.0 409.0 3.5 180.4 75% -118.0 37.7 37.0 3151.2 648.2 1721.0 605.2 4.8 265.0 max -114.3 42.0 52.0 37937.0 6445.0 35682.0 6082.0 15.0 500.0 构建第一个模型尝试预测median_house_value，它将是我们的标签，也称为目标。我们将使用total_rooms作为输入特征。 注意：我们使用的是城市街区级别的数据，因此该特征表示相应街区的房间总数。 为了训练模型，我们将使用 TensorFlow Estimator API 提供的 LinearRegressor 接口。此 API 负责处理大量低级别模型搭建工作，并会提供执行模型训练、评估和推理的便利方法。 第一步：定义特征并配置特征列为了将我们的训练数据导入TensorFlow，我们需要指定每个特征包含的数据类型。在练习中，主要使用一下两类数据： 分类数据： 一种文字数据。 数值数据：一种数字（整数或浮点数）数据以及希望是为数字的数据。 在 TenssorFlow中，使用“特征列”的结构来表示特征的数据类型。特征列仅储存对特征数据的描述；不包含特征数据本身。 一开始，只使用一个数值输入特征total_rooms。以下代码会从 california_housing_dataframe 中提取 total_rooms 数据，并使用 numeric_column 定义特征列，这样会将其数据指定为数值： 12345# 定义输入特征 total_rooms.my_feature = california_housing_dataframe[["total_rooms"]]# 为total_rooms配置一个由数字构成的feature columnfeature_columns = [tf.feature_column.numeric_column("total_rooms")] 注意：total_rooms 数据的形状是一维数组（每个街区的房间总数列表）。这是 numeric_column 的默认形状，因此我们不必将其作为参数传递。 第二步：定义目标接下来，我们将定义目标，也就是 median_house_value。同样，我们可以从 california_housing_dataframe 中提取它： 12# 定义目标targets = california_housing_dataframe["median_house_value"] 第三步：配置LinearRegressor 接下来，我们将使用 LinearRegressor 配置线性回归模型，并使用 GradientDescentOptimizer（它会实现小批量随机梯度下降法 (SGD)）训练该模型。learning_rate 参数可控制梯度步长的大小。 注意：为了安全起见，我们还会通过 clip_gradients_by_norm 将梯度裁剪应用到我们的优化器。梯度裁剪可确保梯度大小在训练期间不会变得过大，梯度过大会导致梯度下降法失败。 12345678910# 使用梯度下降作为训练模型的优化器my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0000001)my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)# 使用我们的特性列和优化器配置线性回归模型# 为梯度下降设置0.0000001的学习率linear_regresor = tf.estimator.LinearRegressor( feature_columns=feature_columns, optimizer=my_optimizer) 第四步：定义输入函数 要将加利福尼亚州住房数据导入 LinearRegressor，我们需要定义一个输入函数，让它告诉 TensorFlow 如何对数据进行预处理，以及在模型训练期间如何批处理、随机处理和重复数据。 首先，我们将 Pandas 特征数据转换成 NumPy 数组字典。然后，我们可以使用 TensorFlow Dataset API 根据我们的数据构建 Dataset 对象，并将数据拆分成大小为 batch_size 的多批数据，以按照指定周期数 (num_epochs) 进行重复。 注意：如果将默认值 num_epochs=None 传递到 repeat()，输入数据会无限期重复。 然后，如果 shuffle 设置为 True，则我们会对数据进行随机处理，以便数据在训练期间以随机方式传递到模型。buffer_size 参数会指定 shuffle 将从中随机抽样的数据集的大小。 最后，输入函数会为该数据集构建一个迭代器，并向 LinearRegressor 返回下一批数据。 123456789101112131415161718192021222324252627def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None): """训练一个特征的线性回归模型 参数: features: pandas DataFrame 类型的 features targets: pandas DataFrame 类型的 targets batch_size: Size of batches to be passed to the model shuffle: True or False. 是否随机化 data. num_epochs: 数据应重复的周期数. None = repeat indefinitely Returns: Tuple of (features, labels) for next data batch 元组：下一个数据批处理的(特征、标签/目标) """ # 将pandas数据转换为numpy数组字典 features = &#123;key: np.array(value) for key, value in dict(features).items()&#125; # 构建dataset，并且拆分为batch_size个数据 ds = Dataset.from_tensor_slices((features, targets)) # 限制为最大2GB ds = ds.batch(batch_size).repeat(num_epochs) # shuffle if shuffle: ds = ds.shuffle(buffer_size=10000) # 返回下一批数据 features, labels = ds.make_one_shot_iterator().get_next() return features, labels 第五步：训练数据 现在，我们可以在 linear_regressor 上调用 train() 来训练模型。我们会将 my_input_fn 封装在 lambda 中，以便可以将 my_feature 和 target 作为参数传入（有关详情，请参阅此 TensorFlow 输入函数教程），首先，我们会训练 100 步。 1234_ = linear_regresor.train( input_fn = lambda: my_input_fn(my_feature, targets), steps=100) 第六步： 评估模型我们基于该训练数据做一次预测，看看我们的模型在训练期间与这些数据的拟合情况。 注意：训练误差可以衡量您的模型与训练数据的拟合情况，但并_不能_衡量模型泛化到新数据的效果。在后面的练习中，您将探索如何拆分数据以评估模型的泛化能力。 123456789101112131415# 为预测创建一个输入函数predication_input_fn = lambda: my_input_fn(my_feature, targets, num_epochs=1, shuffle=False)# 在 linear_regressor 上调用predict()进行预测predictions = linear_regresor.predict(input_fn=predication_input_fn)# 将预测格式化为一个NumPy的数组，这样我们就可以计算错误度量predictions = np.array([item['predictions'][0] for item in predictions])# 输出方差mean_squared_error = metrics.mean_squared_error(predictions, targets)root_mean_squared_error = math.sqrt(mean_squared_error)print("Mean Squared Error (on training data): %0.3f" % mean_squared_error)print("Root Mean Squared Error (on training data): %0.3f" % root_mean_squared_error) Mean Squared Error (on training data): 56367.025 Root Mean Squared Error (on training data): 237.417 如何判断误差有多大？由于均方误差 (MSE) 很难解读，因此我们经常查看的是均方根误差 (RMSE)。RMSE 的一个很好的特性是，它可以在与原目标相同的规模下解读。 我们来比较一下 RMSE 与目标最大值和最小值的差值： 12345678min_house_value = california_housing_dataframe["median_house_value"].min()max_house_value = california_housing_dataframe["median_house_value"].max()min_max_difference = max_house_value - min_house_valueprint("Min. Median House Value: %0.3f" % min_house_value)print("Max. Median House Value: %0.3f" % max_house_value)print("Difference between Min. and Max.: %0.3f" % min_max_difference)print("Root Mean Squared Error: %0.3f" % root_mean_squared_error) Min. Median House Value: 14.999 Max. Median House Value: 500.001 Difference between Min. and Max.: 485.002 Root Mean Squared Error: 237.417 我们的误差跨越目标值近一般范围，为了进一步缩小误差，首先了解一下我们的预测predictions和targets的总体统计信息。 1234calibration_data = pd.DataFrame()calibration_data["predictions"] = pd.Series(predictions)calibration_data["targets"] = pd.Series(targets)calibration_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } predictions targets count 17000.0 17000.0 mean 0.1 207.3 std 0.1 116.0 min 0.0 15.0 25% 0.1 119.4 50% 0.1 180.4 75% 0.2 265.0 max 1.9 500.0 此信息也许有些帮助，比较看相差还是蛮大的，通过数据可视化来观察下 我们知道，单个特征的线性回归可以绘制一条将输入 x 映射到输出 y 的线。 首先，获得均匀分布的随机数据样本，以便绘制可辨识的散点图。 1sample = california_housing_dataframe.sample(n=300) # 随机抽样300个来观察 然后，根据模型的偏差项和特征权重绘制学习线，并绘制散点图。 1234567891011121314151617181920212223# 获取 total_rooms 最大与最小值x_0 = sample["total_rooms"].min()x_1 = sample["total_rooms"].max()# 检索训练过程中产生的最终权重和偏差weight = linear_regresor.get_variable_value("linear/linear_model/total_rooms/weights")[0]bias = linear_regresor.get_variable_value("linear/linear_model/bias_weights")# 获取total_rooms预测的最小和最大值y_0 = weight * x_0 + biasy_1 = weight * x_1 + bias# 现在我们有两个坐标后 画出回归线plt.plot([x_0, x_1], [y_0, y_1], c='r')# 写上每个轴代表的含义plt.ylabel("median_house_value")plt.xlabel("total_rooms")# 画出sample 数据的散点图plt.scatter(sample["total_rooms"], sample["median_house_value"])plt.show() 这条线看起来明显和目标相差很大，综上所述，这些初级健全性检查提示我们也许可以找到更好的线。 调整模型超参数我们把以上所学的东西整理到一个函数中，以方便我们更容易的调整参数和观察变化。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091def train_model(learning_rate, steps, batch_size, input_feature="total_rooms"): """一个线性回归的训练模型 参数： learning_rate: 学习速率 float steps: 训练总次数 int batch_size: 批处理大小 非0 int input_feature: 一个' string '，指定一个来自' california_housing_dataframe '的列用作输入特性。 """ periods = 10 # 周期 steps_per_period = steps / periods my_feature = input_feature my_feature_data = california_housing_dataframe[[my_feature]] my_label = "median_house_value" targets = california_housing_dataframe[my_label] # 创建 feature columns feature_columns = [tf.feature_column.numeric_column(my_feature)] # 创建 input feature training_input_fn = lambda: my_input_fn(my_feature_data, targets, batch_size=batch_size) prediction_input_fn = lambda: my_input_fn(my_feature_data, targets, num_epochs=1, shuffle=True) # 创建 一个线性回归对象 my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate) my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0) linear_regressor = tf.estimator.LinearRegressor( feature_columns=feature_columns, optimizer=my_optimizer ) # 设置回归线的状态 plt.figure(figsize=(15, 6)) plt.subplot(1, 2, 1) plt.title("Learned Line by Period") plt.ylabel(my_label) plt.xlabel(my_feature) sample = california_housing_dataframe.sample(n=300) plt.scatter(sample[my_feature], sample[my_label]) colors = [cm.coolwarm(x) for x in np.linspace(-1, 1, periods)] # 训练模型，但是在循环中这样做，这样我们就可以周期性地评估 # 损失指标 print("Training model...") print("RMSE (on training data):") root_mean_squared_errors = [] for period in range(0, periods): # 训练模型，从之前的状态开始 linear_regressor.train( input_fn=training_input_fn, steps=steps_per_period ) # 计算预测值 predictions = linear_regressor.predict(input_fn=prediction_input_fn) predictions = np.array([item['predictions'][0] for item in predictions]) # 计算损失 root_mean_squared_error = math.sqrt( metrics.mean_squared_error(predictions, targets)) print(" period %02d : %0.2f" % (period, root_mean_squared_error)) # 添加loss 到list root_mean_squared_errors.append(root_mean_squared_error) # 纪录权重和偏差 y_extents = np.array([0, sample[my_label].max()]) weight = linear_regressor.get_variable_value('linear/linear_model/%s/weights' % input_feature)[0] bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights') x_extents = (y_extents - bias) / weight x_extents = np.maximum(np.minimum(x_extents, sample[my_feature].max()), sample[my_feature].min()) y_extents = weight * x_extents + bias plt.plot(x_extents, y_extents, color=colors[period]) print("Model training finished.") # 输出一个周期内损失指标的图表。 plt.subplot(1, 2, 2) plt.ylabel('RMSE') plt.xlabel('Periods') plt.title("Root Mean Squared Error vs. Periods") plt.tight_layout() plt.plot(root_mean_squared_errors) # 输出带有校准数据的表 calibration_data = pd.DataFrame() calibration_data["predictions"] = pd.Series(predictions) calibration_data["targets"] = pd.Series(targets) display.display(calibration_data.describe()) print("Final RMSE (on training data): %0.2f" % root_mean_squared_error) return calibration_data 设置参数初步训练下一试试 12345calibration_data = train_model( learning_rate=0.00001, steps=100, batch_size=1) Training model... RMSE (on training data): period 00 : 236.40 period 01 : 235.26 period 02 : 234.10 period 03 : 232.96 period 04 : 231.87 period 05 : 230.78 period 06 : 229.62 period 07 : 228.54 period 08 : 227.36 period 09 : 226.38 Model training finished. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } predictions targets count 17000.0 17000.0 mean 13.2 207.3 std 10.9 116.0 min 0.0 15.0 25% 7.3 119.4 50% 10.6 180.4 75% 15.8 265.0 max 189.7 500.0 Final RMSE (on training data): 226.38 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } predictions targets 0 9.7 66.9 1 10.6 80.1 2 7.0 85.7 3 13.6 73.4 4 14.0 65.5 ... ... ... 16995 14.1 111.4 16996 7.5 79.0 16997 20.9 103.6 16998 12.6 85.8 16999 10.9 94.6 17000 rows × 2 columns 差距还是蛮大的，改变参数试试 12345calibration_data = train_model( learning_rate=0.00002, steps=500, batch_size=5) Training model... RMSE (on training data): period 00 : 226.36 period 01 : 216.00 period 02 : 206.55 period 03 : 198.01 period 04 : 191.20 period 05 : 185.74 period 06 : 182.91 period 07 : 179.02 period 08 : 176.59 period 09 : 175.67 Model training finished. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } predictions targets count 17000.0 17000.0 mean 115.8 207.3 std 95.5 116.0 min 0.1 15.0 25% 64.0 119.4 50% 93.2 180.4 75% 138.0 265.0 max 1661.6 500.0 Final RMSE (on training data): 175.67 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } predictions targets 0 110.0 66.9 1 51.7 80.1 2 144.4 85.7 3 79.7 73.4 4 77.4 65.5 ... ... ... 16995 47.5 111.4 16996 138.7 79.0 16997 248.7 103.6 16998 117.8 85.8 16999 46.4 94.6 17000 rows × 2 columns 有适用于模型调整的标准启发法吗？降低RMSE，这是一个常见的问题。简短的答案是，不同超参数的效果取决于数据。因此，不存在必须遵循的规则，您需要对自己的数据进行测试。 即便如此，我们仍在下面列出了几条可为您提供指导的经验法则： 训练误差应该稳步减小，刚开始是急剧减小，最终应随着训练收敛达到平稳状态。 如果训练尚未收敛，尝试运行更长的时间。 如果训练误差减小速度过慢，则提高学习速率也许有助于加快其减小速度。 但有时如果学习速率过高，训练误差的减小速度反而会变慢。 如果训练误差变化很大，尝试降低学习速率。 较低的学习速率和较大的步数/较大的批量大小通常是不错的组合。 批量大小过小也会导致不稳定情况。不妨先尝试 100 或 1000 等较大的值，然后逐渐减小值的大小，直到出现性能降低的情况。 重申一下，切勿严格遵循这些经验法则，因为效果取决于数据。请始终进行试验和验证。 尝试其他特征使用 population 特征替换 total_rooms 特征，看看能否取得更好的效果。 123456calibration_data = train_model( learning_rate=0.00002, steps=1000, batch_size=5, input_feature="population") Training model... RMSE (on training data): period 00 : 225.40 period 01 : 214.37 period 02 : 204.42 period 03 : 195.76 period 04 : 188.29 period 05 : 182.98 period 06 : 178.93 period 07 : 175.94 period 08 : 174.97 period 09 : 175.14 Model training finished. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } predictions targets count 17000.0 17000.0 mean 120.1 207.3 std 96.4 116.0 min 0.3 15.0 25% 66.4 119.4 50% 98.0 180.4 75% 144.6 265.0 max 2997.3 500.0 Final RMSE (on training data): 175.14 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } predictions targets 0 116.6 66.9 1 829.3 80.1 2 97.7 85.7 3 136.3 73.4 4 41.7 65.5 ... ... ... 16995 56.4 111.4 16996 12.3 79.0 16997 83.9 103.6 16998 84.3 85.8 16999 70.1 94.6 17000 rows × 2 columns 合成特征和离群值尝试合成特征total_rooms 和 population 特征都会统计指定街区的相关总计数据。 但是，如果一个街区比另一个街区的人口更密集，会怎么样？我们可以创建一个合成特征（即 total_rooms 与 population 的比例）来探索街区人口密度与房屋价值中位数之间的关系。 在以下单元格中，创建一个名为 rooms_per_person 的特征，并将其用作 train_model() 的 input_feature。 通过调整学习速率，您使用这一特征可以获得的最佳效果是什么？（效果越好，回归线与数据的拟合度就越高，最终 RMSE 也会越低。） 12345678california_housing_dataframe["rooms_per_person"] = ( california_housing_dataframe["total_rooms"] / california_housing_dataframe["population"])calibration_data = train_model( learning_rate=0.05, steps=500, batch_size=5, input_feature="rooms_per_person") Training model... RMSE (on training data): period 00 : 214.75 period 01 : 193.24 period 02 : 176.45 period 03 : 160.89 period 04 : 150.13 period 05 : 146.30 period 06 : 145.44 period 07 : 146.13 period 08 : 147.65 period 09 : 149.22 Model training finished. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } predictions targets count 17000.0 17000.0 mean 200.9 207.3 std 93.2 116.0 min 44.5 15.0 25% 164.4 119.4 50% 197.8 180.4 75% 226.2 265.0 max 4443.7 500.0 Final RMSE (on training data): 149.22 识别离群值通过创建预测值与目标值的散点图来可视化模型效果。理想情况下，这些值将位于一条完全相关的对角线上。 使用您在任务 1 中训练过的人均房间数模型，并使用 Pyplot 的 scatter() 创建预测值与目标值的散点图。 您是否看到任何异常情况？通过查看 rooms_per_person 中值的分布情况，将这些异常情况追溯到源数据。 12345plt.figure(figsize=(15, 6))plt.subplot(1, 2, 1)plt.scatter(calibration_data["predictions"], calibration_data["targets"])plt.subplot(1, 2, 2)_ = california_housing_dataframe["rooms_per_person"].hist() 校准数据显示，大多数散点与一条线对齐。这条线几乎是垂直的，我们稍后再讲解。现在，我们重点关注偏离这条线的点。我们注意到这些点的数量相对较少。 观察我们绘制 rooms_per_person 的直方图，则会发现我们的输入数据中有少量离群值 截取离群值将 rooms_per_person 的离群值设置为相对合理的最小值或最大值来进一步改进模型拟合情况。 以下是一个如何将函数应用于 Pandas Series 的简单示例，供您参考： clipped_feature = my_dataframe[&quot;my_feature_name&quot;].apply(lambda x: max(x, 0)) 上述 clipped_feature 没有小于 0 的值。 观察直方图发现大多数数值都小于5，我们将rooms_per_person 的值截取为5，然后绘制直方图再次检查结果。 1234california_housing_dataframe["rooms_per_person"] = ( california_housing_dataframe["rooms_per_person"]).apply(lambda x: min(x, 5))_ = california_housing_dataframe["rooms_per_person"].hist() 验证截取是否有效，我们再训练一次模型，并再次输出校准数据： 12345calibration_data = train_model( learning_rate=0.05, steps=500, batch_size=5, input_feature="rooms_per_person") Training model... RMSE (on training data): period 00 : 214.33 period 01 : 192.43 period 02 : 172.30 period 03 : 155.07 period 04 : 142.01 period 05 : 134.29 period 06 : 129.31 period 07 : 128.99 period 08 : 127.60 period 09 : 126.94 Model training finished. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } predictions targets count 17000.0 17000.0 mean 189.5 207.3 std 49.3 116.0 min 45.1 15.0 25% 158.1 119.4 50% 189.6 180.4 75% 216.4 265.0 max 419.5 500.0 Final RMSE (on training data): 126.94 1_ = plt.scatter(calibration_data["predictions"], calibration_data["targets"])]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 2]]></title>
    <url>%2F2018%2F12%2F28%2FTensorFlow_2%2F</url>
    <content type="text"><![CDATA[创建和控制张量矢量加法可以对张量执行金典的数学运算，试着创建一些矢量。 12345678910111213141516171819202122232425from __future__ import print_functionimport tensorflow as tftry: tf.contrib.eager.enable_eager_execution() print("TF imported with eager execution!")except ValueError: print("TF already imported with eager execution!")# 一个包含质数的‘primes’矢量primes = tf.constant([2, 3, 5, 7, 11 ,13], dtype=tf.int32)print("primes:", primes)# 一个值全为 1 的 ones 矢量ones = tf.ones([6], dtype=tf.int32)print(ones)# 一个通过对前两个矢量执行元素级加法而创建的矢量。just_beyond_primes = tf.add(primes, ones)print(just_beyond_primes)# 把primes中的元素乘二twos = tf.constant([2, 2, 2, 2, 2, 2], dtype=tf.int32)primes_doubled = primes * twosprint(primes_doubled) TF imported with eager execution! primes: tf.Tensor([ 2 3 5 7 11 13], shape=(6,), dtype=int32) tf.Tensor([1 1 1 1 1 1], shape=(6,), dtype=int32) tf.Tensor([ 3 4 6 8 12 14], shape=(6,), dtype=int32) tf.Tensor([ 4 6 10 14 22 26], shape=(6,), dtype=int32) 输出的张量不仅会返回值，还会返回形状shape，以及储存在张量中的值的类型。调用numpy方法会以NumPy数组的形式返回。 123some_matrix = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.int32)print(some_matrix)print("\nnumpy matrix:\n", some_matrix.numpy()) tf.Tensor( [[1 2 3] [4 5 6]], shape=(2, 3), dtype=int32) numpy matrix: [[1 2 3] [4 5 6]] 张量形状shape 是用来描述张量维度大小和数量。张量的形状表示为list，其中第 i 个元素表示维度 i 的大小。列表的长度表示张量的阶（即维数）。 如果是二维的则shape=(行数， 列数) 例如shape=(n1, n2, n3, …, x, y)则说明 一共有 (n1 x n2 x n3 x ….)个x行y列的数组构成。 例： 123456789101112131415# 一个标量scalar = tf.zeros([])# 一个有三个元素的向量vector = tf.zeros([3])# 一个两行三列的矩阵matrix = tf.zeros([2, 3])matrix2 = tf.zeros([2, 3, 4, 5])print('scalar has shape', scalar.get_shape(), 'and value:\n', scalar.numpy())print('vector has shape', vector.get_shape(), 'and value:\n', vector.numpy())print('matrix has shape', matrix.get_shape(), 'and value:\n', matrix.numpy())print('matrix2 has shape', matrix2.get_shape(), 'and value:\n', matrix2.numpy()) scalar has shape () and value: 0.0 vector has shape (3,) and value: [0. 0. 0.] matrix has shape (2, 3) and value: [[0. 0. 0.] [0. 0. 0.]] matrix2 has shape (2, 3, 4, 5) and value: [[[[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]]] [[[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]]]] 广播在数学中，您只能对形状相同的张量执行元素级运算（例如，相加和等于）。不过，在 TensorFlow 中，您可以对张量执行传统意义上不可行的运算。TensorFlow 支持广播（一种借鉴自 NumPy 的概念）。利用广播，元素级运算中的较小数组会增大到与较大数组具有相同的形状。例如，通过广播： 如果运算需要大小为 [6] 的张量，则大小为 [1] 或 [] 的张量可以作为运算数。 如果运算需要大小为 [4, 6] 的张量，则以下任何大小的张量都可以作为运算数： [1, 6] [6] [] 如果运算需要大小为 [3, 5, 6] 的张量，则以下任何大小的张量都可以作为运算数： [1, 5, 6] [3, 1, 6] [3, 5, 1] [1, 1, 1] [5, 6] [1, 6] [6] [1] [] 注意：当张量被广播时，从概念上来说，系统会复制其条目（出于性能考虑，实际并不复制。广播专为实现性能优化而设计）。 有关完整的广播规则集，请参阅简单易懂的 NumPy 广播文档。 以下代码执行了与之前一样的张量运算，不过使用的是标量值（而不是全包含 1 或全包含 2 的矢量）和广播。 123456789101112primes = tf.constant([2, 3, 5, 7, 11, 13], dtype=tf.int32)print("primes:", primes)one = tf.constant(1, dtype=tf.int32)print("one:", one)just_beyond_primes = tf.add(primes, one)print("just_beyond_primes:", just_beyond_primes)two = tf.constant(2, dtype=tf.int32)primes_doubled = primes * twoprint(primes_doubled) primes: tf.Tensor([ 2 3 5 7 11 13], shape=(6,), dtype=int32) one: tf.Tensor(1, shape=(), dtype=int32) just_beyond_primes: tf.Tensor([ 3 4 6 8 12 14], shape=(6,), dtype=int32) tf.Tensor([ 4 6 10 14 22 26], shape=(6,), dtype=int32) 练习 1：矢量运算。执行矢量运算以创建一个“just_under_primes_squared”矢量，其中第 i 个元素等于 primes 中第 i 个元素的平方减 1。例如，第二个元素为 3 * 3 - 1 = 8。 使用 tf.multiply 或 tf.pow 操作可求得 primes 矢量中每个元素值的平方。 123456789def solution(primes): primes_squared = tf.pow(primes, 2) # or tf.multiply(primes, primes) one = tf.constant(1, dtype=tf.int32) just_under_primes_squared = tf.subtract(primes_squared, one) return just_under_primes_squaredprimes = tf.constant([2, 3, 5, 7, 11, 13], dtype=tf.int32)just_under_primes_squared = solution(primes)print(just_under_primes_squared) tf.Tensor([ 3 8 24 48 120 168], shape=(6,), dtype=int32) 矩阵乘法在线性代数中，当两个矩阵相乘时，第一个矩阵的列数必须等于第二个矩阵的行数。 3x4 矩阵乘以 4x2 矩阵是 _有效_ 的，可以得出一个 3x2 矩阵。 4x2 矩阵乘以 3x4 矩阵是 _无效_ 的。 1234567891011# 一个3x4的矩阵x = tf.constant([[5, 2, 4, 3], [5, 1, 6, -2], [-1, 3, -1, -2]], dtype=tf.int32)# 一个4x2的矩阵y = tf.constant([[2, 2], [3, 5], [4, 5], [1, 6]], dtype=tf.int32)# 结果是一个3x2的矩阵matrix_multiply_result = tf.matmul(x, y)print(matrix_multiply_result) tf.Tensor( [[35 58] [35 33] [ 1 -4]], shape=(3, 2), dtype=int32) 张量变形由于张量加法和矩阵乘法均对运算数施加了限制条件，TensorFlow 编程者需要频繁改变张量的形状。 您可以使用 tf.reshape 方法改变张量的形状。例如，您可以将 8x2 张量变形为 2x8 张量或 4x4 张量(改变形状形成的新矩阵元素数和之前必须一样)： 此外，您还可以使用 tf.reshape 更改张量的维数（“阶”）。例如，您可以将 8x2 张量变形为三维 2x2x4 张量或一维 16 元素张量。 123456789101112131415161718192021# 创建一个8x2的矩阵matrix = tf.constant( [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16]], dtype=tf.int32)reshaped_2x8_matrix = tf.reshape(matrix, [2, 8])reshaped_4x4_matrix = tf.reshape(matrix, [4, 4])print("Original matrix (8x2):")print(matrix.numpy())print("Reshaped matrix (2x8):")print(reshaped_2x8_matrix.numpy())print("Reshaped matrix (4x4):")print(reshaped_4x4_matrix.numpy())reshaped_2x2x4_tensor = tf.reshape(matrix, [2, 2, 4])one_dimensional_vector = tf.reshape(matrix, [16])print("Reshaped 3-D tensor (2x2x4):")print(reshaped_2x2x4_tensor.numpy())print("1-D vector:")print(one_dimensional_vector.numpy()) Original matrix (8x2): [[ 1 2] [ 3 4] [ 5 6] [ 7 8] [ 9 10] [11 12] [13 14] [15 16]] Reshaped matrix (2x8): [[ 1 2 3 4 5 6 7 8] [ 9 10 11 12 13 14 15 16]] Reshaped matrix (4x4): [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12] [13 14 15 16]] Reshaped 3-D tensor (2x2x4): [[[ 1 2 3 4] [ 5 6 7 8]] [[ 9 10 11 12] [13 14 15 16]]] 1-D vector: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16] 练习 2：改变两个张量的形状，使其能够相乘。下面两个矢量无法进行矩阵乘法运算： a = tf.constant([5, 3, 2, 7, 1, 4]) b = tf.constant([4, 6, 3]) 请改变这两个矢量的形状，使其成为可以进行矩阵乘法运算的运算数。然后，对变形后的张量调用矩阵乘法运算。 12345678910111213a = tf.constant([5, 3, 2, 7, 1, 4])b = tf.constant([4, 6, 3])reshaped_a = tf.reshape(a, [2, 3])reshaped_b = tf.reshape(b, [3, 1])c = tf.matmul(reshaped_a, reshaped_b)print("reshaped_a (2x3):")print(reshaped_a.numpy())print("reshaped_b (3x1):")print(reshaped_b.numpy())print("reshaped_a x reshaped_b (2x1):")print(c.numpy()) reshaped_a (2x3): [[5 3 2] [7 1 4]] reshaped_b (3x1): [[4] [6] [3]] reshaped_a x reshaped_b (2x1): [[44] [46]] 变量、初始化和赋值到目前为止，我们执行的所有运算都针对的是静态值 (tf.constant)；调用 numpy() 始终返回同一结果。在 TensorFlow 中可以定义 Variable 对象，它的值是可以更改的。 创建变量时，您可以明确设置一个初始值，也可以使用初始化程序（例如分布）： 123456789# 创建初始值为3的标量变量v = tf.contrib.eager.Variable([3])# 创建一个形状为[1,4]的矢量变量，其初始值为随机的# 从均值为1，标准差为0.35的正态分布中取样w = tf.contrib.eager.Variable(tf.random_normal([1, 4], mean=1.0, stddev=0.35))print("v:", v.numpy())print("w:", w.numpy()) v: [3] w: [[0.7752843 1.516361 1.1726708 0.9872638]] 要更改变量的值，请使用 assign 操作，并且向变量赋予新值时，其形状必须和之前的形状一致。 1234567891011121314151617v = tf.contrib.eager.Variable([3])print(v.numpy())tf.assign(v, [7])print(v.numpy())v.assign([5])print(v.numpy())v = tf.contrib.eager.Variable([[1, 2, 3], [4, 5, 6]])print(v.numpy())try: print("Assigning [7, 8, 9] to v") v.assign([7, 8, 9])except ValueError as e: print("Exception:", e) [3] [7] [5] [[1 2 3] [4 5 6]] Assigning [7, 8, 9] to v Exception: Shapes (2, 3) and (3,) are incompatible 练习 3：模拟投掷两个骰子 10 次。创建一个骰子模拟，在模拟中生成一个 10x3 二维张量，其中： 列 1 和 2 均存储一个六面骰子（值为 1-6）的一次投掷值。 列 3 存储同一行中列 1 和 2 的值的总和。 例如，第一行中可能会包含以下值： 列 1 存储 4 列 2 存储 3 列 3 存储 7 要完成此任务，您需要浏览 TensorFlow 文档。 123456789die1 = tf.contrib.eager.Variable( tf.random_uniform([10, 1], minval=1, maxval=7, dtype=tf.int32))die2 = tf.contrib.eager.Variable( tf.random_uniform([10, 1], minval=1, maxval=7, dtype=tf.int32))dice_sum = tf.add(die1, die2)resulting_matrix = tf.concat(values=[die1, die2, dice_sum], axis=1)print(resulting_matrix) tf.Tensor( [[ 2 3 5] [ 5 6 11] [ 3 3 6] [ 5 6 11] [ 2 1 3] [ 6 5 11] [ 1 4 5] [ 1 1 2] [ 4 6 10] [ 5 4 9]], shape=(10, 3), dtype=int32)]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 1]]></title>
    <url>%2F2018%2F12%2F26%2FTensorFlow_1%2F</url>
    <content type="text"><![CDATA[Hello World当tensorflow的环境搭建好后我们就可以尝试run下了，先写个hello world看看吧。 12345678910from __future__ import print_functionimport tensorflow as tftry: tf.contrib.eager.enable_eager_execution()except ValueError: passtensor = tf.constant('Hello, world!')tensor_value = tensor.numpy()print(tensor_value) b&apos;Hello, world!&apos; TensorFlow 编程概念TensorFlow的名称源自张量，张量是任意维度的数组。借助TensorFlow，可以操作具有很大维度的张量。 标量是零维数组（零阶张量）。例如：&apos;hi&apos; 或 3 矢量是一维数组（一阶张量）。例如：[2, 3, 5, 7, 11] 或 [3] 矩阵是二维数组（二阶张量）。例如：[[3.1, 8.2, 5.9][4.3, -2.7, 6.5]] TensorFlow指令会创建，销毁和控制张量。典型TensorFlow程序中的大多数代码都是指令。TensorFlow图（也叫 计算图 或 数据流图）是一种图数据结构。很多TensorFlow程序由单个图构成，但是TensorFlow程序可以选择创建多个图。图的节点是指令；图的边是张量。张量流经图，在每个节点由一个指令操控。一个指令的输出张量通常会变成后续指令的输入张量。TensorFlow会实现延迟执行模型，意味着系统仅会根据相关节点的需求在需要时计算节点。 张量可以作为常量或变量储存在图中。常量储存的是值是不会发生更改的张量，而变量储存的值是会发生更改的张量。常量和变量都只是图中的一种指令。常量是始终会返回同一张量值得指令。变量是会返回分配给得任何张量的指令。 要定义常量，使用tf.constant指令，并传入它的值。例如： x = tf.constant([1.2]) 同样，可以创建变量： y = tf.Variable([3]) 改变值： y = y.assign([1]) 创建好变量或常量后，可以对它们使用其他指令（如tf.add）。 图必须在TensorFlow会话中运行，会话储存了它所运行的图的状态： 将 tf.Session()作为会话： initialization = tf.global_variables_initializer() print(y.eval()) 在使用tf.Variable时可以调用tf.global_variables_initializer，以明确初始化这些变量。 注意：会话可以将图分发到多个机器上执行（假设程序在某个分布式计算框架上运行）。 总结TensorFlow编程有两个流程： 1.将常量，变量和指令整合到一个图中。 2.在一个会话中评估这些常量，变量和指令。 创建一个简单的 TensorFlow 程序我们来看看如何编写一个将两个常量相加的简单 TensorFlow 程序。 添加 import 语句想要运行tensorflow程序，必须添加这句： 1import tensorflow as tf 其他常见的import语句包括： import matplotlib.pyplot as plt # 数据可视化 import numpy as np # 较低级的数学python库 import pandas as pd # 较高级的数学python库 123456789101112131415from __future__ import print_functionimport tensorflow as tf# 创建一个图g = tf.Graph()with g.as_default(): # 创建三个量， x = tf.constant(8, name="x_const") y = tf.constant(5, name="y_const") sum = tf.add(x, y, name="x_y_sum") python # 创建一个会话，将会执行默认图 with tf.Session() as sess: print(sum.eval()) TF already imported with eager execution! 13]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow学习前的准备工作]]></title>
    <url>%2F2018%2F12%2F24%2Ftensorflow%E5%AD%A6%E4%B9%A0%E5%89%8D%E7%9A%84%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[工欲善其事必先利其器，在学习tensor flow之前需要先学会使用一些工具，首先是jupyter，这之后关于tensor flow的blog也都会写成jupyter格式的。 JupyterJupyter Notebook 的本质是一个 Web 应用程序，便于创建和共享文学化程序文档，支持实时代码，数学方程，可视化和 markdown。 用途包括：数据清理和转换，数值模拟，统计建模，机器学习等等。 官网：Jupyter Installing Jupyter with pip12345678910111213As an existing or experienced Python user, you may wish to install Jupyter using Python’s package manager, pip, instead of Anaconda.If you have Python 3 installed (which is recommended):python3 -m pip install --upgrade pippython3 -m pip install jupyterIf you have Python 2 installed:python -m pip install --upgrade pippython -m pip install jupyterCongratulations, you have installed Jupyter Notebook! To run the notebook, run the following command at the Terminal (Mac/Linux) or Command Prompt (Windows):run: jupyter notebook 安装成功后执行 jupyter notebook 后会打开一个web，通过网页就可以执行python程序。 还可以把ipynb文件转换为html，md，pdf等格式 12ipython nbconvert --to markdown filename.ipynbipython nbconvert --to html filename.ipynb ipynb转换为html、md、pdf等格式，还有另一种更简单的方法：在jupyter notebook中，选择File-&gt;Download as，直接选择需要转换的格式就可以了。需要注意的是，转换为pdf格式之前，同样要保证已经安装了xelatex。 基础知识基础数学123456789101112131415161718代数变量、系数和函数线性方程式，例如 对数和对数方程式，例如 S 型函数线性代数张量和张量等级矩阵乘法三角学Tanh（作为激活函数进行讲解，无需提前掌握相关知识）统计信息均值、中间值、离群值和标准偏差能够读懂直方图微积分（可选，适合高级主题）导数概念（您不必真正计算导数）梯度或斜率偏导数（与梯度紧密相关）链式法则（带您全面了解用于训练神经网络的反向传播算法） 基础 Python123456789101112131415Python 教程中介绍了以下 Python 基础知识：定义和调用函数：使用位置和关键字参数字典、列表、集合（创建、访问和迭代）for 循环：包含多个迭代器变量的 for 循环（例如 for a, b in [(1,2), (3,4)]）if/else 条件块和条件表达式字符串格式（例如 '%.2f' % 3.14）变量、赋值、基本数据类型（int、float、bool、str）pass 语句 Python 库1234567891011121314151617机器学习速成课程代码示例使用了第三方库提供的以下功能。无需提前熟悉这些库；您可以在需要时查询相关内容。Matplotlib（适合数据可视化）pyplot 模块cm 模块gridspec 模块Seaborn（适合热图）heatmap 函数Pandas（适合数据处理）DataFrame 类NumPy（适合低阶数学运算）linspace 函数random 函数array 函数arange 函数scikit-learn（适合评估指标）metrics 模块 Bash shell知道命令行，会敲命令。 Pandas最后要知道Pandas库中DataFrame的数据结构，看另一篇博客，Pandas简介。 以上都不会也没有关系，毕竟tensor flow是给学龄前儿童玩耍的。 TensorFlow官网这里提供了非常详细的中文教程 我没有搭建实体环境，是通过Colaboratory来实验代码。 ColaboratoryColaboratory是Google的一个研究项目，旨在提供开发者一个云端训练神经网络的工具。它是Jupyter一个笔记本环境，不用做任何配置，完全运行在云端。Colaboratory存储在Google Drive中，可以进行共享。Colaboratory向开发者提供了免费的Tesla K80 GPU使用。 实用的键盘快捷键 ⌘/Ctrl+m,b：在当前选择的单元格下方创建一个空白代码单元格 ⌘/Ctrl+m,i：中断单元格的运行 ⌘/Ctrl+m,h：显示所有键盘快捷键列表 要查看关于任何 TensorFlow API 方法的文档，请将光标放置在其左括号的正后方，然后按 Tab 键：]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas 简介]]></title>
    <url>%2F2018%2F12%2F23%2FPandas%20%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[这个是学习tensorflow前的准备， pandas 是一种列存数据分析 API。它是用于处理和分析输入数据的强大工具，很多机器学习框架都支持将 pandas 数据结构作为输入。 虽然全方位介绍 pandas API 会占据很长篇幅，但它的核心概念非常简单，我们会在下文中进行说明。有关更完整的参考，请访问 pandas 文档网站，其中包含丰富的文档和教程资源。 基本概念导入pandas 并输出版本123from __future__ import print_functionimport pandas as pdprint(pd.__version__) 0.23.4 pandas中的主要数据结构被时限为一下两类： DataFrame： 一个关系型数据表格，其中包含多行和已命名的列，就像excel一样 Series：它是单独的一列，DataFrame中包含一个或多个Series，每个Series都有一个名称。就像我们写个表格在第一列写上每一行代表什么一样。 数据框架是用于数据操控的一种常用抽象实现形式，spark中的rdd，数据库中的table 类似。 创建Series的一种方法是构建Series对象。列入：1pd.Series(['Beijing', 'Shanghai', 'Shenzhen']) 你可以将映射string列名称的dict传递到它们各自的Series，从而创建DataFrame对象。如果Series在长度上不一致，系统会用特殊的NA值填充缺失的值。 12345city_names = pd.Series(['Beijing', 'Shanghai', 'Shenzhen'])population = pd.Series([21534678, 23541023, 120456])data = pd.DataFrame(&#123;'City name': city_names, 'Population': population&#125;)print(data) 1234 City name Population0 Beijing 215346781 Shanghai 235410232 Shenzhen 120456 大多数情况下，我们需要把整个文件加载到DataFrame中，下面我们加载一个包含加利福尼亚州住房的数据文件。并创建特征定义，通过head方法浏览DataFrame前几个纪录 123california_housing_dataframe = pd.read_csv("https://download.mlcc.google.cn/mledu-datasets/california_housing_train.csv", sep=",")california_housing_dataframe.describe()print(california_housing_dataframe.head()) 12345678 longitude latitude ... median_income median_house_value0 -114.31 34.19 ... 1.4936 66900.01 -114.47 34.40 ... 1.8200 80100.02 -114.56 33.69 ... 1.6509 85700.03 -114.57 33.64 ... 3.1917 73400.04 -114.57 33.57 ... 1.9250 65500.0[5 rows x 9 columns] pandas的另一个强大的功能是绘图制表，借助DataFrame.hist，可以快速了解一个列中值的分布。pandas使用的画图库是matplotlib所以我们也可以使用这个库中的方法来操作图表。 123import matplotlib.pyplot as plthist = california_housing_dataframe.hist('housing_median_age')plt.show() 访问数据可以使用 dict 或list 的方法来访问DataFrame数据123456789cities = pd.DataFrame(&#123;'City name': city_names, 'Population': population&#125;)print(type(cities['City name']))print(cities['City name'])print(type(cities['City name'][1]))print(cities['City name'][1])print(type(cities[0:2]))print(cities[0:2]) 12345678910111213&lt;class 'pandas.core.series.Series'&gt;0 Beijing1 Shanghai2 ShenzhenName: City name, dtype: object&lt;class 'str'&gt;Shanghai&lt;class 'pandas.core.frame.DataFrame'&gt; City name Population0 Beijing 215346781 Shanghai 23541023 操控数据可以向series应用Python的基本用算指令。1234population / 10000 21534.6781 23541.0232 120.456 NumPy是一个用于科学计算的常用工具包。pandas series可作用大多数NumPy函数的参数。1234567import numpy as npnp.log(population)0 13.6558921 13.8311722 13.092314dtype: float64 对于更加复杂的单列转换，可以使用Series.apply。像Python映射函数一样，Series.apply将以参数形式接受lambda函数，而该函数会应用与每个值，下面的例子是创建一个population是否超过一定数值的series。 123456print(population.apply(lambda val: val &gt; 1000000))0 True1 True2 Falsedtype: bool DataFrames的修改方式也非常简单。例如，一下代码向现有的DataFrame添加了两个Series。12345678cities['Area square miles'] = pd.Series([98.87, 176.53, 46.92]) # 随便写的数cities['Population density'] = cities['Population'] / cities['Area square miles']print(cities) City name ... Population density0 Beijing ... 217808.0105191 Shanghai ... 133354.2344082 Shenzhen ... 2567.263427 练习1通过添加一个新的布尔值列，修改cities表格 城市以sh开头 城市面积大于50 （上面数都是我随便写的） 注意：布尔值 Series 1 辑与时，应使用 &amp;，而不是 and。12345678910cities['Is wide and has Sh name'] = (cities['Area square miles'] &gt; 50) &amp; cities['City name'].apply(lambda name: name.startswith('Sh'))print(cities) City name ... Is wide and has Sh name0 Beijing ... False1 Shanghai ... True2 Shenzhen ... False[3 rows x 5 columns] 索引Series和DataFrame对象也定义了index属性，改属性向每个Series项或DataFrame行赋一个标识符值。默认情况下，在构造时，pandas会赋可反应数据源数据顺序的索引值。索引值在创建后时稳定的；也就是说，他们不会因为数据重新排序而发生改变。 123456789101112131415161718192021print(city_names.index)RangeIndex(start=0, stop=3, step=1)print(cities.index)RangeIndex(start=0, stop=3, step=1)print(cities.reindex([2, 0, 1])) City name ... Is wide and has Sh name2 Shenzhen ... False0 Beijing ... False1 Shanghai ... True[3 rows x 5 columns]print(cities.reindex(np.random.permutation(cities.index))) City name ... Is wide and has Sh name0 Beijing ... False2 Shanghai ... True1 Shenzhen ... False[3 rows x 5 columns] 练习2reindex方法允许使用未包含在原始DataFrame索引值中的索引值。请示一下，看看如果使用此类值会发生什么。 如果reindex输入数组包含原始DataFrame索引值中没有的值，reindex会为此类“丢失的”索引添加新行，并在所有对应列中填充NaN值 12345678cities.reindex([0,4,5,2]) City name ... Is wide and has Sh name0 Beijing ... False4 NaN ... NaN5 NaN ... NaN2 Shenzhen ... False[4 rows x 5 columns] 这种行为是可取的，因为索引通常是从实际数据中提取的字符串，在这种情况下，如果容许出现“丢失的”索引，将可以轻松的使用外部列表重建索引，因为我们不必担心将输入清理掉。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ex8-anomaly detection and recommendation]]></title>
    <url>%2F2018%2F11%2F20%2Fex8-anomaly%20detection%20and%20recommendation%2F</url>
    <content type="text"><![CDATA[AndrewNg 机器学习习题ex6-anomaly detection and recommendation 这是最后一个练习了，共有两个算法，第一个是异常检测，第二个是推荐系统。 异常检测之前写过了这里就不再重复了：Python实现异常检测算法 推荐系统推荐系统使用的算法就是协同过滤（collaborative ltering learning algorithm） 首先来看提供的数据都有些什么，更具PDF可知，有5个文件是我们需要的数据集合。 数据集名称 内容 movie_ids.txt 电影的列表 ex8data1.mat 用于异常检测的第一个示例数据集 ex8data2.mat 用于异常检测的第二个示例数据集 ex8_movies.mat 电影评论数据集 ex8_movieParams.mat 为调试提供的参数 导入库和检查数据集ex8_movies.mat中有两个标签的数据，Y是1682个电影的评分，每个电影有943条五个级别的评分，R是一个和Y相同维度的二进制数组，0代表评过分，1代表没评分。 % Notes: X - num_movies (1682) x num_features (10) matrix of movie features% Theta - num_users (943) x num_features (10) matrix of user features% Y - num_movies x num_users matrix of user ratings of movies% R - num_movies x num_users matrix, where R(i, j) = 1 if the% i-th movie was rated by the j-th user 1234567891011121314151617181920212223242526#!/usr/bin/python# coding=utf-8import scipy.io as sioimport matplotlib.pyplot as pltimport numpy as npimport seaborn as snsimport pandas as pdsns.set(context="notebook", style="white")# Y是包含从1到5的等级的（数量的电影x数量的用户）数组.R是包含指示用户是否给电影评分的二进制值的“指示符”数组。movies_mat = sio.loadmat('./data/ex8_movies.mat');Y, R = movies_mat.get('Y'), movies_mat.get('R')print(Y.shape, R.shape)# (1682, 943) (1682, 943)m, u = Y.shape# m: how many movies# u: how many usersn = 10# how many features for a movieparam_mat = sio.loadmat('./data/ex8_movieParams.mat')theta, X = param_mat.get('Theta'), param_mat.get('X')print(theta.shape, X.shape)# (943, 10) (1682, 10) cost function 在对feature运算时，我们先把params serialize为只有一个维度的数组，通过deserialize函数来恢复为原状。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def serialize(X, theta): # serialize 2 matrix # X(move, feature), (1682, 10): movie features # theta (user, feature), (943, 10): user preference # 1682*10 + 943*10 = (26250,) return np.concatenate((X.ravel(), theta.ravel()))def deserialize(param, n_movie, n_user, n_featuers): # into ndarray of X(1682, 10), theta(943, 10) return param[:n_movie * n_featuers].reshape(n_movie, n_featuers),\ param[n_movie * n_featuers:].reshape(n_user, n_featuers)# recomendation fndef cost(param, Y, R, n_features): """compute cost for every r(i, j) = 1 arg: param: serialized X, theta Y (movie, user), (1682, 943): (movie, user) rating R (movie, user), (1682, 943): (movie, user) has rating """ # theta (user, feat) # X(movie, feature), (1682, 10): movie features n_movie, n_user = Y.shape X, theta = deserialize(param, n_movie, n_user, n_features) inner = np.multiply(X @ theta.T - Y, R) return np.power(inner, 2).sum() / 2def gradient(param, Y, R, n_features): # theta (user, feature), (943, 10): user preference # X(movie, feature), (1682, 10): movie features n_movies, n_user = Y.shape X, theta = deserialize(param, n_movies, n_user, n_features) inner = np.multiply(X @ theta.T - Y, R) # (1682, 943) # X_grad (1682, 10) X_grad = inner @ theta # theta_grad (943, 10) theta_grad = inner.T @ X # roll them together and return return serialize(X_grad, theta_grad)def regularized_cost(param, Y, R, n_features, l=1): reg_term = np.power(param, 2).sum() * (1/2) return cost(param, Y, R, n_features) + reg_termdef regularized_gradient(param, Y, R, n_features, l=1): grad = gradient(param, Y, R, n_features) reg_term = l * param return grad + reg_term 按照练习8中的参数cost输出为22，验证结果“ 12345678910111213# 按照练习中给出计算结果为22users = 4movies = 5features = 3X_sub = X[:movies, :features]theta_sub = theta[:users, :features]Y_sub = Y[:movies, :users]R_sub = R[:movies, :users]param_sub = serialize(X_sub, theta_sub)c = cost(param_sub, Y_sub, R_sub, features)print(c) # 22.224603725685675 计算一下总的cost 12345# total readl paramsparam = serialize(X, theta)# total costtotal_cost = cost(param, Y, R, 10)print(total_cost) # 27918.64012454421 gradient function 123456n_movie, n_user = Y.shapeX_grad, theta_grad = deserialize(gradient(param, Y, R, 10), n_movie, n_user, 10)assert X_grad.shape == X.shapeassert theta_grad.shape == theta.shape regularized cost and gradient 123456789101112131415# regularized cost# in the ex8_confi.m, lambda = 1.5, and it's using sub data setreg_cost = regularized_cost(param_sub, Y_sub, R_sub, features, l=1.5)print(reg_cost) # 28.304238738078038# total regularized costtotal_cost = regularized_cost(param, Y, R, 10, l=1)print(total_cost) # 32520.682450229557n_movie, n_user = Y.shapeX_grad, theta_grad = deserialize(regularized_gradient(param, Y, R, 10), n_movie, n_user, 10)assert X_grad.shape == X.shapeassert theta_grad.shape == theta.shape parse movie_id.txt12345678# parse movie_id.txtmovie_list = []with open('./data/movie_ids.txt', encoding='latin-1') as f: for line in f: tokens = line.strip().split(' ') movie_list.append(' '.join(tokens[1:]))movie_list = np.array(movie_list) 给电影打分12345678910111213# reproduce my ratingsratings = np.zeros(1682)ratings[0] = 4ratings[6] = 3ratings[11] = 5ratings[53] = 4ratings[63] = 5ratings[65] = 3ratings[68] = 5ratings[97] = 2ratings[182] = 4ratings[225] = 5ratings[354] = 5 数据预处理把我们的评价插入到所有电影的评分中去，把参数theta和X处理为正态分布。 1234567891011121314151617181920212223# prepare data# now I become user 0Y, R = movies_mat.get('Y'), movies_mat.get('R')Y = np.insert(Y, 0, ratings, axis=1)R = np.insert(R, 0, ratings != 0, axis=1)print(Y.shape) # (1682, 944)print(R.shape) # (1682, 944)n_features = 50n_movie, n_user = Y.shapel = 10# 转换为正态分布X = np.random.standard_normal((n_movie, n_features))theta = np.random.standard_normal((n_user, n_features))print(X.shape, theta.shape) # (1682, 50) (944, 50)param = serialize(X, theta)# normalized ratingsY_norm = Y - Y.mean()print(Y_norm.mean()) # 4.6862111343939375e-17 训练123456789# trainingimport scipy.optimize as optres = opt.minimize(fun=regularized_cost, x0=param, args=(Y_norm, R, n_features, l), method='TNC', jac=regularized_gradient)print(res) 稍等一会儿得到一下结果12345678910 fun: 24268.448311691616 jac: array([-12.49378802, 14.209063 , -6.75343791, ..., 0.61519582, -1.32599207, 0.58813019])message: 'Converged (|f_n-f_(n-1)| ~= 0)' nfev: 219 nit: 14 status: 1success: True x: array([-0.30795529, 0.88620348, -0.10899471, ..., 0.18986581, -0.28537047, -0.11540767]) 检查推荐结果y=np.argsort(x)将x中的元素从小到大排列，提取其对应的index(索引)，然后输出到y1234567891011121314X_trained, theta_trained = deserialize(res.x, n_movie, n_user, n_features)print(X_trained.shape, theta_trained.shape)prediction = X_trained @ theta_trained.Tmy_preds = prediction[:, 0] + Y.mean()idx = np.argsort(my_preds)[::-1] # descending orderprint(idx.shape)# top ten idxmy_preds[idx][:10]for m in movie_list[idx][:10]: print(m) 12345678910Godfather, The (1972)Forrest Gump (1994)Star Wars (1977)Titanic (1997)Shawshank Redemption, The (1994)Raiders of the Lost Ark (1981)Return of the Jedi (1983)Usual Suspects, The (1995)Braveheart (1995)Empire Strikes Back, The (1980) 每次得到的结果有个别差别，但是七成时没有变化的。]]></content>
      <categories>
        <category>机器学习习题</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ex7-k means and PCA]]></title>
    <url>%2F2018%2F11%2F07%2Fex7-k%20means%20and%20PCA%2F</url>
    <content type="text"><![CDATA[AndrewNg 机器学习习题ex7-k means and PCA 练习用数据 在本练习中，我们将实现K-means聚类，并使用它来压缩图像。 我们将从一个简单的2D数据集开始，以了解K-means是如何工作的，然后我们将其应用于图像压缩。 我们还将对主成分分析进行实验，并了解如何使用它来找到面部图像的低维表示。 Implementing K-means我们将实施和应用K-means到一个简单的二维数据集，以获得一些直观的工作原理。 K-means是一个迭代的，无监督的聚类算法，将类似的实例组合成簇。 该算法通过猜测每个簇的初始聚类中心开始，然后重复将实例分配给最近的簇，并重新计算该簇的聚类中心。 我们要实现的第一部分是找到数据中每个实例最接近的聚类中心的函数。 可视化数据1234567891011121314151617#!/usr/bin/python# coding=utf-8import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sbfrom scipy.io import loadmat# 数据可视化data = loadmat('data/ex7data2.mat')X = data['X']data2 = pd.DataFrame(data.get('X'), columns=['X1', 'X2'])print(data2.head())sb.set(context="notebook", style="white")sb.lmplot('X1', 'X2', data=data2, fit_reg=False)plt.show() Finding closest centroids$c^{(i)} := j\ that\ minimizes\ ||x^i - u_j||^2 $ 计算每一个特征值到所选取的聚类中心的距离，纪录最短距离的聚类中心编号。 123456789101112131415161718192021def find_closest_centroids(X, centroids): m = X.shape[0] k = centroids.shape[0] idx = np.zeros(m) for i in range(m): min_dist = 1000000 for j in range(k): dist = np.sum((X[i, :] - centroids[j, :]) ** 2) if dist &lt; min_dist: min_dist = dist idx[i] = j return idx initial_centroids = initial_centroids = np.array([[3, 3], [6, 2], [8, 5]])idx = find_closest_centroids(X, initial_centroids)print(idx[: 3]) [0. 2. 1.] 输出与文本中的预期值匹配（记住我们的数组是从零开始索引的，而不是从一开始索引的，所以值比练习中的值低一个）。 Computing centroid means接下来，我们需要一个函数来计算簇的聚类中心。 聚类中心只是当前分配给簇的所有样本的平均值。123456789101112def compute_centroids(X, idx, k): m, n = X.shape centroids = np.zeros((k, n)) for i in range(k): indices = np.where(idx == i) centroids[i, :] = (np.sum(X[indices, :], axis=1) / len(indices[0])).ravel() return centroidsprint(compute_centroids(X, idx, 3)) [[2.42830111 3.15792418] [5.81350331 2.63365645] [7.11938687 3.6166844 ]] 此输出也符合练习中的预期值。 下一部分涉及实际运行该算法的一些迭代次数和可视化结果。 这个步骤是由于并不复杂，我将从头开始构建它。 为了运行算法，我们只需要在将样本分配给最近的簇并重新计算簇的聚类中心。 K-means on example dataset12345678910111213141516171819202122232425262728293031323334353637383940414243444546centroids_trace = np.empty(shape=[0, 2])def run_k_means(X, initial_centroids, max_iters): m, n = X.shape k = initial_centroids.shape[0] idx = np.zeros(m) centroids = initial_centroids global centroids_trace for i in range(max_iters): idx = find_closest_centroids(X, centroids) centroids = compute_centroids(X, idx, k) centroids_trace = np.append(centroids_trace, centroids, axis=0) return idx, centroidsdef init_centroids(X, k): m, n = X.shape centroids = np.zeros((k, n)) idx = np.random.randint(0, m, k) for i in range(k): centroids[i, :] = X[idx[i], :] return centroidsinitial_centroids = init_centroids(X, 3)idx, centroids = run_k_means(X, initial_centroids, 10)cluster1 = X[np.where(idx == 0)[0], :]cluster2 = X[np.where(idx == 1)[0], :]cluster3 = X[np.where(idx == 2)[0], :]fig, ax = plt.subplots(figsize=(12, 8))ax.scatter(cluster1[:, 0], cluster1[:, 1], s=30, color='r', label='Cluster 1')ax.scatter(cluster2[:, 0], cluster2[:, 1], s=30, color='g', label='Cluster 2')ax.scatter(cluster3[:, 0], cluster3[:, 1], s=30, color='b', label='Cluster 3')ax.legend()x = centroids_trace[:, 0]y = centroids_trace[:, 1]ax.scatter(x, y, color='black', s=50, zorder=2)plt.show() Image compression with K-means我们的下一个任务是将K-means应用于图像压缩。 从下面的演示可以看到，我们可以使用聚类来找到最具代表性的少数颜色，并使用聚类分配将原始的24位颜色映射到较低维的颜色空间。 下面是我们要压缩的图像。 1234567891011121314151617181920212223242526272829image_data = loadmat('data/bird_small.mat')print(image_data)A = image_data['A']print(A.shape)# 数据预处理# normalize value rangesA = A / 255# reshape the arrayprint((A.shape[0] * A.shape[1], A.shape[2]))X = np.reshape(A, (128*128, 3))print(X.shape)# randomly initalize the centroidsinitial_centroids = init_centroids(X, 16)# run the algorithmidx, centroids = run_k_means(X, initial_centroids, 10)# gor the closet centroids one last timeidx = find_closest_centroids(X, centroids)# map each poxel to the centroid valueX_recovered = centroids[idx.astype(int), :]print(X_recovered.shape)# reshape to the original dimensionsX_recovered = np.reshape(X_recovered, (A.shape[0], A.shape[1], A.shape[2])) 用scikit-learn来实现K-means1234567891011121314151617from sklearn.cluster import KMeans # 导入kmeans库model = KMeans(n_clusters=16, n_init=100, n_jobs=1)model.fit(X)centroids = model.cluster_centers_print(centroids.shape)C = model.predict(X)print(C.shape)print(centroids[C].shape)compressed_pic = centroids[C].reshape((128, 128, 3))fig, ax = plt.subplots(1, 3)ax[0].imshow(A)ax[1].imshow(X_recovered)ax[2].imshow(compressed_pic)plt.show() 通过K-means算法，我们让图像以更少的色彩来显示实现压缩，但是图像的主要特征仍然存在。 Principal component analysis（主成分分析）PCA是在数据集中找到“主成分”或最大方差方向的线性变换。 它可以用于降维。 在本练习中，我们首先负责实现PCA并将其应用于一个简单的二维数据集，以了解它是如何工作的。 我们从加载和可视化数据集开始。 1234567891011121314151617181920212223242526272829303132333435363738394041data = loadmat('data/ex7data1.mat')X = data['X']def pca(X): # normalize the feature X = (X - X.mean()) / X.std() # compute the covariance matrix X = np.matrix(X) cov = (X.T * X) / X.shape[0] # perform SVD U, S, V = np.linalg.svd(cov) return U, S, VU, S, V = pca(X)print(U, S, V)def project_data(X, U, k): U_reduced = U[:, :k] return np.dot(X, U_reduced)Z = project_data(X, U, 1)def recover_data(Z, U, k): U_reduced = U[:, :k] return np.dot(Z, U_reduced.T)X_recovered = recover_data(Z, U, 1)fig, ax = plt.subplots(1, 2)ax[0].scatter(X[:, 0], X[:, 1])ax[1].scatter(list(X_recovered[:, 0]), list(X_recovered[:, 1]))plt.show() 请注意，第一主成分的投影轴基本上是数据集中的对角线。 当我们将数据减少到一个维度时，我们失去了该对角线周围的变化，所以在我们的再现中，一切都沿着该对角线。 我们在此练习中的最后一个任务是将PCA应用于脸部图像。 通过使用相同的降维技术，我们可以使用比原始图像少得多的数据来捕获图像的“本质” 1234567891011121314151617181920212223242526272829303132333435def plot_n_image(X, n): """ plot first n images n has to be a square number """ pic_size = int(np.sqrt(X.shape[1])) grid_size = int(np.sqrt(n)) first_n_images = X[:n, :] fig, ax_array = plt.subplots(nrows=grid_size, ncols=grid_size, sharey=True, sharex=True, figsize=(8, 8)) for r in range(grid_size): for c in range(grid_size): ax_array[r, c].imshow(first_n_images[grid_size * r + c].reshape((pic_size, pic_size)).T, cmap=plt.cm.gray) plt.xticks(np.array([])) plt.yticks(np.array([]))faces = loadmat('data/ex7faces.mat')X = faces['X']print(X.shape)plot_n_image(X, 100)face1 = np.reshape(X[1,:], (32, 32)).TU, S, V = pca(X)Z = project_data(X, U, 100)print(Z.shape)X_recovered = recover_data(Z, U, 100)face2 = np.reshape(X_recovered[1,:], (32, 32)).Tfig, ax = plt.subplots(1, 2)ax[0].imshow(face1, cmap=plt.cm.gray)ax[1].imshow(face2, cmap=plt.cm.gray)plt.show()# 计算平均均方差误差与训练集方差的比例print(np.sum(S[:100]) / np.sum(S)) # 0.9434273519364477 我们把1024个特征缩减到100个时还保留了94%的差异值。]]></content>
      <categories>
        <category>机器学习习题</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ex6-SVM]]></title>
    <url>%2F2018%2F11%2F01%2Fex6-SVM%2F</url>
    <content type="text"><![CDATA[AndrewNg 机器学习习题ex6-SVM 练习用数据 线性SVM1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#!/usr/bin/python# coding=utf-8import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sbfrom scipy.io import loadmatfrom sklearn import svm# 在一个简单的二位数据集中 SVM中不同的C处理结果raw_data = loadmat('data/ex6data1.mat')print(raw_data)data = pd.DataFrame(raw_data['X'], columns=['X1', 'X2'])data['y'] = raw_data['y']positive = data[data['y'].isin([1])]negative = data[data['y'].isin([0])]fig, ax = plt.subplots(figsize=(12,8))ax.scatter(positive['X1'], positive['X2'], s=50, marker='x', label='Positive')ax.scatter(negative['X1'], negative['X2'], s=50, marker='o', label='Negative')ax.legend()plt.show()svc = svm.LinearSVC(C=1, loss='hinge', max_iter=1000)print(svc)# 首先看下C=1的结果svc.fit(data[['X1', 'X2']], data['y'])score = svc.score(data[['X1', 'X2']], data['y'])print(score) # 0.9803921568627451# 当C=100的时候svc2 = svm.LinearSVC(C=100, loss='hinge', max_iter=1000)svc2.fit(data[['X1', 'X2']], data['y'])score2 = svc2.score(data[['X1', 'X2']], data['y'])print(score2) # 0.9411764705882353 每次执行的结果可能不同 data['SVM 1 Confidence'] = svc.decision_function(data[['X1', 'X2']])fig, ax = plt.subplots(figsize=(12, 8))ax.scatter(data['X1'], data['X2'], s=50, c=data['SVM 1 Confidence'], cmap='seismic')ax.set_title('SVM (C=1) Decision Confidence')plt.show()data['SVM 2 Confidence'] = svc2.decision_function(data[['X1', 'X2']])fig, ax = plt.subplots(figsize=(12,8))ax.scatter(data['X1'], data['X2'], s=50, c=data['SVM 2 Confidence'], cmap='seismic')ax.set_title('SVM (C=100) Decision Confidence')plt.show() 高斯核函数123456789101112# 核函数def gaussian_kernel(x1, x2, sigma): return np.exp(-(np.sum((x1 - x2) ** 2) / (2 * (sigma ** 2))))x1 = np.array([1.0, 2.0, 1.0])x2 = np.array([0.0, 4.0, -1.0])sigma = 2gaussian_kernel(x1, x2, sigma)# 0.32465246735834974 非线性决策边界123456789101112raw_data = loadmat('data/ex6data2.mat')data = pd.DataFrame(raw_data['X'], columns=['X1', 'X2'])data['y'] = raw_data['y']positive = data[data['y'].isin([1])]negative = data[data['y'].isin([0])]fig, ax = plt.subplots(figsize=(12, 8))ax.scatter(positive['X1'], positive['X2'], s=30, marker='x', label='Positive')ax.scatter(negative['X1'], negative['X2'], s=30, marker='o', label='Negative')ax.legend()plt.show() 对于该数据集，我们将使用内置的RBF内核构建支持向量机分类器，并检查其对训练数据的准确性。 为了可视化决策边界，这一次我们将根据实例具有负类标签的预测概率来对点做阴影。 从结果可以看出，它们大部分是正确的。 123456789svc = svm.SVC(C=100, gamma=10, probability=True)print(svc)svc.fit(data[['X1', 'X2']], data['y'])svc.score(data[['X1', 'X2']], data['y'])data['Probability'] = svc.predict_proba(data[['X1', 'X2']])[:,0]fig, ax = plt.subplots(figsize=(12,8))ax.scatter(data['X1'], data['X2'], s=30, c=data['Probability'], cmap='Reds')plt.show() 搜索最佳参数12345678910111213141516171819202122232425# 搜索最佳参数raw_data = loadmat('data/ex6data3.mat')X = raw_data['X']Xval = raw_data['Xval']y = raw_data['y'].ravel()yval = raw_data['yval']. ravel()C_values = [0.001, 0.003, 0.1, 0.3, 1, 3, 10, 30, 100]gamma_values = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]best_score = 0best_params = &#123;'C': None, 'gamma':None&#125;for C in C_values: for gamma in gamma_values: svc = svm.SVC(C=C, gamma=gamma) svc.fit(X, y) score = svc.score(Xval, yval) if score &gt; best_score: best_score = score best_params['C'] = C best_params['gamma'] = gammaprint(best_params, best_score) {‘C’: 0.3, ‘gamma’: 100} 0.965 垃圾邮件过滤现在，我们将进行第二部分的练习。 在这一部分中，我们的目标是使用SVM来构建垃圾邮件过滤器。 在练习文本中，有一个任务涉及一些文本预处理，以获得适合SVM处理的格式的数据。 然而，这个任务很简单（将字词映射到为练习提供的字典中的ID），而其余的预处理步骤（如HTML删除，词干，标准化等）已经完成。 我将跳过机器学习任务，而不是重现这些预处理步骤，其中包括从预处理过的训练集构建分类器，以及将垃圾邮件和非垃圾邮件转换为单词出现次数的向量的测试数据集。12345678910111213# 垃圾邮件过滤mat_tr = loadmat('data/spamTrain.mat')X, y = mat_tr.get('X'), mat_tr.get('y').ravel()print(X.shape, y.shape) # ((4000, 1899), (4000,))mat_test = loadmat('data/spamTest.mat')test_X, test_y = mat_test.get('Xtest'), mat_test.get('ytest').ravel()print(test_X.shape, test_y.shape) # ((1000, 1899), (1000,))svc = svm.SVC()svc.fit(X, y)pred = svc.predict(test_X)print(metrics.classification_report(test_y, pred)) 123456 precision recall f1-score support 0 0.94 0.99 0.97 692 1 0.98 0.87 0.92 308avg / total 0.95 0.95 0.95 1000 这个结果是使用使用默认参数的。 。 然后用逻辑回归来计算后精确的达到了99%12345# 如果是逻辑回归呢？logit = LogisticRegression()logit.fit(X, y)pred = logit.predict(test_X)print(metrics.classification_report(test_y, pred)) 123456 precision recall f1-score support 0 1.00 0.99 0.99 692 1 0.97 0.99 0.98 308avg / total 0.99 0.99 0.99 1000 调整参数后也可以达到和逻辑回归一样的精确度1234svc = svm.SVC(C=100)svc.fit(X, y)pred = svc.predict(test_X)print(metrics.classification_report(test_y, pred)) 123456 precision recall f1-score support 0 1.00 0.99 0.99 692 1 0.97 0.99 0.98 308avg / total 0.99 0.99 0.99 1000]]></content>
      <categories>
        <category>机器学习习题</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ex2-logistic regression]]></title>
    <url>%2F2018%2F10%2F09%2Fex2-logistic%20regression%2F</url>
    <content type="text"><![CDATA[AndrewNg 机器学习习题ex2-logistic regression 练习用数据 练习数据ex2data1.txt和ex2data2.txt都是由三列数字组成的文本文件，前两列是特征，第三列是结果，结果只有0和1两种。 浏览数据画出散点图，观察两个不同结果的分类情况，有明显的决策边界。12345678910111213141516171819202122232425262728import numpy as npimport pandas as pdimport matplotlib.pyplot as pltpath = './data/ex2data1.txt'names=['exam 1', 'exam 2', 'admitted']data = pd.read_csv(path, header=None, names=names)print(data.head())# 可视化def data_visual(data, names, theta=None): positive = data[data[names[2]].isin([1])] negative = data[data[names[2]].isin([0])] fig, ax = plt.subplots(figsize=(12, 8)) ax.scatter(positive[names[0]], positive[names[1]], s=50, c='b', marker='o', label='1') ax.scatter(negative[names[0]], negative[names[1]], s=50, c='r', marker='x', label='0') ax.legend() if theta is not None: x1 = np.arange(20, 100, 5) x2 = (- theta[0] - theta[1] * x1) / theta[2] plt.plot(x1, x2, color='black') plt.show()data_visual(data, names) 激活函数 逻辑回归模型的假设是：$h_\theta \left( x \right)=g\left(\theta^{T}X \right)$其中： $X$ 代表特征向量 $g$ 代表逻辑函数（logistic function)是一个常用的逻辑函数为S形函数（Sigmoid function），公式为： $g\left( z \right)=\frac{1}{1+{{e}^{-z}}}$。 12345678910111213# sigmoid函数def sigmoid(z): return 1 / (1 + np.exp(-z))# 检查激活函数def sigmoid_visual(): nums = np.arange(-10, 10, step=1) plt.plot(nums, sigmoid(nums)) plt.show()sigmoid_visual() 代价函数与预处理代价函数： $J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)]}$ 123456789101112131415161718192021222324252627# 代价函数def cost(theta, X, Y): theta = np.matrix(theta) X = np.matrix(X) Y = np.matrix(Y) first = np.multiply(-Y, np.log(sigmoid(X * theta.T))) second = np.multiply((1 - Y), np.log(1 - sigmoid(X * theta.T))) return np.sum(first - second) / len(X)# 数据预处理# add a ones column - this makes the matrix multiplication work out easierdata.insert(0, 'Ones', 1)# set X (training data) and y (target variable)cols = data.shape[1]X = data.iloc[:, 0: cols - 1]Y = data.iloc[:, cols - 1: cols]# convert to numpy arrays and initalize the parameter array thetaX = np.array(X.values)Y = np.array(Y.values)theta = np.zeros(3)# 检查维度print(X.shape, theta.shape, Y.shape) # (100, 3) (3,) (100, 1)print(cost(theta, X, Y)) # 初始值的代价 初始化的代价函数值为：0.6931471805599453 梯度下降 $\frac{\partial J\left( \theta \right)}{\partial {{\theta }_{j}}}=\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}\left( {{x}^{(i)}} \right)-{{y}^{(i)}})x_{_{j}}^{(i)}}$ 1234567891011121314151617# 梯度下降def gradient(theta, X, Y): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(Y) parameters = int(theta.ravel().shape[1]) grad = np.zeros(parameters) error = sigmoid(X * theta.T) - Y for i in range(parameters): term = np.multiply(error, X[:, i]) grad[i] = np.sum(term) / len(X) return grad 训练数据与决策边界123456789101112131415161718192021# 用SciPy's truncated newton（TNC）实现寻找最优参数result = opt.fmin_tnc(func=cost, x0=theta, fprime=gradient, args=(X, Y))print(result)print(cost(result[0], X, Y))theta = result[0]# 画出决策边界data_visual(data, names, theta)# 计算预测效果def predict(theta, X): probability = sigmoid(X * theta.T) return [1 if x &gt;= 0.5 else 0 for x in probability]theta_min = np.matrix(result[0])predictions = predict(theta_min, X)correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, Y)]accuracy = (sum(map(int, correct)) % len(correct))print('accuracy = &#123;&#125;%'.format(accuracy)) accuracy = 89% 逻辑回归正则化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778path2 = './data/ex2data2.txt'names = ['test1', 'test2', 'accepted']data2 = pd.read_csv(path2, header=None, names=names)print(data2.head())#data_visual(data2, names)degree = 5x1 = data2['test1']x2 = data2['test2']data2.insert(3, 'Ones', 1)for i in range(1, degree): for j in range(0, i): data2['F' + str(i) + str(j)] = np.power(x1, i-j) * np.power(x2, j)data2.drop('test1', axis=1, inplace=True)data2.drop('test2', axis=1, inplace=True)print(data2.head())# 正则化代价函数 learng_rate = λ lambdadef cost_reg(theta, X, Y, learng_rate): theta = np.matrix(theta) X = np.matrix(X) Y = np.matrix(Y) first = np.multiply(-Y, np.log(sigmoid(X * theta.T))) second = np.multiply((1 - Y), np.log(1 - sigmoid(X * theta.T))) reg = (learng_rate / (2 * len(X))) * np.sum(np.power(theta[:, 1: theta.shape[1]], 2)) return np.sum(first - second) / len(X) + regdef gradient_reg(theta, X, Y, learng_rate): theta = np.matrix(theta) X = np.matrix(X) Y = np.matrix(Y) parameters = int(theta.ravel().shape[1]) grad = np.zeros(parameters) error = sigmoid(X * theta.T) - Y for i in range(parameters): term = np.multiply(error, X[:, i]) if(i == 0): grad[i] = np.sum(term) / len(X) else: grad[i] = (np.sum(term) / len(X)) + ((learng_rate / len(X)) * theta[:, i]) return grad# set X and y (remember from above that we moved the label to column 0)cols = data2.shape[1]X2 = data2.iloc[:,1:cols]Y2 = data2.iloc[:, :1]# convert to numpy arrays and initalize the parameter array thetaX2 = np.array(X2.values)Y2 = np.array(Y2.values)theta2 = np.zeros(11)learng_rate = 1print(cost_reg(theta2, X2, Y2, learng_rate))print(gradient_reg(theta2, X2, Y2, learng_rate))result2 = opt.fmin_tnc(func=cost_reg, x0=theta2, fprime=gradient_reg, args=(X2, Y2, learng_rate))print(result2)theta_min = np.matrix(result2[0])predictions = predict(theta_min, X2)correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, Y2)]accuracy = (sum(map(int, correct)) % len(correct))print('accuracy = &#123;0&#125;%'.format(accuracy)) accuracy = 78% 正则化画出决策边界123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport scipy.optimize as optimport seaborn as snsdef get_y(df): # 读取标签# '''assume the last column is the target''' return np.array(df.iloc[:, -1]) # df.iloc[:, -1]是指df的最后一列def sigmoid(z): return 1 / (1 + np.exp(-z))def gradient(theta, X, y):# '''just 1 batch gradient''' return (1 / len(X)) * X.T @ (sigmoid(X @ theta) - y)def cost(theta, X, y): ''' cost fn is -l(theta) for you to minimize''' return np.mean(-y * np.log(sigmoid(X @ theta)) - (1 - y) * np.log(1 - sigmoid(X @ theta)))df = pd.read_csv('./data/ex2data2.txt', names=['test1', 'test2', 'accepted'])df.head()def feature_mapping(x, y, power, as_ndarray=False):# """return mapped features as ndarray or dataframe""" # data = &#123;&#125; # # inclusive # for i in np.arange(power + 1): # for p in np.arange(i + 1): # data["f&#123;&#125;&#123;&#125;".format(i - p, p)] = np.power(x, i - p) * np.power(y, p) data = &#123;"f&#123;&#125;&#123;&#125;".format(i - p, p): np.power(x, i - p) * np.power(y, p) for i in np.arange(power + 1) for p in np.arange(i + 1) &#125; if as_ndarray: return pd.DataFrame(data).as_matrix() else: return pd.DataFrame(data)x1 = np.array(df.test1)x2 = np.array(df.test2)data = feature_mapping(x1, x2, power=6)print(data.shape)print(data.head())theta = np.zeros(data.shape[1])X = feature_mapping(x1, x2, power=6, as_ndarray=True)print(X.shape)y = get_y(df)print(y.shape)def regularized_cost(theta, X, y, l=1):# '''you don't penalize theta_0''' theta_j1_to_n = theta[1:] regularized_term = (l / (2 * len(X))) * np.power(theta_j1_to_n, 2).sum() return cost(theta, X, y) + regularized_term# 正则化代价函数regularized_cost(theta, X, y, l=1)def regularized_gradient(theta, X, y, l=1):# '''still, leave theta_0 alone''' theta_j1_to_n = theta[1:] regularized_theta = (l / len(X)) * theta_j1_to_n # by doing this, no offset is on theta_0 regularized_term = np.concatenate([np.array([0]), regularized_theta]) return gradient(theta, X, y) + regularized_termprint('init cost = &#123;&#125;'.format(regularized_cost(theta, X, y)))res = opt.minimize(fun=regularized_cost, x0=theta, args=(X, y), method='Newton-CG', jac=regularized_gradient)def draw_boundary(power, l):# """# power: polynomial power for mapped feature# l: lambda constant# """ density = 1000 threshhold = 2 * 10**-3 final_theta = feature_mapped_logistic_regression(power, l) x, y = find_decision_boundary(density, power, final_theta, threshhold) df = pd.read_csv('./data/ex2data2.txt', names=['test1', 'test2', 'accepted']) sns.lmplot('test1', 'test2', hue='accepted', data=df, size=6, fit_reg=False, scatter_kws=&#123;"s": 100&#125;) plt.scatter(x, y, c='R', s=10) plt.title('Decision boundary') plt.show()def feature_mapped_logistic_regression(power, l):# """for drawing purpose only.. not a well generealize logistic regression# power: int# raise x1, x2 to polynomial power# l: int# lambda constant for regularization term# """ df = pd.read_csv('./data/ex2data2.txt', names=['test1', 'test2', 'accepted']) x1 = np.array(df.test1) x2 = np.array(df.test2) y = get_y(df) X = feature_mapping(x1, x2, power, as_ndarray=True) theta = np.zeros(X.shape[1]) res = opt.minimize(fun=regularized_cost, x0=theta, args=(X, y, l), method='TNC', jac=regularized_gradient) final_theta = res.x return final_thetadef find_decision_boundary(density, power, theta, threshhold): t1 = np.linspace(-1, 1.5, density) t2 = np.linspace(-1, 1.5, density) cordinates = [(x, y) for x in t1 for y in t2] x_cord, y_cord = zip(*cordinates) mapped_cord = feature_mapping(x_cord, y_cord, power) # this is a dataframe inner_product = mapped_cord.as_matrix() @ theta decision = mapped_cord[np.abs(inner_product) &lt; threshhold] return decision.f10, decision.f01# 寻找决策边界函数draw_boundary(power=6, l=1) # lambda=1draw_boundary(power=6, l=0) # lambda=1 过拟合draw_boundary(power=6, l=100) # lambda=1 欠拟合]]></content>
      <categories>
        <category>机器学习习题</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ex1-linear regression]]></title>
    <url>%2F2018%2F10%2F08%2Fex1-linear%20regression%2F</url>
    <content type="text"><![CDATA[AndrewNg 机器学习习题ex1-linear regression练习用数据 练习数据ex1data1.txt和ex2data2.txt都是以逗号为分割符的文本文件，所以我们也可以把它们看作csv文件处理。 ex1data1中的第一列是一个城市的人口，第二列是这个城市中卡车司机的利润。 ex2data2三列分别是，一个房子的大小，房间数，售价。 浏览数据12345678910111213import numpy as npimport pandas as pdimport matplotlib.pyplot as plt# 加载数据path = './data/ex1data1.txt'data = pd.read_csv(path, header=None, names=['Population', 'Profit'])# 看一下数据的内容print(data.head())print(data.describe())# 画出散点图data.plot(kind='scatter', x='Population', y='Profit', figsize=(12, 8))plt.show() - Population Profit 0 6.1101 17.5920 1 5.5277 9.1302 2 8.5186 13.6620 3 7.0032 11.8540 4 5.8598 6.8233 - Population Profit count 97.000000 97.000000 mean 8.159800 5.839135 std 3.869884 5.510262 min 5.026900 -2.680700 25% 5.707700 1.986900 50% 6.589400 4.562300 75% 8.578100 7.046700 max 22.203000 24.147000 代价函数我们将创建一个以参数θ为特征函数的代价函数 $J\left( \theta \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( {{h}_{\theta }}\left( {{x}^{(i)}} \right)-{{y}^{(i)}} \right)}^{2}}}$ 其中： ${{h}{\theta }}\left( x \right)={{\theta }^{T}}X={{\theta }{0}}{{x}{0}}+{{\theta }{1}}{{x}{1}}+{{\theta }{2}}{{x}{2}}+...+{{\theta }{n}}{{x}_{n}}$ 123def compute_cost(X, y, theta): inner = np.power(((X * theta.T) - y), 2) return np.sum(inner) / (2 * len(X)) 预处理1234567891011121314151617# 预处理data.insert(0, 'Ones', 1) # 添加一列1cols = data.shape[1]X = data.iloc[:, :cols - 1] # 去掉最后一列Y = data.iloc[:, cols - 1: cols] # 最后一列# 检查X和Y 是否正确print(X.head())print(Y.head())# 把X和Y转换为numpy的矩阵X = np.matrix(X.values)Y = np.matrix(Y.values)# 初始化thetatheta = np.matrix(np.array([0, 0]))# 检查维度print(X.shape, Y.shape, theta.shape) # (97, 2) (97, 1) (1, 2) 批量梯度下降我们要这个公式来更新θ。 ${\theta_{j}}:={\theta_{j}}-\alpha \frac{\partial }{\partial {\theta_{j}}}J\left(\theta \right)$ 12345678910111213141516171819202122232425# 梯度下降# X矩阵，Y矩阵，初始的θ，学习速率，迭代次数def gradient_descent(X, Y, theta, alpha, iters): temp = np.matrix(np.zeros(theta.shape)) parameters = int(theta.ravel().shape[1]) cost = np.zeros(iters) for i in range(iters): error = (X * theta.T) - Y for j in range(parameters): term = np.multiply(error, X[:, j]) temp[0, j] = theta[0, j] - ((alpha / len(X)) * np.sum(term)) theta = temp cost[i] = compute_cost(X, Y, theta) return theta, cost# 初始化迭代次数和学习速率alpha = 0.01iters = 1000g, cost = gradient_descent(X, Y, theta, alpha, iters)# 用我们得到的参数g计算代价函数，查看误差print(g, compute_cost(X, Y, theta)) 可视化1234567891011121314# 绘制线性模型以及数据，查看拟合效果def data_visual(data, g): x = np.linspace(data.Population.min(), data.Population.max(), 100) f = g[0, 0] + (g[0, 1] * x) fig, ax = plt.subplots(figsize=(12, 8)) ax.plot(x, f, 'g', label='Prediction') ax.scatter(data.Population, data.Profit, label='Traning Data') ax.legend(loc=2) ax.set_xlabel('Population') ax.set_ylabel('Profit') plt.show()data_visual(data, g) 123456789# 绘制代价向量def cost_visual(cost): fig, ax = plt.subplots(figsize=(12, 8)) ax.plot(np.arange(iters), cost, 'r') ax.set_xlabel('Iterations') ax.set_ylabel('Cost') plt.show()cost_visual(cost) 多变量的线性回归练习1还包括一个房屋价格数据集，其中有2个变量（房子的大小，卧室的数量）和目标（房子的价格）。 我们使用我们已经应用的技术来分析数据集。 1234567891011121314151617181920212223path = './data/ex1data2.txt'data2 = pd.read_csv(path, header=None, names =['Size', 'Bedrooms', 'Price'])print(data2.head())# 特征归一化data2 = (data2 - data2.mean()) / data2.std()# 预处理# add ones columndata2.insert(0, 'Ones', 1)# set X (training data) and y (target variable)cols = data2.shape[1]X2 = data2.iloc[:, : cols - 1]Y2 = data2.iloc[:, cols - 1: cols]# convert to matrices and initialize thetaX2 = np.matrix(X2.values)Y2 = np.matrix(Y2.values)theta2 = np.matrix(np.array([0, 0, 0]))g2, cost2 = gradient_descent(X2, Y2, theta2, alpha, iters)cost_visual(cost2) - Size Bedrooms Price 0 2104 3 399900 1 1600 3 329900 2 2400 3 369000 3 1416 2 232000 4 3000 4 539900 正规方程 $\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}y$ 1234567# 正规方程def normal_func(X ,Y): theta = np.linalg.inv(X.T@X)@X.T@Y return thetag = normal_func(X, Y)data_visual(data, g.T)]]></content>
      <categories>
        <category>机器学习习题</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ex5-bias vs variance]]></title>
    <url>%2F2018%2F09%2F29%2Fex5-bias%20vs%20variance%2F</url>
    <content type="text"><![CDATA[AndrewNg 机器学习习题ex5-bias vs variance 练习用数据 ex5data1.mat文件储存了大坝出水量的数据，由三部分组成： 训练集：X，y 交叉验证集：Xval，yval 测试集：Xtest，ytest 需要的头：123456import numpy as npimport scipy.io as sioimport scipy.optimize as optimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns 数据预处理画出训练集的散点图，给特征集加一列1.12345678910111213def load_data(): d = sio.loadmat('./data/ex5data1.mat') return map(np.ravel, [d['X'], d['y'], d['Xval'], d['yval'], d['Xtest'], d['ytest']])X, y, Xval, yval, Xtest, ytest = load_data()df = pd.DataFrame(&#123;'water_level': X, 'flow': y&#125;)print(df.shape)sns.lmplot('water_level', 'flow', data=df, fit_reg=False)plt.show()X, Xval, Xtest = [np.insert(x.reshape(x.shape[0], 1), 0, np.ones(x.shape[0]), axis=1) for x in (X, Xval, Xtest)]# print(X, Xval, Xtest ) 正则化代价函数是： 梯度下降： ${\theta_{j}}:={\theta_{j}}-\alpha \frac{\partial }{\partial {\theta_{j}}}J\left(\theta \right)$ 正则化线性回归的代价函数为： $J\left( \theta \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{[({{({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})}^{2}}+\lambda \sum\limits_{j=1}^{n}{\theta _{j}^{2}})]}$ 如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对$\theta_0$进行正则化，所以梯度下降算法将分两种情形： $Repeat$ $until$ $convergence${ ${\theta_0}:={\theta_0}-a\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{0}^{(i)}})$ ${\theta_j}:={\theta_j}-a[\frac{1}{m}\sum\limits_{i=1}^{m}{(({h_\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\left( i \right)}}+\frac{\lambda }{m}{\theta_j}]$ $for$ $j=1,2,...n$ } 12345678910111213141516171819202122232425262728293031323334353637# 代价函数def cost(theta, X, y): m = X.shape[0] inner = X @ theta - y # R(m+1) # 1*m @ m*1 = 1*1 矩阵乘法 # 一维矩阵的转置乘以它自己等于每个元素的平方和 return inner.T @ inner / (2 * m)print(cost(theta, X, y,))# 303.9515255535976# 梯度def gradient(theta, X, y): m = X.shape[0] return X.T @ (X @ theta - y) / m # (m, n).T @ (m, 1) -&gt; (n, 1)print(gradient(theta, X, y,))# [-15.30301567 598.16741084]# 正则化def regularized_cost(theta, X, y, l=1): return cost(theta, X, y) + (l / (2 * X.shape[0])) * np.power(theta[1:], 2).sum()def regularized_gradient(theta, X, y, l=1): m = X.shape[0] regularized_term = theta.copy() regularized_term[0] = 0 regularized_term = (l / m) * regularized_term return gradient(theta, X, y) + regularized_termprint(regularized_gradient(theta, X, y, l=1))# [-15.30301567 598.25074417] 训练数据正则化项 $\lambda=0$12345678910111213def linear_regression_np(theta, X, y, l=1): res = opt.fmin_tnc(func=regularized_cost, x0=theta, fprime=regularized_gradient, args=(X, y)) return resfinal_theta = linear_regression_np(theta, X, y)[0]b = final_theta[0]m = final_theta[1]plt.scatter(X[:, 1], y, label="Training data")plt.plot(X[:, 1], X[:, 1]*m + b, label='Prediction')plt.legend(loc=2)plt.show() 学习曲线12345678910111213141516171819def plot_learning_curve(X, y, Xval, yval, l=0): training_cost, cv_cost = [], [] # 计算训练集的代价和交叉验证（cross validation）集的代价 m = X.shape[0] for i in range(1, m + 1): res = linear_regression_np(theta, X[:i, :], y[:i], l=0) tc = regularized_cost(res[0], X[:i, :], y[:i], l=0) cv = regularized_cost(res[0], Xval, yval, l=0) training_cost.append(tc) cv_cost.append(cv) plt.plot(np.arange(1, m + 1), training_cost, label='training cost') plt.plot(np.arange(1, m + 1), cv_cost, label='cv cost') plt.legend(loc=1)plot_learning_curve(X, y, Xval, yval, l=0)plt.show() 观察学习曲线发现拟合的不太好，欠拟合。很显然我们的模型不优秀，改为多项式特征尝试。 多项式特征把特征扩展到8阶，然后归一化特征值。12345678910111213141516171819202122232425262728def poly_features(x, power, as_ndarray=False): data = &#123;'f&#123;&#125;'.format(i): np.power(x, i) for i in range(1, power + 1)&#125; df = pd.DataFrame(data) return df.as_matrix() if as_ndarray else df# 归一化特征值，减去平均数除以标准差def normalize_feature(df): """Applies function along input axis(default 0) of DataFrame.""" return df.apply(lambda column: (column - column.mean()) / column.std())def prepare_poly_data(*args, power): """ args: keep feeding in X, Xval, or Xtest will return in the same order """ def prepare(x): # expand feature df = poly_features(x, power=power) # normalization ndarr = normalize_feature(df).as_matrix() # add intercept term return np.insert(ndarr, 0, np.ones(ndarr.shape[0]), axis=1) return [prepare(x) for x in args] 尝试不同的λ来观察学习曲线123456789X, y, Xval, yval, Xtest, ytest = load_data()X_poly, Xval_poly, Xtest_poly= prepare_poly_data(X, Xval, Xtest, power=8)plot_learning_curve(X_poly, y, Xval_poly, yval, l=0)plt.show()plot_learning_curve(X_poly, y, Xval_poly, yval, l=1)plt.show()plot_learning_curve(X_poly, y, Xval_poly, yval, l=100)plt.show() 当λ取0时，也就是没有正则项时，可以看到训练的代价太低了，不真实. 这是 过拟合了 当训练代价增加了些，不再是0了。 稍减轻了过拟合 当λ取100时，正则化过多，变成了欠拟合。 最优λ12345678910111213141516171819202122232425262728# 找到最佳拟合l_candidate = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]training_cost, cv_cost = [], []for l in l_candidate: theta = np.ones(X_poly.shape[1]) theta = linear_regression_np(theta, X_poly, y, l)[0] tc = cost(theta, X_poly, y) cv = cost(theta, Xval_poly, yval) training_cost.append(tc) cv_cost.append(cv)plt.plot(l_candidate, training_cost, label='training')plt.plot(l_candidate, cv_cost, label='cross validation')plt.legend(loc=2)plt.xlabel('lambda')plt.ylabel('cost')plt.show()# best cv I got from all those candidatesl_candidate[np.argmin(cv_cost)]# use test data to compute the costfor l in l_candidate: theta = np.ones(X_poly.shape[1]) theta = linear_regression_np(theta, X_poly, y, l)[0] print('test cost(l=&#123;&#125;) = &#123;&#125;'.format(l, cost(theta, Xtest_poly, ytest))) 12345678910test cost(l=0) = 9.799399498688892test cost(l=0.001) = 11.054987989655938test cost(l=0.003) = 11.249198861537238test cost(l=0.01) = 10.879605199670008test cost(l=0.03) = 10.022734920552129test cost(l=0.1) = 8.632060998872074test cost(l=0.3) = 7.336602384055533test cost(l=1) = 7.46630349664086test cost(l=3) = 11.643928200535115test cost(l=10) = 27.715080216719304 调参后， lambda = 0.3 是最优选择，这个时候测试代价最小]]></content>
      <categories>
        <category>机器学习习题</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[独热编码One Hot Encoder]]></title>
    <url>%2F2018%2F09%2F29%2F%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81One%20Hot%20Encoder%2F</url>
    <content type="text"><![CDATA[在机器学习时我们通常要进行归一化数据后再进行训练，还有一些其他处理方法比如使用独热编码。 什么是独热码独热编码即 One-Hot 编码，又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都有它独立的寄存器位，并且在任意时候，其中只有一位有效。 可以这样理解，对于每一个特征，如果它有m个可能值，那么经过独热编码后，就变成了m个二元特征。并且，这些特征互斥，每次只有一个激活。因此，数据会变成稀疏的。 例如对六个状态进行编码： 自然顺序码为 000,001,010,011,100,101 独热编码则是 000001,000010,000100,001000,010000,100000 独热码的优点有一些特征无法直接应用在需要数值计算的算法中，例如，用户的性别，爱好，住址等，一般简单粗暴的处理方式时直接将不同的类别映射为一个整数，比如男性为0，女性为1，其他为2，这种简单的实现最大的问题就在于各种类别的特征都被看成是有序的，这显然不符合实际场景。 使用独热码可以处理非连续型数值特征，并且在一定程度上扩充了特征。 1.使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。 2.将离散特征通过one-hot编码映射到欧式空间，是因为，在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。 3.将离散型特征使用one-hot编码，可以会让特征之间的距离计算更加合理。比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = (1), x_2 = (2), x_3 = (3)。两个工作之间的距离是，(x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。那如果使用one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。 编码过程 假如只有一个特征是离散值： {sex：{male， female，other}} 该特征总共有3个不同的分类值，此时需要3个bit位表示该特征是什么值，对应bit位为1的位置对应原来的特征的值（一般情况下可以将原始的特征的取值进行排序，以便于后期使用），此时得到独热码为{100}男性 ，{010}女性，{001}其他 假如多个特征需要独热码编码，那么久按照上面的方法依次将每个特征的独热码拼接起来： {sex：{male， female，other}} {grade：{一年级， 二年级，三年级， 四年级}} 此时对于输入为{sex：male； grade： 四年级}进行独热编码，可以首先将sex按照上面的进行编码得到{100}，然后按照grade进行编码为{0001}，那么两者连接起来得到最后的独热码{1000001}； sklearn中的One Hot Encoder官方文档 12345from sklearn.preprocessing import OneHotEncodery = np.array([[1, 2, 3]]).Tencoder = OneHotEncoder(sparse=False)y_onehot = encoder.fit_transform(y)print(y_onehot) 123[[1. 0. 0.] [0. 1. 0.] [0. 0. 1.]]]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>OneHotEncoder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ex4-NN back propagation]]></title>
    <url>%2F2018%2F09%2F29%2Fex4-NN%20back%20propagation%2F</url>
    <content type="text"><![CDATA[AndrewNg 机器学习习题ex4-NN back propagation 练习用数据 需要的头：123456import matplotlib.pyplot as pltimport numpy as npimport scipy.io as sioimport matplotlibimport scipy.optimize as optfrom sklearn.metrics import classification_report # 这个包是评价报告 Visualizing the data载入数据：12345678910111213141516171819202122232425262728def load_data(path, transpose=True): data = sio.loadmat(path) y = data.get('y') y = y.reshape(y.shape[0]) X = data.get('X') if transpose: X = np.array([im.reshape((20, 20)).T for im in X]) X = np.array([im.reshape(400) for im in X]) return X, yX, y = load_data('./data/ex4data1.mat')def plot_100_image(X): size = int(np.sqrt(X.shape[1])) sample_idx = np.random.choice(np.array(X.shape[0]), 100) sample_images = X[sample_idx, :] fig, ax_array = plt.subplots(nrows=10, ncols=10, sharey=True, sharex=True, figsize=(8, 8)) for r in range(10): for c in range(10): ax_array[r, c].matshow(sample_images[10 * r + c].reshape((size, size)), cmap=matplotlib.cm.binary) plt.xticks(np.array([])) plt.yticks(np.array([])) plt.show()plot_100_image(X) 准备数据特征集合X添加一列全为1的偏差向量，把目标向量y进行OneHot编码。12345678X_raw, y_raw = load_data('./data/ex4data1.mat', transpose=False) # 这里转置X = np.insert(X_raw, 0, np.ones(X_raw.shape[0]), axis=1) # 增加全为1的一列print(y.shape) # (5000,)y = np.array([y_raw]).Tfrom sklearn.preprocessing import OneHotEncoderencoder = OneHotEncoder(sparse=False)y_onehot = encoder.fit_transform(y)print(y_onehot.shape) # (5000, 10) 读取权重先读取出ex4weights.mat中的theta1和theta2，把theta展开后进行扁平化处理。12345678910111213141516171819202122def load_weight(path): data = sio.loadmat(path) return data['Theta1'], data['Theta2']t1, t2 = load_weight('./data/ex4weights.mat')print(t1.shape, t2.shape) # (25, 401) (10, 26)def serialize(a, b): # np.ravel() 降维 # np.concatenate() 拼接 return np.concatenate((np.ravel(a), np.ravel(b)))def deserialize(seq): # 解开为两个theta return seq[:25 * 401].reshape(25, 401), seq[25 * 401:].reshape(10, 26)theta = serialize(t1, t2)print(theta.shape) # (25 * 401) + (10 * 26) = 10285 前向传播 feed forward（400 + 1） -&gt; (25 + 1) -&gt; (1)12345678910111213141516171819def sigmoid(z): return 1 / (1 + np.exp(-z)) def feed_forward(theta, X): t1, t2 = deserialize(theta) m = X.shape[0] a1 = X # 5000 * 401 z2 = a1 @ t1.T a2 = np.insert(sigmoid(z2), 0, np.ones(m), axis=1) # 5000*26 第一列加一列一 z3 = a2 @ t2.T # 5000 * 100 h = sigmoid(z3) # 5000 * 10 这是 h_theta(X) return a1, z2, a2, z3, h # 把每一层的计算都返回#_, _, _, _, h = feed_forward(theta, X)#print(h.shape) # (5000, 10) 代价函数与正则化 123456789101112131415161718192021222324def cost(theta, X, y): m = X.shape[0] _, _, _, _, h = feed_forward(theta, X) pair_computation = -np.multiply(y, np.log(h)) - np.multiply((1 - y), np.log(1 - h)) return pair_computation.sum() / mcost_res = cost(theta, X, y)print("cost:",cost_res)def regularized_cost(theta, X, y, l=1): t1, t2 = deserialize(theta) # t1: (25,401) t2: (10,26) m = X.shape[0] reg_t1 = np.power(t1[:, 1:], 2).sum() reg_t2 = np.power(t2[:, 1:], 2).sum() reg = (1 / (2 * m)) * (reg_t1 + reg_t2) return cost(theta, X, y) + regregularized_cost_res = regularized_cost(theta, X, y)print("reg cost:",regularized_cost_res) 反向传播12345678910111213141516171819202122232425262728293031323334353637383940def sigmoid_gradient(z): return np.multiply(sigmoid(z), 1 - sigmoid(z))print(sigmoid_gradient(0)) #0.25 def gradient(theta, X, y): t1, t2 = deserialize(theta) m = X.shape[0] deltal = np.zeros(t1.shape) delta2 = np.zeros(t2.shape) a1, z2, a2, z3, h = feed_forward(theta, X) for i in range(m): a1i = a1[i, :] z2i = z2[i, :] a2i = a2[i, :] hi = h[i, :] yi = y[i, :] d3i = hi - yi z2i = np.insert(z2i, 0, np.ones(1)) d2i = np.multiply(t2.T @ d3i, sigmoid_gradient(z2i)) delta2 += np.matrix(d3i).T @ np.matrix(a2i) deltal += np.matrix(d2i[1:]).T @ np.matrix(a1i) delta1 = deltal / m delta2 = delta2 / m return serialize(delta1, delta2)d1, d2 = deserialize(gradient(theta, X, y))print(d1.shape, d2.shape) # (25, 401) (10, 26) 梯度校验 梯度正则化： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def regularized_gradient(theta, X, y, l=1): """don't regularize theta of bias terms""" m = X.shape[0] delta1, delta2 = deserialize(gradient(theta, X, y)) t1, t2 = deserialize(theta) t1[:, 0] = 0 reg_term_d1 = (l / m) * t1 delta1 = delta1 + reg_term_d1 t2[:, 0] = 0 reg_term_d2 = (l / m) * t2 delta2 = delta2 + reg_term_d2 return serialize(delta1, delta2)def expand_array(arr): """replicate array into matrix [1, 2, 3] [[1, 2, 3], [1, 2, 3], [1, 2, 3]] """ # turn matrix back to ndarray return np.array(np.matrix(np.ones(arr.shape[0])).T @ np.matrix(arr))def gradient_checking(theta, X, y, epsilon, regularized=False): def a_numeric_grad(plus, minus, regularized=False): """calculate a partial gradient with respect to 1 theta""" if regularized: return (regularized_cost(plus, X, y) - regularized_cost(minus, X, y)) / (epsilon * 2) else: return (cost(plus, X, y) - cost(minus, X, y)) / (epsilon * 2) theta_matrix = expand_array(theta) # expand to (10285, 10285) epsilon_matrix = np.identity(len(theta)) * epsilon plus_matrix = theta_matrix + epsilon_matrix minus_matrix = theta_matrix - epsilon_matrix # calculate numerical gradient with respect to all theta numeric_grad = np.array([a_numeric_grad(plus_matrix[i], minus_matrix[i], regularized) for i in range(len(theta))]) # analytical grad will depend on if you want it to be regularized or not analytic_grad = regularized_gradient(theta, X, y) if regularized else gradient(theta, X, y) # If you have a correct implementation, and assuming you used EPSILON = 0.0001 # the diff below should be less than 1e-9 # this is how original matlab code do gradient checking diff = np.linalg.norm(numeric_grad - analytic_grad) / np.linalg.norm(numeric_grad + analytic_grad) print('If your backpropagation implementation is correct,\nthe relative difference will be smaller than 10e-9 (assume epsilon=0.0001).\nRelative Difference: &#123;&#125;\n'.format(diff))# gradient_checking(theta, X, y, epsilon= 0.0001)#这个运行很慢，谨慎运行 If your backpropagation implementation is correct,the relative difference will be smaller than 10e-9 (assume epsilon=0.0001).Relative Difference: 2.1466000818218673e-09 准备训练模型12345678910111213141516def random_init(size): return np.random.uniform(-0.12, 0.12, size)def nn_training(X, y): init_theta = random_init(10285) # 25 * 401 + 10 * 26 res = opt.minimize(fun=regularized_cost, x0=init_theta, args=(X ,y, 1), method='TNC', jac=regularized_gradient, options=&#123;'maxiter': 400&#125;) return resres = nn_training(X, y) # 慢print(res) Out put：12345678910 fun: 0.32211992072588747 jac: array([ 2.15004329e-04, 3.88985627e-08, -3.33174201e-08, ..., 3.15328424e-05, 2.82831419e-05, -1.68082404e-05])message: 'Max. number of function evaluations reached' nfev: 400 nit: 26 status: 3success: False x: array([ 0.00000000e+00, 1.94492814e-04, -1.66587101e-04, ..., -7.15493763e-01, -1.36561388e+00, -2.90127262e+00]) 显示准确率123456789101112_, y_answer = load_data('./data/ex4data1.mat')final_theta = res.xdef show_accuracy(theta, X, y): _, _, _, _, h = feed_forward(theta, X) y_pred = np.argmax(h, axis=1) + 1 print(classification_report(y, y_pred))show_accuracy(final_theta, X, y_answer) Out Put:1234567891011121314 precision recall f1-score support 1 1.00 0.79 0.88 500 2 0.73 1.00 0.85 500 3 0.82 0.99 0.89 500 4 1.00 0.89 0.94 500 5 1.00 0.86 0.92 500 6 0.94 0.99 0.97 500 7 0.99 0.81 0.89 500 8 0.94 0.95 0.95 500 9 0.96 0.95 0.95 500 10 0.96 0.98 0.97 500avg / total 0.93 0.92 0.92 5000 显示隐藏层123456789101112131415161718def plot_hidden_layer(theta): """ theta: (10285, ) """ final_theta1, _ = deserialize(theta) hidden_layer = final_theta1[:, 1:] # ger rid of bias term theta fig, ax_array = plt.subplots(nrows=5, ncols=5, sharey=True, sharex=True, figsize=(5, 5)) for r in range(5): for c in range(5): ax_array[r, c].matshow(hidden_layer[5 * r + c].reshape((20, 20)), cmap=matplotlib.cm.binary) plt.xticks(np.array([])) plt.yticks(np.array([]))plot_hidden_layer(final_theta)plt.show()]]></content>
      <categories>
        <category>机器学习习题</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ex3-neural network]]></title>
    <url>%2F2018%2F09%2F28%2Fex3-neural%20network%2F</url>
    <content type="text"><![CDATA[AndrewNg 机器学习习题ex3-neural network 练习用数据 ex3data1.mat是一个matlab文件，储存了5000个图像的数据，每个图像是一个20像素×20像素的灰度图，展开后为一个400维的向量，每一个向量都储存为矩阵X的行，所以X的维度是（5000，400）y的每一行代表X所对应的手写数字，y的维度是（5000，1） 需要的头：123456import matplotlib.pyplot as pltimport numpy as npimport scipy.io as sioimport matplotlibimport scipy.optimize as optfrom sklearn.metrics import classification_report # 这个包是评价报告 Visualizing the data载入数据：1234567891011def load_data(path, transpose=True): data = sio.loadmat(path) y = data.get('y') y = y.reshape(y.shape[0]) X = data.get('X') if transpose: X = np.array([im.reshape((20, 20)).T for im in X]) X = np.array([im.reshape(400) for im in X]) return X, yX, y = load_data('./data/ex3data1.mat') 画一个图12345678910def plot_an_image(image): fig, ax = plt.subplots(figsize=(1, 1)) ax.matshow(image.reshape((20, 20)), cmap=matplotlib.cm.binary) plt.xticks(np.array([])) plt.yticks(np.array([])) plt.show()pick_one = np.random.randint(0, 5000)plot_an_image(X[pick_one, :])print('this should be &#123;&#125;'.format(y[pick_one])) 画一百个图123456789101112131415def plot_100_image(X): size = int(np.sqrt(X.shape[1])) sample_idx = np.random.choice(np.array(X.shape[0]), 100) sample_images = X[sample_idx, :] fig, ax_array = plt.subplots(nrows=10, ncols=10, sharey=True, sharex=True, figsize=(8, 8)) for r in range(10): for c in range(10): ax_array[r, c].matshow(sample_images[10 * r + c].reshape((size, size)), cmap=matplotlib.cm.binary) plt.xticks(np.array([])) plt.yticks(np.array([])) plt.show() plot_100_image(X) 准备数据加载好ex3data1.mat文件后我们需要处理一下，首先X是一个(5000,400)的矩阵，我们在第一列加上一列全为1的矩阵为偏差量，y是一个(5000,)的矩阵，需要注意的是，为了兼容Oxtave和matlab，y中0的被标记为了10。我们把y分成10类整理y数据为(10,5000)的一个矩阵。 123扩展 5000*1 到 5000*10 比如 y=10 -&gt; [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]: ndarray 比如 y=1 -&gt; [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]: ndarray 1234567raw_X, raw_y = load_data('./data/ex3data1.mat')X = np.insert(raw_X, 0, values=np.ones(raw_X.shape[0]), axis = 1) # 插入了第一列 全为1y_matrix = []for k in range(1, 11): y_matrix.append((raw_y == k).astype(int))y_matrix = [y_matrix[-1]] + y_matrix[:-1]y = np.array(y_matrix) 训练一维模型处理好数据后接着写，激活函数和代价函数，代价函数的偏导数就是梯度函数，我们期望这个函数最小。给梯度函数和代价函数加入正则项。 12345678910111213141516171819202122def sigmoid(z): return 1 / (1 + np.exp(-z))def cost(theta, X, y): return np.mean(-y * np.log(sigmoid(X @ theta)) - (1 - y) * np.log(1 - sigmoid(X @ theta)))# 梯度就是jθ的在θ偏导def gradient(theta, X, y): # @ 对应元素相乘求和 return (1 / len(X)) * X.T @ (sigmoid(X @ theta) - y)def regularized_cost(theta, X, y, l=1): theta_j1_to_n = theta[1:] regularized_term = (1 / (2 * len(X))) * np.power(theta_j1_to_n, 2).sum() return cost(theta, X, y) + regularized_termdef regularized_gradient(theta, X, y, l=1): theta_j1_to_n = theta[1:] regularized_theta = (l / len(X)) * theta_j1_to_n regularized_term = np.concatenate([np.array([0]), regularized_theta]) # 在theta矩阵前接一个[0] return gradient(theta, X, y) + regularized_term 运用minimize()函数开始迭代，计算出theta，然后验证theta的准确性。 12345678910111213def logistic_regression(X, y, l=1): theta = np.zeros(X.shape[1]) res = opt.minimize(fun=regularized_cost, x0=theta, args=(X, y, l), method='TNC', jac=regularized_gradient, options=&#123;'disp': True&#125;) final_theta = res.x return final_thetadef predict(x, theta): prob = sigmoid(x @ theta) return (prob &gt;= 0.5).astype(int)t0 = logistic_regression(X, y[0])y_pred = predict(X, t0)print('Accuracy=&#123;&#125;'.format(np.mean(y[0] == y_pred))) 最终求得结果为 Accuracy=0.9974 训练K维模型1234567891011121314k_theta = np.array([logistic_regression(X, y[k]) for k in range(10)])print(k_theta.shape) # (10, 401)prob_matrix = sigmoid(X @ k_theta.T)np.set_printoptions(suppress=True) # 科学计数法表示print(prob_matrix.shape) # (5000, 10)y_pred = np.argmax(prob_matrix, axis=1)print(y_pred.shape) # (5000,)y_answer = raw_y.copy()y_answer[y_answer==10] = 0print(classification_report(y_answer, y_pred)) 1234567891011121314 precision recall f1-score support 0 0.97 0.99 0.98 500 1 0.95 0.99 0.97 500 2 0.95 0.92 0.93 500 3 0.95 0.91 0.93 500 4 0.95 0.95 0.95 500 5 0.92 0.92 0.92 500 6 0.97 0.98 0.97 500 7 0.95 0.95 0.95 500 8 0.93 0.92 0.92 500 9 0.92 0.92 0.92 500avg / total 0.94 0.94 0.94 5000 如ex3.pdf中所说，我们成功的分类出94%的例子。 Feedforward Propagation and Prediction 我们的神经网路如上图所示，它有3层构成（一个输入层，一个隐藏层a，一个输出层。）已经提供了一组训练参数（Θ1，Θ2）储存在ex3weights.mat中 123456% Load saved matrices from fileload('ex3weights.mat');% The matrices Theta1 and Theta2 will now be in your Octave% environment% Theta1 has size 25 x 401% Theta2 has size 10 x 26 1234567891011121314151617181920212223242526def load_weight(path): data = sio.loadmat(path) return data['Theta1'], data['Theta2']theta1, theta2 = load_weight('./data/ex3weights.mat')X, y = load_data('./data/ex3data1.mat',transpose=False)X = np.insert(X, 0, values=np.ones(X.shape[0]), axis=1) # intercepta1 = Xz2 = a1 @ theta1.T # (5000, 401) @ (25,401).T = (5000, 25)print(z2.shape)z2 = np.insert(z2, 0, values=np.ones(z2.shape[0]), axis=1)a2 = sigmoid(z2)z3 = a2 @ theta2.Ta3 = sigmoid(z3)y_pred = np.argmax(a3, axis=1) + 1 # numpy is 0 base index, +1 for matlab convention，返回沿轴axis最大值的索引，axis=1代表行print(classification_report(y, y_pred)) 1234567891011121314 precision recall f1-score support 1 0.97 0.98 0.97 500 2 0.98 0.97 0.97 500 3 0.98 0.96 0.97 500 4 0.97 0.97 0.97 500 5 0.98 0.98 0.98 500 6 0.97 0.99 0.98 500 7 0.98 0.97 0.97 500 8 0.98 0.98 0.98 500 9 0.97 0.96 0.96 500 10 0.98 0.99 0.99 500avg / total 0.98 0.98 0.98 5000]]></content>
      <categories>
        <category>机器学习习题</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 28.Implement strStr().]]></title>
    <url>%2F2018%2F09%2F14%2FLeetCode%2028.Implement%20strStr().%2F</url>
    <content type="text"><![CDATA[28.Implement strStr(). Return the index of the first occurrence of needle in haystack, or -1 if needle is not part of haystack. Example 1: Input: haystack = “hello”, needle = “ll” Output: 2 Example 2: Input: haystack = “aaaaa”, needle = “bba” Output: -1Clarification: What should we return when needle is an empty string? This is a great question to ask during an interview. For the purpose of this problem, we will return 0 when needle is an empty string. This is consistent to C’s strstr() and Java’s indexOf(). 三种解法，首先是暴力解法，本来我先写了这个解法试试看，结果leetcode没通过，在倒数第二个case时超时了，在处理aaaa…ab, aa…ab(非常多个a)这种情况时，时间复杂度时n*m，显然不是一个很好的算法，使用KMP算法只有O(n+m)。 感谢UP主 [KMP算法]NEXT数列手算演示 12345678910111213141516171819暴力破解法：int strStr(char* haystack, char* needle) &#123; int i = 0, j = 0, n = 0; if(strlen(haystack) &lt; strlen(needle)) return -1; if(strlen(needle) == 0) return 0; while(i &lt;= strlen(haystack)) &#123; if(haystack[i] == needle[j]) for(j = 0, n = i; j &lt; strlen(needle) &amp;&amp; haystack[n] == needle[j]; j++, n++) ; if(j == strlen(needle)) return i; i++; j = 0; &#125; return -1;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546KMP算法：int* next_arr(char* ptr)&#123; int ptr_num = strlen(ptr); int* next; int i = 0, j = 1; next = calloc(ptr_num, sizeof(int)); for(i = 1, j = 0; i &lt; ptr_num;) &#123; if(ptr[i] == ptr[j]) next[i++] = ++j; else if(j) j = next[j - 1]; else next[i++] = 0; &#125; return next;&#125;int strStr_kmp(char* haystack, char* needle) &#123; int m = strlen(haystack), n = strlen(needle); int i, j; int* next = next_arr(needle); if (!n) return 0; for (i = 0, j = 0; i &lt; m;) &#123; if (haystack[i] == needle[j]) &#123; i++; j++; &#125; if(j == n) return i - j; if (i &lt; m &amp;&amp; haystack[i] != needle[j]) &#123; if (j) j = next[j - 1]; else i++; &#125; &#125; free(next); return -1; &#125; 使用KMP算法运算速率成功的打败了100%的人，然后点了sample 0 ms submission，看到了别人的算法。。。1234567891011121314151617int strStr(char* haystack, char* needle) &#123; int a = strlen(needle); int b = strlen(haystack); if(a==0) return 0; for(int i=0;i&lt;b-a+1;i++)&#123; for(int j = 0; j&lt;b; j++)&#123; if(haystack[i+j] == needle[j])&#123; if(j == a-1) return i; &#125; else break; &#125; &#125; return -1;&#125;]]></content>
      <categories>
        <category>算法练习</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>c</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习总结]]></title>
    <url>%2F2018%2F09%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Andrew Ng的机器学习入门课程已经全部看完了，笔记也写了一些，这里总结所有所学的内容，说实话，现在完全忘记了开始所学的内容了。 什么是机器学习Arthur Samuel。他定义机器学习为，在进行特定编程的情况下给予计算学习能力的领域。 Tom Mitchell。他定义的的机器学习是，一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E后，经过P评判，程序再处理T时的性能有所提升。 周志华。他再机器学习一书中的意思是，让机器从数据中学习，进而得到一个更加符合现实规律的模型，通过对模型的使用使得机器比以往表现的更好，这就是机器学习。 我的愚见。机器学习就是在已有的数据中发现规律再寻找符合这个规律的数据。 监督学习回归（房价预测），分类（肿瘤预测），给出特征值与其对应的结果。 无监督学习聚类（新闻、邮件的分类），只根据特征值寻找其中的规律。 线性回归模型表示m：训练集中实例的数量 x：特征值/输入变量 y：目标值/输出变量 （x，y）：训练集中的实例 第i个实例：$(x^i, y^i)$ h：学习算法中的解决方案或函数，也称为假设（hypothesis） $h_\theta(x)=\theta_0+\theta_1x$ 线性回归代价函数预测函数$h_\theta(x)$是关于$x$的函数,而代价函数是一个关于$(\theta_0,\theta_1)$的函数 $J(\theta_0,\theta_1) = \frac{1}{2m} \sum^m_{i=1}(h_\theta(x^i)-y^i)^2$ 优化目标：$minimize J(\theta_0,\theta_1)$ 梯度下降 梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数$J(\theta_0,\theta_1)$的最小值。 梯度下降背后的思想是：开始时我们随机选择一个参数组合$(\theta_0, \theta_1, ......,\theta_n)$，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到一个局部最小值，因为我们没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否是全局最小值，选择不同的初始参数组合，可能回找到不同的局部最小值。 线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即： $\frac{\partial }{\partial {{\theta }{j}}}J({{\theta }{0}},{{\theta }{1}})=\frac{\partial }{\partial {{\theta }{j}}}\frac{1}{2m}{{\sum\limits_{i=1}^{m}{\left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}}^{2}}$ $j=0$ 时：$\frac{\partial }{\partial {{\theta }{0}}}J({{\theta }{0}},{{\theta }{1}})=\frac{1}{m}{{\sum\limits{i=1}^{m}{\left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}}}$ $j=1$ 时：$\frac{\partial }{\partial {{\theta }{1}}}J({{\theta }{0}},{{\theta }{1}})=\frac{1}{m}\sum\limits{i=1}^{m}{\left( \left( {{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)\cdot {{x}^{(i)}} \right)}$ 则算法写成： Repeat { ​ ${\theta_{0}}:={\theta_{0}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{ \left({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)}$ ​ ${\theta_{1}}:={\theta_{1}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{\left( \left({{h}_{\theta }}({{x}^{(i)}})-{{y}^{(i)}} \right)\cdot {{x}^{(i)}} \right)}$ ​ } 特征缩放 尝试将所有特征的尺度都尽量缩放到-1到1之间， 最简单的方法是令：${{x}{n}}=\frac{{{x}{n}}-{{\mu}{n}}}{{{s}{n}}}$，其中 ${\mu_{n}}$是平均值，${s_{n}}$是标准差。 学习速率梯度下降算法的每次迭代受到学习率的影响，如果学习率$a$过小，则达到收敛所需的迭代次数会非常高；如果学习率$a$过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。 通常可以考虑尝试些学习率： $\alpha=0.01，0.03，0.1，0.3，1，3，10$ 正规方程正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\frac{\partial}{\partial{\theta_{j}}}J\left( {\theta_{j}} \right)=0$ 。 假设我们的训练集特征矩阵为 $X$（包含了 ${{x}_{0}}=1$）并且我们的训练集结果为向量 $y$，则利用正规方程解出向量 $\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}y$ 。 梯度下降与正规方程的比较 梯度下降 正规方程 需要选择学习速率 不需要 需要多次迭代 需要计算${{\left( {X^T}X \right)}^{-1}}{X^{T}}$如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂读为$O(n^3)$，通常来说n小于一万时还可以接受 适用于各种类型的模型 只适用于线性模型，不适合逻辑回归等其他模型 总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数$\theta $的替代方法。具体地说，只要特征变量数量小于一万，通常使用标准方程法，而不使用梯度下降法。 逻辑回归逻辑回归(Logistic Regression)一般用在分类问题中。 假设函数 $h_\theta(x) = g(\theta^TX)$ $g\left( z \right)=\frac{1}{1+{{e}^{-z}}}$ X代表特征向量，g代表逻辑函数(Logistic function)，常用的逻辑函数为S形函数(Sigmoid function) 判定边界在逻辑回归中，我们预测： 当${h_\theta}\left( x \right)&gt;=0.5$时，预测 $y=1$。 当${h_\theta}\left( x \right)&lt;0.5$时，预测 $y=0$。 根据 S 形函数图像，我们知道当 $z=0$ 时 $g(z)=0.5$ $z&gt;0$ 时 $g(z)&gt;0.5$ $z&lt;0$ 时 $g(z)&lt;0.5$ 又 $z={\theta^{T}}x$，即： ${\theta^{T}}x&gt;=0$ 时，预测 $y=1$. ${\theta^{T}}x&lt;0$ 时，预测 $y=0$ 接下来看价函数 逻辑回归代价函数逻辑回归的代价函数为：$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{{Cost}\left( {h_\theta}\left( {x}^{\left( i \right)} \right),{y}^{\left( i \right)} \right)}$. ${h_\theta}\left( x \right)$与 $Cost\left( {h_\theta}\left( x \right),y \right)$之间的关系如下图所示：.这样构建的$Cost\left( {h_\theta}\left( x \right),y \right)$函数的特点是：当实际的 $y=1$ 且${h_\theta}\left( x \right)$也为 1 时误差为 0，当 $y=1$ 但${h_\theta}\left( x \right)$不为1时误差随着${h_\theta}\left( x \right)$变小而变大；当实际的 $y=0$ 且${h_\theta}\left( x \right)$也为 0 时代价为 0，当$y=0$ 但${h_\theta}\left( x \right)$不为 0时误差随着 ${h_\theta}\left( x \right)$的变大而变大。 将构建的 $Cost\left( {h_\theta}\left( x \right),y \right)$简化如下： $Cost\left( {h_\theta}\left( x \right),y \right)=-y\times log\left( {h_\theta}\left( x \right) \right)-(1-y)\times log\left( 1-{h_\theta}\left( x \right) \right)$ 带入代价函数得到：$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)]}$.即：$J\left( \theta \right)=-\frac{1}{m}\sum\limits_{i=1}^{m}{[{{y}^{(i)}}\log \left( {h_\theta}\left( {{x}^{(i)}} \right) \right)+\left( 1-{{y}^{(i)}} \right)\log \left( 1-{h_\theta}\left( {{x}^{(i)}} \right) \right)]}$.在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为：Repeat { $\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j} J(\theta)$ (simultaneously update all ) } 求导后得到： Repeat { $\theta_j := \theta_j - \alpha \frac{1}{m}\sum\limits_{i=1}^{m}{{\left( {h_\theta}\left( \mathop{x}^{\left( i \right)} \right)-\mathop{y}^{\left( i \right)} \right)}}\mathop{x}_{j}^{(i)}$ (simultaneously update all ) } 高级优化共轭梯度法 BFGS (变尺度法) L-BFGS (限制变尺度法) 线性搜索(line search) 正则化正则化可以改善或者减少过拟合问题。 $...+\frac{\lambda}{2m}\sum\limits_{j=1}^{n}\theta_j^2$ 神经网络当特征他多时，需要神经网络。 标记方法训练样本数：$m$ 输入信号：$x$ 输出信号：$y$ 神经网络层数：$L$ 每层的neuron个数：$S_1$ - $S_L$ 神经网络的分类二类分类：$S_L = 0, y = 0 or 1$ K类分类：$S_L = k, y_i = 1 (k &gt; 2)$ 代价函数$\newcommand{\subk}[1]{ #1_k }$ $$h_\theta\left(x\right)\in \mathbb{R}^{K}$$ $${\left({h_\theta}\left(x\right)\right)}_{i}={i}^{th} \text{output}$$ $J(\Theta) = -\frac{1}{m} \left[ \sum\limits_{i=1}^{m} \sum\limits_{k=1}^{k} {y_k}^{(i)} \log \subk{(h_\Theta(x^{(i)}))} + \left( 1 - y_k^{(i)} \right) \log \left( 1- \subk{\left( h_\Theta \left( x^{(i)} \right) \right)} \right) \right] + \frac{\lambda}{2m} \sum\limits_{l=1}^{L-1} \sum\limits_{i=1}^{s_l} \sum\limits_{j=1}^{s_l+1} \left( \Theta_{ji}^{(l)} \right)^2$ 反向传播向前传播的算法是: 反向传播的算法就是先正向传播计算出每一层的激活单元，然后利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播计算出直至第二层的所有误差。 在求出了$\Delta_{ij}^{(l)}$之后，我们便可以计算代价函数的偏导数了，计算方法如下： $ D_{ij}^{(l)} :=\frac{1}{m}\Delta_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}$ ${if}; j \neq 0$ $ D_{ij}^{(l)} :=\frac{1}{m}\Delta_{ij}^{(l)}$ ${if}; j = 0$ 神经网络的总结网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。 第一层的单元数即我们训练集的特征数量。 最后一层的单元数是我们训练集的结果的类的数量。 如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。 我们真正要决定的是隐藏层的层数和每个中间层的单元数。 训练神经网络： 参数的随机初始化 利用正向传播方法计算所有的$h_{\theta}(x)$ 编写计算代价函数 $J$ 的代码 利用反向传播方法计算所有偏导数 利用数值检验方法检验这些偏导数 使用优化算法来最小化代价函数]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python实现异常检测算法]]></title>
    <url>%2F2018%2F08%2F27%2FPython%E5%AE%9E%E7%8E%B0%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[应用高斯分布开发异常检测算法，这个比较简单，高斯分布也叫做正态分布，高中就学过，如果我们的数据符合高斯分布或者比较像高斯分布的时候可以使用这个算法，通过训练集计算高斯分布函数，与交叉验证集比较设置合适的Σ，当测试数据小于Σ时则为异常 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109#!/usr/bin/python# coding=utf-8import numpy as npimport matplotlib.pyplot as pltimport scipy.io as siodata = sio.loadmat('./data/ex8data1.mat');X = data['X'] # 训练集Xval = data['Xval'] # 交叉验证集Yval = data['yval']X1 = X[:, [0]]X2 = X[:, [1]]fig, ax = plt.subplots(figsize=(12,8))ax.scatter(X1, X2)# plt.plot(Xval[:, [0]], Xval[:, [1]], '.', markerfacecolor='g', markeredgecolor="k", markersize=14)plt.show()# 计算平均数 𝜇 和 方差 𝜎2def aveandvar(x): sum = np.array([0]) for i in x: sum = sum + i ave = sum/len(x) sum = 0 for i in x: sum += (i - ave)*(i - ave) var = sum/len(x) return ave, var# 计算概率密度p(x); 特征集, 平均值, 平方差def gaussian_distribution(x, u, s): px = [] for i in x: p = 1/(np.sqrt((2 * np.pi * s))) * np.exp(-((i - u) * (i - u))/(2 * s)) px.append(p) px = np.array(px) return px# 选择阈值def select_threshold(pval, yval): best_epsilon = 0 best_f1 = 0 f1 = 0 step = (pval.max() - pval.min()) / 10000 for epsilon in np.arange(pval.min(), pval.max(), step): preds = pval &lt; epsilon tp = np.sum(np.logical_and(preds == 1, yval == 1)).astype(float) fp = np.sum(np.logical_and(preds == 1, yval == 0)).astype(float) fn = np.sum(np.logical_and(preds == 0, yval == 1)).astype(float) precision = tp / (tp + fp) recall = tp / (tp + fn) f1 = (2 * precision * recall) / (precision + recall) if f1 &gt; best_f1: best_f1 = f1 best_epsilon = epsilon return best_epsilon, best_f1# 画一下这两个特征值的高斯曲线 首先要排下序X1.T.sort()X2.T.sort()u,s = aveandvar(X1)px1 = gaussian_distribution(X1, u, s)u,s = aveandvar(X2)px2 = gaussian_distribution(X2, u, s)plt.subplot(211)plt.title('X1')plt.plot(X1, px1 )plt.subplot(212)plt.title('X2')plt.plot(X2, px2 )plt.show()# 计算训练集u,s = aveandvar(X)px = gaussian_distribution(X, u, s)# 计算测试集u,s = aveandvar(Xval)tpx = gaussian_distribution(X, u, s)epsilon, f1 = select_threshold(tpx, Yval)print(epsilon, f1)# 标记出异常数据outliers = np.where(px &lt; epsilon)fig, ax = plt.subplots(figsize=(12,8))ax.scatter(X[:,0], X[:,1])ax.scatter(X[outliers[0],0], X[outliers[0],1], s=50, color='r', marker='o')plt.show()# 调库验证from scipy import statspx = np.zeros((X.shape[0], X.shape[1]))px[:,0] = stats.norm(u[0], s[0]).pdf(X[:,0])px[:,1] = stats.norm(u[1], s[1]).pdf(X[:,1])outliers = np.where(px &lt; epsilon)fig, ax = plt.subplots(figsize=(12,8))ax.scatter(X[:,0], X[:,1])ax.scatter(X[outliers[0],0], X[outliers[0],1], s=50, color='r', marker='o')plt.show() 这是我们的训练集合，明显有六个是异常数据 画出连个特征的高斯函数，比较像高斯分布 通过我自己写的高斯密度函数计算，有些过拟合，多拟合到了两个点，不知道为什么。 调用scipy的高斯函数库计算后完美的检测到了异常数据]]></content>
      <categories>
        <category>机器学习代码实现</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python实现PCA降维算法]]></title>
    <url>%2F2018%2F08%2F21%2FPython%E5%AE%9E%E7%8E%B0PCA%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[这是第二个无监督学习的算法，是一个降维算法，可以把多个特征进行压缩，我在压缩后计算了与原数据的偏差，当我把四个特征压缩为三个时偏差只有0.5%，压缩为一个特征时偏差也只有7%，当只有一个特征时把数据展开也可以轻易的分为三类，所以这是一个非常优秀的算法。 值得注意的点是在计算奇异矩阵时遇到的问题，首先我们有一个m×n（m个数据，n个特征）的矩阵X，我们希望得到一个m×k的矩阵Z，具体降维过程分三步： ·第一步：均值归一化，就是把每一个数都减去总数的平均值，得到的一个和平均数差距的新矩阵Xj。 ·第二部：计算协方差矩阵，在这里要注意的时，Xj(i)是一个n×1的矩阵，Xj(i)的转置是一个1×n的矩阵，所以他俩相乘得到一个n×n的矩阵Σ，其实就是的到一个奇异矩阵，因为只有奇异矩阵才可能有特征值。 ·第三部：奇异值分解，计算∑的特征值，使用svd()函数分解出U,S,V三个向量，U也是一个n×n的矩阵，在U中选取k个向量，获得一个n×k的矩阵Ureduce，新的特征矩阵z就等于Ureduce的转置(k×n)乘以X(n×m)结果得到一个k×m的新矩阵 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111#!/usr/bin/python# coding=utf-8from sklearn.datasets import load_irisimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dimport numpy as npfrom sklearn.cluster import KMeansclass MYPCA: def __init__(self, data, k): self.m = len(data) # 训练数据个数 self.n = len(data[0]) # 现在的特征数 self.k = k # 优化后的特征数 self.X = data # 第一步是均值归一化。我们需要计算出所有特征的均值 def data_preprocess(self): sum = 0 for i in self.X: sum += i u = sum/self.m self.newX = np.empty([0, self.n]) for i in self.X: self.newX = np.row_stack((self.newX, i - u)) return self.newX # 第二步计算协方差矩阵 传入均值归一化后的矩阵 Σ=1𝑚Σ(𝑥(𝑖))𝑛𝑖=1(𝑥(𝑖))𝑇 def covariance_matrix(self, X): sum = 0 for i in X: i = i[np.newaxis, :] sum += np.dot(i.T, i) sigma = sum/self.m return sigma # 计算新的特征向量Z def get_z(self, U, X): z = np.empty([self.k, 0]) Ureduce = U[...,0:self.k] for i in X: i = i[np.newaxis, :] t = np.dot(Ureduce.T, i.T) z = np.column_stack((z, t)) return z # 计算训练集误差 def error_analysis(self): S = self.S sigmaK = 0 sigmaN = 0 for i in range(self.n): if i &lt; self.k: sigmaK += S[i] if i &lt; self.n: sigmaN += S[i] return 1 - sigmaK/sigmaN # 恢复到之前维度 def rovecor_dimensional(self): Ureduce = self.U[..., 0:self.k] Xappox = np.dot(Ureduce, self.z) return Xappox def train(self): newX = self.data_preprocess() sigma = self.covariance_matrix(newX) self.U, self.S, self.V = np.linalg.svd(sigma) # 这里使用均值归一化后的X和原X对结果没有影响 #self.z = self.get_z(self.U, self.X) self.z = self.get_z(self.U, newX) return self.z.T# 构造训练集：引入鸢尾花数据集来作为训练集, 具有四个特征,分三类iris = load_iris()data = iris.datadata = np.array(data[:])m = len(data)#np.random.shuffle(data)# 把四个特征压缩为三个irispca = MYPCA(data, 3)z = irispca.train()error = irispca.error_analysis()print(error)x1 = z[:, [0]]x2 = z[:, [1]]x3 = z[:, [2]]fig = plt.figure()ax = fig.add_subplot(111, projection='3d')ax.scatter(x1, x2, x3, c='r', marker='*')ax.set_xlabel('x1 Label')ax.set_ylabel('x2 Label')ax.set_zlabel('x3 Label')plt.show()# 把四个特征压缩为一个irispca = MYPCA(data, 1)z = irispca.train()plt.plot(z, '.')error = irispca.error_analysis()print(error)# 使用Kmeans的算法验证一下是否还可以正确分类kmeans = KMeans(n_clusters=3, random_state=0).fit(z)kmeans_u = kmeans.cluster_centers_u = np.transpose(kmeans_u)plt.plot([m/6, m/2, 5*m/6], u[0], '*', markerfacecolor='g', markeredgecolor="k", markersize=14)plt.show()]]></content>
      <categories>
        <category>机器学习代码实现</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python实现K-均值算法]]></title>
    <url>%2F2018%2F08%2F16%2FPython%E5%AE%9E%E7%8E%B0K-%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[第一个无监督学习算法，K-均值，这是一个非常普及的聚类算法，实现起来也比较简单，学习了Andrew Ng的视频讲解，直接纪录一下重点吧。 首先训练集合选取了sklearn自带的多类单标签数据集make_blobs 初始化变量有m:训练集的个数，Feature:训练集的维度，K：要分成几类，u：一个K*Feature维度的数组，储存聚类中心，c:储存每次迭代的分类结果，uDict:储存分类结果的字典 总结：因为数据量比较少，根据观察畸变函的结果数，基本迭代三次就分类成功了，说明这是一个非常优秀的算法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import numpy as npimport matplotlib.pyplot as pltfrom sklearn.datasets.samples_generator import make_blobsfrom sklearn.cluster import KMeansimport random# 构建单标签数据集center = [[1,1],[-1,-1],[1,-1]]cluster_std = 0.3X,labels = make_blobs(n_samples=200, centers=center, n_features=2, cluster_std=cluster_std, random_state=0)unique_lables = set(labels)colors=plt.cm.Spectral(np.linspace(0, 1, len(unique_lables)))for k,col in zip(unique_lables, colors): x_k=X[labels==k] plt.plot(x_k[:,0], x_k[:,1], 'o', markerfacecolor=col, markeredgecolor="k", markersize=14)##########Feature = 2 # 特征数m = 200 # 训练数据个数# 初始化K 和 聚类中心uK = 3u = np.empty([K, Feature])for i in range(K): u[i] = random.choice(X)# c 储存分类结果和距离c = np.zeros([m,2])# 画出初始聚类中心t = np.transpose(u)plt.plot(t[0], t[1], '+', markerfacecolor='g', markeredgecolor="k", markersize=14)# 储存分类结果的字典uDict = &#123;&#125;# 移动聚类中心def MoveK(c): u = np.empty([K, Feature]) for i in range(K): uDict[i] = [] for i in range(m): for j in range(K): if(c[i][0] == j): uDict[j].append(X[i]) for i in range(K): sum = np.zeros([1, Feature]) for j in uDict[i]: sum = np.add(sum, j) u[i] = sum/len(uDict[i]) return u# 畸变函数 Distortion functiondef Distortion(u): sum = 0 for i in uDict.keys(): for j in uDict[i]: dis = np.linalg.norm(j - u[i]) sum += dis * dis return sum/m# 开始迭代for t in range(5): # 我希望找到 c[i](代表第i个数据) 距离 u[k]（聚类中心） 最小 for i in range(m): flag = True for j in range(K): dis = np.linalg.norm(X[i]-u[j]) if(flag or dis &lt; c[i][1]): flag = False c[i][0] = j c[i][1] = dis u = MoveK(c) print(Distortion(u))# 验证结果print("my kemans cluster enters:", u)kmeans = KMeans(n_clusters=3, random_state=0).fit(X)kmeans_u = kmeans.cluster_centers_print("sklearn kemans cluster enters:", kmeans_u)t = np.transpose(u)plt.plot(t[0], t[1], '*', markerfacecolor='blue', markeredgecolor="k", markersize=14)plt.show() 执行结果： 0.80930644677085140.27707959685843420.172880244245511540.172880244245511540.17288024424551154 my kemans cluster enters: [[ 0.95712283 -1.02057236] [ 1.01281413 1.06595402] [-1.03507066 -1.03233287]] sklearn kemans cluster enters: [[ 0.95712283 -1.02057236] [ 1.01281413 1.06595402] [-1.03507066 -1.03233287]] 算法成功的从+号的位置移动到五角星的位置。]]></content>
      <categories>
        <category>机器学习代码实现</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习（十二）]]></title>
    <url>%2F2018%2F08%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[十七、大规模机器学习(Large Scale Machine Learning) 17.1 大型数据集的学习参考视频: 17 - 1 - Learning With Large Datasets (6 min).mkv 如果我们有一个低方差的模型，增加数据集的规模可以帮助你获得更好的结果。我们应该怎样应对一个有100万条记录的训练集？ 以线性回归模型为例，每一次梯度下降迭代，我们都需要计算训练集的误差的平方和，如果我们的学习算法需要有20次迭代，这便已经是非常大的计算代价。 首先应该做的事是去检查一个这么大规模的训练集是否真的必要，也许我们只用1000个训练集也能获得较好的效果，我们可以绘制学习曲线来帮助判断。 17.2 随机梯度下降法参考视频: 17 - 2 - Stochastic Gradient Descent (13 min).mkv 如果我们一定需要一个大规模的训练集，我们可以尝试使用随机梯度下降法来代替批量梯度下降法。 在随机梯度下降法中，我们定义代价函数为一个单一训练实例的代价： ​ $$cost\left( \theta, \left( {x}^{(i)} , {y}^{(i)} \right) \right) = \frac{1}{2}\left( {h}_{\theta}\left({x}^{(i)}\right)-{y}^{{(i)}} \right)^{2}$$ **随机**梯度下降算法为：首先对训练集随机“洗牌”，然后： Repeat (usually anywhere between1-10){ **for** $i = 1:m${ ​ $\theta:={\theta}_{j}-\alpha\left( {h}_{\theta}\left({x}^{(i)}\right)-{y}^{(i)} \right){{x}_{j}}^{(i)}$ ​ (**for** $j=0:n$) ​ } } 随机梯度下降算法在每一次计算之后便更新参数 ${{\theta }}$ ，而不需要首先将所有的训练集求和，在梯度下降算法还没有完成一次迭代时，随机梯度下降算法便已经走出了很远。但是这样的算法存在的问题是，不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊。 17.3 小批量梯度下降参考视频: 17 - 3 - Mini-Batch Gradient Descent (6 min).mkv 小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算常数$b$次训练实例，便更新一次参数 ${{\theta }}$ 。 **Repeat** { **for** $i = 1:m${ ​ $\theta:={\theta}_{j}-\alpha\frac{1}{b}\sum_\limits{k=i}^{i+b-1}\left( {h}_{\theta}\left({x}^{(k)}\right)-{y}^{(k)} \right){{x}_{j}}^{(k)}$ ​ (**for** $j=0:n$) ​ $ i +=10 $ ​ } } 通常我们会令 $b$ 在 2-100 之间。这样做的好处在于，我们可以用向量化的方式来循环 $b$个训练实例，如果我们用的线性代数函数库比较好，能够支持平行处理，那么算法的总体表现将不受影响（与随机梯度下降相同）。 17.4 随机梯度下降收敛参考视频: 17 - 4 - Stochastic Gradient Descent Convergence (12 min). mkv 现在我们介绍随机梯度下降算法的调试，以及学习率 $α$ 的选取。 在批量梯度下降中，我们可以令代价函数$J$为迭代次数的函数，绘制图表，根据图表来判断梯度下降是否收敛。但是，在大规模的训练集的情况下，这是不现实的，因为计算代价太大了。 在随机梯度下降中，我们在每一次更新 ${{\theta }}$ 之前都计算一次代价，然后每$x$次迭代后，求出这$x$次对训练实例计算代价的平均值，然后绘制这些平均值与$x$次迭代的次数之间的函数图表。 当我们绘制这样的图表时，可能会得到一个颠簸不平但是不会明显减少的函数图像（如上面左下图中蓝线所示）。我们可以增加$α$来使得函数更加平缓，也许便能看出下降的趋势了（如上面左下图中红线所示）；或者可能函数图表仍然是颠簸不平且不下降的（如洋红色线所示），那么我们的模型本身可能存在一些错误。 如果我们得到的曲线如上面右下方所示，不断地上升，那么我们可能会需要选择一个较小的学习率$α$。 我们也可以令学习率随着迭代次数的增加而减小，例如令： ​ $$\alpha = \frac{const1}{iterationNumber + const2}$$ 随着我们不断地靠近全局最小值，通过减小学习率，我们迫使算法收敛而非在最小值附近徘徊。但是通常我们不需要这样做便能有非常好的效果了，对$α$进行调整所耗费的计算通常不值得 总结下，这段视频中，我们介绍了一种方法，近似地监测出随机梯度下降算法在最优化代价函数中的表现，这种方法不需要定时地扫描整个训练集，来算出整个样本集的代价函数，而是只需要每次对最后1000个，或者多少个样本，求一下平均值。应用这种方法，你既可以保证随机梯度下降法正在正常运转和收敛，也可以用它来调整学习速率$α$的大小。 17.5 在线学习参考视频: 17 - 5 - Online Learning (13 min).mkv 在这个视频中，讨论一种新的大规模的机器学习机制，叫做在线学习机制。在线学习机制让我们可以模型化问题。 今天，许多大型网站或者许多大型网络公司，使用不同版本的在线学习机制算法，从大批的涌入又离开网站的用户身上进行学习。特别要提及的是，如果你有一个由连续的用户流引发的连续的数据流，进入你的网站，你能做的是使用一个在线学习机制，从数据流中学习用户的偏好，然后使用这些信息来优化一些关于网站的决策。 假定你有一个提供运输服务的公司，用户们来向你询问把包裹从A地运到B地的服务，同时假定你有一个网站，让用户们可多次登陆，然后他们告诉你，他们想从哪里寄出包裹，以及包裹要寄到哪里去，也就是出发地与目的地，然后你的网站开出运输包裹的的服务价格。比如，我会收取\$50来运输你的包裹，我会收取\$20之类的，然后根据你开给用户的这个价格，用户有时会接受这个运输服务，那么这就是个正样本，有时他们会走掉，然后他们拒绝购买你的运输服务，所以，让我们假定我们想要一个学习算法来帮助我们，优化我们想给用户开出的价格。 一个算法来从中学习的时候来模型化问题在线学习算法指的是对数据流而非离线的静态数据集的学习。许多在线网站都有持续不断的用户流，对于每一个用户，网站希望能在不将数据存储到数据库中便顺利地进行算法学习。 假使我们正在经营一家物流公司，每当一个用户询问从地点A至地点B的快递费用时，我们给用户一个报价，该用户可能选择接受（$y=1$）或不接受（$y=0$）。 现在，我们希望构建一个模型，来预测用户接受报价使用我们的物流服务的可能性。因此报价是我们的一个特征，其他特征为距离，起始地点，目标地点以及特定的用户数据。模型的输出是:$p(y=1)$。 在线学习的算法与随机梯度下降算法有些类似，我们对单一的实例进行学习，而非对一个提前定义的训练集进行循环。 Repeat forever (as long as the website is running) { Get $\left(x,y\right)$ corresponding to the current user ​ $\theta:={\theta}_{j}-\alpha\left( {h}_{\theta}\left({x}\right)-{y} \right){{x}_{j}}$ ​ (**for** $j=0:n$) } 一旦对一个数据的学习完成了，我们便可以丢弃该数据，不需要再存储它了。这种方式的好处在于，我们的算法可以很好的适应用户的倾向性，算法可以针对用户的当前行为不断地更新模型以适应该用户。 每次交互事件并不只产生一个数据集，例如，我们一次给用户提供3个物流选项，用户选择2项，我们实际上可以获得3个新的训练实例，因而我们的算法可以一次从3个实例中学习并更新模型。 这些问题中的任何一个都可以被归类到标准的，拥有一个固定的样本集的机器学习问题中。或许，你可以运行一个你自己的网站，尝试运行几天，然后保存一个数据集，一个固定的数据集，然后对其运行一个学习算法。但是这些是实际的问题，在这些问题里，你会看到大公司会获取如此多的数据，真的没有必要来保存一个固定的数据集，取而代之的是你可以使用一个在线学习算法来连续的学习，从这些用户不断产生的数据中来学习。这就是在线学习机制，然后就像我们所看到的，我们所使用的这个算法与随机梯度下降算法非常类似，唯一的区别的是，我们不会使用一个固定的数据集，我们会做的是获取一个用户样本，从那个样本中学习，然后丢弃那个样本并继续下去，而且如果你对某一种应用有一个连续的数据流，这样的算法可能会非常值得考虑。当然，在线学习的一个优点就是，如果你有一个变化的用户群，又或者你在尝试预测的事情，在缓慢变化，就像你的用户的品味在缓慢变化，这个在线学习算法，可以慢慢地调试你所学习到的假设，将其调节更新到最新的用户行为。 17.6 映射化简和数据并行参考视频: 17 - 6 - Map Reduce and Data Parallelism (14 min).mkv 映射化简和数据并行对于大规模机器学习问题而言是非常重要的概念。之前提到，如果我们用批量梯度下降算法来求解大规模数据集的最优解，我们需要对整个训练集进行循环，计算偏导数和代价，再求和，计算代价非常大。如果我们能够将我们的数据集分配给不多台计算机，让每一台计算机处理数据集的一个子集，然后我们将计所的结果汇总在求和。这样的方法叫做映射简化。 具体而言，如果任何学习算法能够表达为，对训练集的函数的求和，那么便能将这个任务分配给多台计算机（或者同一台计算机的不同CPU 核心），以达到加速处理的目的。 例如，我们有400个训练实例，我们可以将批量梯度下降的求和任务分配给4台计算机进行处理： 很多高级的线性代数函数库已经能够利用多核CPU的多个核心来并行地处理矩阵运算，这也是算法的向量化实现如此重要的缘故（比调用循环快）。 十八、应用实例：图片文字识别(Application Example: Photo OCR)18.1 问题描述和流程图参考视频: 18 - 1 - Problem Description and Pipeline (7 min).mkv 图像文字识别应用所作的事是，从一张给定的图片中识别文字。这比从一份扫描文档中识别文字要复杂的多。 为了完成这样的工作，需要采取如下步骤： 文字侦测（Text detection）——将图片上的文字与其他环境对象分离开来 字符切分（Character segmentation）——将文字分割成一个个单一的字符 字符分类（Character classification）——确定每一个字符是什么可以用任务流程图来表达这个问题，每一项任务可以由一个单独的小队来负责解决： 18.2 滑动窗口参考视频: 18 - 2 - Sliding Windows (15 min).mkv 滑动窗口是一项用来从图像中抽取对象的技术。假使我们需要在一张图片中识别行人，首先要做的是用许多固定尺寸的图片来训练一个能够准确识别行人的模型。然后我们用之前训练识别行人的模型时所采用的图片尺寸在我们要进行行人识别的图片上进行剪裁，然后将剪裁得到的切片交给模型，让模型判断是否为行人，然后在图片上滑动剪裁区域重新进行剪裁，将新剪裁的切片也交给模型进行判断，如此循环直至将图片全部检测完。 一旦完成后，我们按比例放大剪裁的区域，再以新的尺寸对图片进行剪裁，将新剪裁的切片按比例缩小至模型所采纳的尺寸，交给模型进行判断，如此循环。 滑动窗口技术也被用于文字识别，首先训练模型能够区分字符与非字符，然后，运用滑动窗口技术识别字符，一旦完成了字符的识别，我们将识别得出的区域进行一些扩展，然后将重叠的区域进行合并。接着我们以宽高比作为过滤条件，过滤掉高度比宽度更大的区域（认为单词的长度通常比高度要大）。下图中绿色的区域是经过这些步骤后被认为是文字的区域，而红色的区域是被忽略的。 以上便是文字侦测阶段。下一步是训练一个模型来完成将文字分割成一个个字符的任务，需要的训练集由单个字符的图片和两个相连字符之间的图片来训练模型。 模型训练完后，我们仍然是使用滑动窗口技术来进行字符识别。 以上便是字符切分阶段。最后一个阶段是字符分类阶段，利用神经网络、支持向量机或者逻辑回归算法训练一个分类器即可。 18.3 获取大量数据和人工数据参考视频: 18 - 3 - Getting Lots of Data and Artificial Data (16 min).mkv 如果我们的模型是低方差的，那么获得更多的数据用于训练模型，是能够有更好的效果的。问题在于，我们怎样获得数据，数据不总是可以直接获得的，我们有可能需要人工地创造一些数据。 以我们的文字识别应用为例，我们可以字体网站下载各种字体，然后利用这些不同的字体配上各种不同的随机背景图片创造出一些用于训练的实例，这让我们能够获得一个无限大的训练集。这是从零开始创造实例。 另一种方法是，利用已有的数据，然后对其进行修改，例如将已有的字符图片进行一些扭曲、旋转、模糊处理。只要我们认为实际数据有可能和经过这样处理后的数据类似，我们便可以用这样的方法来创造大量的数据。 有关获得更多数据的几种方法： 人工数据合成 手动收集、标记数据 众包 18.4 上限分析：哪部分管道的接下去做参考视频: 18 - 4 - Ceiling Analysis_ What Part of the Pipeline to Work on Next(14 min).mkv 在机器学习的应用中，我们通常需要通过几个步骤才能进行最终的预测，我们如何能够知道哪一部分最值得我们花时间和精力去改善呢？这个问题可以通过上限分析来回答。 回到我们的文字识别应用中，我们的流程图如下： 流程图中每一部分的输出都是下一部分的输入，上限分析中，我们选取一部分，手工提供100%正确的输出结果，然后看应用的整体效果提升了多少。假使我们的例子中总体效果为72%的正确率。 如果我们令文字侦测部分输出的结果100%正确，发现系统的总体效果从72%提高到了89%。这意味着我们很可能会希望投入时间精力来提高我们的文字侦测部分。 接着我们手动选择数据，让字符切分输出的结果100%正确，发现系统的总体效果只提升了1%，这意味着，我们的字符切分部分可能已经足够好了。 最后我们手工选择数据，让字符分类输出的结果100%正确，系统的总体效果又提升了10%，这意味着我们可能也会应该投入更多的时间和精力来提高应用的总体表现。 十九、总结(Conclusion)19.1 总结和致谢参考视频: 19 - 1 - Summary and Thank You (5 min).mkv 欢迎来到《机器学习》课的最后一段视频。我们已经一起学习很长一段时间了。在最后这段视频中，我想快速地回顾一下这门课的主要内容，然后简单说几句想说的话。 作为这门课的结束时间，那么我们学到了些什么呢？在这门课中，我们花了大量的时间介绍了诸如线性回归、逻辑回归、神经网络、支持向量机等等一些监督学习算法，这类算法具有带标签的数据和样本，比如${{x}^{\left( i \right)}}$、${{y}^{\left( i \right)}}$。 然后我们也花了很多时间介绍无监督学习。例如 **K-均值**聚类、用于降维的主成分分析，以及当你只有一系列无标签数据 ${{x}^{\left( i \right)}}$ 时的异常检测算法。 当然，有时带标签的数据，也可以用于异常检测算法的评估。此外，我们也花时间讨论了一些特别的应用或者特别的话题，比如说推荐系统。以及大规模机器学习系统，包括并行系统和映射化简方法，还有其他一些特别的应用。比如，用于计算机视觉技术的滑动窗口分类算法。 最后，我们还提到了很多关于构建机器学习系统的实用建议。这包括了怎样理解某个机器学习算法是否正常工作的原因，所以我们谈到了偏差和方差的问题，也谈到了解决方差问题的正则化，同时我们也讨论了怎样决定接下来怎么做的问题，也就是说当你在开发一个机器学习系统时，什么工作才是接下来应该优先考虑的问题。因此我们讨论了学习算法的评价法。介绍了评价矩阵，比如：查准率、召回率以及F1分数，还有评价学习算法比较实用的训练集、交叉验证集和测试集。我们也介绍了学习算法的调试，以及如何确保学习算法的正常运行，于是我们介绍了一些诊断法，比如学习曲线，同时也讨论了误差分析、上限分析等等内容。 所有这些工具都能有效地指引你决定接下来应该怎样做，让你把宝贵的时间用在刀刃上。现在你已经掌握了很多机器学习的工具，包括监督学习算法和无监督学习算法等等。 但除了这些以外，我更希望你现在不仅仅只是认识这些工具，更重要的是掌握怎样有效地利用这些工具来建立强大的机器学习系统。所以，以上就是这门课的全部内容。如果你跟着我们的课程一路走来，到现在，你应该已经感觉到自己已经成为机器学习方面的专家了吧？ 我们都知道，机器学习是一门对科技、工业产生深远影响的重要学科，而现在，你已经完全具备了应用这些机器学习工具来创造伟大成就的能力。我希望你们中的很多人都能在相应的领域，应用所学的机器学习工具，构建出完美的机器学习系统，开发出无与伦比的产品和应用。并且我也希望你们通过应用机器学习，不仅仅改变自己的生活，有朝一日，还要让更多的人生活得更加美好！ 我也想告诉大家，教这门课对我来讲是一种享受。所以，谢谢大家！ 最后，在结束之前，我还想再多说一点：那就是，也许不久以前我也是一个学生，即使是现在，我也尽可能挤出时间听一些课，学一些新的东西。所以，我深知要坚持学完这门课是很需要花一些时间的，我知道，也许你是一个很忙的人，生活中有很多很多事情要处理。正因如此，你依然挤出时间来观看这些课程视频。我知道，很多视频的时间都长达数小时，你依然花了好多时间来做这些复习题。你们中好多人，还愿意花时间来研究那些编程练习，那些又长又复杂的编程练习。我对你们表示衷心的感谢！我知道你们很多人在这门课中都非常努力，很多人都在这门课上花了很多时间，很多人都为这门课贡献了自己的很多精力。所以，我衷心地希望你们能从这门课中有所收获！ 最后我想说！再次感谢你们选修这门课程！ Andew Ng]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习（十一）]]></title>
    <url>%2F2018%2F08%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[十五、异常检测(Anomaly Detection) 15.1 问题的动机参考文档: 15 - 1 - Problem Motivation (8 min).mkv 在接下来的一系列视频中，我将向大家介绍异常检测(Anomaly detection)问题。这是机器学习算法的一个常见应用。这种算法的一个有趣之处在于：它虽然主要用于非监督学习问题，但从某些角度看，它又类似于一些监督学习问题。 什么是异常检测呢？为了解释这个概念，让我举一个例子吧： 假想你是一个飞机引擎制造商，当你生产的飞机引擎从生产线上流出时，你需要进行QA(质量控制测试)，而作为这个测试的一部分，你测量了飞机引擎的一些特征变量，比如引擎运转时产生的热量，或者引擎的振动等等。 这样一来，你就有了一个数据集，从$x^{(1)}$到$x^{(m)}$，如果你生产了$m$个引擎的话，你将这些数据绘制成图表，看起来就是这个样子： 这里的每个点、每个叉，都是你的无标签数据。这样，异常检测问题可以定义如下：我们假设后来有一天，你有一个新的飞机引擎从生产线上流出，而你的新飞机引擎有特征变量$x_{test}$。所谓的异常检测问题就是：我们希望知道这个新的飞机引擎是否有某种异常，或者说，我们希望判断这个引擎是否需要进一步测试。因为，如果它看起来像一个正常的引擎，那么我们可以直接将它运送到客户那里，而不需要进一步的测试。 给定数据集 $x^{(1)},x^{(2)},..,x^{(m)}$，我们假使数据集是正常的，我们希望知道新的数据 $x_{test}$ 是不是异常的，即这个测试数据不属于该组数据的几率如何。我们所构建的模型应该能根据该测试数据的位置告诉我们其属于一组数据的可能性 $p(x)$。 上图中，在蓝色圈内的数据属于该组数据的可能性较高，而越是偏远的数据，其属于该组数据的可能性就越低。 这种方法称为密度估计，表达如下： $$ if \quad p(x) \begin{cases} < \varepsilon & anomaly \\ > =\varepsilon & normal \end{cases} $$ 欺诈检测： $x^{(i)} = {用户的第i个活动特征}$ 模型$p(x)$ 为我们其属于一组数据的可能性，通过$p(x) &lt; \varepsilon$检测非正常用户。 异常检测主要用来识别欺骗。例如在线采集而来的有关用户的数据，一个特征向量中可能会包含如：用户多久登录一次，访问过的页面，在论坛发布的帖子数量，甚至是打字速度等。尝试根据这些特征构建一个模型，可以用这个模型来识别那些不符合该模式的用户。 再一个例子是检测一个数据中心，特征可能包含：内存使用情况，被访问的磁盘数量，CPU的负载，网络的通信量等。根据这些特征可以构建一个模型，用来判断某些计算机是不是有可能出错了。 15.2 高斯分布参考视频: 15 - 2 - Gaussian Distribution (10 min).mkv 在这个视频中，我将介绍高斯分布，也称为正态分布。回顾高斯分布的基本知识。 通常如果我们认为变量 $x$ 符合高斯分布 $x \sim N(\mu, \sigma^2)$则其概率密度函数为：$p(x,\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$我们可以利用已有的数据来预测总体中的$μ$和$σ^2$的计算方法如下：$\mu=\frac{1}{m}\sum\limits_{i=1}^{m}x^{(i)}$ $\sigma^2=\frac{1}{m}\sum\limits_{i=1}^{m}(x^{(i)}-\mu)^2$ 高斯分布样例： 注：机器学习中对于方差我们通常只除以$m$而非统计学中的$(m-1)$。这里顺便提一下，在实际使用中，到底是选择使用$1/m$还是$1/(m-1)$其实区别很小，只要你有一个还算大的训练集，在机器学习领域大部分人更习惯使用$1/m$这个版本的公式。这两个版本的公式在理论特性和数学特性上稍有不同，但是在实际使用中，他们的区别甚小，几乎可以忽略不计。 15.3 算法参考视频: 15 - 3 - Algorithm (12 min).mkv 在本节视频中，我将应用高斯分布开发异常检测算法。 异常检测算法： 对于给定的数据集 $x^{(1)},x^{(2)},…,x^{(m)}$，我们要针对每一个特征计算 $\mu$ 和 $\sigma^2$ 的估计值。 $\mu_j=\frac{1}{m}\sum\limits_{i=1}^{m}x_j^{(i)}$ $\sigma_j^2=\frac{1}{m}\sum\limits_{i=1}^m(x_j^{(i)}-\mu_j)^2$ 一旦我们获得了平均值和方差的估计值，给定新的一个训练实例，根据模型计算 $p(x)$： $p(x)=\prod\limits_{j=1}^np(x_j;\mu_j,\sigma_j^2)=\prod\limits_{j=1}^1\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})$ 当$p(x) &lt; \varepsilon$时，为异常。 下图是一个由两个特征的训练集，以及特征的分布情况： 下面的三维图表表示的是密度估计函数，$z$轴为根据两个特征的值所估计$p(x)$值： 我们选择一个$\varepsilon$，将$p(x) = \varepsilon$作为我们的判定边界，当$p(x) &gt; \varepsilon$时预测数据为正常数据，否则为异常。 在这段视频中，我们介绍了如何拟合$p(x)$，也就是 $x$的概率值，以开发出一种异常检测算法。同时，在这节课中，我们也给出了通过给出的数据集拟合参数，进行参数估计，得到参数 $\mu$ 和 $\sigma$，然后检测新的样本，确定新样本是否是异常。 在接下来的课程中，我们将深入研究这一算法，同时更深入地介绍，怎样让算法工作地更加有效。 15.4 开发和评价一个异常检测系统参考视频: 15 - 4 - Developing and Evaluating an Anomaly Detection System (13 min). mkv 异常检测算法是一个非监督学习算法，意味着我们无法根据结果变量 $ y$ 的值来告诉我们数据是否真的是异常的。我们需要另一种方法来帮助检验算法是否有效。当我们开发一个异常检测系统时，我们从带标记（异常或正常）的数据着手，我们从其中选择一部分正常数据用于构建训练集，然后用剩下的正常数据和异常数据混合的数据构成交叉检验集和测试集。 例如：我们有10000台正常引擎的数据，有20台异常引擎的数据。 我们这样分配数据： 6000台正常引擎的数据作为训练集 2000台正常引擎和10台异常引擎的数据作为交叉检验集 2000台正常引擎和10台异常引擎的数据作为测试集 具体的评价方法如下： 根据测试集数据，我们估计特征的平均值和方差并构建$p(x)$函数 对交叉检验集，我们尝试使用不同的$\varepsilon$值作为阀值，并预测数据是否异常，根据F1值或者查准率与查全率的比例来选择 $\varepsilon$ 选出 $\varepsilon$ 后，针对测试集进行预测，计算异常检验系统的$F1$值，或者查准率与查全率之比 15.5 异常检测与监督学习对比参考视频: 15 - 5 - Anomaly Detection vs. Supervised Learning (8 min).mkv 之前我们构建的异常检测系统也使用了带标记的数据，与监督学习有些相似，下面的对比有助于选择采用监督学习还是异常检测： 两者比较： 异常检测 监督学习 非常少量的正向类（异常数据 $y=1$）, 大量的负向类（$y=0$） 同时有大量的正向类和负向类 许多不同种类的异常，非常难。根据非常 少量的正向类数据来训练算法。 有足够多的正向类实例，足够用于训练 算法，未来遇到的正向类实例可能与训练集中的非常近似。 未来遇到的异常可能与已掌握的异常、非常的不同。 例如： 欺诈行为检测 生产（例如飞机引擎）检测数据中心的计算机运行状况 例如：邮件过滤器 天气预报 肿瘤分类 希望这节课能让你明白一个学习问题的什么样的特征，能让你把这个问题当做是一个异常检测，或者是一个监督学习的问题。另外，对于很多技术公司可能会遇到的一些问题，通常来说，正样本的数量很少，甚至有时候是0，也就是说，出现了太多没见过的不同的异常类型，那么对于这些问题，通常应该使用的算法就是异常检测算法。 15.6 选择特征参考视频: 15 - 6 - Choosing What Features to Use (12 min).mkv 对于异常检测算法，我们使用的特征是至关重要的，下面谈谈如何选择特征： 异常检测假设特征符合高斯分布，如果数据的分布不是高斯分布，异常检测算法也能够工作，但是最好还是将数据转换成高斯分布，例如使用对数函数：$x= log(x+c)$，其中 $c$ 为非负常数； 或者 $x=x^c$，$c$为 0-1 之间的一个分数，等方法。(编者注：在python中，通常用np.log1p()函数，$log1p$就是 $log(x+1)$，可以避免出现负数结果，反向函数就是np.expm1()) 误差分析： 一个常见的问题是一些异常的数据可能也会有较高的$p(x)$值，因而被算法认为是正常的。这种情况下误差分析能够帮助我们，我们可以分析那些被算法错误预测为正常的数据，观察能否找出一些问题。我们可能能从问题中发现我们需要增加一些新的特征，增加这些新特征后获得的新算法能够帮助我们更好地进行异常检测。 异常检测误差分析： 我们通常可以通过将一些相关的特征进行组合，来获得一些新的更好的特征（异常数据的该特征值异常地大或小），例如，在检测数据中心的计算机状况的例子中，我们可以用CPU负载与网络通信量的比例作为一个新的特征，如果该值异常地大，便有可能意味着该服务器是陷入了一些问题中。 在这段视频中，我们介绍了如何选择特征，以及对特征进行一些小小的转换，让数据更像正态分布，然后再把数据输入异常检测算法。同时也介绍了建立特征时，进行的误差分析方法，来捕捉各种异常的可能。希望你通过这些方法，能够了解如何选择好的特征变量，从而帮助你的异常检测算法，捕捉到各种不同的异常情况。 15.7 多元高斯分布（选修）参考视频: 15 - 7 - Multivariate Gaussian Distribution (Optional) (14 min).mkv 假使我们有两个相关的特征，而且这两个特征的值域范围比较宽，这种情况下，一般的高斯分布模型可能不能很好地识别异常数据。其原因在于，一般的高斯分布模型尝试的是去同时抓住两个特征的偏差，因此创造出一个比较大的判定边界。 下图中是两个相关特征，洋红色的线（根据ε的不同其范围可大可小）是一般的高斯分布模型获得的判定边界，很明显绿色的X所代表的数据点很可能是异常值，但是其$p(x)$值却仍然在正常范围内。多元高斯分布将创建像图中蓝色曲线所示的判定边界。 在一般的高斯分布模型中，我们计算 $p(x)$ 的方法是：通过分别计算每个特征对应的几率然后将其累乘起来，在多元高斯分布模型中，我们将构建特征的协方差矩阵，用所有的特征一起来计算 $p(x)$。 我们首先计算所有特征的平均值，然后再计算协方差矩阵：$p(x)=\prod_{j=1}^np(x_j;\mu,\sigma_j^2)=\prod_{j=1}^n\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})$ $\mu=\frac{1}{m}\sum_{i=1}^mx^{(i)}$ $\Sigma = \frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T=\frac{1}{m}(X-\mu)^T(X-\mu)$ 注:其中$\mu $ 是一个向量，其每一个单元都是原特征矩阵中一行数据的均值。最后我们计算多元高斯分布的$p\left( x \right)$: $p(x)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)$ 其中： $|\Sigma|$是定矩阵，在 Octave 中用 det(sigma)计算 $\Sigma1$ 是逆矩阵，下面我们来看看协方差矩阵是如何影响模型的： 上图是5个不同的模型，从左往右依次分析： 是一个一般的高斯分布模型 通过协方差矩阵，令特征1拥有较小的偏差，同时保持特征2的偏差 通过协方差矩阵，令特征2拥有较大的偏差，同时保持特征1的偏差 通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的正相关性 通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的负相关性 多元高斯分布模型与原高斯分布模型的关系： 可以证明的是，原本的高斯分布模型是多元高斯分布模型的一个子集，即像上图中的第1、2、3，3个例子所示，如果协方差矩阵只在对角线的单位上有非零的值时，即为原本的高斯分布模型了。 原高斯分布模型和多元高斯分布模型的比较： 原高斯分布模型 多元高斯分布模型 不能捕捉特征之间的相关性 但可以通过将特征进行组合的方法来解决 自动捕捉特征之间的相关性 计算代价低，能适应大规模的特征 计算代价较高 训练集较小时也同样适用 必须要有 $m&gt;n$，不然的话协方差矩阵 不可逆的，通常需要 $m&gt;10n$ 另外特征冗余也会导致协方差矩阵不可逆 原高斯分布模型被广泛使用着，如果特征之间在某种程度上存在相互关联的情况，我们可以通过构造新新特征的方法来捕捉这些相关性。 如果训练集不是太大，并且没有太多的特征，我们可以使用多元高斯分布模型。 15.8 使用多元高斯分布进行异常检测（可选）参考视频: 15 - 8 - Anomaly Detection using the Multivariate Gaussian Distribution (Optional) (14 min).mkv 在我们谈到的最后一个视频，关于多元高斯分布，看到的一些建立的各种分布模型，当你改变参数，$\mu$ 和 $\Sigma$。在这段视频中，让我们用这些想法，并应用它们制定一个不同的异常检测算法。 要回顾一下多元高斯分布和多元正态分布： 分布有两个参数， $\mu$ 和 $\Sigma$。其中$\mu$这一个$n$维向量和 $\Sigma$ 的协方差矩阵，是一种$n\times n$的矩阵。而这里的公式$x$的概率，如按 $\mu$ 和参数化 $\Sigma$，和你的变量 $\mu$ 和 $\Sigma$，你可以得到一个范围的不同分布一样，你知道的，这些都是三个样本，那些我们在以前的视频看过了。 因此，让我们谈谈参数拟合或参数估计问题： 我有一组样本${{{ x^{(1)},x^{(2)},...,x^{(m)}} }}$是一个$n$维向量，我想我的样本来自一个多元高斯分布。我如何尝试估计我的参数 $\mu$ 和 $\Sigma$ 以及标准公式？ 估计他们是你设置 $\mu$ 是你的训练样本的平均值。 $\mu=\frac{1}{m}\sum_{i=1}^{m}x^{(i)}$并设置$\Sigma$：$\Sigma=\frac{1}{m}\sum_{i=1}^{m}(x^{(i)}-\mu)(x^{(i)}-\mu)^T$这其实只是当我们使用PCA算法时候，有 $\Sigma$ 时写出来。所以你只需插入上述两个公式，这会给你你估计的参数 $\mu$ 和你估计的参数 $\Sigma$。所以，这里给出的数据集是你如何估计 $\mu$ 和 $\Sigma$。让我们以这种方法而只需将其插入到异常检测算法。那么，我们如何把所有这一切共同开发一个异常检测算法？ 首先，我们把我们的训练集，和我们的拟合模型，我们计算$p(x)$，要知道，设定$\mu$和描述的一样$\Sigma$。 如图，该分布在中央最多，越到外面的圈的范围越小。 并在该点是出路这里的概率非常低。 原始模型与多元高斯模型的关系如图： 其中：协方差矩阵$\Sigma$为： 原始模型和多元高斯分布比较如图： 十六、推荐系统(Recommender Systems)16.1 问题形式化参考视频: 16 - 1 - Problem Formulation (8 min).mkv 在接下来的视频中，我想讲一下推荐系统。我想讲推荐系统有两个原因： 第一、仅仅因为它是机器学习中的一个重要的应用。在过去几年，我偶尔访问硅谷不同的技术公司，我常和工作在这儿致力于机器学习应用的人们聊天，我常问他们，最重要的机器学习的应用是什么，或者，你最想改进的机器学习应用有哪些。我最常听到的答案是推荐系统。现在，在硅谷有很多团体试图建立很好的推荐系统。因此，如果你考虑网站像亚马逊，或网飞公司或易趣，或iTunes Genius，有很多的网站或系统试图推荐新产品给用户。如，亚马逊推荐新书给你，网飞公司试图推荐新电影给你，等等。这些推荐系统，根据浏览你过去买过什么书，或过去评价过什么电影来判断。这些系统会带来很大一部分收入，比如为亚马逊和像网飞这样的公司。因此，对推荐系统性能的改善，将对这些企业的有实质性和直接的影响。 推荐系统是个有趣的问题，在学术机器学习中因此，我们可以去参加一个学术机器学习会议，推荐系统问题实际上受到很少的关注，或者，至少在学术界它占了很小的份额。但是，如果你看正在发生的事情，许多有能力构建这些系统的科技企业，他们似乎在很多企业中占据很高的优先级。这是我为什么在这节课讨论它的原因之一。 我想讨论推荐系统地第二个原因是：这个班视频的最后几集我想讨论机器学习中的一些大思想，并和大家分享。这节课我们也看到了，对机器学习来说，特征是很重要的，你所选择的特征，将对你学习算法的性能有很大的影响。因此，在机器学习中有一种大思想，它针对一些问题，可能并不是所有的问题，而是一些问题，有算法可以为你自动学习一套好的特征。因此，不要试图手动设计，而手写代码这是目前为止我们常干的。有一些设置，你可以有一个算法，仅仅学习其使用的特征，推荐系统就是类型设置的一个例子。还有很多其它的，但是通过推荐系统，我们将领略一小部分特征学习的思想，至少，你将能够了解到这方面的一个例子，我认为，机器学习中的大思想也是这样。因此，让我们开始讨论推荐系统问题形式化。 我们从一个例子开始定义推荐系统的问题。 假使我们是一个电影供应商，我们有 5 部电影和 4 个用户，我们要求用户为电影打分。 前三部电影是爱情片，后两部则是动作片，我们可以看出Alice和Bob似乎更倾向与爱情片， 而 Carol 和 Dave 似乎更倾向与动作片。并且没有一个用户给所有的电影都打过分。我们希望构建一个算法来预测他们每个人可能会给他们没看过的电影打多少分，并以此作为推荐的依据。 下面引入一些标记： $n_u$ 代表用户的数量 $n_m$ 代表电影的数量 $r(i, j)$ 如果用户j给电影 $i$ 评过分则 $r(i,j)=1$ $y^{(i, j)}$ 代表用户 $j$ 给电影i的评分 $m_j$代表用户 $j$ 评过分的电影的总数 16.2 基于内容的推荐系统参考视频: 16 - 2 - Content Based Recommendations (15 min).mkv 在一个基于内容的推荐系统算法中，我们假设对于我们希望推荐的东西有一些数据，这些数据是有关这些东西的特征。 在我们的例子中，我们可以假设每部电影都有两个特征，如$x_1$代表电影的浪漫程度，$x_2$ 代表电影的动作程度。 则每部电影都有一个特征向量，如$x^{(1)}$是第一部电影的特征向量为[0.9 0]。 下面我们要基于这些特征来构建一个推荐系统算法。 假设我们采用线性回归模型，我们可以针对每一个用户都训练一个线性回归模型，如${{\theta }^{(1)}}$是第一个用户的模型的参数。 于是，我们有： $\theta^{(j)}$用户 $j$ 的参数向量 $x^{(i)}$电影 $i$ 的特征向量 对于用户 $j$ 和电影 $i$，我们预测评分为：$(\theta^{(j)})^T x^{(i)}$ 代价函数 针对用户 $j$，该线性回归模型的代价为预测误差的平方和，加上正则化项：$$\min_{\theta (j)}\frac{1}{2}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\left(\theta_{k}^{(j)}\right)^2$$ 其中 $i:r(i,j)$表示我们只计算那些用户 $j$ 评过分的电影。在一般的线性回归模型中，误差项和正则项应该都是乘以$1/2m$，在这里我们将$m$去掉。并且我们不对方差项$\theta_0$进行正则化处理。 上面的代价函数只是针对一个用户的，为了学习所有用户，我们将所有用户的代价函数求和： $$ \min_{\theta^{(1)},...,\theta^{(n_u)}} \frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2 $$ 如果我们要用梯度下降法来求解最优解，我们计算代价函数的偏导数后得到梯度下降的更新公式为： $$\theta_k^{(j)}:=\theta_k^{(j)}-\alpha\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_{k}^{(i)} \quad (\text{for} \, k = 0)$$ $$\theta_k^{(j)}:=\theta_k^{(j)}-\alpha\left(\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_{k}^{(i)}+\lambda\theta_k^{(j)}\right) \quad (\text{for} \, k\neq 0)$$ 16.3 协同过滤参考视频: 16 - 3 - Collaborative Filtering (10 min).mkv 在之前的基于内容的推荐系统中，对于每一部电影，我们都掌握了可用的特征，使用这些特征训练出了每一个用户的参数。相反地，如果我们拥有用户的参数，我们可以学习得出电影的特征。 $$ \mathop{min}\limits_{x^{(1)},...,x^{(n_m)}}\frac{1}{2}\sum_{i=1}^{n_m}\sum_{j{r(i,j)=1}}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(i)})^2 $$ 但是如果我们既没有用户的参数，也没有电影的特征，这两种方法都不可行了。协同过滤算法可以同时学习这两者。 我们的优化目标便改为同时针对$x$和$\theta$进行。$$J(x^{(1)},…x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})=\frac{1}{2}\sum_{(i:j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(j)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2$$ 对代价函数求偏导数的结果如下： $$x_k^{(i)}:=x_k^{(i)}-\alpha\left(\sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\theta_k^{j}+\lambda x_k^{(i)}\right)$$ $$\theta_k^{(i)}:=\theta_k^{(i)}-\alpha\left(\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}x_k^{(i)}+\lambda \theta_k^{(j)}\right)$$ 注：在协同过滤从算法中，我们通常不使用方差项，如果需要的话，算法会自动学得。协同过滤算法使用步骤如下： 初始 $x^{(1)},x^{(1)},…x^{(nm)},\ \theta^{(1)},\theta^{(2)},…,\theta^{(n_u)}$为一些随机小值 使用梯度下降算法最小化代价函数 在训练完算法后，我们预测$(\theta^{(j)})^Tx^{(i)}$为用户 $j$ 给电影 $i$ 的评分 通过这个学习过程获得的特征矩阵包含了有关电影的重要数据，这些数据不总是人能读懂的，但是我们可以用这些数据作为给用户推荐电影的依据。 例如，如果一位用户正在观看电影 $x^{(i)}$，我们可以寻找另一部电影$x^{(j)}$，依据两部电影的特征向量之间的距离$\left\| {{x}^{(i)}}-{{x}^{(j)}} \right\|$的大小。 16.4 协同过滤算法参考视频: 16 - 4 - Collaborative Filtering Algorithm (9 min).mkv 协同过滤优化目标： 给定$x^{(1)},...,x^{(n_m)}$，估计$\theta^{(1)},...,\theta^{(n_u)}$： $$ \min_{\theta^{(1)},...,\theta^{(n_u)}}\frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2 $$ 给定$\theta^{(1)},…,\theta^{(n_u)}$，估计$x^{(1)},…,x^{(n_m)}$： 同时最小化$x^{(1)},…,x^{(n_m)}$和$\theta^{(1)},…,\theta^{(n_u)}$：$$J(x^{(1)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})=\frac{1}{2}\sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(i)})^2+\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}(\theta_k^{(j)})^2$$ $$ \min_{x^{(1)},...,x^{(n_m)} \\\ \theta^{(1)},...,\theta^{(n_u)}}J(x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)}) $$ 16.5 向量化：低秩矩阵分解参考视频: 16 - 5 - Vectorization_ Low Rank Matrix Factorization (8 min).mkv 在上几节视频中，我们谈到了协同过滤算法，本节视频中我将会讲到有关该算法的向量化实现，以及说说有关该算法你可以做的其他事情。 举例子： 当给出一件产品时，你能否找到与之相关的其它产品。 一位用户最近看上一件产品，有没有其它相关的产品，你可以推荐给他。 我将要做的是：实现一种选择的方法，写出协同过滤算法的预测情况。 我们有关于五部电影的数据集，我将要做的是，将这些用户的电影评分，进行分组并存到一个矩阵中。 我们有五部电影，以及四位用户，那么 这个矩阵 $Y$ 就是一个5行4列的矩阵，它将这些电影的用户评分数据都存在矩阵里： Movie Alice (1) Bob (2) Carol (3) Dave (4) Love at last 5 5 0 0 Romance forever 5 ? ? 0 Cute puppies of love ? 4 0 ? Nonstop car chases 0 0 5 4 Swords vs. karate 0 0 5 ? 推出评分： 找到相关影片： 现在既然你已经对特征参数向量进行了学习，那么我们就会有一个很方便的方法来度量两部电影之间的相似性。例如说：电影 $i$ 有一个特征向量$x^{(i)}$，你是否能找到一部不同的电影 $j$，保证两部电影的特征向量之间的距离$x^{(i)}$和$x^{(j)}$很小，那就能很有力地表明电影$i$和电影 $j$ 在某种程度上有相似，至少在某种意义上，某些人喜欢电影 $i$，或许更有可能也对电影 $j$ 感兴趣。总结一下，当用户在看某部电影 $i$ 的时候，如果你想找5部与电影非常相似的电影，为了能给用户推荐5部新电影，你需要做的是找出电影 $j$，在这些不同的电影中与我们要找的电影 $i$ 的距离最小，这样你就能给你的用户推荐几部不同的电影了。 通过这个方法，希望你能知道，如何进行一个向量化的计算来对所有的用户和所有的电影进行评分计算。同时希望你也能掌握，通过学习特征参数，来找到相关电影和产品的方法。 16.6 推行工作上的细节：均值归一化参考视频: 16 - 6 - Implementational Detail_ Mean Normalization (9 min).mkv 让我们来看下面的用户评分数据： 如果我们新增一个用户 Eve，并且 Eve 没有为任何电影评分，那么我们以什么为依据为Eve推荐电影呢？ 我们首先需要对结果 $Y $矩阵进行均值归一化处理，将每一个用户对某一部电影的评分减去所有用户对该电影评分的平均值： 然后我们利用这个新的 $Y$ 矩阵来训练算法。如果我们要用新训练出的算法来预测评分，则需要将平均值重新加回去，预测$(\theta^{(j)})^T x^{(i)}+\mu_i$，对于Eve，我们的新模型会认为她给每部电影的评分都是该电影的平均分。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习（十）]]></title>
    <url>%2F2018%2F08%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%EF%BC%89%2F</url>
    <content type="text"><![CDATA[十三、聚类(Clustering)无监督学习：简介参考视频: 13 - 1 - Unsupervised Learning_ Introduction (3 min).mkv 在这个视频中，我将开始介绍聚类算法。这将是一个激动人心的时刻，因为这是我们学习的第一个非监督学习算法。我们将要让计算机学习无标签数据，而不是此前的标签数据。 那么，什么是非监督学习呢？在课程的一开始，我曾简单的介绍过非监督学习，然而，我们还是有必要将其与监督学习做一下比较。 在一个典型的监督学习中，我们有一个有标签的训练集，我们的目标是找到能够区分正样本和负样本的决策边界，在这里的监督学习中，我们有一系列标签，我们需要据此拟合一个假设函数。与此不同的是，在非监督学习中，我们的数据没有附带任何标签，我们拿到的数据就是这样的： 在这里我们有一系列点，却没有标签。因此，我们的训练集可以写成只有$x^{(1)}$,$x^{(2)}$…..一直到$x^{(m)}$。我们没有任何标签$y$。因此，图上画的这些点没有标签信息。也就是说，在非监督学习中，我们需要将一系列无标签的训练数据，输入到一个算法中，然后我们告诉这个算法，快去为我们找找这个数据的内在结构给定数据。我们可能需要某种算法帮助我们寻找一种结构。图上的数据看起来可以分成两个分开的点集（称为簇），一个能够找到我圈出的这些点集的算法，就被称为聚类算法。 这将是我们介绍的第一个非监督学习算法。当然，此后我们还将提到其他类型的非监督学习算法，它们可以为我们找到其他类型的结构或者其他的一些模式，而不只是簇。 我们将先介绍聚类算法。此后，我们将陆续介绍其他算法。那么聚类算法一般用来做什么呢？ 在这门课程的早些时候，我曾经列举过一些应用：比如市场分割。也许你在数据库中存储了许多客户的信息，而你希望将他们分成不同的客户群，这样你可以对不同类型的客户分别销售产品或者分别提供更适合的服务。社交网络分析：事实上有许多研究人员正在研究这样一些内容，他们关注一群人，关注社交网络，例如Facebook，Google+，或者是其他的一些信息，比如说：你经常跟哪些人联系，而这些人又经常给哪些人发邮件，由此找到关系密切的人群。因此，这可能需要另一个聚类算法，你希望用它发现社交网络中关系密切的朋友。我有一个朋友正在研究这个问题，他希望使用聚类算法来更好的组织计算机集群，或者更好的管理数据中心。因为如果你知道数据中心中，那些计算机经常协作工作。那么，你可以重新分配资源，重新布局网络。由此优化数据中心，优化数据通信。 最后，我实际上还在研究如何利用聚类算法了解星系的形成。然后用这个知识，了解一些天文学上的细节问题。好的，这就是聚类算法。这将是我们介绍的第一个非监督学习算法。在下一个视频中，我们将开始介绍一个具体的聚类算法。 K-均值算法参考视频: 13 - 2 - K-Means Algorithm (13 min).mkv K-均值是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。 K-均值是一个迭代算法，假设我们想要将数据聚类成n个组，其方法为: 首先选择$K$个随机的点，称为聚类中心（cluster centroids）； 对于数据集中的每一个数据，按照距离$K$个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。 计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。 重复步骤2-4直至中心点不再变化。 下面是一个聚类示例： 迭代 1 次 迭代 3 次 迭代 10 次 用$μ^1$,$μ^2$,…,$μ^k$ 来表示聚类中心，用$c^{(1)}$,$c^{(2)}$,…,$c^{(m)}$来存储与第$i$个实例数据最近的聚类中心的索引，K-均值算法的伪代码如下： 1234567891011Repeat &#123;for i = 1 to mc(i) := index (form 1 to K) of cluster centroid closest to x(i)for k = 1 to Kμk := average (mean) of points assigned to cluster k&#125; 算法分为两个步骤，第一个for循环是赋值步骤，即：对于每一个样例$i$，计算其应该属于的类。第二个for循环是聚类中心的移动，即：对于每一个类$K$，重新计算该类的质心。 K-均值算法也可以很便利地用于将数据分为许多不同组，即使在没有非常明显区分的组群的情况下也可以。下图所示的数据集包含身高和体重两项特征构成的，利用K-均值算法将数据分为三类，用于帮助确定将要生产的T-恤衫的三种尺寸。 优化目标参考视频: 13 - 3 - Optimization Objective (7 min).mkv K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此K-均值的代价函数（又称畸变函数 Distortion function）为 $$J(c^{(1)},...,c^{(m)},μ_1,...,μ_K)=\dfrac {1}{m}\sum^{m}_{i=1}\left\| X^{\left( i\right) }-\mu_{c^{(i)}}\right\| ^{2}$$ 其中${{\mu }_{{{c}^{(i)}}}}$代表与${{x}^{(i)}}$最近的聚类中心点。我们的的优化目标便是找出使得代价函数最小的 $c^{(1)}$,$c^{(2)}$,...,$c^{(m)}$和$μ^1$,$μ^2$,...,$μ^k$ 回顾刚才给出的:K-均值迭代算法，我们知道，第一个循环是用于减小$c^{(i)}$引起的代价，而第二个循环则是用于减小${{\mu }_{i}}$引起的代价。迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。 随机初始化参考视频: 13 - 4 - Random Initialization (8 min).mkv 在运行K-均值算法的之前，我们首先要随机初始化所有的聚类中心点，下面介绍怎样做： 我们应该选择$K&lt;m$，即聚类中心点的个数要小于所有训练集实例的数量 随机选择$K$个训练实例，然后令$K$个聚类中心分别与这$K$个训练实例相等 K-均值的一个问题在于，它有可能会停留在一个局部最小值处，而这取决于初始化的情况。 为了解决这个问题，我们通常需要多次运行K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行K-均值的结果，选择代价函数最小的结果。这种方法在$K$较小的时候（2–10）还是可行的，但是如果$K$较大，这么做也可能不会有明显地改善。 选择聚类数参考视频: 13 - 5 - Choosing the Number of Clusters (8 min).mkv 没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考我们运用K-均值算法聚类的动机是什么，然后选择能最好服务于该目的标聚类数。 当人们在讨论，选择聚类数目的方法时，有一个可能会谈及的方法叫作“肘部法则”。关于“肘部法则”，我们所需要做的是改变$K$值，也就是聚类类别数目的总数。我们用一个聚类来运行K均值聚类方法。这就意味着，所有的数据都会分到一个聚类里，然后计算成本函数或者计算畸变函数$J$。$K$代表聚类数字。 我们可能会得到一条类似于这样的曲线。像一个人的肘部。这就是“肘部法则”所做的，让我们来看这样一个图，看起来就好像有一个很清楚的肘在那儿。好像人的手臂，如果你伸出你的胳膊，那么这就是你的肩关节、肘关节、手。这就是“肘部法则”。你会发现这种模式，它的畸变值会迅速下降，从1到2，从2到3之后，你会在3的时候达到一个肘点。在此之后，畸变值就下降的非常慢，看起来就像使用3个聚类来进行聚类是正确的，这是因为那个点是曲线的肘点，畸变值下降得很快，$K=3$之后就下降得很慢，那么我们就选$K=3$。当你应用“肘部法则”的时候，如果你得到了一个像上面这样的图，那么这将是一种用来选择聚类个数的合理方法。 例如，我们的 T-恤制造例子中，我们要将用户按照身材聚类，我们可以分成3个尺寸:$S,M,L$，也可以分成5个尺寸$XS,S,M,L,XL$，这样的选择是建立在回答“聚类后我们制造的T-恤是否能较好地适合我们的客户”这个问题的基础上作出的。聚类参考资料：1.相似度/距离计算方法总结(1). 闵可夫斯基距离Minkowski（其中欧式距离：$p=2$)$dist(X,Y)={{\left( {{\sum\limits_{i=1}^{n}{\left| {{x}_{i}}-{{y}_{i}} \right|}}^{p}} \right)}^{\frac{1}{p}}}$ (2). 杰卡德相似系数(Jaccard)： $J(A,B)=\frac{\left| A\cap B \right|}{\left|A\cup B \right|}$ (3). 余弦相似度(cosine similarity)： $n$维向量$x$和$y$的夹角记做$\theta$，根据余弦定理，其余弦值为： $cos (\theta )=\frac{{{x}^{T}}y}{\left|x \right|\cdot \left| y \right|}=\frac{\sum\limits_{i=1}^{n}{{{x}_{i}}{{y}_{i}}}}{\sqrt{\sum\limits_{i=1}^{n}{{{x}_{i}}^{2}}}\sqrt{\sum\limits_{i=1}^{n}{{{y}_{i}}^{2}}}}$ (4). Pearson皮尔逊相关系数：${{\rho }_{XY}}=\frac{\operatorname{cov}(X,Y)}{{{\sigma }_{X}}{{\sigma }_{Y}}}=\frac{E[(X-{{\mu }_{X}})(Y-{{\mu }_{Y}})]}{{{\sigma }_{X}}{{\sigma }_{Y}}}=\frac{\sum\limits_{i=1}^{n}{(x-{{\mu }_{X}})(y-{{\mu }_{Y}})}}{\sqrt{\sum\limits_{i=1}^{n}{{{(x-{{\mu }_{X}})}^{2}}}}\sqrt{\sum\limits_{i=1}^{n}{{{(y-{{\mu }_{Y}})}^{2}}}}}$ Pearson相关系数即将$x$、$y$坐标向量各自平移到原点后的夹角余弦。 2.聚类的衡量指标 (1). 均一性：$p$ 类似于精确率，一个簇中只包含一个类别的样本，则满足均一性。其实也可以认为就是正确率(每个 聚簇中正确分类的样本数占该聚簇总样本数的比例和) (2). 完整性：$r$ 类似于召回率，同类别样本被归类到相同簇中，则满足完整性;每个聚簇中正确分类的样本数占该类型的总样本数比例的和 (3). V-measure: 均一性和完整性的加权平均 $V = \frac{(1+\beta^2)*pr}{\beta^2*p+r}$ (4). 轮廓系数 样本$i$的轮廓系数：$s(i)$ 簇内不相似度:计算样本$i$到同簇其它样本的平均距离为$a(i)$，应尽可能小。 簇间不相似度:计算样本$i$到其它簇$C_j$的所有样本的平均距离$b_{ij}$，应尽可能大。 轮廓系数：$s(i)$值越接近1表示样本$i$聚类越合理，越接近-1，表示样本$i$应该分类到 另外的簇中，近似为0，表示样本$i$应该在边界上;所有样本的$s(i)$的均值被成为聚类结果的轮廓系数。$s(i) = \frac{b(i)-a(i)}{max\{a(i),b(i)\}}$ (5). ARI 数据集$S$共有$N$个元素， 两个聚类结果分别是： $X=\{{{X}_{1}},{{X}_{2}},...,{{X}_{r}}\},Y=\{{{Y}_{1}},{{Y}_{2}},...,{{Y}_{s}}\}$ $X$和$Y$的元素个数为： $a=\{{{a}_{1}},{{a}_{2}},...,{{a}_{r}}\},b=\{{{b}_{1}},{{b}_{2}},...,{{b}_{s}}\}$ 记：${{n}_{ij}}=\left| {{X}_{i}}\cap {{Y}_{i}} \right|$ $ARI=\frac{\sum\limits_{i,j}{C_{{{n}_{ij}}}^{2}}-\left[ \left( \sum\limits_{i}{C_{{{a}_{i}}}^{2}} \right)\cdot \left( \sum\limits_{i}{C_{{{b}_{i}}}^{2}} \right) \right]/C_{n}^{2}}{\frac{1}{2}\left[ \left( \sum\limits_{i}{C_{{{a}_{i}}}^{2}} \right)+\left( \sum\limits_{i}{C_{{{b}_{i}}}^{2}} \right) \right]-\left[ \left( \sum\limits_{i}{C_{{{a}_{i}}}^{2}} \right)\cdot \left( \sum\limits_{i}{C_{{{b}_{i}}}^{2}} \right) \right]/C_{n}^{2}}$ 十四、降维(Dimensionality Reduction)动机一：数据压缩参考视频: 14 - 1 - Motivation I_ Data Compression (10 min).mkv 这个视频，我想开始谈论第二种类型的无监督学习问题，称为降维。有几个不同的的原因使你可能想要做降维。一是数据压缩，后面我们会看了一些视频后，数据压缩不仅允许我们压缩数据，因而使用较少的计算机内存或磁盘空间，但它也让我们加快我们的学习算法。 但首先，让我们谈论降维是什么。作为一种生动的例子，我们收集的数据集，有许多，许多特征，我绘制两个在这里。 假设我们未知两个的特征：$x_1$:长度：用厘米表示；$x_2$：是用英寸表示同一物体的长度。 所以，这给了我们高度冗余表示，也许不是两个分开的特征$x_1$和$x_2$，这两个基本的长度度量，也许我们想要做的是减少数据到一维，只有一个数测量这个长度。这个例子似乎有点做作，这里厘米英寸的例子实际上不是那么不切实际的，两者并没有什么不同。 将数据从二维降至一维：假使我们要采用两种不同的仪器来测量一些东西的尺寸，其中一个仪器测量结果的单位是英寸，另一个仪器测量的结果是厘米，我们希望将测量的结果作为我们机器学习的特征。现在的问题的是，两种仪器对同一个东西测量的结果不完全相等（由于误差、精度等），而将两者都作为特征有些重复，因而，我们希望将这个二维的数据降至一维。 从这件事情我看到的东西发生在工业上的事。如果你有几百个或成千上万的特征，它是它这往往容易失去你需要的特征。有时可能有几个不同的工程团队，也许一个工程队给你二百个特征，第二工程队给你另外三百个的特征，第三工程队给你五百个特征，一千多个特征都在一起，它实际上会变得非常困难，去跟踪你知道的那些特征，你从那些工程队得到的。其实不想有高度冗余的特征一样。 多年我一直在研究直升飞机自动驾驶。诸如此类。如果你想测量——如果你想做，你知道，做一个调查或做这些不同飞行员的测试——你可能有一个特征：$x_1$，这也许是他们的技能（直升机飞行员），也许$x_2$可能是飞行员的爱好。这是表示他们是否喜欢飞行，也许这两个特征将高度相关。你真正关心的可能是这条红线的方向，不同的特征，决定飞行员的能力。 将数据从三维降至二维：这个例子中我们要将一个三维的特征向量降至一个二维的特征向量。过程是与上面类似的，我们将三维向量投射到一个二维的平面上，强迫使得所有的数据都在同一个平面上，降至二维的特征向量。 这样的处理过程可以被用于把任何维度的数据降到任何想要的维度，例如将1000维的特征降至100维。 正如我们所看到的，最后，这将使我们能够使我们的一些学习算法运行也较晚，但我们会在以后的视频提到它。 动机二：数据可视化参考视频: 14 - 2 - Motivation II_ Visualization (6 min).mkv 在许多及其学习问题中，如果我们能将数据可视化，我们便能寻找到一个更好的解决方案，降维可以帮助我们。 假使我们有有关于许多不同国家的数据，每一个特征向量都有50个特征（如GDP，人均GDP，平均寿命等）。如果要将这个50维的数据可视化是不可能的。使用降维的方法将其降至2维，我们便可以将其可视化了。 这样做的问题在于，降维的算法只负责减少维数，新产生的特征的意义就必须由我们自己去发现了。 主成分分析问题参考视频: 14 - 3 - Principal Component Analysis Problem Formulation (9 min). mkv 主成分分析(PCA)是最常见的降维算法。 在PCA中，我们要做的是找到一个方向向量（Vector direction），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。 下面给出主成分分析问题的描述： 问题是要将$n$维数据降至$k$维，目标是找到向量$u^{(1)}$,$u^{(2)}$,…,$u^{(k)}$使得总的投射误差最小。主成分分析与线性回顾的比较： 主成分分析与线性回归是两种不同的算法。主成分分析最小化的是投射误差（Projected Error），而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。 上图中，左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差（垂直于红线投影）。 PCA将$n$个特征降维到$k$个，可以用来进行数据压缩，如果100维的向量最后可以用10维来表示，那么压缩率为90%。同样图像处理领域的KL变换使用PCA做图像压缩。但PCA 要保证降维后，还要保证数据的特性损失最小。 PCA技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。 PCA技术的一个很大的优点是，它是完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。 但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。 主成分分析算法参考视频: 14 - 4 - Principal Component Analysis Algorithm (15 min).mkv PCA 减少$n$维到$k$维： 第一步是均值归一化。我们需要计算出所有特征的均值，然后令 $x_j= x_j-μ_j$。如果特征是在不同的数量级上，我们还需要将其除以标准差 $σ^2$。第二步是计算协方差矩阵（covariance matrix）$Σ$：$\sum=\dfrac {1}{m}\sum^{n}_{i=1}\left( x^{(i)}\right) \left( x^{(i)}\right) ^{T}$ 第三步是计算协方差矩阵$Σ$的特征向量（eigenvectors）: 在 Octave 里我们可以利用奇异值分解（singular value decomposition）来求解，[U, S, V]= svd(sigma)。 $$Sigma=\dfrac {1}{m}\sum^{n}_{i=1}\left( x^{(i)}\right) \left( x^{(i)}\right) ^{T}$$ 对于一个 $n×n$维度的矩阵，上式中的$U$是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从$n$维降至$k$维，我们只需要从$U$中选取前$k$个向量，获得一个$n×k$维度的矩阵，我们用$U_{reduce}$表示，然后通过如下计算获得要求的新特征向量$z^{(i)}$:$$z^{(i)}=U^{T}_{reduce}*x^{(i)}$$ 其中$x$是$n×1$维的，因此结果为$k×1$维度。注，我们不对方差特征进行处理。 选择主成分的数量参考视频: 14 - 5 - Choosing The Number Of Principal Components (13 min).mkv 主要成分分析是减少投射的平均均方误差： 训练集的方差为：$\dfrac {1}{m}\sum^{m}_{i=1}\left\| x^{\left( i\right) }\right\| ^{2}$ 我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的$k$值。 如果我们希望这个比例小于1%，就意味着原本数据的偏差有99%都保留下来了，如果我们选择保留95%的偏差，便能非常显著地降低模型中特征的维度了。 我们可以先令$k=1$，然后进行主要成分分析，获得$U_{reduce}$和$z$，然后计算比例是否小于1%。如果不是的话再令$k=2$，如此类推，直到找到可以使得比例小于1%的最小$k$ 值（原因是各个特征之间通常情况存在某种相关性）。 还有一些更好的方式来选择$k$，当我们在Octave中调用“svd”函数的时候，我们获得三个参数：[U, S, V] = svd(sigma)。 其中的$S$是一个$n×n$的矩阵，只有对角线上有值，而其它单元都是0，我们可以使用这个矩阵来计算平均均方误差与训练集方差的比例：$$\dfrac {\dfrac {1}{m}\sum^{m}_{i=1}\left\| x^{\left( i\right) }-x^{\left( i\right) }_{approx}\right\| ^{2}}{\dfrac {1}{m}\sum^{m}_{i=1}\left\| x^{(i)}\right\| ^{2}}=1-\dfrac {\Sigma^{k}_{i=1}S_{ii}}{\Sigma^{m}_{i=1}S_{ii}}\leq 1\%$$ 也就是：$$\frac {\Sigma^{k}_{i=1}s_{ii}}{\Sigma^{n}_{i=1}s_{ii}}\geq0.99$$ 在压缩过数据后，我们可以采用如下方法来近似地获得原有的特征：$$x^{\left( i\right) }_{approx}=U_{reduce}z^{(i)}$$ 重建的压缩表示参考视频: 14 - 6 - Reconstruction from Compressed Representation (4 min).mkv 在以前的视频中，我谈论PCA作为压缩算法。在那里你可能需要把1000维的数据压缩100维特征，或具有三维数据压缩到一二维表示。所以，如果这是一个压缩算法，应该能回到这个压缩表示，回到你原有的高维数据的一种近似。 所以，给定的$z^{(i)}$，这可能100维，怎么回到你原来的表示$x^{(i)}$，这可能是1000维的数组？ PCA算法，我们可能有一个这样的样本。如图中样本$x^{(1)}$,$x^{(2)}$。我们做的是，我们把这些样本投射到图中这个一维平面。然后现在我们需要只使用一个实数，比如$z^{(1)}$，指定这些点的位置后他们被投射到这一个三维曲面。给定一个点$z^{(1)}$，我们怎么能回去这个原始的二维空间呢？$x$为2维，z为1维，$z=U^{T}_{reduce}x$，相反的方程为：$x_{appox}=U_{reduce}\cdot z$,$x_{appox}\approx x$。如图： 如你所知，这是一个漂亮的与原始数据相当相似。所以，这就是你从低维表示$z$回到未压缩的表示。我们得到的数据的一个之间你的原始数据 $x$，我们也把这个过程称为重建原始数据。 当我们认为试图重建从压缩表示 $x$ 的初始值。所以，给定未标记的数据集，您现在知道如何应用PCA，你的带高维特征$x$和映射到这的低维表示$z$。这个视频，希望你现在也知道如何采取这些低维表示$z$，映射到备份到一个近似你原有的高维数据。 现在你知道如何实施应用PCA，我们将要做的事是谈论一些技术在实际使用PCA很好，特别是，在接下来的视频中，我想谈一谈关于如何选择$k$。 主成分分析法的应用建议参考视频: 14 - 7 - Advice for Applying PCA (13 min).mkv 假使我们正在针对一张 100×100像素的图片进行某个计算机视觉的机器学习，即总共有10000 个特征。 第一步是运用主要成分分析将数据压缩至1000个特征 然后对训练集运行学习算法 在预测时，采用之前学习而来的$U_{reduce}$将输入的特征$x$转换成特征向量$z$，然后再进行预测 注：如果我们有交叉验证集合测试集，也采用对训练集学习而来的$U_{reduce}$。 错误的主要成分分析情况：一个常见错误使用主要成分分析的情况是，将其用于减少过拟合（减少了特征的数量）。这样做非常不好，不如尝试正则化处理。原因在于主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。 另一个常见的错误是，默认地将主要成分分析作为学习过程中的一部分，这虽然很多时候有效果，最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习（九）]]></title>
    <url>%2F2018%2F08%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%2F</url>
    <content type="text"><![CDATA[十二、支持向量机(Support Vector Machines) 12.1 优化目标参考视频: 12 - 1 - Optimization Objective (15 min).mkv 到目前为止,你已经见过一系列不同的学习算法。在监督学习中，许多学习算法的性能都非常类似，因此，重要的不是你该选择使用学习算法A还是学习算法B，而更重要的是，应用这些算法时，所创建的大量数据在应用这些算法时，表现情况通常依赖于你的水平。比如：你为学习算法所设计的特征量的选择，以及如何选择正则化参数，诸如此类的事。还有一个更加强大的算法广泛的应用于工业界和学术界，它被称为支持向量机(Support Vector Machine)。与逻辑回归和神经网络相比，支持向量机，或者简称SVM，在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。因此，在接下来的视频中，我会探讨这一算法。在稍后的课程中，我也会对监督学习算法进行简要的总结。当然，仅仅是作简要描述。但对于支持向量机，鉴于该算法的强大和受欢迎度，在本课中，我会花许多时间来讲解它。它也是我们所介绍的最后一个监督学习算法。 正如我们之前开发的学习算法，我们从优化目标开始。那么，我们开始学习这个算法。为了描述支持向量机，事实上，我将会从逻辑回归开始展示我们如何一点一点修改来得到本质上的支持向量机。 那么，在逻辑回归中我们已经熟悉了这里的假设函数形式，和右边的S型激励函数。然而，为了解释一些数学知识.我将用$z$ 表示$\theta^Tx$。 现在考虑下我们想要逻辑回归做什么：如果有一个 $y=1$的样本，我的意思是不管是在训练集中或是在测试集中，又或者在交叉验证集中，总之是 $y=1$，现在我们希望${{h}_{\theta }}\left( x \right)$ 趋近1。因为我们想要正确地将此样本分类，这就意味着当 ${{h}_{\theta }}\left( x \right)$趋近于1时，$\theta^Tx$ 应当远大于0，这里的$>>$意思是远远大于0。这是因为由于 $z$ 表示 $\theta^Tx$，当 $z$远大于0时，即到了该图的右边，你不难发现此时逻辑回归的输出将趋近于1。相反地，如果我们有另一个样本，即$y=0$。我们希望假设函数的输出值将趋近于0，这对应于$\theta^Tx$，或者就是 $z$ 会远小于0，因为对应的假设函数的输出值趋近0。 如果你进一步观察逻辑回归的代价函数，你会发现每个样本 $(x,y)$都会为总代价函数，增加这里的一项，因此，对于总代价函数通常会有对所有的训练样本求和，并且这里还有一个$1/m$项，但是，在逻辑回归中，这里的这一项就是表示一个训练样本所对应的表达式。现在，如果我将完整定义的假设函数代入这里。那么，我们就会得到每一个训练样本都影响这一项。 现在，先忽略 $1/m$ 这一项，但是这一项是影响整个总代价函数中的这一项的。 现在，一起来考虑两种情况： 一种是$y$等于1的情况；另一种是 $y$ 等于0的情况。 在第一种情况中，假设 $y=1$ ，此时在目标函数中只需有第一项起作用，因为$y=1$时，$(1-y)$项将等于0。因此，当在 $y=1$ 的样本中时，即在 $(x, y) $中 ，我们得到 $y=1$ $-\log(1-\frac{1}{1+e^{-z}})$这样一项，这里同上一张幻灯片一致。 我用 $z$ 表示$\theta^Tx$，即： $z= \theta^Tx$。当然，在代价函数中，$y$ 前面有负号。我们只是这样表示，如果 $y=1$ 代价函数中，这一项也等于1。这样做是为了简化此处的表达式。如果画出关于$z$ 的函数，你会看到左下角的这条曲线，我们同样可以看到，当$z$ 增大时，也就是相当于$\theta^Tx$增大时，$z$ 对应的值会变的非常小。对整个代价函数而言，影响也非常小。这也就解释了，为什么逻辑回归在观察到正样本$y=1$时，试图将$\theta^Tx$设置得非常大。因为，在代价函数中的这一项会变的非常小。 现在开始建立支持向量机，我们从这里开始： 我们会从这个代价函数开始，也就是$-\log(1-\frac{1}{1+e^{-z}})$一点一点修改，让我取这里的$z=1$ 点，我先画出将要用的代价函数。 新的代价函数将会水平的从这里到右边(图外)，然后我再画一条同逻辑回归非常相似的直线，但是，在这里是一条直线，也就是我用紫红色画的曲线，就是这条紫红色的曲线。那么，到了这里已经非常接近逻辑回归中使用的代价函数了。只是这里是由两条线段组成，即位于右边的水平部分和位于左边的直线部分，先别过多的考虑左边直线部分的斜率，这并不是很重要。但是，这里我们将使用的新的代价函数，是在$y=1$的前提下的。你也许能想到，这应该能做同逻辑回归中类似的事情，但事实上，在之后的优化问题中，这会变得更坚定，并且为支持向量机，带来计算上的优势。例如，更容易计算股票交易的问题等等。 目前，我们只是讨论了$y=1$的情况，另外一种情况是当$y=0$时，此时如果你仔细观察代价函数只留下了第二项，因为第一项被消除了。如果当$y=0$时，那么这一项也就是0了。所以上述表达式只留下了第二项。因此，这个样本的代价或是代价函数的贡献。将会由这一项表示。并且，如果你将这一项作为$z$的函数，那么，这里就会得到横轴$z$。现在，你完成了支持向量机中的部分内容，同样地，我们要替代这一条蓝色的线，用相似的方法。 如果我们用一个新的代价函数来代替，即这条从0点开始的水平直线，然后是一条斜线，像上图。那么，现在让我给这两个方程命名，左边的函数，我称之为${\cos}t_1{(z)}$，同时，右边函数我称它为${\cos}t_0{(z)}$。这里的下标是指在代价函数中，对应的 $y=1$ 和 $y=0$ 的情况，拥有了这些定义后，现在，我们就开始构建支持向量机。 这是我们在逻辑回归中使用代价函数$J(\theta)$。也许这个方程看起来不是非常熟悉。这是因为之前有个负号在方程外面，但是，这里我所做的是，将负号移到了表达式的里面，这样做使得方程看起来有些不同。对于支持向量机而言，实质上我们要将这替换为${\cos}t_1{(z)}$，也就是${\cos}t_1{(\theta^Tx)}$，同样地，我也将这一项替换为${\cos}t_0{(z)}$，也就是代价${\cos}t_0{(\theta^Tx)}$。这里的代价函数${\cos}t_1$，就是之前所提到的那条线。此外，代价函数${\cos}t_0$，也是上面所介绍过的那条线。因此，对于支持向量机，我们得到了这里的最小化问题，即: 然后，再加上正则化参数。现在，按照支持向量机的惯例，事实上，我们的书写会稍微有些不同，代价函数的参数表示也会稍微有些不同。 首先，我们要除去$1/m$这一项，当然，这仅仅是由于人们使用支持向量机时，对比于逻辑回归而言，不同的习惯所致，但这里我所说的意思是：你知道，我将要做的是仅仅除去$1/m$这一项，但是，这也会得出同样的 $\theta$ 最优值，好的，因为$1/m$ 仅是个常量，因此，你知道在这个最小化问题中，无论前面是否有$1/m$ 这一项，最终我所得到的最优值$\theta$都是一样的。这里我的意思是，先给你举一个实例，假定有一最小化问题：即要求当$(u-5)^2+1$取得最小值时的$u$值，这时最小值为：当$u=5$时取得最小值。 现在，如果我们想要将这个目标函数乘上常数10，这里我的最小化问题就变成了：求使得$10×(u-5)^2+10$最小的值$u$，然而，使得这里最小的$u$值仍为5。因此将一些常数乘以你的最小化项，这并不会改变最小化该方程时得到$u$值。因此，这里我所做的是删去常量$m$。也相同的，我将目标函数乘上一个常量$m$，并不会改变取得最小值时的$\theta$值。 第二点概念上的变化，我们只是指在使用支持向量机时，一些如下的标准惯例，而不是逻辑回归。因此，对于逻辑回归，在目标函数中，我们有两项：第一个是训练样本的代价，第二个是我们的正则化项，我们不得不去用这一项来平衡。这就相当于我们想要最小化$A$加上正则化参数$\lambda$，然后乘以其他项$B$对吧？这里的$A$表示这里的第一项，同时我用B表示第二项，但不包括$\lambda$，我们不是优化这里的$A+\lambda\times B$。我们所做的是通过设置不同正则参数$\lambda$达到优化目的。这样，我们就能够权衡对应的项，是使得训练样本拟合的更好。即最小化$A$。还是保证正则参数足够小，也即是对于B项而言，但对于支持向量机，按照惯例，我们将使用一个不同的参数替换这里使用的$\lambda$来权衡这两项。你知道，就是第一项和第二项我们依照惯例使用一个不同的参数称为$C$，同时改为优化目标，$C×A+B$因此，在逻辑回归中，如果给定$\lambda$，一个非常大的值，意味着给予B更大的权重。而这里，就对应于将$C$ 设定为非常小的值，那么，相应的将会给$B$比给$A$更大的权重。因此，这只是一种不同的方式来控制这种权衡或者一种不同的方法，即用参数来决定是更关心第一项的优化，还是更关心第二项的优化。当然你也可以把这里的参数$C$ 考虑成$1/\lambda$，同 $1/\lambda$所扮演的角色相同，并且这两个方程或这两个表达式并不相同，因为$C=1/\lambda$，但是也并不全是这样，如果当$C=1/\lambda$时，这两个优化目标应当得到相同的值，相同的最优值 $\theta$。因此，就用它们来代替。那么，我现在删掉这里的$\lambda$，并且用常数$C$来代替。因此，这就得到了在支持向量机中我们的整个优化目标函数。然后最小化这个目标函数，得到SVM 学习到的参数$C$。 最后有别于逻辑回归输出的概率。在这里，我们的代价函数，当最小化代价函数，获得参数$\theta$时，支持向量机所做的是它来直接预测$y$的值等于1，还是等于0。因此，这个假设函数会预测1。当$\theta^Tx$大于或者等于0时，或者等于0时，所以学习参数$\theta$就是支持向量机假设函数的形式。那么，这就是支持向量机数学上的定义。 在接下来的视频中，让我们再回去从直观的角度看看优化目标，实际上是在做什么，以及SVM的假设函数将会学习什么，同时也会谈谈如何做些许修改，学习更加复杂、非线性的函数。 12.2 大边界的直观理解参考视频: 12 - 2 - Large Margin Intuition (11 min).mkv 人们有时将支持向量机看作是大间距分类器。在这一部分，我将介绍其中的含义，这有助于我们直观理解SVM模型的假设是什么样的。 这是我的支持向量机模型的代价函数，在左边这里我画出了关于z的代价函数${\cos}t_1{(z)}$，此函数用于正样本，而在右边这里我画出了关于$z$的代价函数${\cos}t_0{(z)}$，横轴表示$z$，现在让我们考虑一下，最小化这些代价函数的必要条件是什么。如果你有一个正样本，$y=1$，则只有在$z>=1$时，代价函数${\cos}t_1{(z)}$才等于0。 换句话说，如果你有一个正样本，我们会希望$\theta^Tx$>=1，反之，如果$y=0$，我们观察一下，函数${\cos}t_0{(z)}$，它只有在$z0大的话，我们的模型代价函数值为0，类似地，如果你有一个负样本，则仅需要$\theta^Tx$\0，我们需要的是比0值大很多，比如大于等于1，我也想这个比0小很多，比如我希望它小于等于-1，这就相当于在支持向量机中嵌入了一个额外的安全因子，或者说安全的间距因子。 当然，逻辑回归做了类似的事情。但是让我们看一下，在支持向量机中，这个因子会导致什么结果。具体而言，我接下来会考虑一个特例。我们将这个常数$C$设置成一个非常大的值。比如我们假设$C$的值为100000或者其它非常大的数，然后来观察支持向量机会给出什么结果？ 如果 $C$非常大，则最小化代价函数的时候，我们将会很希望找到一个使第一项为0的最优解。因此，让我们尝试在代价项的第一项为0的情形下理解该优化问题。比如我们可以把$C$设置成了非常大的常数，这将给我们一些关于支持向量机模型的直观感受。 ​ $$\min_\limits{\theta}C\sum_\limits{i=1}^{m}\left[y^{(i)}{\cos}t_{1}\left(\theta^{T}x^{(i)}\right)+\left(1-y^{(i)}\right){\cos}t\left(\theta^{T}x^{(i)}\right)\right]+\frac{1}{2}\sum_\limits{i=1}^{n}\theta^{2}_{j}$$ 我们已经看到输入一个训练样本标签为$y=1$，你想令第一项为0，你需要做的是找到一个$\theta$，使得$\theta^Tx>=1$，类似地，对于一个训练样本，标签为$y=0$，为了使${\cos}t_0{(z)}$ 函数的值为0，我们需要$\theta^Tx=1$，如果 $y^{(i)}$是等于1 的，$\theta^Tx^{(i)}=1$ 或者$θ^Tx^{(i)}=1$这个约束所代替的。因为$θ^Tx^{(i)}=p^{(i)}\cdot{\left\| \theta \right\|}$ ，将其写入我们的优化目标。我们将会得到没有了约束，$θ^Tx^{(i)}$而变成了$p^{(i)}\cdot{\left\| \theta \right\|}$。 需要提醒一点，我们之前曾讲过这个优化目标函数可以被写成等于$\frac{1}{2}\left\| \theta \right\|^2$。 现在让我们考虑下面这里的训练样本。现在，继续使用之前的简化，即${{\theta }_{0}}=0$，我们来看一下支持向量机会选择什么样的决策界。这是一种选择，我们假设支持向量机会选择这个决策边界。这不是一个非常好的选择，因为它的间距很小。这个决策界离训练样本的距离很近。我们来看一下为什么支持向量机不会选择它。 对于这样选择的参数$\theta$，可以看到参数向量$\theta$事实上是和决策界是90度正交的，因此这个绿色的决策界对应着一个参数向量$\theta$这个方向,顺便提一句${{\theta }_{0}}=0$的简化仅仅意味着决策界必须通过原点$(0,0)$。现在让我们看一下这对于优化目标函数意味着什么。 比如这个样本，我们假设它是我的第一个样本$x^{(1)}$，如果我考察这个样本到参数$\theta$的投影，投影是这个短的红线段，就等于$p^{(1)}$，它非常短。类似地，这个样本如果它恰好是$x^{(2)}$，我的第二个训练样本，则它到$\theta$的投影在这里。我将它画成粉色，这个短的粉色线段是$p^{(2)}$，即第二个样本到我的参数向量$\theta$的投影。因此，这个投影非常短。$p^{(2)}$事实上是一个负值，$p^{(2)}$是在相反的方向，这个向量和参数向量$\theta$的夹角大于90度，$p^{(2)}$的值小于0。 我们会发现这些$p^{(i)}$将会是非常小的数，因此当我们考察优化目标函数的时候，对于正样本而言，我们需要$p^{(i)}\cdot{\left\| \theta \right\|}>=1$,但是如果 $p^{(i)}$在这里非常小,那就意味着我们需要$\theta$的范数非常大.因为如果 $p^{(1)}$ 很小,而我们希望$p^{(1)}\cdot{\left\| \theta \right\|}>=1$,令其实现的唯一的办法就是这两个数较大。如果 $p^{(1)}$ 小，我们就希望$\theta$的范数大。类似地，对于负样本而言我们需要$p^{(2)}\cdot{\left\|\theta \right\|}1，则因为$p^{(1)}$变大了，$\theta$的范数就可以变小了。因此这意味着通过选择右边的决策界，而不是左边的那个，支持向量机可以使参数$\theta$的范数变小很多。因此，如果我们想令$\theta$的范数变小，从而令$\theta$范数的平方变小，就能让支持向量机选择右边的决策界。这就是支持向量机如何能有效地产生大间距分类的原因。看这条绿线，这个绿色的决策界。我们希望正样本和负样本投影到$\theta$的值大。要做到这一点的唯一方式就是选择这条绿线做决策界。这是大间距决策界来区分开正样本和负样本这个间距的值。这个间距的值就是$p^{(1)},p^{(2)},p^{(3)}$等等的值。通过让间距变大，即通过这些$p^{(1)},p^{(2)},p^{(3)}$等等的值，支持向量机最终可以找到一个较小的$\theta$范数。这正是支持向量机中最小化目标函数的目的。以上就是为什么支持向量机最终会找到大间距分类器的原因。因为它试图极大化这些$p^{(i)}$的范数，它们是训练样本到决策边界的距离。最后一点，我们的推导自始至终使用了这个简化假设，就是参数$θ_0=0$。就像我之前提到的。这个的作用是：$θ_0=0$的意思是我们让决策界通过原点。如果你令$θ_0$不是0的话，含义就是你希望决策界不通过原点。我将不会做全部的推导。实际上，支持向量机产生大间距分类器的结论，会被证明同样成立，证明方式是非常类似的，是我们刚刚做的证明的推广。之前视频中说过，即便$θ_0$不等于0，支持向量机要做的事情都是优化这个目标函数对应着$C$值非常大的情况，但是可以说明的是，即便$θ_0$不等于0，支持向量机仍然会找到正样本和负样本之间的大间距分隔。总之，我们解释了为什么支持向量机是一个大间距分类器。在下一节我们，将开始讨论如何利用支持向量机的原理，应用它们建立一个复杂的非线性分类器。### 12.4 核函数1参考视频: 12 - 4 - Kernels I (16 min).mkv回顾我们之前讨论过可以使用高级数的多项式模型来解决无法用直线进行分隔的分类问题：为了获得上图所示的判定边界，我们的模型可能是${{\theta }_{0}}+{{\theta }_{1}}{{x}_{1}}+{{\theta }_{2}}{{x}_{2}}+{{\theta }_{3}}{{x}_{1}}{{x}_{2}}+{{\theta }_{4}}x_{1}^{2}+{{\theta }_{5}}x_{2}^{2}+\cdots $的形式。我们可以用一系列的新的特征f来替换模型中的每一项。例如令：${{f}_{1}}={{x}_{1}},{{f}_{2}}={{x}_{2}},{{f}_{3}}={{x}_{1}}{{x}_{2}},{{f}_{4}}=x_{1}^{2},{{f}_{5}}=x_{2}^{2}$ …得到$h_θ(x)=f_1+f_2+…+f_n$。然而，除了对原有的特征进行组合以外，有没有更好的方法来构造$f_1,f_2,f_3$？我们可以利用核函数来计算出新的特征。 给定一个训练实例$x$，我们利用$x$的各个特征与我们预先选定的地标(landmarks)$l^{(1)},l^{(2)},l^{(3)}$的近似程度来选取新的特征$f_1,f_2,f_3$。 例如：${{f}_{1}}=similarity(x,{{l}^{(1)}})=e(-\frac{{{\left\| x-{{l}^{(1)}} \right\|}^{2}}}{2{{\sigma }^{2}}})$ 其中：${{\left\| x-{{l}^{(1)}} \right\|}^{2}}=\sum{_{j=1}^{n}}{{({{x}_{j}}-l_{j}^{(1)})}^{2}}$，为实例$x$中所有特征与地标$l^{(1)}$之间的距离的和。上例中的$similarity(x,{{l}^{(1)}})$就是核函数，具体而言，这里是一个高斯核函数(Gaussian Kernel)。 注：这个函数与正态分布没什么实际上的关系，只是看上去像而已。 这些地标的作用是什么？如果一个训练实例$x$与地标$L$之间的距离近似于0，则新特征 $f$近似于$e^{-0}=1$，如果训练实例$x$与地标$L$之间距离较远，则$f$近似于$e^{-(一个较大的数)}=0$。 假设我们的训练实例含有两个特征[$x_{1}$ $x{_2}$]，给定地标$l^{(1)}$与不同的$\sigma$值，见下图： 图中水平面的坐标为 $x_{1}$，$x_{2}$而垂直坐标轴代表$f$。可以看出，只有当$x$与$l^{(1)}$重合时$f$才具有最大值。随着$x$的改变$f$值改变的速率受到$\sigma^2$的控制。 在下图中，当实例处于洋红色的点位置处，因为其离$l^{(1)}$更近，但是离$l^{(2)}$和$l^{(3)}$较远，因此$f_1$接近1，而$f_2$,$f_3$接近0。因此$h_θ(x)=θ_0+θ_1f_1+θ_2f_2+θ_1f_3>0$，因此预测$y=1$。同理可以求出，对于离$l^{(2)}$较近的绿色点，也预测$y=1$，但是对于蓝绿色的点，因为其离三个地标都较远，预测$y=0$。 这样，图中红色的封闭曲线所表示的范围，便是我们依据一个单一的训练实例和我们选取的地标所得出的判定边界，在预测时，我们采用的特征不是训练实例本身的特征，而是通过核函数计算出的新特征$f_1,f_2,f_3$。 12.5 核函数2参考视频: 12 - 5 - Kernels II (16 min).mkv 在上一节视频里，我们讨论了核函数这个想法，以及怎样利用它去实现支持向量机的一些新特性。在这一节视频中，我将补充一些缺失的细节，并简单的介绍一下怎么在实际中使用应用这些想法。 如何选择地标？ 我们通常是根据训练集的数量选择地标的数量，即如果训练集中有$m$个实例，则我们选取$m$个地标，并且令:$l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},.....,l^{(m)}=x^{(m)}$。这样做的好处在于：现在我们得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的，即： 下面我们将核函数运用到支持向量机中，修改我们的支持向量机假设为： • 给定$x$，计算新特征$f$，当$θ^Tf&gt;=0$ 时，预测 $y=1$，否则反之。 相应地修改代价函数为：$\sum{_{j=1}^{n=m}}\theta _{j}^{2}={{\theta}^{T}}\theta $， $min C\sum\limits_{i=1}^{m}{[{{y}^{(i)}}cos {{t}_{1}}}( {{\theta }^{T}}{{f}^{(i)}})+(1-{{y}^{(i)}})cos {{t}_{0}}( {{\theta }^{T}}{{f}^{(i)}})]+\frac{1}{2}\sum\limits_{j=1}^{n=m}{\theta _{j}^{2}}$. 在具体实施过程中，我们还需要对最后的正则化项进行些微调整，在计算$\sum{_{j=1}^{n=m}}\theta _{j}^{2}={{\theta}^{T}}\theta $时，我们用$θ^TMθ$代替$θ^Tθ$，其中$M$是根据我们选择的核函数而不同的一个矩阵。这样做的原因是为了简化计算。 理论上讲，我们也可以在逻辑回归中使用核函数，但是上面使用 $M$来简化计算的方法不适用与逻辑回归，因此计算将非常耗费时间。 在此，我们不介绍最小化支持向量机的代价函数的方法，你可以使用现有的软件包（如liblinear,libsvm等）。在使用这些软件包最小化我们的代价函数之前，我们通常需要编写核函数，并且如果我们使用高斯核函数，那么在使用之前进行特征缩放是非常必要的。 另外，支持向量机也可以不使用核函数，不使用核函数又称为线性核函数(linear kernel)，当我们不采用非常复杂的函数，或者我们的训练集特征非常多而实例非常少的时候，可以采用这种不带核函数的支持向量机。 下面是支持向量机的两个参数$C$和$\sigma$的影响： $C=1/\lambda$ $C$ 较大时，相当于$\lambda$较小，可能会导致过拟合，高方差； $C$ 较小时，相当于$λ$较大，可能会导致低拟合，高偏差； $\sigma$较大时，可能会导致低方差，高偏差； $\sigma$较小时，可能会导致低偏差，高方差。 如果你看了本周的编程作业，你就能亲自实现这些想法，并亲眼看到这些效果。这就是利用核函数的支持向量机算法，希望这些关于偏差和方差的讨论，能给你一些对于算法结果预期的直观印象。 12.6 使用支持向量机参考视频: 12 - 6 - Using An SVM (21 min).mkv 目前为止，我们已经讨论了SVM比较抽象的层面，在这个视频中我将要讨论到为了运行或者运用SVM。你实际上所需要的一些东西：支持向量机算法，提出了一个特别优化的问题。但是就如在之前的视频中我简单提到的，我真的不建议你自己写软件来求解参数$\theta$，因此由于今天我们中的很少人，或者其实没有人考虑过自己写代码来转换矩阵，或求一个数的平方根等我们只是知道如何去调用库函数来实现这些功能。同样的，用以解决SVM最优化问题的软件很复杂，且已经有研究者做了很多年数值优化了。因此你提出好的软件库和好的软件包来做这样一些事儿。然后强烈建议使用高优化软件库中的一个，而不是尝试自己落实一些数据。有许多好的软件库，我正好用得最多的两个是liblinear和libsvm，但是真的有很多软件库可以用来做这件事儿。你可以连接许多你可能会用来编写学习算法的主要编程语言。 在高斯核函数之外我们还有其他一些选择，如： 多项式核函数（Polynomial Kernel） 字符串核函数（String kernel） 卡方核函数（ chi-square kernel） 直方图交集核函数（histogram intersection kernel） 等等… 这些核函数的目标也都是根据训练集和地标之间的距离来构建新特征，这些核函数需要满足Mercer’s定理，才能被支持向量机的优化软件正确处理。 多类分类问题 假设我们利用之前介绍的一对多方法来解决一个多类分类问题。如果一共有$k$个类，则我们需要$k$个模型，以及$k$个参数向量$\theta $。我们同样也可以训练k个支持向量机来解决多类分类问题。但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。 尽管你不去写你自己的SVM的优化软件，但是你也需要做几件事： 1、是提出参数$C$的选择。我们在之前的视频中讨论过误差/方差在这方面的性质。 2、你也需要选择内核参数或你想要使用的相似函数，其中一个选择是：我们选择不需要任何内核参数，没有内核参数的理念，也叫线性核函数。因此，如果有人说他使用了线性核的SVM（支持向量机），这就意味这他使用了不带有核函数的SVM（支持向量机）。 从逻辑回归模型，我们得到了支持向量机模型，在两者之间，我们应该如何选择呢？ 下面是一些普遍使用的准则： $n$为特征数，$m$为训练样本数。 (1)如果相较于$m$而言，$n$要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。 (2)如果$n$较小，而且$m$大小中等，例如$n$在 1-1000 之间，而$m$在10-10000之间，使用高斯核函数的支持向量机。 (3)如果$n$较小，而$m$较大，例如$n$在1-1000之间，而$m$大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。 值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。 今天的SVM包会工作得很好，但是它们仍然会有一些慢。当你有非常非常大的训练集，且用高斯核函数是在这种情况下，我经常会做的是尝试手动地创建，拥有更多的特征变量，然后用逻辑回归或者不带核函数的支持向量机。如果你看到这个幻灯片，看到了逻辑回归，或者不带核函数的支持向量机。在这个两个地方，我把它们放在一起是有原因的。原因是：逻辑回归和不带核函数的支持向量机它们都是非常相似的算法，不管是逻辑回归还是不带核函数的SVM，通常都会做相似的事情，并给出相似的结果。但是根据你实现的情况，其中一个可能会比另一个更加有效。但是在其中一个算法应用的地方，逻辑回归或不带核函数的SVM另一个也很有可能很有效。但是随着SVM的复杂度增加，当你使用不同的内核函数来学习复杂的非线性函数时，这个体系，你知道的，当你有多达1万（10,000）的样本时，也可能是5万（50,000），你的特征变量的数量这是相当大的。那是一个非常常见的体系，也许在这个体系里，不带核函数的支持向量机就会表现得相当突出。你可以做比这困难得多需要逻辑回归的事情。 最后，神经网络使用于什么时候呢？ 对于所有的这些问题，对于所有的这些不同体系一个设计得很好的神经网络也很有可能会非常有效。有一个缺点是，或者说是有时可能不会使用神经网络的原因是：对于许多这样的问题，神经网络训练起来可能会特别慢，但是如果你有一个非常好的SVM实现包，它可能会运行得比较快比神经网络快很多，尽管我们在此之前没有展示，但是事实证明，SVM具有的优化问题，是一种凸优化问题。因此，好的SVM优化软件包总是会找到全局最小值，或者接近它的值。对于SVM你不需要担心局部最优。在实际应用中，局部最优不是神经网络所需要解决的一个重大问题，所以这是你在使用SVM的时候不需要太去担心的一个问题。根据你的问题，神经网络可能会比SVM慢，尤其是在这样一个体系中，至于这里给出的参考，看上去有些模糊，如果你在考虑一些问题，这些参考会有一些模糊，但是我仍然不能完全确定，我是该用这个算法还是改用那个算法，这个没有太大关系，当我遇到机器学习问题的时候，有时它确实不清楚这是否是最好的算法，但是就如在之前的视频中看到的算法确实很重要。但是通常更加重要的是：你有多少数据，你有多熟练是否擅长做误差分析和排除学习算法，指出如何设定新的特征变量和找出其他能决定你学习算法的变量等方面，通常这些方面会比你使用逻辑回归还是SVM这方面更加重要。但是，已经说过了，SVM仍然被广泛认为是一种最强大的学习算法，这是一个体系，包含了什么时候一个有效的方法去学习复杂的非线性函数。因此，实际上与逻辑回归、神经网络、SVM一起使用这些方法来提高学习算法，我认为你会很好地建立很有技术的状态。（编者注：当时GPU计算比较慢，神经网络还不流行。） 机器学习系统对于一个宽泛的应用领域来说，这是另一个在你军械库里非常强大的工具，你可以把它应用到很多地方，如硅谷、在工业、学术等领域建立许多高性能的机器学习系统。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习（八）]]></title>
    <url>%2F2018%2F08%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AB%EF%BC%89%2F</url>
    <content type="text"><![CDATA[好吧，从这里开始博客就不是我自己手敲的了，因为一些原因，没有时间再写下去了，但又不想放弃，只能抄一份过来了，感谢Huang Haiguang老师的笔记。https://github.com/fengdu78 十、应用机器学习的建议(Advice for Applying Machine Learning) 10.1 决定下一步做什么参考视频: 10 - 1 - Deciding What to Try Next (6 min).mkv ​ 到目前为止，我们已经介绍了许多不同的学习算法，如果你一直跟着这些视频的进度学习，你会发现自己已经不知不觉地成为一个了解许多先进机器学习技术的专家了。 ​ 然而，在懂机器学习的人当中依然存在着很大的差距，一部分人确实掌握了怎样高效有力地运用这些学习算法。而另一些人他们可能对我马上要讲的东西，就不是那么熟悉了。他们可能没有完全理解怎样运用这些算法。因此总是把时间浪费在毫无意义的尝试上。我想做的是确保你在设计机器学习的系统时，你能够明白怎样选择一条最合适、最正确的道路。因此，在这节视频和之后的几段视频中，我将向你介绍一些实用的建议和指导，帮助你明白怎样进行选择。具体来讲，我将重点关注的问题是假如你在开发一个机器学习系统，或者想试着改进一个机器学习系统的性能，你应如何决定接下来应该选择哪条道路？为了解释这一问题，我想仍然使用预测房价的学习例子，假如你已经完成了正则化线性回归，也就是最小化代价函数$J$的值，假如，在你得到你的学习参数以后，如果你要将你的假设函数放到一组新的房屋样本上进行测试，假如说你发现在预测房价时产生了巨大的误差，现在你的问题是要想改进这个算法，接下来应该怎么办？ ​ 实际上你可以想出很多种方法来改进这个算法的性能，其中一种办法是使用更多的训练样本。具体来讲，也许你能想到通过电话调查或上门调查来获取更多的不同的房屋出售数据。遗憾的是，我看到好多人花费了好多时间想收集更多的训练样本。他们总认为，要是我有两倍甚至十倍数量的训练数据，那就一定会解决问题的是吧？但有时候获得更多的训练数据实际上并没有作用。在接下来的几段视频中，我们将解释原因。 ​ 我们也将知道怎样避免把过多的时间浪费在收集更多的训练数据上，这实际上是于事无补的。另一个方法，你也许能想到的是尝试选用更少的特征集。因此如果你有一系列特征比如$x_1,x_2,x_3$等等。也许有很多特征，也许你可以花一点时间从这些特征中仔细挑选一小部分来防止过拟合。或者也许你需要用更多的特征，也许目前的特征集，对你来讲并不是很有帮助。你希望从获取更多特征的角度来收集更多的数据，同样地，你可以把这个问题扩展为一个很大的项目，比如使用电话调查来得到更多的房屋案例，或者再进行土地测量来获得更多有关，这块土地的信息等等，因此这是一个复杂的问题。同样的道理，我们非常希望在花费大量时间完成这些工作之前，我们就能知道其效果如何。我们也可以尝试增加多项式特征的方法，比如$x_1$的平方，$x_2$的平方，$x_1,x_2$的乘积，我们可以花很多时间来考虑这一方法，我们也可以考虑其他方法减小或增大正则化参数$\lambda$的值。我们列出的这个单子，上面的很多方法都可以扩展开来扩展成一个六个月或更长时间的项目。遗憾的是，大多数人用来选择这些方法的标准是凭感觉的，也就是说，大多数人的选择方法是随便从这些方法中选择一种，比如他们会说“噢，我们来多找点数据吧”，然后花上六个月的时间收集了一大堆数据，然后也许另一个人说：“好吧，让我们来从这些房子的数据中多找点特征吧”。我很遗憾不止一次地看到很多人花了至少六个月时间来完成他们随便选择的一种方法，而在六个月或者更长时间后，他们很遗憾地发现自己选择的是一条不归路。幸运的是，有一系列简单的方法能让你事半功倍，排除掉单子上的至少一半的方法，留下那些确实有前途的方法，同时也有一种很简单的方法，只要你使用，就能很轻松地排除掉很多选择，从而为你节省大量不必要花费的时间。最终达到改进机器学习系统性能的目的假设我们需要用一个线性回归模型来预测房价，当我们运用训练好了的模型来预测未知数据的时候发现有较大的误差，我们下一步可以做什么？ 获得更多的训练实例——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。 尝试减少特征的数量 尝试获得更多的特征 尝试增加多项式特征 尝试减少正则化程度$\lambda$ 尝试增加正则化程度$\lambda$ ​ 我们不应该随机选择上面的某种方法来改进我们的算法，而是运用一些机器学习诊断法来帮助我们知道上面哪些方法对我们的算法是有效的。 ​ 在接下来的两段视频中，我首先介绍怎样评估机器学习算法的性能，然后在之后的几段视频中，我将开始讨论这些方法，它们也被称为”机器学习诊断法”。“诊断法”的意思是：这是一种测试法，你通过执行这种测试，能够深入了解某种算法到底是否有用。这通常也能够告诉你，要想改进一种算法的效果，什么样的尝试，才是有意义的。在这一系列的视频中我们将介绍具体的诊断法，但我要提前说明一点的是，这些诊断法的执行和实现，是需要花些时间的，有时候确实需要花很多时间来理解和实现，但这样做的确是把时间用在了刀刃上，因为这些方法让你在开发学习算法时，节省了几个月的时间，因此，在接下来几节课中，我将先来介绍如何评价你的学习算法。在此之后，我将介绍一些诊断法，希望能让你更清楚。在接下来的尝试中，如何选择更有意义的方法。 10.2 评估一个假设参考视频: 10 - 2 - Evaluating a Hypothesis (8 min).mkv ​ 在本节视频中我想介绍一下怎样用你学过的算法来评估假设函数。在之后的课程中，我们将以此为基础来讨论如何避免过拟合和欠拟合的问题。 ​ 当我们确定学习算法的参数的时候，我们考虑的是选择参量来使训练误差最小化，有人认为得到一个非常小的训练误差一定是一件好事，但我们已经知道，仅仅是因为这个假设具有很小的训练误差，并不能说明它就一定是一个好的假设函数。而且我们也学习了过拟合假设函数的例子，所以这推广到新的训练集上是不适用的。 ​ 那么，你该如何判断一个假设函数是过拟合的呢？对于这个简单的例子，我们可以对假设函数$h(x)$进行画图，然后观察图形趋势，但对于特征变量不止一个的这种一般情况，还有像有很多特征变量的问题，想要通过画出假设函数来进行观察，就会变得很难甚至是不可能实现。 ​ 因此，我们需要另一种方法来评估我们的假设函数过拟合检验。 ​ 为了检验算法是否过拟合，我们将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。很重要的一点是训练集和测试集均要含有各种类型的数据，通常我们要对数据进行“洗牌”，然后再分成训练集和测试集。 ​ 测试集评估在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算误差： 对于线性回归模型，我们利用测试集数据计算代价函数$J$ 对于逻辑回归模型，我们除了可以利用测试数据集来计算代价函数外： $$ J_{test}{(\theta)} = -\frac{1}{{m}_{test}}\sum_\limits{i=1}^{m_{test}}\log{h_{\theta}(x^{(i)}_{test})}+(1-{y^{(i)}_{test}})\log{h_{\theta}(x^{(i)}_{test})}$$ 误分类的比率，对于每一个测试集实例，计算： 然后对计算结果求平均。 10.3 模型选择和交叉验证集参考视频: 10 - 3 - Model Selection and Train_Validation_Test Sets (12 min).mkv ​ 假设我们要在10个不同次数的二项式模型之间进行选择： ​ 显然越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能推广至一般情况，我们应该选择一个更能适应一般情况的模型。我们需要使用交叉验证集来帮助选择模型。 ​ 即：使用60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用20%的数据作为测试集 模型选择的方法为： 使用训练集训练出10个模型 用10个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值） 选取代价函数值最小的模型 用步骤3中选出的模型对测试集计算得出推广误差（代价函数的值） Train/validation/test error Training error: ​ $J_{train}(\theta) = \frac{1}{2m}\sum_\limits{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$ Cross Validation error: ​ $J_{cv}(\theta) = \frac{1}{2m_{cv}}\sum_\limits{i=1}^{m}(h_{\theta}(x^{(i)}{cv})-y^{(i)}{cv})^2​$ Test error: ​ $J_{test}(\theta)=\frac{1}{2m_{test}}\sum_\limits{i=1}^{m_{test}}(h_{\theta}(x^{(i)}{cv})-y^{(i)}{cv})^2$ 10.4 诊断偏差和方差参考视频: 10 - 4 - Diagnosing Bias vs. Variance (8 min).mkv ​ 当你运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。那么这两种情况，哪个和偏差有关，哪个和方差有关，或者是不是和两个都有关？搞清楚这一点非常重要，因为能判断出现的情况是这两种情况中的哪一种。其实是一个很有效的指示器，指引着可以改进算法的最有效的方法和途径。在这段视频中，我想更深入地探讨一下有关偏差和方差的问题，希望你能对它们有一个更深入的理解，并且也能弄清楚怎样评价一个学习算法，能够判断一个算法是偏差还是方差有问题，因为这个问题对于弄清如何改进学习算法的效果非常重要，高偏差和高方差的问题基本上来说是欠拟合和过拟合的问题。 ​ 我们通常会通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析： Bias/variance Training error: $J_{train}(\theta) = \frac{1}{2m}\sum_\limits{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$ Cross Validation error: $J_{cv}(\theta) = \frac{1}{2m_{cv}}\sum_\limits{i=1}^{m}(h_{\theta}(x^{(i)}{cv})-y^{(i)}{cv})^2$ ​ 对于训练集，当 $d$ 较小时，模型拟合程度更低，误差较大；随着 $d$ 的增长，拟合程度提高，误差减小。 ​ 对于交叉验证集，当 $d$ 较小时，模型拟合程度低，误差较大；但是随着 $d$ 的增长，误差呈现先减小后增大的趋势，转折点是我们的模型开始过拟合训练数据集的时候。 ​ 如果我们的交叉验证集误差较大，我们如何判断是方差还是偏差呢？根据上面的图表，我们知道: ​ 训练集误差和交叉验证集误差近似时：偏差/欠拟合 ​ 交叉验证集误差远大于训练集误差时：方差/过拟合 10.5 正则化和偏差/方差参考视频: 10 - 5 - Regularization and Bias_Variance (11 min).mkv ​ 在我们在训练模型的过程中，一般会使用一些正则化方法来防止过拟合。但是我们可能会正则化的程度太高或太小了，即我们在选择λ的值时也需要思考与刚才选择多项式模型次数类似的问题。 ​ 我们选择一系列的想要测试的 $\lambda$ 值，通常是 0-10之间的呈现2倍关系的值（如：$0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10$共12个）。 我们同样把数据分为训练集、交叉验证集和测试集。 选择$\lambda$的方法为： 使用训练集训练出12个不同程度正则化的模型 用12个模型分别对交叉验证集计算的出交叉验证误差 选择得出交叉验证误差最小的模型 运用步骤3中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与λ的值绘制在一张图表上： ​ • 当 $\lambda$ 较小时，训练集误差较小（过拟合）而交叉验证集误差较大 ​ • 随着 $\lambda$ 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加 10.6 学习曲线参考视频: 10 - 6 - Learning Curves (12 min).mkv ​ 学习曲线就是一种很好的工具，我经常使用学习曲线来判断某一个学习算法是否处于偏差、方差问题。学习曲线是学习算法的一个很好的合理检验（sanity check）。学习曲线是将训练集误差和交叉验证集误差作为训练集实例数量（$m$）的函数绘制的图表。 ​ 即，如果我们有100行数据，我们从1行数据开始，逐渐学习更多行的数据。思想是：当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据。 ​ 如何利用学习曲线识别高偏差/欠拟合：作为例子，我们尝试用一条直线来适应下面的数据，可以看出，无论训练集有多么大误差都不会有太大改观： ​ 也就是说在高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助。 ​ 如何利用学习曲线识别高方差/过拟合：假设我们使用一个非常高次的多项式模型，并且正则化非常小，可以看出，当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。 ​ 也就是说在高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。 10.7 决定下一步做什么参考视频: 10 - 7 - Deciding What to Do Next Revisited (7 min).mkv ​ 我们已经介绍了怎样评价一个学习算法，我们讨论了模型选择问题，偏差和方差的问题。那么这些诊断法则怎样帮助我们判断，哪些方法可能有助于改进学习算法的效果，而哪些可能是徒劳的呢？ ​ 让我们再次回到最开始的例子，在那里寻找答案，这就是我们之前的例子。回顾 1.1 中提出的六种可选的下一步，让我们来看一看我们在什么情况下应该怎样选择： 获得更多的训练实例——解决高方差 尝试减少特征的数量——解决高方差 尝试获得更多的特征——解决高偏差 尝试增加多项式特征——解决高偏差 尝试减少正则化程度λ——解决高偏差 尝试增加正则化程度λ——解决高方差 神经网络的方差和偏差： ​ 使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。 ​ 通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。 ​ 对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络，然后选择交叉验证集代价最小的神经网络。 ​ 好的，以上就是我们介绍的偏差和方差问题，以及诊断该问题的学习曲线方法。在改进学习算法的表现时，你可以充分运用以上这些内容来判断哪些途径可能是有帮助的。而哪些方法可能是无意义的。如果你理解了以上几节视频中介绍的内容，并且懂得如何运用。那么你已经可以使用机器学习方法有效的解决实际问题了。你也能像硅谷的大部分机器学习从业者一样，他们每天的工作就是使用这些学习算法来解决众多实际问题。我希望这几节中提到的一些技巧，关于方差、偏差，以及学习曲线为代表的诊断法能够真正帮助你更有效率地应用机器学习，让它们高效地工作。 十一、机器学习系统的设计(Machine Learning System Design)11.1 首先要做什么参考视频: 11 - 1 - Prioritizing What to Work On (10 min).mkv ​ 在接下来的视频中，我将谈到机器学习系统的设计。这些视频将谈及在设计复杂的机器学习系统时，你将遇到的主要问题。同时我们会试着给出一些关于如何巧妙构建一个复杂的机器学习系统的建议。下面的课程的的数学性可能不是那么强，但是我认为我们将要讲到的这些东西是非常有用的，可能在构建大型的机器学习系统时，节省大量的时间。 ​ 本周以一个垃圾邮件分类器算法为例进行讨论。 ​ 为了解决这样一个问题，我们首先要做的决定是如何选择并表达特征向量$x$。我们可以选择一个由100个最常出现在垃圾邮件中的词所构成的列表，根据这些词是否有在邮件中出现，来获得我们的特征向量（出现为1，不出现为0），尺寸为100×1。 为了构建这个分类器算法，我们可以做很多事，例如： 收集更多的数据，让我们有更多的垃圾邮件和非垃圾邮件的样本 基于邮件的路由信息开发一系列复杂的特征 基于邮件的正文信息开发一系列复杂的特征，包括考虑截词的处理 为探测刻意的拼写错误（把watch 写成w4tch）开发复杂的算法 ​ 在上面这些选项中，非常难决定应该在哪一项上花费时间和精力，作出明智的选择，比随着感觉走要更好。当我们使用机器学习时，总是可以“头脑风暴”一下，想出一堆方法来试试。实际上，当你需要通过头脑风暴来想出不同方法来尝试去提高精度的时候，你可能已经超越了很多人了。大部分人并不尝试着列出可能的方法，他们做的只是某天早上醒来，因为某些原因有了一个突发奇想：”让我们来试试用Honey Pot项目收集大量的数据吧。” ​ 我们将在随后的课程中讲误差分析，我会告诉你怎样用一个更加系统性的方法，从一堆不同的方法中，选取合适的那一个。因此，你更有可能选择一个真正的好方法，能让你花上几天几周，甚至是几个月去进行深入的研究。 11.2 误差分析参考视频: 11 - 2 - Error Analysis (13 min).mkv ​ 在本次课程中，我们将会讲到误差分析（Error Analysis）的概念。这会帮助你更系统地做出决定。如果你准备研究机器学习的东西，或者构造机器学习应用程序，最好的实践方法不是建立一个非常复杂的系统，拥有多么复杂的变量；而是构建一个简单的算法，这样你可以很快地实现它。 ​ 每当我研究机器学习的问题时，我最多只会花一天的时间，就是字面意义上的24小时，来试图很快的把结果搞出来，即便效果不好。坦白的说，就是根本没有用复杂的系统，但是只是很快的得到的结果。即便运行得不完美，但是也把它运行一遍，最后通过交叉验证来检验数据。一旦做完，你可以画出学习曲线，通过画出学习曲线，以及检验误差，来找出你的算法是否有高偏差和高方差的问题，或者别的问题。在这样分析之后，再来决定用更多的数据训练，或者加入更多的特征变量是否有用。这么做的原因是：这在你刚接触机器学习问题时是一个很好的方法，你并不能提前知道你是否需要复杂的特征变量，或者你是否需要更多的数据，还是别的什么。提前知道你应该做什么，是非常难的，因为你缺少证据，缺少学习曲线。因此，你很难知道你应该把时间花在什么地方来提高算法的表现。但是当你实践一个非常简单即便不完美的方法时，你可以通过画出学习曲线来做出进一步的选择。你可以用这种方式来避免一种电脑编程里的过早优化问题，这种理念是：我们必须用证据来领导我们的决策，怎样分配自己的时间来优化算法，而不是仅仅凭直觉，凭直觉得出的东西一般总是错误的。除了画出学习曲线之外，一件非常有用的事是误差分析，我的意思是说：当我们在构造垃圾邮件分类器时，我会看一看我的交叉验证数据集，然后亲自看一看哪些邮件被算法错误地分类。因此，通过这些被算法错误分类的垃圾邮件与非垃圾邮件，你可以发现某些系统性的规律：什么类型的邮件总是被错误分类。经常地这样做之后，这个过程能启发你构造新的特征变量，或者告诉你：现在这个系统的短处，然后启发你如何去提高它。 ​ 构建一个学习算法的推荐方法为： ​ 1. 从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法 ​ 2.绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择 ​ 3.进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的实例，看看这些实例是否有某种系统化的趋势 ​ 以我们的垃圾邮件过滤器为例，误差分析要做的既是检验交叉验证集中我们的算法产生错误预测的所有邮件，看：是否能将这些邮件按照类分组。例如医药品垃圾邮件，仿冒品垃圾邮件或者密码窃取邮件等。然后看分类器对哪一组邮件的预测误差最大，并着手优化。 ​ 思考怎样能改进分类器。例如，发现是否缺少某些特征，记下这些特征出现的次数。 ​ 例如记录下错误拼写出现了多少次，异常的邮件路由情况出现了多少次等等，然后从出现次数最多的情况开始着手优化。 ​ 误差分析并不总能帮助我们判断应该采取怎样的行动。有时我们需要尝试不同的模型，然后进行比较，在模型比较时，用数值来判断哪一个模型更好更有效，通常我们是看交叉验证集的误差。 ​ 在我们的垃圾邮件分类器例子中，对于“我们是否应该将discount/discounts/discounted/discounting处理成同一个词？”如果这样做可以改善我们算法，我们会采用一些截词软件。误差分析不能帮助我们做出这类判断，我们只能尝试采用和不采用截词软件这两种不同方案，然后根据数值检验的结果来判断哪一种更好。 ​ 因此，当你在构造学习算法的时候，你总是会去尝试很多新的想法，实现出很多版本的学习算法，如果每一次你实践新想法的时候，你都要手动地检测这些例子，去看看是表现差还是表现好，那么这很难让你做出决定。到底是否使用词干提取，是否区分大小写。但是通过一个量化的数值评估，你可以看看这个数字，误差是变大还是变小了。你可以通过它更快地实践你的新想法，它基本上非常直观地告诉你：你的想法是提高了算法表现，还是让它变得更坏，这会大大提高你实践算法时的速度。所以我强烈推荐在交叉验证集上来实施误差分析，而不是在测试集上。但是，还是有一些人会在测试集上来做误差分析。即使这从数学上讲是不合适的。所以我还是推荐你在交叉验证向量上来做误差分析。 ​ 总结一下，当你在研究一个新的机器学习问题时，我总是推荐你实现一个较为简单快速、即便不是那么完美的算法。我几乎从未见过人们这样做。大家经常干的事情是：花费大量的时间在构造算法上，构造他们以为的简单的方法。因此，不要担心你的算法太简单，或者太不完美，而是尽可能快地实现你的算法。当你有了初始的实现之后，它会变成一个非常有力的工具，来帮助你决定下一步的做法。因为我们可以先看看算法造成的错误，通过误差分析，来看看他犯了什么错，然后来决定优化的方式。另一件事是：假设你有了一个快速而不完美的算法实现，又有一个数值的评估数据，这会帮助你尝试新的想法，快速地发现你尝试的这些想法是否能够提高算法的表现，从而你会更快地做出决定，在算法中放弃什么，吸收什么误差分析可以帮助我们系统化地选择该做什么。 11.3 类偏斜的误差度量参考视频: 11 - 3 - Error Metrics for Skewed Classes (12 min).mkv ​ 在前面的课程中，我提到了误差分析，以及设定误差度量值的重要性。那就是，设定某个实数来评估你的学习算法，并衡量它的表现，有了算法的评估和误差度量值。有一件重要的事情要注意，就是使用一个合适的误差度量值，这有时会对于你的学习算法造成非常微妙的影响，这件重要的事情就是偏斜类（skewed classes）的问题。类偏斜情况表现为我们的训练集中有非常多的同一种类的实例，只有很少或没有其他类的实例。 ​ 例如我们希望用算法来预测癌症是否是恶性的，在我们的训练集中，只有0.5%的实例是恶性肿瘤。假设我们编写一个非学习而来的算法，在所有情况下都预测肿瘤是良性的，那么误差只有0.5%。然而我们通过训练而得到的神经网络算法却有1%的误差。这时，误差的大小是不能视为评判算法效果的依据的。 ​ 查准率（Precision）和查全率（Recall） 我们将算法预测的结果分成四种情况： ​ 1. 正确肯定（True Positive,TP）：预测为真，实际为真 ​ 2.正确否定（True Negative,TN）：预测为假，实际为假 ​ 3.错误肯定（False Positive,FP）：预测为真，实际为假 ​ 4.错误否定（False Negative,FN）：预测为假，实际为真 ​ 则：查准率=TP/(TP+FP)。例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 ​ 查全率=TP/(TP+FN)。例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。 ​ 这样，对于我们刚才那个总是预测病人肿瘤为良性的算法，其查全率是0。 预测值 Positive Negtive 实际值 Positive TP FN Negtive FP TN 11.4 查准率和查全率之间的权衡参考视频: 11 - 4 - Trading Off Precision and Recall (14 min).mkv ​ 在之前的课程中，我们谈到查准率和召回率，作为遇到偏斜类问题的评估度量值。在很多应用中，我们希望能够保证查准率和召回率的相对平衡。 ​ 在这节课中，我将告诉你应该怎么做，同时也向你展示一些查准率和召回率作为算法评估度量值的更有效的方式。继续沿用刚才预测肿瘤性质的例子。假使，我们的算法输出的结果在0-1 之间，我们使用阀值0.5 来预测真和假。 ​ 查准率(Precision)=TP/(TP+FP)例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。 ​ 查全率(Recall)=TP/(TP+FN)例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。 ​ 如果我们希望只在非常确信的情况下预测为真（肿瘤为恶性），即我们希望更高的查准率，我们可以使用比0.5更大的阀值，如0.7，0.9。这样做我们会减少错误预测病人为恶性肿瘤的情况，同时却会增加未能成功预测肿瘤为恶性的情况。 ​ 如果我们希望提高查全率，尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断，我们可以使用比0.5更小的阀值，如0.3。 ​ 我们可以将不同阀值情况下，查全率与查准率的关系绘制成图表，曲线的形状根据数据的不同而不同： ​ 我们希望有一个帮助我们选择这个阀值的方法。一种方法是计算F1 值（F1 Score），其计算公式为： ${{F}_{1}}Score:2\frac{PR}{P+R}$ 我们选择使得F1值最高的阀值。 11.5 机器学习的数据参考视频: 11 - 5 - Data For Machine Learning (11 min).mkv ​ 在之前的视频中，我们讨论了评价指标。在这个视频中，我要稍微转换一下，讨论一下机器学习系统设计中另一个重要的方面，这往往涉及到用来训练的数据有多少。在之前的一些视频中，我曾告诫大家不要盲目地开始，而是花大量的时间来收集大量的数据，因为数据有时是唯一能实际起到作用的。但事实证明，在一定条件下，我会在这个视频里讲到这些条件是什么。得到大量的数据并在某种类型的学习算法中进行训练，可以是一种有效的方法来获得一个具有良好性能的学习算法。而这种情况往往出现在这些条件对于你的问题都成立。并且你能够得到大量数据的情况下。这可以是一个很好的方式来获得非常高性能的学习算法。因此，在这段视频中，让我们一起讨论一下这个问题。 ​ 很多很多年前，我认识的两位研究人员Michele Banko 和Eric Brill进行了一项有趣的研究，他们尝试通过机器学习算法来区分常见的易混淆的单词，他们尝试了许多种不同的算法，并发现数据量非常大时，这些不同类型的算法效果都很好。 ​ 比如，在这样的句子中：早餐我吃了__个鸡蛋(to,two,too)，在这个例子中，“早餐我吃了2个鸡蛋”，这是一个易混淆的单词的例子。于是他们把诸如这样的机器学习问题，当做一类监督学习问题，并尝试将其分类，什么样的词，在一个英文句子特定的位置，才是合适的。他们用了几种不同的学习算法，这些算法都是在他们2001年进行研究的时候，都已经被公认是比较领先的。因此他们使用了一个方差，用于逻辑回归上的一个方差，被称作”感知器”(perceptron)。他们也采取了一些过去常用，但是现在比较少用的算法，比如 Winnow算法，很类似于回归问题，但在一些方面又有所不同，过去用得比较多，但现在用得不太多。还有一种基于内存的学习算法，现在也用得比较少了，但是我稍后会讨论一点，而且他们用了一个朴素算法。这些具体算法的细节不那么重要，我们下面希望探讨，什么时候我们会希望获得更多数据，而非修改算法。他们所做的就是改变了训练数据集的大小，并尝试将这些学习算法用于不同大小的训练数据集中，这就是他们得到的结果。 ​ 这些趋势非常明显，首先大部分算法，都具有相似的性能，其次，随着训练数据集的增大，在横轴上代表以百万为单位的训练集大小，从0.1个百万到1000百万，也就是到了10亿规模的训练集的样本，这些算法的性能也都对应地增强了。 ​ 事实上，如果你选择任意一个算法，可能是选择了一个”劣等的”算法，如果你给这个劣等算法更多的数据，那么从这些例子中看起来的话，它看上去很有可能会其他算法更好，甚至会比”优等算法”更好。由于这项原始的研究非常具有影响力，因此已经有一系列许多不同的研究显示了类似的结果。这些结果表明，许多不同的学习算法有时倾向于表现出非常相似的表现，这还取决于一些细节，但是真正能提高性能的，是你能够给一个算法大量的训练数据。像这样的结果，引起了一种在机器学习中的普遍共识：”取得成功的人不是拥有最好算法的人，而是拥有最多数据的人”。 ​ 那么这种说法在什么时候是真，什么时候是假呢？因为如果我们有一个学习算法，并且如果这种说法是真的，那么得到大量的数据通常是保证我们具有一个高性能算法的最佳方式，而不是去争辩应该用什么样的算法。 ​ 假如有这样一些假设，在这些假设下有大量我们认为有用的训练集，我们假设在我们的机器学习问题中，特征值$x$包含了足够的信息，这些信息可以帮助我们用来准确地预测$y$，例如，如果我们采用了一些容易混淆的词，如：two、to、too，假如说它能够描述$x$，捕捉到需要填写的空白处周围的词语，那么特征捕捉到之后，我们就希望有对于“早饭我吃了__鸡蛋”，那么这就有大量的信息来告诉我中间我需要填的词是“两个”(two)，而不是单词 to 或too，因此特征捕捉，哪怕是周围词语中的一个词，就能够给我足够的信息来确定出标签 $y$是什么。换句话说，从这三组易混淆的词中，我应该选什么词来填空。 ​ 那么让我们来看一看，大量的数据是有帮助的情况。假设特征值有足够的信息来预测$y$值，假设我们使用一种需要大量参数的学习算法，比如有很多特征的逻辑回归或线性回归，或者用带有许多隐藏单元的神经网络，那又是另外一种带有很多参数的学习算法，这些都是非常强大的学习算法，它们有很多参数，这些参数可以拟合非常复杂的函数，因此我要调用这些，我将把这些算法想象成低偏差算法，因为我们能够拟合非常复杂的函数，而且因为我们有非常强大的学习算法，这些学习算法能够拟合非常复杂的函数。很有可能，如果我们用这些数据运行这些算法，这种算法能很好地拟合训练集，因此，训练误差就会很低了。 ​ 现在假设我们使用了非常非常大的训练集，在这种情况下，尽管我们希望有很多参数，但是如果训练集比参数的数量还大，甚至是更多，那么这些算法就不太可能会过度拟合。也就是说训练误差有希望接近测试误差。 ​ 另一种考虑这个问题的角度是为了有一个高性能的学习算法，我们希望它不要有高的偏差和方差。 ​ 因此偏差问题，我么将通过确保有一个具有很多参数的学习算法来解决，以便我们能够得到一个较低偏差的算法，并且通过用非常大的训练集来保证。 ​ 我们在此没有方差问题，我们的算法将没有方差，并且通过将这两个值放在一起，我们最终可以得到一个低误差和低方差的学习算法。这使得我们能够很好地测试测试数据集。从根本上来说，这是一个关键的假设：特征值有足够的信息量，且我们有一类很好的函数，这是为什么能保证低误差的关键所在。它有大量的训练数据集，这能保证得到更多的方差值，因此这给我们提出了一些可能的条件，如果你有大量的数据，而且你训练了一种带有很多参数的学习算法，那么这将会是一个很好的方式，来提供一个高性能的学习算法。 ​ 我觉得关键的测试：首先，一个人类专家看到了特征值 $x$，能很有信心的预测出$y$值吗？因为这可以证明 $ y$ 可以根据特征值$x$被准确地预测出来。其次，我们实际上能得到一组庞大的训练集，并且在这个训练集中训练一个有很多参数的学习算法吗？如果你不能做到这两者，那么更多时候，你会得到一个性能很好的学习算法。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转】神经网络浅讲：从神经元到深度学习]]></title>
    <url>%2F2018%2F05%2F30%2F%E3%80%90%E8%BD%AC%E3%80%91%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%B5%85%E8%AE%B2%EF%BC%9A%E4%BB%8E%E7%A5%9E%E7%BB%8F%E5%85%83%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[这又是我常看的大神写的一篇博客，从最初的从机器学习谈起使我入门机器学习，到现在学到了神经网络，又找到了他的博客，希望对我学习神经网络有所帮助。 原文地址：https://www.cnblogs.com/subconscious/p/5058741.html#first 作者：计算机的潜意识 神经网络是一门重要的机器学习技术。它是目前最为火热的研究方向–深度学习的基础。学习神经网络不仅可以让你掌握一门强大的机器学习方法，同时也可以更好地帮助你理解深度学习技术。 本文以一种简单的，循序的方式讲解神经网络。适合对神经网络了解不多的同学。本文对阅读没有一定的前提要求，但是懂一些机器学习基础会更好地帮助理解本文。 神经网络是一种模拟人脑的神经网络以期能够实现类人工智能的机器学习技术。人脑中的神经网络是一个非常复杂的组织。成人的大脑中估计有1000亿个神经元之多。图1 人脑神经网络&nbsp; 那么机器学习中的神经网络是如何实现这种模拟的，并且达到一个惊人的良好效果的？通过本文，你可以了解到这些问题的答案，同时还能知道神经网络的历史，以及如何较好地学习它。 由于本文较长，为方便读者，以下是本文的目录： 一.前言 二.神经元 三.单层神经网络（感知器） 四.两层神经网络（多层感知器） 五.多层神经网络（深度学习） 六.回顾 七.展望 八.总结 九.后记 十.备注&nbsp;一. 前言 让我们来看一个经典的神经网络。这是一个包含三个层次的神经网络。红色的是输入层，绿色的是输出层，紫色的是中间层（也叫隐藏层）。输入层有3个输入单元，隐藏层有4个单元，输出层有2个单元。后文中，我们统一使用这种颜色来表达神经网络的结构。图2 神经网络结构图&nbsp; 在开始介绍前，有一些知识可以先记在心里：设计一个神经网络时，输入层与输出层的节点数往往是固定的，中间层则可以自由指定；神经网络结构图中的拓扑与箭头代表着预测过程时数据的流向，跟训练时的数据流有一定的区别；结构图里的关键不是圆圈（代表“神经元”），而是连接线（代表“神经元”之间的连接）。每个连接线对应一个不同的权重（其值称为权值），这是需要训练得到的。&nbsp;&nbsp; 除了从左到右的形式表达的结构图，还有一种常见的表达形式是从下到上来表示一个神经网络。这时候，输入层在图的最下方。输出层则在图的最上方，如下图：图3&nbsp;从下到上的神经网络结构图&nbsp;&nbsp; 从左到右的表达形式以Andrew&nbsp;Ng和LeCun的文献使用较多，Caffe里使用的则是从下到上的表达。在本文中使用Andrew&nbsp;Ng代表的从左到右的表达形式。 下面从简单的神经元开始说起，一步一步介绍神经网络复杂结构的形成。&nbsp;二.&nbsp;神经元 1.引子 对于神经元的研究由来已久，1904年生物学家就已经知晓了神经元的组成结构。 一个神经元通常具有多个树突，主要用来接受传入信息；而轴突只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“突触”。 人脑中的神经元形状可以用下图做简单的说明：图4 神经元&nbsp;&nbsp; 1943年，心理学家McCulloch和数学家Pitts参考了生物神经元的结构，发表了抽象的神经元模型MP。在下文中，我们会具体介绍神经元模型。&nbsp;&nbsp;&nbsp;图5 Warren McCulloch（左）和&nbsp;Walter Pitts（右）&nbsp;&nbsp; 2.结构&nbsp; 神经元模型是一个包含输入，输出与计算功能的模型。输入可以类比为神经元的树突，而输出可以类比为神经元的轴突，计算则可以类比为细胞核。 下图是一个典型的神经元模型：包含有3个输入，1个输出，以及2个计算功能。 注意中间的箭头线。这些线称为“连接”。每个上有一个“权值”。图6 神经元模型&nbsp;&nbsp; 连接是神经元中最重要的东西。每一个连接上都有一个权重。 一个神经网络的训练算法就是让权重的值调整到最佳，以使得整个网络的预测效果最好。 我们使用a来表示输入，用w来表示权值。一个表示连接的有向箭头可以这样理解：在初端，传递的信号大小仍然是a，端中间有加权参数w，经过这个加权后的信号会变成aw，因此在连接的末端，信号的大小就变成了aw。 在其他绘图模型里，有向箭头可能表示的是值的不变传递。而在神经元模型里，每个有向箭头表示的是值的加权传递。图7 连接（connection）&nbsp;&nbsp;&nbsp; 如果我们将神经元图中的所有变量用符号表示，并且写出输出的计算公式的话，就是下图。图8 神经元计算&nbsp;&nbsp;&nbsp; 可见z是在输入和权值的线性加权和叠加了一个函数g的值。在MP模型里，函数g是sgn函数，也就是取符号函数。这个函数当输入大于0时，输出1，否则输出0。 下面对神经元模型的图进行一些扩展。首先将sum函数与sgn函数合并到一个圆圈里，代表神经元的内部计算。其次，把输入a与输出z写到连接线的左上方，便于后面画复杂的网络。最后说明，一个神经元可以引出多个代表输出的有向箭头，但值都是一样的。 神经元可以看作一个计算与存储单元。计算是神经元对其的输入进行计算功能。存储是神经元会暂存计算结果，并传递到下一层。图9 神经元扩展&nbsp;&nbsp; 当我们用“神经元”组成网络以后，描述网络中的某个“神经元”时，我们更多地会用“单元”（unit）来指代。同时由于神经网络的表现形式是一个有向图，有时也会用“节点”（node）来表达同样的意思。&nbsp; 3.效果&nbsp; 神经元模型的使用可以这样理解： 我们有一个数据，称之为样本。样本有四个属性，其中三个属性已知，一个属性未知。我们需要做的就是通过三个已知属性预测未知属性。 具体办法就是使用神经元的公式进行计算。三个已知属性的值是a1，a2，a3，未知属性的值是z。z可以通过公式计算出来。 这里，已知的属性称之为特征，未知的属性称之为目标。假设特征与目标之间确实是线性关系，并且我们已经得到表示这个关系的权值w1，w2，w3。那么，我们就可以通过神经元模型预测新样本的目标。 4.影响 1943年发布的MP模型，虽然简单，但已经建立了神经网络大厦的地基。但是，MP模型中，权重的值都是预先设置的，因此不能学习。 1949年心理学家Hebb提出了Hebb学习率，认为人脑神经细胞的突触（也就是连接）上的强度上可以变化的。于是计算科学家们开始考虑用调整权值的方法来让机器学习。这为后面的学习算法奠定了基础。图10 Donald Olding Hebb&nbsp;&nbsp; 尽管神经元模型与Hebb学习律都已诞生，但限于当时的计算机能力，直到接近10年后，第一个真正意义的神经网络才诞生。&nbsp;三. 单层神经网络（感知器） 1.引子 1958年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络。他给它起了一个名字–“感知器”（Perceptron）（有的文献翻译成“感知机”，下文统一用“感知器”来指代）。 感知器是当时首个可以学习的人工神经网络。Rosenblatt现场演示了其学习识别简单图像的过程，在当时的社会引起了轰动。 人们认为已经发现了智能的奥秘，许多学者和科研机构纷纷投入到神经网络的研究中。美国军方大力资助了神经网络的研究，并认为神经网络比“原子弹工程”更重要。这段时间直到1969年才结束，这个时期可以看作神经网络的第一次高潮。图11 Rosenblat与感知器&nbsp; 2.结构 下面来说明感知器模型。 在原来MP模型的“输入”位置添加神经元节点，标志其为“输入单元”。其余不变，于是我们就有了下图：从本图开始，我们将权值w1, w2, w3写到“连接线”的中间。图12 单层神经网络&nbsp;&nbsp; 在“感知器”中，有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算。 我们把需要计算的层次称之为“计算层”，并把拥有一个计算层的网络称之为“单层神经网络”。有一些文献会按照网络拥有的层数来命名，例如把“感知器”称为两层神经网络。但在本文里，我们根据计算层的数量来命名。 假如我们要预测的目标不再是一个值，而是一个向量，例如[2,3]。那么可以在输出层再增加一个“输出单元”。 下图显示了带有两个输出单元的单层神经网络，其中输出单元z1的计算公式如下图。图13 单层神经网络(Z1)&nbsp; 可以看到，z1的计算跟原先的z并没有区别。 我们已知一个神经元的输出可以向多个神经元传递，因此z2的计算公式如下图。图14 单层神经网络(Z2)&nbsp; 可以看到，z2的计算中除了三个新的权值：w4，w5，w6以外，其他与z1是一样的。 整个网络的输出如下图。图15 单层神经网络(Z1和Z2)&nbsp; 目前的表达公式有一点不让人满意的就是：w4，w5，w6是后来加的，很难表现出跟原先的w1，w2，w3的关系。 因此我们改用二维的下标，用wx,y来表达一个权值。下标中的x代表后一层神经元的序号，而y代表前一层神经元的序号（序号的顺序从上到下）。 例如，w1,2代表后一层的第1个神经元与前一层的第2个神经元的连接的权值（这种标记方式参照了Andrew Ng的课件）。根据以上方法标记，我们有了下图。图16 单层神经网络(扩展)&nbsp; 如果我们仔细看输出的计算公式，会发现这两个公式就是线性代数方程组。因此可以用矩阵乘法来表达这两个公式。 例如，输入的变量是[a1，a2，a3]T（代表由a1，a2，a3组成的列向量），用向量a来表示。方程的左边是[z1，z2]T，用向量z来表示。 系数则是矩阵W（2行3列的矩阵，排列形式与公式中的一样）。 于是，输出公式可以改写成：g(W a) = z;&nbsp; 这个公式就是神经网络中从前一层计算后一层的矩阵运算。 3.效果 与神经元模型不同，感知器中的权值是通过训练得到的。因此，根据以前的知识我们知道，感知器类似一个逻辑回归模型，可以做线性分类任务。 我们可以用决策分界来形象的表达分类的效果。决策分界就是在二维的数据平面中划出一条直线，当数据的维度是3维的时候，就是划出一个平面，当数据的维度是n维时，就是划出一个n-1维的超平面。 下图显示了在二维平面中划出决策分界的效果，也就是感知器的分类效果。图17 单层神经网络（决策分界） 4.影响 感知器只能做简单的线性分类任务。但是当时的人们热情太过于高涨，并没有人清醒的认识到这点。于是，当人工智能领域的巨擘Minsky指出这点时，事态就发生了变化。 Minsky在1969年出版了一本叫《Perceptron》的书，里面用详细的数学证明了感知器的弱点，尤其是感知器对XOR（异或）这样的简单分类任务都无法解决。 Minsky认为，如果将计算层增加到两层，计算量则过大，而且没有有效的学习算法。所以，他认为研究更深层的网络是没有价值的。（本文成文后一个月，即2016年1月，Minsky在美国去世。谨在本文中纪念这位著名的计算机研究专家与大拿。）&nbsp; &nbsp;图18 Marvin Minsky 由于Minsky的巨大影响力以及书中呈现的悲观态度，让很多学者和实验室纷纷放弃了神经网络的研究。神经网络的研究陷入了冰河期。这个时期又被称为“AI winter”。 接近10年以后，对于两层神经网络的研究才带来神经网络的复苏。&nbsp;四. 两层神经网络（多层感知器） 1.引子 两层神经网络是本文的重点，因为正是在这时候，神经网络开始了大范围的推广与使用。 Minsky说过单层神经网络无法解决异或问题。但是当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。不过两层神经网络的计算是一个问题，没有一个较好的解法。 1986年，Rumelhar和Hinton等人提出了反向传播（Backpropagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。目前，大量的教授神经网络的教材，都是重点介绍两层（带一个隐藏层）神经网络的内容。&nbsp; 这时候的Hinton还很年轻，30年以后，正是他重新定义了神经网络，带来了神经网络复苏的又一春。&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;图19 David&nbsp;Rumelhart（左）以及&nbsp;Geoffery Hinton（右）&nbsp; 2.结构 两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。我们扩展上节的单层神经网络，在右边新加一个层次（只含有一个节点）。 现在，我们的权值矩阵增加到了两个，我们用上标来区分不同层次之间的变量。 例如ax(y)代表第y层的第x个节点。z1，z2变成了a1(2)，a2(2)。下图给出了a1(2)，a2(2)的计算公式。图20 两层神经网络（中间层计算）&nbsp; 计算最终输出z的方式是利用了中间层的a1(2)，a2(2)和第二个权值矩阵计算得到的，如下图。图21 两层神经网络（输出层计算）&nbsp; 假设我们的预测目标是一个向量，那么与前面类似，只需要在“输出层”再增加节点即可。 我们使用向量和矩阵来表示层次中的变量。a(1)，a(2)，z是网络中传输的向量数据。W(1)和W(2)是网络的矩阵参数。如下图。图22 两层神经网络（向量形式）&nbsp; 使用矩阵运算来表达整个计算公式的话如下：&nbsp; g(W(1)&nbsp;&nbsp;a(1)) =&nbsp;a(2);&nbsp;g(W(2)&nbsp;&nbsp;a(2)) =&nbsp;z;&nbsp; 由此可见，使用矩阵运算来表达是很简洁的，而且也不会受到节点数增多的影响（无论有多少节点参与运算，乘法两端都只有一个变量）。因此神经网络的教程中大量使用矩阵运算来描述。 需要说明的是，至今为止，我们对神经网络的结构图的讨论中都没有提到偏置节点（bias unit）。事实上，这些节点是默认存在的。它本质上是一个只含有存储功能，且存储值永远为1的单元。在神经网络的每个层次中，除了输出层以外，都会含有这样一个偏置单元。正如线性回归模型与逻辑回归模型中的一样。 偏置单元与后一层的所有节点都有连接，我们设这些参数值为向量b，称之为偏置。如下图。图23 两层神经网络（考虑偏置节点）&nbsp; 可以看出，偏置节点很好认，因为其没有输入（前一层中没有箭头指向它）。有些神经网络的结构图中会把偏置节点明显画出来，有些不会。一般情况下，我们都不会明确画出偏置节点。&nbsp; 在考虑了偏置以后的一个神经网络的矩阵运算如下：&nbsp;&nbsp;g(W(1)&nbsp;&nbsp;a(1)&nbsp;+ b(1)) =&nbsp;a(2);&nbsp;g(W(2)&nbsp;&nbsp;a(2)&nbsp;+ b(2)) =&nbsp;z;&nbsp; 需要说明的是，在两层神经网络中，我们不再使用sgn函数作为函数g，而是使用平滑函数sigmoid作为函数g。我们把函数g也称作激活函数（active&nbsp;function）。 事实上，神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系。初学者可能认为画神经网络的结构图是为了在程序中实现这些圆圈与线，但在一个神经网络的程序中，既没有“线”这个对象，也没有“单元”这个对象。实现一个神经网络最需要的是线性代数库。 3.效果 与单层神经网络不同。理论证明，两层神经网络可以无限逼近任意连续函数。 这是什么意思呢？也就是说，面对复杂的非线性分类任务，两层（带一个隐藏层）神经网络可以分类的很好。 下面就是一个例子（此两图来自colah的博客），红色的线与蓝色的线代表数据。而红色区域和蓝色区域代表由神经网络划开的区域，两者的分界线就是决策分界。图24 两层神经网络（决策分界） 可以看到，这个两层神经网络的决策分界是非常平滑的曲线，而且分类的很好。有趣的是，前面已经学到过，单层网络只能做线性分类任务。而两层神经网络中的后一层也是线性分类层，应该只能做线性分类任务。为什么两个线性分类任务结合就可以做非线性分类任务？ 我们可以把输出层的决策分界单独拿出来看一下。就是下图。图25 两层神经网络（空间变换）&nbsp; 可以看到，输出层的决策分界仍然是直线。关键就是，从输入层到隐藏层时，数据发生了空间变换。也就是说，两层神经网络中，隐藏层对原始的数据进行了一个空间变换，使其可以被线性分类，然后输出层的决策分界划出了一个线性分类分界线，对其进行分类。 这样就导出了两层神经网络可以做非线性分类的关键–隐藏层。联想到我们一开始推导出的矩阵公式，我们知道，矩阵和向量相乘，本质上就是对向量的坐标空间进行一个变换。因此，隐藏层的参数矩阵的作用就是使得数据的原始坐标空间从线性不可分，转换成了线性可分。 两层神经网络通过两层的线性模型模拟了数据内真实的非线性函数。因此，多层的神经网络的本质就是复杂函数拟合。 下面来讨论一下隐藏层的节点数设计。在设计一个神经网络时，输入层的节点数需要与特征的维度匹配，输出层的节点数要与目标的维度匹配。而中间层的节点数，却是由设计者指定的。因此，“自由”把握在设计者的手中。但是，节点数设置的多少，却会影响到整个模型的效果。如何决定这个自由层的节点数呢？目前业界没有完善的理论来指导这个决策。一般是根据经验来设置。较好的方法就是预先设定几个可选值，通过切换这几个值来看整个模型的预测效果，选择效果最好的值作为最终选择。这种方法又叫做Grid&nbsp;Search（网格搜索）。 了解了两层神经网络的结构以后，我们就可以看懂其它类似的结构图。例如EasyPR字符识别网络架构（下图）。图26 EasyPR字符识别网络&nbsp; EasyPR使用了字符的图像去进行字符文字的识别。输入是120维的向量。输出是要预测的文字类别，共有65类。根据实验，我们测试了一些隐藏层数目，发现当值为40时，整个网络在测试集上的效果较好，因此选择网络的最终结构就是120，40，65。 4.训练 下面简单介绍一下两层神经网络的训练。 在Rosenblat提出的感知器模型中，模型中的参数可以被训练，但是使用的方法较为简单，并没有使用目前机器学习中通用的方法，这导致其扩展性与适用性非常有限。从两层神经网络开始，神经网络的研究人员开始使用机器学习相关的技术进行神经网络的训练。例如用大量的数据（1000-10000左右），使用算法进行优化等等，从而使得模型训练可以获得性能与数据利用上的双重优势。 机器学习模型训练的目的，就是使得参数尽可能的与真实的模型逼近。具体做法是这样的。首先给所有参数赋上随机值。我们使用这些随机生成的参数值，来预测训练数据中的样本。样本的预测目标为yp，真实目标为y。那么，定义一个值loss，计算公式如下。loss = (yp&nbsp;- y)2&nbsp; 这个值称之为损失（loss），我们的目标就是使对所有训练数据的损失和尽可能的小。 如果将先前的神经网络预测的矩阵公式带入到yp中（因为有z=yp），那么我们可以把损失写为关于参数（parameter）的函数，这个函数称之为损失函数（loss function）。下面的问题就是求：如何优化参数，能够让损失函数的值最小。 此时这个问题就被转化为一个优化问题。一个常用方法就是高等数学中的求导，但是这里的问题由于参数不止一个，求导后计算导数等于0的运算量很大，所以一般来说解决这个优化问题使用的是梯度下降算法。梯度下降算法每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。 在神经网络模型中，由于结构复杂，每次计算梯度的代价很大。因此还需要使用反向传播算法。反向传播算法是利用了神经网络的结构进行的计算。不一次计算所有参数的梯度，而是从后往前。首先计算输出层的梯度，然后是第二个参数矩阵的梯度，接着是中间层的梯度，再然后是第一个参数矩阵的梯度，最后是输入层的梯度。计算结束以后，所要的两个参数矩阵的梯度就都有了。 反向传播算法可以直观的理解为下图。梯度的计算从后往前，一层层反向传播。前缀E代表着相对导数的意思。图27 反向传播算法&nbsp; 反向传播算法的启示是数学中的链式法则。在此需要说明的是，尽管早期神经网络的研究人员努力从生物学中得到启发，但从BP算法开始，研究者们更多地从数学上寻求问题的最优解。不再盲目模拟人脑网络是神经网络研究走向成熟的标志。正如科学家们可以从鸟类的飞行中得到启发，但没有必要一定要完全模拟鸟类的飞行方式，也能制造可以飞天的飞机。 优化问题只是训练中的一个部分。机器学习问题之所以称为学习问题，而不是优化问题，就是因为它不仅要求数据在训练集上求得一个较小的误差，在测试集上也要表现好。因为模型最终是要部署到没有见过训练数据的真实场景。提升模型在测试集上的预测效果的主题叫做泛化（generalization），相关方法被称作正则化（regularization）。神经网络中常用的泛化技术有权重衰减等。 5.影响 两层神经网络在多个地方的应用说明了其效用与价值。10年前困扰神经网络界的异或问题被轻松解决。神经网络在这个时候，已经可以发力于语音识别，图像识别，自动驾驶等多个领域。 历史总是惊人的相似，神经网络的学者们再次登上了《纽约时报》的专访。人们认为神经网络可以解决许多问题。就连娱乐界都开始受到了影响，当年的《终结者》电影中的阿诺都赶时髦地说一句：我的CPU是一个神经网络处理器，一个会学习的计算机。 但是神经网络仍然存在若干的问题：尽管使用了BP算法，一次神经网络的训练仍然耗时太久，而且困扰训练优化的一个问题就是局部最优解问题，这使得神经网络的优化较为困难。同时，隐藏层的节点数需要调参，这使得使用不太方便，工程和研究人员对此多有抱怨。 90年代中期，由Vapnik等人发明的SVM（Support Vector Machines，支持向量机）算法诞生，很快就在若干个方面体现出了对比神经网络的优势：无需调参；高效；全局最优解。基于以上种种理由，SVM迅速打败了神经网络算法成为主流。图28 Vladimir Vapnik&nbsp; 神经网络的研究再次陷入了冰河期。当时，只要你的论文中包含神经网络相关的字眼，非常容易被会议和期刊拒收，研究界那时对神经网络的不待见可想而知。&nbsp;五. 多层神经网络（深度学习） 1.引子 在被人摒弃的10年中，有几个学者仍然在坚持研究。这其中的棋手就是加拿大多伦多大学的Geoffery Hinton教授。 2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。与传统的训练方式不同，“深度信念网络”有一个“预训练”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“微调”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词–“深度学习”。&nbsp; 很快，深度学习在语音识别领域暂露头角。接着，2012年，深度学习技术又在图像识别领域大展拳脚。Hinton与他的学生在ImageNet竞赛中，用多层的卷积神经网络成功地对包含一千类别的一百万张图片进行了训练，取得了分类错误率15%的好成绩，这个成绩比第二名高了近11个百分点，充分证明了多层神经网络识别效果的优越性。 在这之后，关于深度神经网络的研究与应用不断涌现。图29 Geoffery Hinton&nbsp;&nbsp; 由于篇幅原因，本文不介绍CNN（Conventional Neural Network，卷积神经网络）与RNN（Recurrent Neural Network，递归神经网络）的架构，下面我们只讨论普通的多层神经网络。 2.结构 我们延续两层神经网络的方式来设计一个多层神经网络。 在两层神经网络的输出层后面，继续添加层次。原来的输出层变成中间层，新加的层次成为新的输出层。所以可以得到下图。图30 多层神经网络&nbsp; 依照这样的方式不断添加，我们可以得到更多层的多层神经网络。公式推导的话其实跟两层神经网络类似，使用矩阵运算的话就仅仅是加一个公式而已。 在已知输入a(1)，参数W(1)，W(2)，W(3)的情况下，输出z的推导公式如下：&nbsp; &nbsp; &nbsp;g(W(1)&nbsp;&nbsp;a(1)) =&nbsp;a(2);&nbsp;&nbsp; &nbsp; g(W(2)&nbsp;&nbsp;a(2)) =&nbsp;a(3);g(W(3)&nbsp;&nbsp;a(3)) = z;&nbsp; 多层神经网络中，输出也是按照一层一层的方式来计算。从最外面的层开始，算出所有单元的值以后，再继续计算更深一层。只有当前层所有单元的值都计算完毕以后，才会算下一层。有点像计算向前不断推进的感觉。所以这个过程叫做“正向传播”。 下面讨论一下多层神经网络中的参数。 首先我们看第一张图，可以看出W(1)中有6个参数，W(2)中有4个参数，W(3)中有6个参数，所以整个神经网络中的参数有16个（这里我们不考虑偏置节点，下同）。&nbsp;图31 多层神经网络（较少参数）&nbsp; 假设我们将中间层的节点数做一下调整。第一个中间层改为3个单元，第二个中间层改为4个单元。 经过调整以后，整个网络的参数变成了33个。&nbsp;图32 多层神经网络（较多参数）&nbsp; 虽然层数保持不变，但是第二个神经网络的参数数量却是第一个神经网络的接近两倍之多，从而带来了更好的表示（represention）能力。表示能力是多层神经网络的一个重要性质，下面会做介绍。 在参数一致的情况下，我们也可以获得一个“更深”的网络。&nbsp;图33 多层神经网络（更深的层次）&nbsp; 上图的网络中，虽然参数数量仍然是33，但却有4个中间层，是原来层数的接近两倍。这意味着一样的参数数量，可以用更深的层次去表达。 3.效果 与两层层神经网络不同。多层神经网络中的层数增加了很多。 增加更多的层次有什么好处？更深入的表示特征，以及更强的函数模拟能力。 更深入的表示特征可以这样理解，随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。 关于逐层特征学习的例子，可以参考下图。&nbsp;图34 多层神经网络（特征学习）&nbsp; 更强的函数模拟能力是由于随着层数的增加，整个网络的参数就越多。而神经网络其实本质就是模拟特征与目标之间的真实关系函数的方法，更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的容量（capcity）去拟合真正的关系。 通过研究发现，在参数数量一样的情况下，更深的网络往往具有比浅层的网络更好的识别效率。这点也在ImageNet的多次大赛中得到了证实。从2012年起，每年获得ImageNet冠军的深度神经网络的层数逐年增加，2015年最好的方法GoogleNet是一个多达22层的神经网络。 在最新一届的ImageNet大赛上，目前拿到最好成绩的MSRA团队的方法使用的更是一个深达152层的网络！关于这个方法更多的信息有兴趣的可以查阅ImageNet网站。 4.训练 在单层神经网络时，我们使用的激活函数是sgn函数。到了两层神经网络时，我们使用的最多的是sigmoid函数。而到了多层神经网络时，通过一系列的研究发现，ReLU函数在训练多层神经网络时，更容易收敛，并且预测性能更好。因此，目前在深度学习中，最流行的非线性函数是ReLU函数。ReLU函数不是传统的非线性函数，而是分段线性函数。其表达式非常简单，就是y=max(x,0)。简而言之，在x大于0，输出就是输入，而在x小于0时，输出就保持为0。这种函数的设计启发来自于生物神经元对于激励的线性响应，以及当低于某个阈值后就不再响应的模拟。 在多层神经网络中，训练的主题仍然是优化和泛化。当使用足够强的计算芯片（例如GPU图形加速卡）时，梯度下降算法以及反向传播算法在多层神经网络中的训练中仍然工作的很好。目前学术界主要的研究既在于开发新的算法，也在于对这两个算法进行不断的优化，例如，增加了一种带动量因子（momentum）的梯度下降算法。 在深度学习中，泛化技术变的比以往更加的重要。这主要是因为神经网络的层数增加了，参数也增加了，表示能力大幅度增强，很容易出现过拟合现象。因此正则化技术就显得十分重要。目前，Dropout技术，以及数据扩容（Data-Augmentation）技术是目前使用的最多的正则化技术。 5.影响 目前，深度神经网络在人工智能界占据统治地位。但凡有关人工智能的产业报道，必然离不开深度学习。神经网络界当下的四位引领者除了前文所说的Ng，Hinton以外，还有CNN的发明人Yann Lecun，以及《Deep Learning》的作者Bengio。 前段时间一直对人工智能持谨慎态度的马斯克，搞了一个OpenAI项目，邀请Bengio作为高级顾问。马斯克认为，人工智能技术不应该掌握在大公司如Google，Facebook的手里，更应该作为一种开放技术，让所有人都可以参与研究。马斯克的这种精神值得让人敬佩。&nbsp; &nbsp;图35 Yann LeCun（左）和&nbsp;Yoshua Bengio（右）&nbsp; 多层神经网络的研究仍在进行中。现在最为火热的研究技术包括RNN，LSTM等，研究方向则是图像理解方面。图像理解技术是给计算机一幅图片，让它用语言来表达这幅图片的意思。ImageNet竞赛也在不断召开，有更多的方法涌现出来，刷新以往的正确率。&nbsp;六. 回顾 1.影响 我们回顾一下神经网络发展的历程。神经网络的发展历史曲折荡漾，既有被人捧上天的时刻，也有摔落在街头无人问津的时段，中间经历了数次大起大落。 从单层神经网络（感知器）开始，到包含一个隐藏层的两层神经网络，再到多层的深度神经网络，一共有三次兴起过程。详见下图。&nbsp;图36 三起三落的神经网络&nbsp; 上图中的顶点与谷底可以看作神经网络发展的高峰与低谷。图中的横轴是时间，以年为单位。纵轴是一个神经网络影响力的示意表示。如果把1949年Hebb模型提出到1958年的感知机诞生这个10年视为落下（没有兴起）的话，那么神经网络算是经历了“三起三落”这样一个过程，跟“小平”同志类似。俗话说，天将降大任于斯人也，必先苦其心志，劳其筋骨。经历过如此多波折的神经网络能够在现阶段取得成功也可以被看做是磨砺的积累吧。 历史最大的好处是可以给现在做参考。科学的研究呈现螺旋形上升的过程，不可能一帆风顺。同时，这也给现在过分热衷深度学习与人工智能的人敲响警钟，因为这不是第一次人们因为神经网络而疯狂了。1958年到1969年，以及1985年到1995，这两个十年间人们对于神经网络以及人工智能的期待并不现在低，可结果如何大家也能看的很清楚。 因此，冷静才是对待目前深度学习热潮的最好办法。如果因为深度学习火热，或者可以有“钱景”就一窝蜂的涌入，那么最终的受害人只能是自己。神经网络界已经两次有被人们捧上天了的境况，相信也对于捧得越高，摔得越惨这句话深有体会。因此，神经网络界的学者也必须给这股热潮浇上一盆水，不要让媒体以及投资家们过分的高看这门技术。很有可能，三十年河东，三十年河西，在几年后，神经网络就再次陷入谷底。根据上图的历史曲线图，这是很有可能的。 2.效果 下面说一下神经网络为什么能这么火热？简而言之，就是其学习效果的强大。随着神经网络的发展，其表示性能越来越强。 从单层神经网络，到两层神经网络，再到多层神经网络，下图说明了，随着网络层数的增加，以及激活函数的调整，神经网络所能拟合的决策分界平面的能力。&nbsp;图37 表示能力不断增强&nbsp; 可以看出，随着层数增加，其非线性分界拟合能力不断增强。图中的分界线并不代表真实训练出的效果，更多的是示意效果。 神经网络的研究与应用之所以能够不断地火热发展下去，与其强大的函数拟合能力是分不开关系的。 3.外因 当然，光有强大的内在能力，并不一定能成功。一个成功的技术与方法，不仅需要内因的作用，还需要时势与环境的配合。神经网络的发展背后的外在原因可以被总结为：更强的计算性能，更多的数据，以及更好的训练方法。只有满足这些条件时，神经网络的函数拟合能力才能得已体现，见下图。&nbsp;图38 发展的外在原因&nbsp; 之所以在单层神经网络年代，Rosenblat无法制作一个双层分类器，就在于当时的计算性能不足，Minsky也以此来打压神经网络。但是Minsky没有料到，仅仅10年以后，计算机CPU的快速发展已经使得我们可以做两层神经网络的训练，并且还有快速的学习算法BP。 但是在两层神经网络快速流行的年代。更高层的神经网络由于计算性能的问题，以及一些计算方法的问题，其优势无法得到体现。直到2012年，研究人员发现，用于高性能计算的图形加速卡（GPU）可以极佳地匹配神经网络训练所需要的要求：高并行性，高存储，没有太多的控制需求，配合预训练等算法，神经网络才得以大放光彩。 互联网时代，大量的数据被收集整理，更好的训练方法不断被发现。所有这一切都满足了多层神经网络发挥能力的条件。 “时势造英雄”，正如Hinton在2006年的论文里说道的 “…&nbsp;provided&nbsp;that&nbsp;computers&nbsp;were&nbsp;fast&nbsp;enough,&nbsp;data&nbsp;sets&nbsp;were&nbsp;big&nbsp;enough,&nbsp;and&nbsp;the&nbsp;initial&nbsp;weights&nbsp;were&nbsp;close&nbsp;enough&nbsp;to&nbsp;a&nbsp;good&nbsp;solution.&nbsp;All&nbsp;three&nbsp;conditions&nbsp;are&nbsp;now&nbsp;satisfied.”，&nbsp; 外在条件的满足也是神经网络从神经元得以发展到目前的深度神经网络的重要因素。 除此以外，一门技术的发扬没有“伯乐”也是不行的。在神经网络漫长的历史中，正是由于许多研究人员的锲而不舍，不断钻研，才能有了现在的成就。前期的Rosenblat，Rumelhart没有见证到神经网络如今的流行与地位。但是在那个时代，他们为神经网络的发展所打下的基础，却会永远流传下去，不会退色。&nbsp;七. 展望 1.量子计算 回到我们对神经网络历史的讨论，根据历史趋势图来看，神经网络以及深度学习会不会像以往一样再次陷入谷底？作者认为，这个过程可能取决于量子计算机的发展。 根据一些最近的研究发现，人脑内部进行的计算可能是类似于量子计算形态的东西。而且目前已知的最大神经网络跟人脑的神经元数量相比，仍然显得非常小，仅不及1%左右。所以未来真正想实现人脑神经网络的模拟，可能需要借助量子计算的强大计算能力。 各大研究组也已经认识到了量子计算的重要性。谷歌就在开展量子计算机D-wave的研究，希望用量子计算来进行机器学习，并且在前段时间有了突破性的进展。国内方面，阿里和中科院合作成立了量子计算实验室，意图进行量子计算的研究。 如果量子计算发展不力，仍然需要数十年才能使我们的计算能力得以突飞猛进的发展，那么缺少了强大计算能力的神经网络可能会无法一帆风顺的发展下去。这种情况可以类比为80-90年时期神经网络因为计算能力的限制而被低估与忽视。假设量子计算机真的能够与神经网络结合，并且助力真正的人工智能技术的诞生，而且量子计算机发展需要10年的话，那么神经网络可能还有10年的发展期。直到那时期以后，神经网络才能真正接近实现AI这一目标。&nbsp;图39 量子计算&nbsp; 2.人工智能 最后，作者想简单地谈谈对目前人工智能的看法。虽然现在人工智能非常火热，但是距离真正的人工智能还有很大的距离。就拿计算机视觉方向来说，面对稍微复杂一些的场景，以及易于混淆的图像，计算机就可能难以识别。因此，这个方向还有很多的工作要做。 就普通人看来，这么辛苦的做各种实验，以及投入大量的人力就是为了实现一些不及孩童能力的视觉能力，未免有些不值。但是这只是第一步。虽然计算机需要很大的运算量才能完成一个普通人简单能完成的识图工作，但计算机最大的优势在于并行化与批量推广能力。使用计算机以后，我们可以很轻易地将以前需要人眼去判断的工作交给计算机做，而且几乎没有任何的推广成本。这就具有很大的价值。正如火车刚诞生的时候，有人嘲笑它又笨又重，速度还没有马快。但是很快规模化推广的火车就替代了马车的使用。人工智能也是如此。这也是为什么目前世界上各著名公司以及政府都对此热衷的原因。 目前看来，神经网络要想实现人工智能还有很多的路要走，但方向至少是正确的，下面就要看后来者的不断努力了。图40 人工智能&nbsp;八 总结 本文回顾了神经网络的发展历史，从神经元开始，历经单层神经网络，两层神经网络，直到多层神经网络。在历史介绍中穿插讲解神经网络的结构，分类效果以及训练方法等。本文说明了神经网络内部实际上就是矩阵计算，在程序中的实现没有“点”和“线”的对象。本文说明了神经网络强大预测能力的根本，就是多层的神经网络可以无限逼近真实的对应函数，从而模拟数据之间的真实关系。除此之外，本文回顾了神经网络发展的历程，分析了神经网络发展的外在原因，包括计算能力的增强，数据的增多，以及方法的创新等。最后，本文对神经网络的未来进行了展望，包括量子计算与神经网络结合的可能性，以及探讨未来人工智能发展的前景与价值。&nbsp;九.&nbsp;后记 本篇文章可以视为作者一年来对神经网络的理解与总结，包括实验的体会，书籍的阅读，以及思考的火花等。神经网络虽然重要，但学习并不容易。这主要是由于其结构图较为难懂，以及历史发展的原因，导致概念容易混淆，一些介绍的博客与网站内容新旧不齐。本篇文章着眼于这些问题，没有太多的数学推导，意图以一种简单的，直观的方式对神经网络进行讲解。在2015年最后一天终于写完。希望本文可以对各位有所帮助。&nbsp;&nbsp; 作者很感谢能够阅读到这里的读者。如果看完觉得好的话，还请轻轻点一下赞，你们的鼓励就是作者继续行文的动力。本文的备注部分是一些对神经网络学习的建议，供补充阅读与参考。 目前为止，EasyPR的1.4版已经将神经网络（ANN）训练的模块加以开放，开发者们可以使用这个模块来进行自己的字符模型的训练。有兴趣的可以下载。&nbsp;十. 备注 神经网络虽然很重要，但是对于神经网络的学习，却并不容易。这些学习困难主要来自以下三个方面：概念，类别，教程。下面简单说明这三点。 1.概念 对于一门技术的学习而言，首先最重要的是弄清概念。只有将概念理解清楚，才能顺畅的进行后面的学习。由于神经网络漫长的发展历史，经常会有一些概念容易混淆，让人学习中产生困惑。这里面包括历史的术语，不一致的说法，以及被遗忘的研究等。 历史的术语 这个的代表就是多层感知器（MLP）这个术语。起初看文献时很难理解的一个问题就是，为什么神经网络又有另一个名称：MLP。其实MLP（Multi-Layer Perceptron）的名称起源于50-60年代的感知器（Perceptron）。由于我们在感知器之上又增加了一个计算层，因此称为多层感知器。值得注意的是，虽然叫“多层”，MLP一般都指的是两层（带一个隐藏层的）神经网络。 MLP这个术语属于历史遗留的产物。现在我们一般就说神经网络，以及深度神经网络。前者代表带一个隐藏层的两层神经网络，也是EasyPR目前使用的识别网络，后者指深度学习的网络。 不一致的说法 这个最明显的代表就是损失函数loss&nbsp;function，这个还有两个说法是跟它完全一致的意思，分别是残差函数error&nbsp;function，以及代价函数cost&nbsp;function。loss&nbsp;function是目前深度学习里用的较多的一种说法，caffe里也是这么叫的。cost&nbsp;function则是Ng在coursera教学视频里用到的统一说法。这三者都是同一个意思，都是优化问题所需要求解的方程。虽然在使用的时候不做规定，但是在听到各种讲解时要心里明白。 再来就是权重weight和参数parameter的说法，神经网络界由于以前的惯例，一般会将训练得到的参数称之为权重，而不像其他机器学习方法就称之为参数。这个需要记住就好。不过在目前的使用惯例中，也有这样一种规定。那就是非偏置节点连接上的值称之为权重，而偏置节点上的值称之为偏置，两者统一起来称之为参数。 另外一个同义词就是激活函数active function和转移函数transfer function了。同样，他们代表一个意思，都是叠加的非线性函数的说法。 被遗忘的研究 由于神经网络发展历史已经有70年的漫长历史，因此在研究过程中，必然有一些研究分支属于被遗忘阶段。这里面包括各种不同的网络，例如SOM（Self-Organizing Map，自组织特征映射网络），SNN（Synergetic Neural Network，协同神经网络），ART（Adaptive Resonance Theory，自适应共振理论网络）等等。所以看历史文献时会看到许多没见过的概念与名词。 有些历史网络甚至会重新成为新的研究热点，例如RNN与LSTM就是80年代左右开始的研究，目前已经是深度学习研究中的重要一门技术，在语音与文字识别中有很好的效果。 对于这些易于混淆以及弄错的概念，务必需要多方参考文献，理清上下文，这样才不会在学习与阅读过程中迷糊。 2.类别 下面谈一下关于神经网络中的不同类别。 其实本文的名字“神经网络浅讲”并不合适，因为本文并不是讲的是“神经网络”的内容，而是其中的一个子类，也是目前最常说的前馈神经网络。根据下图的分类可以看出。&nbsp;图41 神经网络的类别&nbsp; 神经网络其实是一个非常宽泛的称呼，它包括两类，一类是用计算机的方式去模拟人脑，这就是我们常说的ANN（人工神经网络），另一类是研究生物学上的神经网络，又叫生物神经网络。对于我们计算机人士而言，肯定是研究前者。 在人工神经网络之中，又分为前馈神经网络和反馈神经网络这两种。那么它们两者的区别是什么呢？这个其实在于它们的结构图。我们可以把结构图看作是一个有向图。其中神经元代表顶点，连接代表有向边。对于前馈神经网络中，这个有向图是没有回路的。你可以仔细观察本文中出现的所有神经网络的结构图，确认一下。而对于反馈神经网络中，结构图的有向图是有回路的。反馈神经网络也是一类重要的神经网络。其中Hopfield网络就是反馈神经网络。深度学习中的RNN也属于一种反馈神经网络。 具体到前馈神经网络中，就有了本文中所分别描述的三个网络：单层神经网络，双层神经网络，以及多层神经网络。深度学习中的CNN属于一种特殊的多层神经网络。另外，在一些Blog中和文献中看到的BP神经网络是什么？其实它们就是使用了反向传播BP算法的两层前馈神经网络。也是最普遍的一种两层神经网络。 通过以上分析可以看出，神经网络这种说法其实是非常广义的，具体在文章中说的是什么网络，需要根据文中的内容加以区分。 3.教程 如何更好的学习神经网络，认真的学习一门课程或者看一本著作都是很有必要的。 说到网络教程的话，这里必须说一下Ng的机器学习课程。对于一个初学者而言，Ng的课程视频是非常有帮助的。Ng一共开设过两门机器学习公开课程：一个是2003年在Standford开设的，面向全球的学生，这个视频现在可以在网易公开课上找到；另一个是2010年专门为Coursera上的用户开设的，需要登陆Coursera上才能学习。 但是，需要注意点是，这两个课程对待神经网络的态度有点不同。早些的课程一共有20节课，Ng花了若干节课去专门讲SVM以及SVM的推导，而当时的神经网络，仅仅放了几段视频，花了大概不到20分钟（一节课60分钟左右）。而到了后来的课程时，总共10节的课程中，Ng给了完整的两节给神经网络，详细介绍了神经网络的反向传播算法。同时给SVM只有一节课，并且没有再讲SVM的推导过程。下面两张图分别是Ng介绍神经网络的开篇，可以大致看出一些端倪。&nbsp;图42 Ng与神经网络&nbsp; 为什么Ng对待神经网络的反应前后相差那么大？事实上就是深度学习的原因。Ng实践了深度学习的效果，认识到深度学习的基础–神经网络的重要性。这就是他在后面重点介绍神经网络的原因。总之，对于神经网络的学习而言，我更推荐Coursera上的。因为在那个时候，Ng才是真正的把神经网络作为一门重要的机器学习方法去传授。你可以从他上课的态度中感受到他的重视，以及他希望你能学好的期望。&nbsp;版权说明： 本文中的所有文字，图片，代码的版权都是属于作者和博客园共同所有。欢迎转载，但是务必注明作者与出处。任何未经允许的剽窃以及爬虫抓取都属于侵权，作者和博客园保留所有权利。&nbsp;&nbsp;参考文献： 1.Neural Networks 2.Andrew Ng&nbsp;Neural Networks&nbsp; 3.神经网络简史 4.中科院 史忠植 神经网络&nbsp;讲义 5.深度学习 胡晓林&nbsp;L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":110,"height":220},"mobile":{"show":false},"log":false,"tagMode":false});]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>机器学习，神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微软笔试算法（一）]]></title>
    <url>%2F2018%2F05%2F29%2F%E5%BE%AE%E8%BD%AF%E7%AC%94%E8%AF%95%E7%AE%97%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[最近求职中接到的好多外包公司的电话，一般我都是拒绝的，其中告知外包到微软的就有三家，有一家说是微软小冰项目，给了我八道算法题，都不是太难，有的在LeetCode上还做过，在这里记录一下，正好也整理下LeetCode上刷过的题。 这里记录的算法是只用python实现，也是我目前能想到的最优解，一共八个问题。 完成一个函数：def Add(num1, num2):其中，两个输入都是数字，都是字符串（如：“12345678986543210123456789”），要求计算两个数字的和，返回一个字符串，不能使用内置函数，如int，long等。例如，输入两个数字是：“1000000000000000”和“-1”，返回“999999999999999”。 这个问题比较简单了，主要就是三个小问题： 字符串和数字的互转 加法的进位 减法的借位 字符串和数字的互转不让用int函数，这个简单，python只要写个字典就解决了，C的话就要写switch case来一个一个比较了。这里就解决了字符转数字的问题了。 到的数字开始计算，因为要设计借位和进位，答案会有四种情况 正数 + 正数 = 正数 2 + 3 = 5 负数 + 负数 = 负数 -2 + -3 = -5 大正数 + 负数 = 正数 -2 + 3 = 1 正数 + 大负数 = 负数 2 + -3 = -1 貌似就是这样，但是观察结果除了负号就是两种，所以程序一开始应该先分析负号，进位的情况有1,2，借位情况是3,4，正好这两个结果除了负号答案一样，所以分两类就OK 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# 问题 1def question1_add(num1, num2): # 创建个字典用来把字符转为int便于计算 def strtoint(str): numdict = &#123;"0": 0, "1": 1, "2": 2, "3": 3, "4": 4, "5": 5, "6": 6, "7": 7, "8": 8, "9": 9&#125; return numdict.get(str) # 把列表中的数字转会字符串 def inttostr(lt): ls2 = [str(i) for i in lt] return ''.join(ls2) lt, x, y = [], [], [] # 两个数是否为负数的标记 minusflag1, minusflag2 = 0, 0 # 首先先提取出负号，记录负号的情况 if num1[0] is "-": x = list(map(strtoint, num1[1:])) minusflag1 = 1 else: x = list(map(strtoint, num1)) if num2[0] is "-": y = list(map(strtoint, num2[1:])) minusflag2 = 1 else: y = list(map(strtoint, num2)) # 把 x作为最长的那列，y是短的那列 if len(x) &lt; len(y): x, y = y, x maxlen, minlen = len(x), len(y) # 在短的那列前面插上0让两列一样齐以便于计算 for i in range(maxlen-minlen): y.insert(0, 0) # 计算通常是从最低位开始，所以i从最低位开始 # carry记录借位和进位情况 i, carry = 0, 0 # 两个数都是正数或负数 if (minusflag1 == 0 and minusflag2 == 0) or (minusflag1 == 1 and minusflag2 == 1): # +1是为了防止最高位有进位的情况，给最高位加一个0 for j in range(maxlen+1): i -= 1 if j &lt; maxlen: sum = x[i] + y[i] + carry else: sum = carry # 两个数相加大于十代表有进位， 把结果和10的商给新列表，余给进位标志 if sum &gt;= 10: lt.insert(0, sum%10) carry = sum//10 else: lt.insert(0, sum) carry = 0 # 如果最高位没有进位 把0删除 if lt[0] == 0: lt.pop(0) # 如果都为负数 加 - if minusflag1 == 1 and minusflag2 == 1: lt.insert(0, "-") return inttostr(lt) else: return inttostr(lt) # 一正一负 if (minusflag1 == 1 and minusflag2 == 0) or (minusflag1 == 0 and minusflag2 == 1): for j in range(maxlen): i -= 1 sum = x[i] - y[i] + carry if sum &lt; 0: lt.insert(0, sum%10) carry = sum//10 else: lt.insert(0, sum) carry = 0 while lt[0] == 0: lt.pop(0) if len(lt) == 1: break # 判断是否应该加 - 号 if (num1[0] is "-" and len(num1[1:]) &gt; len(num2)) or num2[0] is "-" and len(num2[1:]) &gt; len(num1) : lt.insert(0, "-") return inttostr(lt) return inttostr(lt) 给定一个数组nums，然后对其排序，使得排序结果满足nums[0] &lt; nums[1] &gt; nums[2] &lt; nums[3]…。 例如给定数组nums=[1,2,3,4,5,6,7,8,9],其中一个满足条件的结果是12345.给出一个结果即可（可能无解）。最优解法是O(n)时间复杂度和O(1)空间复杂度。 原题位置：https://leetcode.com/problems/wiggle-sort-ii/ 这道题就比较有趣了，LeetCode上大神给出的算法挺多，但是最优解不是O(n)，用python实现的话还能比O(n)小。 先说我看到题第一眼的想法和看了LeetCode上大神的方法吧，很简单就三行 123nums.sort()median = len(nums[::2]) - 1nums[::2], nums[1::2] = nums[median::-1], nums[:median:-1] 首先排序，找到中位数，然后把中位数左右两边的交叉相排就OK了。 好吧，详细一点吧，S代表比中位数小的，L代表比中位数大的，M代表中位数 1234Small half: M . S . S . S Small half: M . S . S . S .Large half: . L . L . M . Large half: . L . L . L . M-------------------------- --------------------------Together: M L S L S M S Together: M L S L S L S M 根据这个原理还可以写成其他形式123nums.sort()median = len(nums[::2])nums[1::2], nums[::2] = nums[median:],nums[:median] 这个方式简单但是不满足O(n)时间复杂度的要求，这个方法是LeetCode上大神想出的叫virtual indexing，地址123456解释一下site()函数的作用为了防止median的元素挨在一起，也就是说奇数位置上的值是median，同时与他相邻的某个偶数位置上的值也是median导致排序失败为了避免这个问题，可以采用一种方法，即另j 每次移动两步，也就是说先另j直线奇数位置，再另j指向偶数位置，所以对于大小为10的序列，j的变化可能像是这样 1 -&gt; 3 -&gt; 5 -&gt; 7 -&gt; 9 -&gt; 0 -&gt; 2 -&gt; 4 -&gt; 6 -&gt; 8123456789101112131415161718192021222324252627暂且先不考虑怎么实现这样的改变，先说一下这样做带来的好处由上面j的变化可知，j的改变是每次移动两步，所以，根据算法描述。所有和median相等的元素一定是最后才固定位置，又因为当j指向的值等于median时，是不与i和k指向的任何一个元素交换的。所以，每次移动两步带来的结果是这些median永远不可能相邻。换句话说就是永远不会出现两个median挨着的情况```pythondef question2_sort2(nums): def site(n): return (1 + 2 * n) % (len(nums) | 1) nums.sort() if len(nums) % 2 == 0: median = (nums[len(nums) // 2] + nums[len(nums) // 2 - 1]) / 2 else: median = nums[len(nums) // 2] i, j, k = 0, 0, len(nums) - 1 while j &lt;= k: if nums[site(j)] &gt; median: nums[site(i)], nums[site(j)] = nums[site(j)], nums[site(i)] i += 1 j += 1 elif nums[site(j)] &lt; median: nums[site(j)], nums[site(k)] = nums[site(k)], nums[site(j)] k -= 1 j += 1 # 没错就是这里，我认为执行过交换后j就在正确的位置上了，所以我让j+1结果就是循环次数减少了，如果进行j+1则会循环N次，两种的结果不同但都复合条件 else: j += 1 return nums 写一个函数，输入是两个int数组A和B。要求从A和B中分别取出一个数，使他们的和为20。打印出所有的组合。要求数字在数组中的位置和数字本身。比如输入为 A = [18, 2, 7, 8, 3], B = [17, 1, 19]，输出为 3 (A4) + 17 (B0) = 20，表示A的第4个元素是3，B的第0个元素是17 这就是道送分题了，嵌套循环就OK了，没什么好说的。1234567def question3_take(a, b): for i in range(len(a)): for j in range(len(b)): if a[i] + b[j] == 20: print("%d (A%d) + %d (B%d) = 20" % (a[i], i, b[j], j)) return print(" No solution!") 写一个函数，输入一个随机的01序列，打印出这个序列中最长的01交替出现的序列的起始位置和结束位置。例如：输入“000101010101101”，输出起始位置2, 结束位置10 这题让我纠结了下，因为需要记录的位置有点多 因为我们要遍历这个序列中所有的01所以要两个指针同时移动才能确保01的出现，当出现01我们计数器n +1，然后判断是否到结尾或者下一个不是01的情况，我们就结束这段，记录这段的长度和结束开始的位置，如果下一段出现的01长我们就替换掉上次的记录。 值得注意的是01可能出现在序列的奇数或偶数位上，所以要遍历两次，再比较奇偶上哪个长，返回长的那段。 12345678910111213141516171819202122232425def func(i, j, num): start, end, n, count = 0, 0, 0, 0 while j &lt; len(num): if num[i] is '0' and num[j] is '1': n += 1 if j+1 &gt;= len(num) - 1 or num[i+2] is '1' or num[j+2] is '0': if n &gt; count: count = n n = 0 end = j + 1 start = end - count*2 i += 2 j += 2 return start, enddef question4(num): i, j = 0, 1 start1, end1 = func(i, j, num) i, j = 1, 2 start2, end2 = func(i, j, num) if end1 - start1 &gt; end2- start2: return start1, end1 else: return start2, end2]]></content>
      <categories>
        <category>算法练习</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转】机器学习法则：ML工程的最佳实践]]></title>
    <url>%2F2018%2F05%2F21%2F%5B%E8%BD%AC%5D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B3%95%E5%88%99%EF%BC%9AML%E5%B7%A5%E7%A8%8B%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[机器学习法则：ML工程的最佳实践 作者无邪机器学习研究者，人工智障推进者。Martin Zinkevich 在2016年将 google 内容多年关于机器学习相关的经验分享了出来，这篇文章是对该分享的一些翻译+解读，如果想查看原文请参见：https://developers.google.com/machine-learning/rules-of-ml/&nbsp;。术语在说到具体的相关经验之前，先来了解下常用的术语。示例（Instance）：那些你要为其做出预测的事物称为示例。例如，示例可能是一个网页，你要将其归为“关于猫的”网页或者“不是关于猫的”网页。标签（Label）：预测任务的答案或结果称为标签。无论是机器学习系统的答案或结果，还是训练数据的答案或结果，都可以称为标签。例如，将网页标记为“关于猫的”。特征（Feature）：预测任务中示例的属性即为“特征”。例如，网页可以有“包含词汇‘猫’”的特征。特征栏（Feature Column）：特征栏是相关特征的集合，如用户所住地区存在的所有可能国籍的集合。在同一个样本的同一个特征栏中可能有一个或多个特征。特征栏相当于（雅虎或微软的）虚拟机系统的 “命名空间（namespace）”或“域（field）”。样本（Example）：样本包含示例（具有各种特征）和一个标签。模型（Model）：模型是预测任务的数学表达形式。先是通过样本训练模型，而后利用模型做出预测。指标（Metric）：指标是指一系列的数字，这些数字直接或间接的都被优化过。目标（Objective）：目标是指算法经过优化，努力要达到的度量标准。工作流（Pipeline）：工作流指的是围绕机器学习算法而存在的基础架构。从前端搜集数据、将搜集到的数据放入训练数据文件夹、训练一个或多个模型以及将模型用于生产等过程，都属于工作流。点击率（Click-through Rate）：用户浏览网页时对包含广告链接的点击次数占浏览次数的百分比。概述要想创造出优秀的产品：你需要以一位优秀工程师的身份去运用深度学习！记住！你不单单是一位机器学习的研究者！事实上，你所面临的大多数问题都是工程问题。即便拥有足以媲美机器学习专家的理论知识，要想有所突破，大多数情况下都在依赖示例的良好特征，而非优秀的机器学习算法。因此，基本方法如下：确保你的工作流各连接端十分可靠从树立合理的目标开始用简单的方式，添加符合常识的特征确保你的工作流始终可靠这种方法能带来相当多的盈利，也能在较长时间里令许多人都满意，甚至还可能实现双赢。只有在简单技巧不发挥任何作用的情况下，才考虑使用复杂的一些的方法。方法越复杂，产品最终输出速度慢。当所有的简单技巧用完后，很可能就要考虑最前沿机器学习术了。本文档主要由四部分组成：第一部分：帮助你明白是否到了需要构建一个机器学习系统第二部分：部署你的第一个工作流第三部分：往工作流增加新特征时的发布和迭代，以及如何评价模型和训练-服务倾斜（training-serving shew)第四部分：达到稳定阶段后该继续做什么。在机器学习之前Rule #1:Don’t be afraid to launch a product without machine learning.法则 1:不要害怕发布一款没有用到机器学习的产品机器学习是很酷，但它需要数据。如果你认为机器学习可以提高 100% 收益，那么启发式规则可以获得 50% 收益。Rule #2: First, design and implement metrics.法则2：首先需要设计和实现评估指标在构建具体的机器学习系统之前，首先在当前系统中记录尽量详细的历史信息。原因如下：在早期，更容易获得系统用户的权限许可（获得系统用户权限后，更容易收集各种数据）。如果你觉得某个问题以后会受到关注，最好是从现状开始就搜集历史数据。如果设计系统的时候考虑了评估指标，这对将来会大有益处。具体来说，这是为了让你以后不用在日志文件中寻找相关的字符串。你能够注意到什么（随着时间）改变了，什么（随着时间）没有改变。举个例子，假设你想要直接优化一天的活跃用户量。然而在早期对系统的处理中可能会发现用户体验的变化并没有显著改变活跃用户量的度量。Rule #3: Choose machine learning over a complex heuristic.法则3：优先选择机器学习而不是复杂的启发式规则简单的启发式方法可以轻松应用到产品上，而复杂的启发式方法却难以维护。一旦你拥有了足够的数据，并且对要实现的目标有了基本的概念，那就转向机器学习吧。在大多数软件工程中，不管使用的是启发方法还是机器学习模型，都需要经常更新算法。但是你会发现，使用机器学习的模型更容易更新和维护。机器学习阶段 1：第一条工作流构建第一个机器学习工作流时，一定要更多关注系统基础架构的建设。虽然机器学习的算法令人激动，但是因为基础架构不给力找不到问题时会令人抓狂。Rule #4: Keep the first model simple and get the infrastructure right.法则4：第一个模型要简单，但是基础架构要正确第一个模型对你的产品提高最大，因此它不需要有多花哨。相反，你会碰到比你想象的多的基础架构方面的问题。在别人使用你的的新机器学习系统前，你需要确定：如何为你的学习算法得到样本为你的系统初步定义“好”与“坏”的标准如何将模型集成到应用程序中。你可以直接将模型应用到在线应用程序中，也可以在离线样本的基础上对模型进行预计算（pre－compute），然后把与计算的结果储存在表格中。选择简单的特征，这样会更容易确保：特征正确应用到算法中模型能够学习到合理的权重特征正确应用到服务器模型（也就是生产环境的模型）中你的系统如果能够可靠地遵守这三点，你就完成了大多数工作。你的简单模型能够提供基准指标和基准行为，你可以用来测量更加复杂的模型。Rule #5: Test the infrastructure independently from the machine learning.法则5：独立于机器学习来测试架构流程不仅需要确保基础架构的可测试性，还需要确保系统的学习部分（learning part）是封装好的（encapsulated），这样才能测试所有与之相关的部件。具体来说：测试输入到算法中的数据。检查应该填充的特征栏是否正确填充。测试模型在训练算法之外的运行情况。确保模型的训练环境中和服务环境中的得分相同。机器学习的一个特点就是不可预测性。因此，你必须确保在训练和实际运行中创造样本的代码能被测试，并且在实际运行中始终使用同一个固定的模型。Rule #6: Be careful about dropped data when copying pipelines.法则6：复制工作流时留意丢失的数据我们有时候会通过复制已经存在的工作流来创建一个新的工作流。在新的工作流中需要的数据，很可能在旧的数据流就丢弃了。Rule #7: Turn heuristics into features, or handle them externally.法则 7: 将启发规则转化为特征，或者在外部处理它们机器学习系统解决的问题通常都不是新问题，而是对已有问题的进一步优化。这意味着有很多已有的规则或者启发式规则可供使用。这部分信息应该被充分利用。下面是几种启发式规则可以被使用的方式：用启发式规则进行预处理。 若特征相当完美，则可以采用这个方法。举个例子，在垃圾邮件过滤器中，如果发件人已经被加入黑名单了，则可以不用重新学习“黑名单”的概念。直接阻止该信息就可以！这种方法在二元分类（binary classification）任务中很有用。创建特征。 直接从启发式规则中创建特征会很便捷。举个例子，若要用启发式规则为某个查询结果计算相关度，你可以把分数纳入特征的值中。接下来，用机器学习的方法来处理这些值（例如，把这些值转化为由一系列离散值组成的有限集，或者也可以与其它特征相结合），但是要从启发式方法生成的原始数据入手。挖掘启发式方法的原始输入数据。 对于某款 app，若存在一个启发式方法，其包含安装量、文本字符数和当天日期等要素，可以考虑将这些原始信息单独作为特征使用。修改标签。当你发觉启发式方法捕捉了一些信息，而这些信息没有包含在标记中，这时可以考虑该选项。举个例子，如果你想让下载量达到最大，但同时对内容的质量有要求，那么可以用 app 的平均评级乘以标记来解决问题。监控一般来说，所有系统都要设置良好的警示程序，警报系统需要顺利执行，或者设置一个仪表板页面（dashboard page）。Rule #8: Know the freshness requirements of your system.法则 8: 了解你系统对新鲜度的要求如果你使用的是一天前的旧模型，运行状况会下降多少？如果是一周前的呢？或一个季度前的呢？知道何时该刷新系统能帮助你划分监控的优先级。如果你的模型一天没有更新，受益便下降 10%，因此有必要指派一名工程师时时关注它的动态。大多数广告服务系统每天都会有新的广告需要处理和更新。此外，要留意系统对新鲜度的要求会随着时间变化，特别是在添加或移除特征栏的时候，需要尤为注意。Rule #9: Detect problems before exporting models.法则 9: 输出（发布）模型前发现问题许多机器学习系统都存在这样一个阶段：直接把模型输出运行。如果问题出现在模型输出之后，那么这个问题就是用户所面临的问题。而如果问题出现在模型输出之前，就是训练过程中的问题，用户不会发现。输出模型之前请做好完整性检查（sanity check）。具体来讲，确保模型在留存数据上运行合理，例如AUC。Rule #10: Watch for silent failures.法则10：注意隐藏性故障比起其它系统，机器学习系统更容易出现潜在的问题。假设系统的某个特定的表格不再进行更新，整个系统通过调整仍会保持良好的运行水准，但是会慢慢衰减。有时有些表格几个月都不会刷新一次，而只需简单的刷新就能大幅度提升系统的运行水准，效果甚至超过该季度最新发布的那些模型！例如，由于系统实现（implementation）发生变化，特征的覆盖范围也会发生相应的变化：比如，某个特征栏刚开始可能包含 90%的样本，接下来却可能突然下降到 60％。解决方法是是对关键数据的统计信息进行监控，并且周期性对关键数据进行人工检查。Rule #11: Give feature columns owners and documentation.法则 11：为特征栏指定负责人并记录文档如果系统的规模比较大，并且特征栏比较多，那么必须清楚每个特征栏的创建者或者维护者。如果某个了解该特征栏的人离开了，一定要确保另外还有人了解这部分信息。虽然很多特征栏的名字非常直观，但最好还是使用更详尽的文档来描述这些特征的内容、来自哪里以及它们的作用。你的第一个目标（Objective）objective 是模型试图优化的值，而 metric 指的是任何用来评估系统的值。Rule #12: Don’t overthink which objective you choose to directly optimize.法则 12: 不要过于纠结该优化哪个目标你有成千上万关心的指标，这些指标也值得你去测试。但是，在机器学习过程的早期，你会发现，即使你并没有直接去优化，他们也都会上升。比如，你关心点击次数，停留时间以及每日活跃用户数。如果仅优化了点击次数，通常也会看到停留时间增加了。所以，当提高所有的指标都不难的时候，就没必要花心思来如何权衡不同的指标。不过过犹不及：不要混淆了你的目标和系统的整体健康度。Rule #13: Choose a simple, observable and attributable metric for your first objective.法则 13：选择一个简单、可观测并且可归类的评估指标（metric）作为你的第一个目标（objective）有时候你自以为你清楚真实的目标,但随着你对数据的观察，对老系统和新的机器学习系统的分析，你会发现你又想要调整。而且，不同的团队成员对于真实目标并不能达成一致。机器学习的目标必须是能很容易测量的，并且一定是“真实”目标的代言。因此，在简单的机器学习目标上训练，并创建一个“决策层”，以允许你在上面增加额外的逻辑（这些逻辑，越简单越好）来形成最后的排序。最容易建模的是那些可以直接观察并可归属到系统的某个动作的用户行为：排序的链接被点击了吗？排序的物品被下载了吗？排序的物品被转发/回复/邮件订阅了吗？排序的物品被评价了吗？展示的物品是否被标注为垃圾/色情/暴力？最开始要避免对间接效果建模：用户第二天会来访吗？用户访问时间是多长？每日活跃用户是什么样的？间接效果是非常重要的指标，在A/B test和发布决定的时候可以使用。最后，不要试图让机器学习来回答以下问题：用户使用你的产品是否开心用户是否有满意的体验产品是否提高了用户的整体幸福感这些是否影响了公司的整体健康度这些都很重要，但太难评估了。与其如此，不如考虑其他代替的：比如，用户如果高兴，那停留时间就应该更长。如果用户满意，他就会再次造访。Rule #14: Starting with an interpretable model makes debugging easier.法则 14：从容易解释的模型入手会让调试过程更加容易线性回归，逻辑回归和泊松回归直接由概率模型激发。每个预测可解释为概率或期望值。这使得他们比那些使用目标来直接优化分类准确性和排序性能的模型要更容易调试。比如，如果训练时的概率和预测时的概率，或者生产系统上的查看到的概率有偏差，那说明存在某种问题。Rule #15: Separate Spam Filtering and Quality Ranking in a Policy Layer.法则 15：在策略层将垃圾信息过滤和质量排名分开质量排名是一门艺术，而垃圾过滤是一场战争。那些使用你系统的人非常清楚你采用什么来评价一篇帖子的质量，所以他们会想尽办法来使得他们的帖子具有这些属性。因此，质量排序应该关注对哪些诚实发布的内容进行排序。如果将垃圾邮件排高名次，那质量排序学习器就大打折扣。同理也要将粗俗的内容从质量排序中拿出分开处理。垃圾过滤就是另外一回事。你必须考虑到要生成的特征会经常性的改变。你会输入很多明显的规则到系统中。至少要保证你的模型是每日更新的。同时，要重点考虑内容创建者的信誉问题。机器学习阶段 2：特征工程在机器学习系统研发周期的第一阶段，重点是把训练数据导入学习系统，得到感兴趣的评价指标，并创建基础架构。当你有了一个端对端的系统，并且该系统的单元和测试都仪表化之后，第二阶段便开始了。第二阶段需要纳入尽可能多的有效特征，并依据直观的感觉组合起来。在这个阶段，所有的评估指标仍然会上升。Rule #16: Plan to launch and iterate.法则16：做好持续迭代上线的准备不要期望现在发布的这个模型是最终的模型。因此，考虑你给当前这个模型增加的复杂度会不会减慢后续的发布。许多团队每季度推出一个模型或者更多年。之所以不断发布新模型，有三个基本原因：你会不断地想到新的特征。你会不断地调整并以新的方式组合旧的特征。你会不断调优目标。Rule #17: Start with directly observed and reported features as opposed to learned features.法则 17：优先使用直接观测或收集到的特征，而不是学习出来的特征（learned features）先描述一下什么是学习出来的特征（learned features）。学习出来的特征（learned features）是由外部系统（比如无监督聚类系统）或学习者本身（比如因子模型、深度学习）生成的特征。两种方式生成的特征都很有用，但也有很多问题，因此不应当用在第一个模型中。Rule #18: Explore with features of content that generalize across contexts.法则 18：探索使用可以跨场景的内容特征通常情况下，机器学习只占到一个大系统中的很小一部分，因此你必须要试着从不同角度审视一个用户行为。比如热门推荐这一场景，一般情况下论坛里“热门推荐”里的帖子都会有许多评论、分享和阅读量，如果利用这些统计数据对模型展开训练，然后对一个新帖子进行优化，就有可能使其成为热门帖子。另一方面，YouTube上自动播放的下一个视频也有许多选择，例如可以根据大部分用户的观看顺序推荐，或者根据用户评分推荐等。总之，如果你将一个用户行为用作模型的标记（label），那么在不同的上下文条件下审视这一行为，可能会得到更丰富的特征（feature），也就更利于模型的训练。需要注意的是这与个性化不同：个性化是确定用户是否在特定的上下文环境中喜欢某一内容，并发现哪些用户喜欢，喜欢的程度如何。Rule #19: Use very specific features when you can.法则 19：尽量使用非常具体的特征在海量数据的支持下，即使学习数百万个简单的特征也比仅仅学习几个复杂的特征要容易实现。由于被检索的文本标识与规范化的查询并不会提供太多的归一化信息，只会调整头部查询中的标记排序。因此你不必担心虽然整体的数据覆盖率高达90%以上，但针对每个特征组里的单一特征却没有多少训练数据可用的情况。另外，你也可以尝试正则化的方法来增加每个特征所对应的样本数。Rule #20: Combine and modify existing features to create new features in human–understandable ways.法则 20: 用人类可理解的方式对已有特征进行组合和修改有很多种方法组合和改良特征。像 TensorFlow 这样的机器学习系统，它允许通过 transformations 预处理数据。其最标准的两种方法分别是“discretization（离散化）”和“crosses（叉积）”。Discretization 会根据一个连续的特征创建许多离散的特征。假定年龄是一个连续的特征。我们可以创建如下特征，当年龄小于 18 时记为 1，或者当年龄在 18 到35 岁之间时为 1，以此类推。不用过多考虑这些数据的边界问题：简单的数字可以给你最直观的冲击。Cross由两个或多个特征栏组成。根据TensorFlow给出的解释， 特征栏是一组同类的特征。（如｛男，女｝、｛美国，加拿大，墨西哥｝等）。而Cross是一个新的特征栏，可以用｛男，女｝×｛美国，加拿大，墨西哥｝等来简单的表示。新的特征栏会包含以下特征，如｛男，加拿大｝。使用TensorFlow时可以让它帮你创建cross。｛男，加拿大｝可以在样本中代表男性加拿大人。注意若模型使用三个以上的特征栏组成的cross，则需要大量的数据来训练模型。Cross 会产生庞大的特征栏，有可能导致过拟合现象。举个例子，假设你要做某种搜索。检索词构成一个特征栏，文档中的词构成另一个特征栏。你可以通过cross 来组合它们，但这样会出现很多特征。处理文本时，有两个替代性方案。最苛刻的方案是 dot product（点积）。点积仅统计检索词和文档词中的公共词汇。得到的特征可以被离散化。另一种方案是取intersection（交集）：因此，我们有一个特征来表示当“pony（色情）”这个词同时出现在文档和检索词中，另一个特征表示“the”同时出现在文档和检索词中。Rule #21: The number of feature weights you can learn in a linear model is roughly proportional to the amount of data you have.法则 21：线性模型中的特征权重的数量应大致和样本数量形成一定的比例关于模型究竟多复杂才合适，统计学习理论有许多有趣的结论。但总的来说，这一条法则就足够了。我曾和一些人交流过，在他们看来，要想学到些东西，一千个样本远远不够，至少需要一百万个样本。这是因为，他们被特定的学习方法束缚了手脚。而诀窍就是，根据数据大小调整学习方法：如果你在开发一个搜索排名系统，并且有数百万不同的词汇存在于文档和检索词中，而你仅有 1000 个带有标记的样本。那么你应该使用文档和检索词的点积特征、 TF－IDF 以及其它六个人工设计的特征。 1000 个样本，对应 12个左右的特征。如果有一百万个的样本，那就通过 regularization 或特征 selection，取文档特征栏和检索词特征栏的交集。这样你能得到数百万个特征，但 regularization会帮你减少些许的特征。一千万个样本，对应大约十万个特征。如果有十亿个乃至几千亿个样本，你可以通过 regularization 和特征选取，取文档特征栏和 query token 的叉积。如果有十亿个样本，那么你会得到一千万个特征。Rule #22: Clean up features you are no longer using.法则22：清理不再使用的特征当决定要清除哪些特征时，需要考虑其覆盖率，即该项特征覆盖了多少样本。举个例子，如果你有一些比较特别的特征，但只有 8% 的用户与之相关，那么这些特征就无足轻重了。同时，有些特征可能超越它们的权重。比如某个特征仅覆盖 1% 的数据，但 90% 的正样本都含有这种特征。那么，也应当将这个特征添加进来。系统的人工分析在进入机器学习第三阶段前，有一些在机器学习课程上学习不到的内容也非常值得关注：如何检测一个模型并改进它。这与其说是门科学，还不如说是一门艺术。这里再介绍几种要避免的反模式（anti-patterns）Rule #23: You are not a typical end user.法则 23: 你并非典型终端用户这可能是让一个团队陷入困境的最简单的方法。虽然fishfooding（只在团队内部使用原型）和dogfooding（只在公司内部使用原型）都有许多优点，但无论哪一种，开发者都应该首先确认这种方式是否符合性能要求。要避免使用一个明显不好的改变，同时，任何看起来合理的产品策略也应该进一步的测试，不管是通过让非专业人士来回答问题，还是通过一个对真实用户的线上实验。Rule #24: Measure the delta between models.法则24：测量模型间的差异在将你的模型发布上线前，一个最简单，有时也是最有效的测试是比较你当前的模型和已经交付的模型生产的结果之间的差异。如果差异很小，那不再需要做实验，你也知道你这个模型不会带来什么改变。如果差异很大，那就要继续确定这种改变是不是好的。检查对等差分很大的查询能帮助理解改变的性质（是变好，还是变坏）。但是，使用不同模型进行比较前，需要确保该模型和它本身比较，这个差异很小（理想情况应该是无任何差异）。Rule #25: When choosing models, utilitarian performance trumps predictive power.法则 25: 选择模型时，性能表现比预测力更重要虽然我们训练模型时 objective 一般都是 logloss，也就是说实在追求模型的预测能力。但是我们在上层应用中却可能有多种用途，例如可能会用来排序，那么这时具体的预测能力就不如排序能力重要；如果用来划定阈值然后跟根据阈值判断垃圾邮件，那么准确率就更重要。当然大多数情况下这几个指标是一致的。Rule #26: Look for patterns in the measured errors, and create new features.法则 26: 在错误中寻找规律，然后创建新特征假设你的模型在某个样本中预测错误。在分类任务中，这可能是误报或漏报。在排名任务中，这可能是一个正向判断弱于逆向判断的组。但更重要的是，在这个样本中机器学习系统知道它错了，需要修正。如果你此时给模型一个允许它修复的特征，那么模型将尝试自行修复这个错误。另一方面，如果你尝试基于未出错的样本创建特征，那么该特征将很可能被系统忽略。例如，假设在 Google Play商店的应用搜索中，有人搜索“免费游戏”，但其中一个排名靠前的搜索结果却是一款其他App，所以你为其他App创建了一个特征。但如果你将其他App的安装数最大化，即人们在搜索免费游戏时安装了其他App，那么这个其他App的特征就不会产生其应有的效果。所以，正确的做法是一旦出现样本错误，那么应该在当前的特征集之外寻找解决方案。例如，如果你的系统降低了内容较长的帖子的排名，那就应该普遍增加帖子的长度。而且也不要拘泥于太具体的细节。例如你要增加帖子的长度，就不要猜测长度的具体含义，而应该直接添加几个相关的特征，交给模型自行处理，这才是最简单有效的方法。Rule #27: Try to quantify observed undesirable behavior.法则 27：尝试量化观察到的异常行为如果在系统中观察到了模型没有优化到的问题，典型的例如推荐系统逼格不够这种问题，这时应该努力将这种不满意转化为具体的数字，具体来讲可以通过人工标注等方法标注出不满意的物品，然后进行统计。如果问题可以被量化，后面就可以将其用作特征、objective或者metric。整体原则就是“先量化，再优化”。Rule #28: Be aware that identical short-term behavior does not imply identical long-term behavior.法则 28：短期行为相同并不代表长期行为也相同假设你有一个新系统，它可以查看每个doc_id和exact_query，然后根据每个文档的每次查询行为计算其点击率。你发现它的行为几乎与当前系统的并行和A/B测试结果完全相同，而且它很简单，于是你启动了这个系统。却没有新的应用显示，为什么？由于你的系统只基于自己的历史查询记录显示文档，所以不知道应该显示一个新的文档。要了解一个系统在长期行为中如何工作的唯一办法，就是让它只基于当前的模型数据展开训练。这一点非常困难。训练偏差（Training－Serving Skew）训练偏差是指训练时的表现和在生产环境中实际运行时的表现的差别。这种偏差可能由以下因素引起：在训练时和在实际工作流中用不同的方式处理数据。训练中的数据和在实际运行中的分布不同。模型和算法之间存在反馈循环。解决这类问题的核心是对系统和数据的变化进行监控，确保一切差异都在监控之内，不会悄悄进入系统。Rule #29: The best way to make sure that you train like you serve is to save the set of features used at serving time, and then pipe those features to a log to use them at training time.法则 29： 要让实际产品和训练时表现一样好，最好的方法是实际运行中保留特征集，并记录到日志中以便训练中使用即使你不能对每个样例都这样做，做一小部分也比什么也不做好，这样你就可以验证服务和训练之间的一致性（见规则37）。在 Google 采取了这项措施的团队有时候会对其效果感到惊讶。比如YouTube主页在服务时会切换到日志记录特征，这不仅大大提高了服务质量，而且减少了代码复杂度。目前有许多团队都已经在其基础设施上采用了这种策略。Rule #30: Importance-weight sampled data, don’t arbitrarily drop it!法则30：给抽样数据按重要性赋权重，不要随意丢弃它们当我们有太多训练数据时，我们会只取其中的一部分。但这是错误的。正确的做法是，如果你给某条样本30%的采样权重，那么在训练时就给它10/3的训练权重。通过这样的重要性赋权（importance weight），整个训练结果的校准性（calibration）就还能够保证。Rule #31: Beware that if you join data from a table at training and serving time, the data in the table may change.法则 31：如果要从表格中组合数据，注意训练时和实际运行时表格可能发生改变假设你要把文档 id 和包含文档特征的表格（比如评论或点击的数量）结合起来。从训练和实际运行，表格中的特征可能会改变（例如用户对物品的评论数），模型对同一文档做的预测也能不同。要避免这这类问题，最简单的办法就是记录所有实际运行时的特征。若表格只是缓慢的变化，你也可以按照每小时或每天的频率对其做出记录，得到足够相近的数据。注意这样不能完美的解决问题。Rule #32: Re-use code between your training pipeline and your serving pipeline whenever possible.法则 32: 尽量在训练流和实际运行流中使用重复代码首先需要明确一点：批处理和在线处理并不一样。在线处理中，你必须及时处理每一个请求（比如，必须为每个查询单独查找），而批处理，你可以合并完成。服务时，你要做的是在线处理，而训练是批处理任务。尽管如此，还是有很多可以重用代码的地方。比如说，你可以创建特定于系统的对象，其中的所有联结和查询结果都以人类可读的方式存储，错误也可以被简单地测试。然后，一旦在服务或训练期间收集了所有信息，你就可以通过一种通用方法在这个特定对象和机器学习系统需要的格式之间形成互通，训练和服务的偏差也得以消除。因此，尽量不要在训练时和服务时使用不同的变成语言，毕竟这样会让你没法重用代码。Rule #33: If you produce a model based on the data until January 5th, test the model on the data from January 6th and after.法则 33: 如果训练数据是1月5日之前的，那么测试数据要从1月6日开始测试模型时应当使用的比训练模型时更加新的数据，因为这更能反映你的系统实际运行表现。如果你用 1 月 5 日前的数据生成了一个模型，那就得用 1月 6 号之后的数据测试它。你会发现，在新的数据下模型表现得没那么好，但也不会差到哪里去。这个结果更加接近真实运行时的表现。Rule #34: In binary classification for filtering (such as spam detection or determining interesting emails), make small short-term sacrifices in performance for very clean data.法则 34：在过滤类的任务中，被标记为负的样本是不会展示给用户的，例如可能会把75%标记为负的样本阻拦住不展现给用户。但如果你只从展示给用户的结果中获取下次训练的样本，显然你的训练样本是有偏的更好的做法是使用一定比例的流量（例如1%）专门收集训练数据，在这部分流量中的用户会看到所有的样本。这样显然会影响线上的真实过滤效果，但是会收集到更好的数据，更有利于系统的长远发展。否则系统会越训练越偏，慢慢就不可用了。同时还能保证至少过滤掉74%的负样本，对系统的影响也不是很大。但是如果你的系统会过滤掉95%或者更多的负样本，这种做法就不那么可行了。即使如此，为了准确衡量模型的效果，你仍然可以通过构造一个更小的数据集（0.1%或者更小）来测试。十万级别的样本足够给出准确的评价指标了。Rule #35: Beware of the inherent skew in ranking problems.法则 35: 注意排序问题存在固有偏差当你对排序算法做出足够多的改动时，一方面会引起完全不同的排序结果，另一方面也可能在很大程度上改变算法未来可能要处理的数据。这会引入一些固有偏差，因此你必须事先充分认识到这一点。以下这些方法可以有效帮你优化训练数据。对涵盖更多查询的特征进行更高的正则化，而不是那些只覆盖单一查询的特征。这种方式使得模型更偏好那些针对个别查询的特征，而不是那些能够泛化到全部查询的特征。这种方式能够帮助阻止非常流行的结果进入不相关查询。这点和更传统的建议不一样，传统建议应该对更独特的特征集进行更高的正则化。只允许特征具有正向权重，这样一来就能保证任何好特征都会比未知特征合适。不要有那些仅仅偏文档（document-only）的特征。这是法则1的极端版本。比如，不管搜索请求是什么，即使一个给定的应用程序是当前的热门下载，你也不会想在所有地方都显示它。没有仅仅偏文档类特征，这会很容易实现。Rule #36: Avoid feedback loops with positional features.法则 36：用位置特征来避免反馈回路大家都知道排序位置本身就会影响用户是否会对物品产生互动，例如点击。所以如果模型中没有位置特征，本来由于位置导致的影响会被算到其他特征头上去，导致模型不够准。可以用加入位置特征的方法来避免这种问题，具体来讲，在训练时加入位置特征，预测时去掉位置特征，或者给所有样本一样的位置特征。这样会让模型更正确地分配特征的权重。需要注意的是，位置特征要保持相对独立，不要与其他特征发生关联。可以将位置相关的特征用一个函数表达，然后将其他特征用另外的函数表达，然后组合起来。具体应用中，可以通过位置特征不与任何其他特征交叉来实现这个目的。Measure Training/Serving Skew.法则 37: 衡量训练和服务之间的差异很多情况会引起偏差。大致上分为一些几种：训练集和测试集之间的差异。这种差异会经常存在，而且不一定是坏事。测试集和“第二天”数据间的差异。这种差异也会一直存在，而这个“第二天”数据上的表现是我们应该努力优化的，例如通过正则化。这两者之间差异如果过大，可能是因为用到了一些时间敏感的特征，导致模型效果变化明显。“第二天”数据和线上数据间的差异。如果同样一条样本，在训练时给出的结果和线上服务时给出的结果不一致，那么这意味着工程实现中出现了bug。机器学习第三阶段：放慢速度、优化细化和复杂的模型一般会有一些明确的信号来标识第二阶段的尾声。首先，每月的提升会逐步降低。你开始在不同指标之间做权衡，有的上升有的下降。这将会变得越来越有趣。增长越来越难实现，必须要考虑更加复杂的机器学习。警告：相对于前面两个阶段，这部分会有很多开放式的法则。第一阶段和第二阶段的机器学习是快乐的。当到了第三阶段，每个团队就不能不去找到他们自己的途径了。Rule #38: Don’t waste time on new features if unaligned objectives have become the issue.法则 38： 如果目标没有达成一致，就不要在新特征上浪费时间当达到评估指标瓶颈，你的团队开始关注机器学习系统目标范围之外的问题。如同之前提到的，如果产品目标没有包括在算法目标之内，你就得修改其中一个。比如说，你也许优化的是点击数、点赞或者下载量，但发布决策还是依赖于人类评估者。Rule #39: Launch decisions are a proxy for long-term product goals.法则 39：模型发布决策是长期产品目标的代理这个法则字面上有点难以理解，其实作者核心就是在讲一件事情：系统、产品甚至公司的长远发展需要通过多个指标来综合衡量，而新模型是否上线要综合考虑这些指标。所谓代理，指的就是优化这些综合指标就是在优化产品、公司的长远目标。决策只有在所有指标都在变好的情况下才会变得简单。但常常事情没那么简单，尤其是当不同指标之间无法换算的时候，例如A系统有一百万日活和四百万日收入，B系统有两百万日活和两百万日收入，你会从A切换到B吗？或者反过来？答案是或许都不会，因为你不知道某个指标的提升是否会cover另外一个指标的下降。关键是，没有任何一个指标能回答：“五年后我的产品在哪里”？而每个个体，尤其是工程师们，显然更喜欢能够直接优化的目标，而这也是机器学习系统常见的场景 。现在也有一些多目标学习系统在试图解决这种问题。但仍然有很多目标无法建模为机器学习问题，比如用户为什么会来访问你的网站等等。作者说这是个AI-complete问题，也常被称为强AI问题，简单来说就是不能用某个单一算法解决的问题。Rule #40: Keep ensembles simple.法则 40: 保持模型集合（ensembles）的简单性接收原始特征、直接对内容排序的统一模型，是最容易理解、最容易修补漏洞的模型。但是，一个集成模型（一个把其他模型得分组合在一起的“模型”）的效果会更好。为保持简洁，每个模型应该要么是一个只接收其他模型的输入的集成模型，要么是一个有多种特征的基础模型，但不能两者皆是。如果你有单独训练、基于其它模型的模型，把它们组合到一起会导致不好的行为。只使用简单模型来集成那些仅仅把你的基础模型输出当做输入。你同样想要给这些集成模型加上属性。比如，基础模型生成得分的提高，不应该降低集成模型的分数。另外，如果连入模型在语义上可解释（比如校准了的）就最好了，这样其下层模型的改变不会影响集成模型。此外，强行让下层分类器预测的概率升高，不会降低集成模型的预测概率。Rule #41: When performance plateaus, look for qualitatively new sources of information to add rather than refining existing signals.法则 41：当效果进入瓶颈期，寻找本质上新的信息源，而不是优化已有的信号。你先是添加了一些用户的人口信息，又添加了一些文档词汇的信息，接着你又浏览了一遍模版，而后又调整了规则，但是最后，关键度量却只提升了不到 1%。现在怎么办？这时候应该用完全不同的特征搭建基础架构，比如用户昨天／上周／去年访问的文档的历史记录。利用 wikidata 或对公司来说比较重要的东西（比如 Google 的知识图）。你或许需要使用深度学习。开始调整你对投资回报的期望，并作出相应努力。如同所有工程项目，你需要平衡新增加的特征与提高的复杂度。Rule #42: Don’t expect diversity, personalization, or relevance to be as correlated with popularity as you think they are.法则 42：不要期望多样性、个性化、相关性和受欢迎程度之间有紧密联系一系列内容的多样性能意味着许多东西，内容来源的多样性最为普遍。个性化意味着每个用户都能获得它自己感兴趣的结果。相关性意味着一个特定的查询对于某个查询总比其他更合适。显然，这三个属性的定义和标准都不相同。问题是标准很难打破。注意：如果你的系统在统计点击量、耗费时间、浏览数、点赞数、分享数等等，你事实上在衡量内容的受欢迎程度。有团队试图学习具备多样性的个性化模型。为个性化，他们加入允许系统进行个性化的特征（有的特征代表用户兴趣），或者加入多样性（表示该文档与其它返回文档有相同特征的特征，比如作者和内容），然后发现这些特征比他们预想的得到更低的权重（有时是不同的信号）。这不意味着多样性、个性化和相关性就不重要。就像之前的规则指出的，你可以通过后处理来增加多样性或者相关性。如果你看到更长远的目标增长了，那至少你可以声称，除了受欢迎度，多样性/相关性是有价值的。你可以继续使用后处理，或者你也可以基于多样性或相关性直接修改你的目标。Rule #43: Your friends tend to be the same across different products. Your interests tend not to be.法则 43: 在不同的产品中，你的朋友可能相同，但兴趣却不尽然Google 经常在不同产品上使用同样的好友关系预测模型，并且取得了很好的效果，这证明不同的产品上好友关系是可以迁移的，毕竟他们是固定的同一批人。但他们尝试将一个产品上的个性化特征使用到另外一个产品上时却常常得不到好结果。可行的做法是使用一个数据源上的原始数据来预测另外数据源上的行为，而不是使用加工后的特征。此外，用户在另一个数据源上的行为历史也会有用。]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习（七）]]></title>
    <url>%2F2018%2F05%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%2F</url>
    <content type="text"><![CDATA[[toc] 这里开始学习神经网络，前一个视屏的结尾吴恩达教授说会线性回归和逻辑回归就超过硅谷的大部分程序员了，而且那些程序员混的还不错，听到这里当然是很开心啊，但是想视频是2011年的，七年前懂这些的确是不易了，现在是2018年了，我才开始学，是不是太迟了？ 非线性假设（Non linear Hypotheses）神经网络实际上是一个相对古老的算法，是20世纪80年代时期出现的，但是没有成为发展的热点，随着现代计算机计算能力的提升，近年来，神经网络又成为机器学习算法中的一个热点。 之前已经学习过线性回归和逻辑回归算法，为什么还要研究神经网络呢？为了阐述研究神经网络算法目的，我们首先来看几个机器学习的问题作为例子，这几个例子都依赖于复杂的非线性分类器 考虑这个监督学习的分类问题，我们已经有了对应的训练集，如果利用逻辑回归算法来解决这个问题，首先需要构造一个包含很多非线性项的逻辑回归函数，这里g仍是s型函数（即f(x)=1/(1+e^-x)）。我们能让函数包含许多像这样的多项式，当多项式的项数足够的的时候你能够得到一个分开正样本和负样本的判定边界。 例如当只有两项时比如x1和x2，这种方法确实能够得到不错的结果，因为你可以把x1和x2的所有组合都包含到多项式中，但是对于许多复杂的机器学习问题而言，设计的多项式往往多于两项。 例如我们之前讨论过的房价预测问题，假设现在处理的是关于房屋的分类问题而不是一个回归问题。假设你对一栋房子的多方面特点都有所了解，你想预测房屋在未来半年内能够被卖出去的概率，这是一个分类问题。 我们可以想到许多特征，对于不同的房子有可能有上百个特征，对于这类问题如果要包含所有的二次项，即使只包含二项式或多项式的计算，最终的多项式也可能有很多项，比如x1^2 ,x1x2 ,x1x3 ,x1x4直到x1x100,接着还有x2^2, x2x3等等很多项。因此即使只考虑二阶项，也就是说两个项的乘积x1乘以x1等等类似于此的项，那么，在n=100的情况下最终也有5050个二次项。 而且随着特征个数n的增加，二次项的个数大约以 n^2 的量级增长，其中n是原始项的个数，即我们之前说过的x1到x100这些项。事实上二次项的个数大约是（n^2）/2个，因此要包含所有的二次项是很困难的，所以这可能不是一个好的做法。 而且由于项数的过多，最后的结果可能是过拟合的，此外，在处理这么多项时也存在运算量过大的问题。当然，我们也可以试试只包含上边这些二次项的子集，例如，我们只考虑x1^2， x2^2， x3^3直到 x100^2这些项，这样就可以将二次项的数量大幅度减少，减少到只有100个二次项。但是由于忽略了太多项，在处理类似左上角的数据时，不太可能得到理想的结果。 实际上如果只考虑x1的平方到x100的平方这一百个二次项，那么你可能会拟合出一些特别的假设，比如可能拟合出一个椭圆状的曲线，但是肯定不会拟合出左上角这个数据集的分界线，所以5000个二次项看起来已经很多了。 而现在的假设还包括三次项， 例如x1x2x3, x1^2x2, x10x11x17等等，类似的三次项有很多很多，事实上，三次项的个数是n^3的量级增加。当n=100时，可以计算出来最后能得到大概17000个三次项。 所以，当初始特征个数n增大时，这些高阶多项式将以几何级数递增，特征空间也随之急剧膨胀。当特征值个数n很大时，如果找出附加项来建立一些分类器，这并不是一个好做法。对于许多实际的机器学习问题，特征个数n是很大的。 我们看看下边这个例子，这是关于计算机视觉中的一个问题。假设你想要使用机器学习算法来训练一个分类器，使他检测一个图像是否为一辆汽车。很多人可能会好奇，觉得这对计算器视觉来说有什么难的？ 当我们自己看这幅图像时里面有什么事一目了然的事情，你肯定会奇怪，为什么学算法会不知道图像是什么。 为了解答这个问题，我们取出这幅图像的一部分，将其放大，比如这幅图中，汽车的门把手，红框中的部分，人肉眼看到一辆车时，计算机看到的是一个这样的数据矩阵。 它们表示了像素强度值，告诉我们图像中每个像素的亮度值。因此，对于计算机视觉来说问题就变成了，根据这个像素点亮度矩阵来告诉我们这些数值是否代表一个汽车门把手。 具体而言，当机器学习算法构造一个汽车识别器时，我们想出一个带标签的样本集，其中一些样本是各类汽车，而另一部分样本是其他任何东西。将这个样本输入给学习算法以训练出一个分类器，当训练完毕后，我们输入一副新的图片，让分类器判别“这是什么东西？”理想情况下，分类器能识别出这是一辆汽车。 为了理解引入非线性分类器的必要性，我们从学习算法的训练样本中挑选出一些汽车的图片和非汽车的图片。让我们从其中每幅图片中挑出一组像素电，例如上图像素点1的位置和像素2的位置。 在坐标系中标出这幅汽车的位置，其他坐标系中的位置取决于像素点1和像素点2的亮度。让我们用同样的方法在坐标系中标出其他图片中汽车的位置。接着我们在坐标系中继续画上两个非汽车样本。 然后我们继续在坐标系中画上更多新样本，用“+”表示汽车图片，用“-”表示非汽车图片，我们将发现汽车样本和非汽车样本分布在坐标系中的不同区域，因此我们现在需要一个非线性分类器，来尽量分开这两类样本。 这个分类问题中特征空间的维度是多少？ 显然在真实情况下，我们不可能只取两个像素点来做特征。假设我们用50*50像素的图片，注意，我们的图片已经足够小了，长宽只各有50个像素，但这依然是25000个像素点，因此，我们的特征向量的元素数量 n=2500。特征向量X包含了所有像素点的亮度值。 对于典型的计算机图片表示方法，如果储存的每个像素点灰度值（色彩的强烈程度），那么每个元素的值应该在0 到255之间。因此，这个问题中n=2500 但是这只是使用灰度图片的情况，如果我们用的是RGB彩色图像，每个像素点包含红，绿，蓝三个子像素，那么n=7500。 因此，如果我们非要通过包含所有的二次项来解决这个非线性问题，那么仅仅二次项 xi * xj总共有大约300万个（2500^2/2），这个数字大的有点离谱了。对于每个样本来说，要发现并表示所有这300万个项，这个计算成本太高。因此，只是简单的增加二次项或者三次项之类的逻辑回归算法并不是一个解决复杂线性问题的好办法。因为n很大时，将会产生非常多的特征项。 接下来，我们会讨论神经网络，他在解决复杂的非线性分类问题上，被证明是一种好的多的算法，及时你输入的特征空间或输入的特征维度n很大，也能轻松搞定。 神经元和大脑（Neurons and the brain）神经网络是一种很古老的算法，他最初产生的目的是制造模拟大脑的机器。我们将会讨论神经网络，因为他能很好的解决不同的机器学习问题，而不是只因为他们在逻辑上行的通。 神经网络产生的原因是人们想尝试设计出模拟大脑的计算。从某种意义上说，如果我们想要建立学习系统那为什么不去模拟我们所认识的最神奇的学习机器–人类的大脑的？ 神经网络逐渐兴起于二十世纪八九十年代，应用的非常广泛。但由于各种原因在90年代的后期应用减少，其中一个原因是神经网络是一种计算量有些偏大的算法，但是最近神经网络又东山再起了，大概 由于近年来计算机的运行速度变快，才足以真正运行起大规模的神经网络。 正式由于这个原因和其他一些我们后面会讨论的技术因素，如今的神经网络对于许多应用来说是最先进的技术。 当你模拟大脑时，是指想制造出于人类大脑效果相同的机器。大脑可以学会去看而不是听的方式处理图像，学会处理我们的触觉。我们能学习数学，学习计算微积分，而且大脑能处理各种不同的令人惊奇的事情。似乎如果你想要模仿它，你需要写许多不同的软件来模拟所有大脑告诉我们这些五花八门的奇妙的事情。 如果假设大脑处理所有这些不同事情不需要上千个程序去实现他，相反，大脑只需要一个简单的学习算法就可以了呢？ 尽管这只是一个假设，不过让我和你分享一些这方面的证据。 如图大脑这个部分，这一小片红色区域是你的听觉皮层，如果你通过我说的话来理解我表达的内容，那么是靠耳朵接收到声音信号并把声音信号传递给你的听觉表皮层，正因如此，你才能明白我的话。 神经系统科学家做了一个有趣的实验，把耳朵到听觉表皮的神经切断。在这种情况下，将其重新接到一个动物的大脑上，这样从眼睛看到的视觉神经的信号最终将传到听觉表皮层，结果表明，听觉表皮层将会学会“看”。 所以，如果你对动物这样做那么动物就可以完成视觉辨别任务，他们可以看图像，并根据图像做出适当的决定。它们正是通过脑组织中的这个部分完成的。 下面在举另一个例子，这块红色的脑组织是你的躯体感觉皮层，这是你用来处理触觉的。如果你做一个和刚才类似的重接实验，那么躯体感觉皮层也能会“看”，这个实验和其他一些类似的实验被称为神经重接实验。从这个意义上说，如果人体有同一块脑组织可以处理光、声或触觉信号，那么也许存在一种学习方法可以同时处理视觉，听觉和触觉，而不是需要成千个不同的程序或者算法来做这些。 大脑能够完成的成千上万的事，我们需要做的就是找出一些近似的或实际的大脑学习算法，然后实现它，大脑通过自学掌握如何处理这些不同类型的数据，在很大程度上可以猜想，如果我们把任何一种传感器接到大脑的任何一个部位，大脑就会学会处理它。 再看上图的几个例子，左上角这张图是用舌头学会“看”的一个例子。这实际上是一个名为BrainPort的系统，他现在正在FDA（美国食品药物管理局）的临床试验阶段，他帮助失明人士看见事物。他的原理是，在你前额戴上一个灰度摄像头，他能够获取你面前的事物的低分辨率的灰度图像。你连接一根线到舌头上安装的电极阵列上，那么每个像素都被映射到你舌头上的某个位置。可能电压值高的点对应一个暗像素，电压值的点对应亮像素。 即使依靠它现在的功能，使用这种系统就能够让人在几十分钟里学会用我们的舌头看东西。 下面是第二个例子，关于人体回声定位或者说人体声纳。你有两种方法可以实现，你可以弹响指或者咂舌头。现在有失明人士确实在学校里接受这样的培训，并学会解读从环境反弹回来的声波模式—这就是声纳。 如果你搜索 YouTube 之后，就会发现有些视频讲述了一个令人称奇的孩子，他因为癌症眼球惨遭移除，虽然失去了眼球，但是通过打响指他可以四处走动而不撞到任何东西。他能滑滑板，他可以将篮球投入篮框中。 第三个例子是触觉皮带，如果你把它戴在腰上，蜂鸣器会响，而且总是朝向北时发出嗡嗡声。它可以使人拥有方向感，用类似于鸟类感知方向的方式。 还有一些离奇的例子，如果你在青蛙身上插入第三只眼，青蛙也能学会使用那只眼睛。 因此，这将会非常令人惊奇。如果你能把几乎任何传感器接入到大脑中，大脑的学习算法就能找出学习数据的方法并处理这些数据。从某种意义上来说如果我们能找出大脑的学习算法，然后在计算机上执行大脑学习算法或与之相似的算法，也许这将是我们向人工智能迈进做出的最好的尝试。 模型表示（Model Representation）神经网络是在模仿大脑中的神经元或者神经网络时发明的。因此，要解释如何表示模型假设，我们不妨先来看单个神经元在大脑中是什么样的。 我们的大脑中充满了如上图所示的这样的的神经元，神经元是大脑中俄细胞，其中有两点值得我们注意，一是神经元有像这样的细胞主题（Nucleus），二是神经元有一定数量的输入神经和输出神经。这些输入神经叫做树突（dendrite），可以把他们想象成输入电线，他们接受来自其他神经元的信息。神经元的输出神经焦作轴突（Axon），这些输出神经是用来给其他神经元传递信号或者传递信息的。 简而言之，神经元是一个计算单元，他从输入神经接受一定数目的信息，并做一些计算，然后将结果通过他的轴突传送到其他节点或者大脑中的其他神经元。 下面是神经元的示意图： 神经元利用微弱的电流进行沟通。这些弱电流也称作动作电位，其实就是微弱的电流。所以如果神经元想要传递一个消息，他就会通过它的轴突发送一段微弱电流给其他神经元。 上图中，黄色的圆圈就代表了一个神经元，X为输入向量，θ代表神经元的权重（就是我们之前所说的模型参数），hθ(x)代表激励函数（在神经网络术语中，激励函数只对类似非线性函数（g(z)的另一个术语称呼，g(z)等于1除以1加e的-z次方）。 实际上你可以这样理解，神经元就是权重θ。 当将输入送进神经元后，经计算（就是X^Tθ）会有一个输出，这个输出再送进激励函数中，便得到了神经元的真实输出。 注意：当我们绘制一个神经网络时，通常我会只绘制节点x1,x2,x3等等，但有时可以增加一个额外的节点x0，这个x0节点有时被称作偏置神经元。但因为x0总是等于1，去哦们会画出它，有时我们不会画出，这要看画出他是否对例子有利。 神经网络就是不同的神经元组合在一起，第一层为输入层，最后一层为输出层，而且中间的所有层均为隐藏层。 注意：输入单元x1，x2，x3，再说一次，有时也可以画上额外的节点x0.同时，这里有三个神经元，我在里面写了a1(2) 、 a2(2) 和a3(2) ,然后再次说明这里我可以添加一个a0(2) ，这和x0一样，代表一个额外的偏度单位，它的值永远是1，注意：a1(2) 、 a2(2) 和a3(2)中计算的是g（X^Tθ）的值，而a0(2)中存放的就是偏置1。 如果一个网络在第j层有sj个单元，在j+1层有sj+1个单元，那么矩阵θ(j)即控制第j层到第j+1层的映射。 矩阵θ(j)的维度是s(j+1)*(sj+1),s(j+1)行，(sj+1)列 总之，上面的图展示了是怎样定义一个人工网络的。这个神经网络定义了函数h:从输入x到输出y的映射。我将这些假设的参数记为大写的θ，这样一来不同的θ对应不同的假设，所以我们有不同的函数，比如说从x到y的映射。 以上就是我们怎么从数学上定义神经网络的假设 下面将讲解如何高效的进行计算，并展示一个向量化的实现方法，更重要的是让你明白这样表示神经网络是一个好方法，并且明白它们怎样帮助我们学习复杂的非线性假设 以前我们说过计算出假设输出的步骤，通过左边的这些方程计算出三个隐藏的单元的激励值，然后利用这些值来计算假设函数h(x)的最终输出，接下来我要定义一些额外的项，因此，上图中蓝色线的项把他定义为z上标（2）下标1，这样一来就有了a(2)1 这个项，等于g(z(2)1)(上标2的意思是与第二层相关，即神经网络的隐藏层有关)接下来画红线的项同样定义为z(2)2，最后一项定义为z(2)3，这样我们就有a(2)3=g(z(2)3)，所以这些Z的值是线性组合，是输入值x1,x2,x3的加权线性组合，他将进入一个特定的神经元，类似于矩阵向量的乘法。 现在看一线灰色框里的一维数组，你可能会注意到这一块对应了矩阵向量的运算x1乘以向量x，观察到这一点我就能够将神经网络的运算向量化，具体而言我们定义特征向量x为x0,x1,x2,x3组成的向量，其中x0=1，并定义z^2为这些值组成的向量，注意：这里的Z(2)是一个三维向量。 下面我们可以这样向量化1(2) 、 a2(2) 和a3(2)的计算我们只用两个步骤z(2)等于θ(1)乘以x，然后a(2)等于g(z(2))，需要明白的是这里的z(2)是一个三维向量，并且a(2)也是一个三维向量因此这里的激励将s函数逐元素作用于z(2)中的每个元素z(2)就等于θ(1)乘以a(1)。当然x也有偏置单元x0， 顺便说一下，为了让我们的符号和接下来的工作一致，在输入层，虽然我们有输入x但是我们还可以把这些想成是第一层的激励，所以我们可以定义第一层的激励a(1)=x,因此a(1)就是一个向量了，我们可以把这里的x替换成a(1) 现在我们得到了a1，a2，a3的值，但是我们同样需要一个值a0，他对应隐藏层得到这个输出的偏置单元，这时a(2)就是一个四维的特征向量。 为了计算假设的实际输出值h，我们只需要计算z(3),z(3)等于绿色框框中的项目，最后假设函数h(x)输出他等于a(3),a(3)是输出层唯一的单元，他是一个实数。 这个h(x)的计算过程也成为向前传播(forward propagation),这样的命名是因为我们是从输入层的激励开始，然后进行向前传播给隐藏层，并计算隐藏层，然后我们继续向前传播，计算输出层的激励，这个从输入层到隐藏层再到输出层一次计算激励的过程叫向前传播。 我们刚刚得到了这一过程的向量化实现方法，如果用右边的公式计算，会得到一个有效的计算h(x)的方法 这种向前传播的角度，可以帮助我们了解神经网络的原理，帮助我们学习非线性假设 看上面这幅图，我们先盖住图片左边的部分，如果只看右边，这看起来很像逻辑回归，在逻辑回归中我们用最后一个节点，也就是最后一个逻辑回归单元来预测h(x)的值，具体来说，假设输出的h(x)等于s型激励函数g(Θa1+Θa2…)。其中a由那三个单元一样，为了和我们之前的定义保持一致，需要添加红色的上标和下标1，因为我们只有一个输出单元，但如果你只观察蓝色的部分，这看起来非常像标准的逻辑回归模型，不同之处在于，我现在用的是大写的Θ，而不是小写的Θ，这样做完我们只得到了逻辑回归，但是逻辑回归输入特征值是通过隐藏层计算的。 再说一遍，神经网络所做的就像逻辑回归，但是它不是使用x1，x2，x3作为输入特征，而是用a1，a2，a3作为新的输入特征，同样的我们需要把上标加上来和之前的记号保持一致，有趣的是特征值a1，a2，a2是当做输入函数来学习的，具体来说，就是从第一层映射到第二层的函数，这个函数由其他一组参数θ(1)决定，而在神经网络上，他没有用输入特征x1，x2，x3，来训练逻辑回归而是自己训练逻辑回归的输入a1，a2，a3，可以想象，如果在θ1中选择不同的参数，可以学习一些很有趣和复杂的特征，就可以得到一个更好的假设，比使用原始输入x1，x2或x3时得到的假设更好。 你也可以x1，x2，x3等作为输入项，但是这个算法可以灵活的快速学习任意的特征项，把这些a1，a2，a3,输入这个最后的单元，实际上他是逻辑回归。 还可以用其他类型图表示神经网络，神经网络中神经元相连接的方式，称为神经网络的架构，所以架构是指，不同的神经元是如何相互连接的，这里有个一不同的神经网络架构的例子，你可以意识到这个第二层是如何工作的，我们有三个隐藏单元，它们根据输入层计算一个复杂的函数，然后到第三层，我们可以将第二层训练出的特征项作为输入，并在第三层计算一些更复杂的函数，这样你在第四次，即输出层时，就可以利用第三层训练出的更复杂的特征项作为输入，以此得到非常有趣的非线性假设。顺便说下，在这个网络中，第一层被称为输入层，第四层仍然是我们的输出层，这个网络有两个隐藏层，所以都被称为隐藏层任何一个不是输入层或者输出层的。 示例和直觉（Examples and Intuitions）接下来讲解两个例子来说明神经网络是如何计算的。 关于输入的复杂的非线性函数，希望这个例子可以让你了解，神经网络可以用来学习复杂的非线性假设 我们有x1，x2要么取0要么取1，所以x1和x2只能有两种取值，在这个例子中，我只画出了，两个正样本和两个负样本，你可以认为这是一个复杂样本的简单版本，咋这个复杂问题中，我们可能在右上角有一堆正样本，和左上角一堆用圈圈表示的负样本，我们想要学习一种非线性的决策边界来区分正负样本。 我们用左边的例子来说明，具体来讲我们需要计算的是目标函数y等于x1异或x2，或者y也可以等于x1异或非x2，其中异或非表示x1异或x2后取反，x1异或x2为真当且仅当这两个值，x1或者x2中有且仅有一个为1，如果我用xNOR的例子比用NOT作为例子结果会好一些，但这两个其实是相同的，这就意味着在x1异或x2后再取反，即他们同时为真或者同时为假的时候，我们将会获得y等于1，y为0的结果是，如果他们中仅有一个为真，则y为0。 我们能否找到一个神经网络模型来拟合这种训练集，为了建立能够拟合XNOR运算，我们先拟合一个简单的神经网络，它拟合了“且运算”。 假设我们有输入x1，x2都是二进制，即要么是0要么是1，我们的目标函数y等于x1且x2，一个逻辑与运算，那么我们怎样得到一个具有单个神经元的神经网络来计算这个逻辑与呢？ 我们给这个网络分配一些权重或参数，-30，+20，+20，即我们给x的前面系数赋值，所以我们的h(x)=g(-30+20x1+20x2),右上角的图就是我们的s型函数，然后我们看四种输入的可能性，就是与运算的结果。 同样我们用神经网络实现或运算然后讲解更为赋值的神经网络。 我们只要在x1前面放入一个很大的负数，就可以实现非的功能。 我们现在把这三个功能放在一起，就可以实现x1 XNOR x2的功能。 当层数很多的时候，你有一个相对简单的输入量的函数作为第二层，而第三层可以建立在此基础上建立一些更加复杂的函数，然后在下一层又在计算一个稍微复杂的函数，我们可以运用更深层的函数计算更加复杂的函数。 神经网络还可以用于识别手写数字。 它使用的输入是不同的图像或者说就是一些原始的像素点。第一层计算出一些特征，然后下一层再计算出一些稍复杂的特征，然后是更复杂的特征，然后这些特征实际上被最终传递给最后一层逻辑回归分类器上，使其准确地预测出神经网络“看”到的数字。 多类分类（Multiclass Classification）在多分类问题中我们如何处理？ 和处理逻辑回归的多分类问题一样。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习（六）]]></title>
    <url>%2F2018%2F05%2F10%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这里记录了视频第七节的内容，主要关于正则化的问题 欠拟合(under fitting)和过拟合(over fitting)现在已经学习了一些不同的机器学习算法，包括线性回归和逻辑回归，它们能够有效的解决许多问题，但是将它们应用到某些特定的机器学习中时，就会出现欠拟合或者过拟合的问题，导致效果很差，通过正则化的方法可以改善算法，下面介绍什么是过拟合与欠拟合。继续用线性回归预测房价的例子： 首先看第一幅图，使用一条直线函数来拟合数据，很显然随着房子面积的增大，房屋价格的变化越稳定或者说是越像右越趋于平滑，因此线性回归并没有很好拟合训练数据。==我们把此类情况称为欠拟合(underfitting)，或者叫作叫做高偏差(bias)。== 这两种说法大致相似，都表示没有很好地拟合训练数据。高偏差这个词是 machine learning 的研究初期传下来的一个专业名词，具体到这个问题，意思就是说如果用线性回归这个算法去拟合训练数据，那么该算法实际上会产生一个非常大的偏差或者说存在一个很强的偏见。 在第二幅图我们用了一个二次函数去拟合数据，可以拟合出一条合理的曲线，事实证明这是一个很好的拟合效果。 第三幅图，在这里我们有五个训练数据，所以使用了五个参数θ0到θ4的一个四次多项式去拟合数据，它似乎是一个很好的拟合，因为它成功的通过了我们的所有训练数据，但是它非常的扭曲，在上下波动，所以事实上我们并不认为它是一个预测房价的好模型。 ==我们把这类情况叫做过拟合(overfitting)，也叫高方差(variance)。== 与高偏差一样，高方差同样也是一个历史上的叫法。从第一印象上来说，如果我们拟合一个高阶多项式，那么这个函数能很好的拟合训练集（能拟合几乎所有的训练数据），但这也就面临函数可能太过庞大的问题，变量太多。==同时如果我们没有足够的数据集（训练集）去约束这个变量过多的模型，那么就会发生过拟合。==12为什么足够多的多项式可以完美的拟合数据？泰勒公式展开式。 概括的说，过拟合将会发生在变量（特征）过多的时候，这时候训练出的方程总能够很好的拟合训练数据，我们的代价函数无限趋于0或者就是0，但是这样千方百计拟合训练数据的曲线无法泛化到新的样本数据中，以至于无法预测新的样本价格。术语泛化指的是一个假设模型能够应用到新样本的能力，新样本指的是不在训练集中的样本数据。 过拟合和欠拟合的情况不仅出现在线性回归也会出现在逻辑回归的问题过多的变量（特征），同时只有非常少的训练数据，会导致出现过度拟合的问题 如何避免过拟合呢？有以下两个方式来避免过拟合 减少选取变量的数量，保留更加重要的特征 具体而言，我们可以人工检查每一项变量，并以此来确定哪些变量更为重要，然后，保留那些更为重要的特征变量。至于，哪些变量应该舍弃，我们以后在讨论，这会涉及到模型选择算法，这种算法是可以自动选择采用哪些特征变量，自动舍弃不需要的变量。这类做法非常有效，但是其缺点是当你舍弃一部分特征变量时，你也舍弃了问题中的一些信息。例如，也许所有的特征变量对于预测房价都是有用的，我们实际上并不想舍弃一些信息或者说舍弃这些特征变量。 正则化 正则化中我们将保留所有的特征变量，但是会减小特征变量的数量级（参数数值的大小θ(j)）。 这个方法非常有效，当我们有很多特征变量时，其中每一个变量都能对预测产生一点影响。正如我们在房价预测的例子中看到的那样，我们可以有很多特征变量，其中每一个变量都是有用的，因此我们不希望把它们删掉，这就导致了正则化概念的发生。 接下来我们会讨论怎样应用正则化和什么叫做正则化均值，然后将开始讨论怎样使用正则化来使学习算法正常工作，并避免过拟合。 机器学习的正则化在前面介绍了用二次函数去拟合这些数据，他的拟合效果是很好的，然而我们用更高次的多项式去拟合，最终到的一个曲线，尽管他很好的拟合了训练集，但并不是一个好的结果，因为他过度拟合了数据，所以一般性不好。 然后我们考虑下面的假设，我们想要加上惩罚项从而使参数 θ3 和 θ4 足够小，上面函数是我们的优化目标，也就是说我们要尽量减少代价函数的均方差，对于这个函数我们对它添加一些项，加上 1000 乘以 θ3 的平方，再加上 1000 乘以 θ4 的平方，于是出现了下面的式子： 1000 只是我随便写的某个较大的数字而已。现在，如果我们要最小化这个函数，那么为了最小化这个新的代价函数，我们要让 θ3 和 θ4 尽可能小。因为，如果你在原有代价函数的基础上加上 1000 乘以 θ3 这一项 ，那么这个新的代价函数将变得很大，所以，当我们最小化这个新的代价函数时， 我们将使 θ3 的值接近于 0，同样 θ4 的值也接近于 0，就像我们忽略了这两个值一样。如果我们做到这一点（ θ3 和 θ4 接近 0 ），那么我们将得到一个近似的二次函数。 因此，我们最终恰当地拟合了数据，我们所使用的正是二次函数加上一些非常小，贡献很小项（因为这些项的 θ3、 θ4 非常接近于0）。显然，这是一个更好的假设。 正则化背后的思路，如果我们的参数值对应一个较小的值的（参数值较小），那么我们会道道一个形式更简单的假设。 在上面的例子中，我们的惩罚的只是θ3和θ4，使这两个值均接近于零，从而得到了一个更简单的假设，实际上这个假设类似一个二次函数。 更简单的讲，如果我们像惩罚θ3和θ4这样惩罚其他参数，那么我们往往可以得到一个相对简单的假设函数。 ==实际上，这些参数的值越小，对应的函数曲线越平滑，也就是更加简单的函数，因此，就不易发生过拟合的问题。== 为什么越小的参数对应一个相对简单的假设函数，具体看下面这个例子。 对于房屋价格的预测我们可能又上百个特征，与刚刚所说的多项式例子不同，我们并不知道θ3和θ4是高阶多项式的项，所以，我们有一百个特征，但是我们那并不知道如何选择关联度更好的参数，如何缩小参数的数目等等。 因此在正则化里，我们要做的事情，就是减小我们的代价函数所有的参数值，因为我们并不知道哪一个或几个要去缩小，所以我们要修改代价函数，在后面添加一项，就像我们在方括号里的这项，当我们添加一个额外的正则化项的时候，我们缩小了每一个参数。 顺便说一下，按照惯例，我们没有去惩罚 θ0，因此 θ0的值是大的。这就是一个约定从 1 到 n 的求和，而不是从 0 到 n 的求和。但其实在实践中这只会有非常小的差异，无论你是否包括这θ0这项。但是按照惯例，通常情况下我们还是只从 θ1 到 θn 进行正则化。 带λ的的这项就是一个正则化项，并且λ在这里我们称做正则化参数。 ==λ 要做的就是控制在两个不同的目标中的平衡关系。== 第一个目标就是我们想要使假设函数更好的拟合训练数据 第二个目标是要保持我们的参数较小通过正则化 而λ这个正则化参数需要控制的是两者之间的平衡，既平衡拟合训练的目标和保持参数值较小的目标。从而保持假设的形式相对简单，来避免过拟合。 对于房屋价格预测来说，我们之前所用的非常多的高阶多项式来拟合，我们将会得到一个非常弯曲和复杂的曲线函数，现在只需要通过正则化的优化，就可以得到一个更加合适的曲线，这个曲线不是一个真正的二次函数曲线，而是更加的流畅和简单的一个曲线。这样就得到了对于这个数据集更好的假设函数。 再一次说明下，这部分内容的确有些难以明白，为什么加上参数的影响可以具有这种效果？但如果你亲自实现了正规化，你将能够看到这种影响的最直观的感受。 在正则化线性回归中，如果正则化参数值λ被设定的非常大，那么会发生什么呢？我们非常大的惩罚参数θ1 θ2 θ3 θ4 … 也就是说，我们最终惩罚θ1 θ2 θ3 θ4 … 在一个非常大的程度，那么我们会使所有这些参数接近于零。 如果我们这么做，那么就意味着我们的假设相当于去掉了这些项，并且使我们只留下了一个简单的假设，这个假设只能表明房屋价格等于θ0的值，那就是类似与一条水平的直线，对于数据来说就是一个欠拟合。这是一个失败的假设直线，对于训练集来说这就是一条平滑的直线，它没有任何趋势，它不会去趋向大部分的训练样本的任何值。 另一个说法就是这种假设有过于强烈的偏见或者说使高偏差，认为预测的价格只等于θ0，对于数据来说只是一条水平线。 因此，为了使正则化运行良好，我们应当注意一些方面，应该去选择一个不错的正则化参数λ，当我们以后讲到多重选择时我们将讨论一种方法来自动选择正则化参数 λ，为了使用正则化，接下来我们将把这些概念应用到到线性回归和逻辑回归中去，那么我们就可以让他们避免过度拟合了。 正则化的线性回归 （Regularized Linear Regression）之前写过线性回归的代价函数如下： 对于线性回归的求解，我们之前运用了两种学习算法，一种基于梯度下降，一种基于正规方程。 梯度下降 正规方程 不可逆情况当出现样本数量M比特征数N少或等于时，矩阵XTX将出现不可逆或者奇异（singluar）矩阵，用另一个说法就是矩阵的退化（degenerate），这时我们就没办法用正规方程来求 θ。 正则化可以解决这个问题，具体的说只要正则参数是严格大于零，实际上，可以证明上图的蓝括号部分是可逆的（invertable），因此正则化可以解决任何XTX不可逆的问题。 所以现在可以实现线性回归避免过度拟合的问题，即使是一个相对较小的训练集合里面有很多的特征值。 正则化的逻辑回归（Regularized Logistic Regression）逻辑回归的正则化实际上和线性回归的正则化十分的相似。 同样使用梯度下降： 如果在高级优化算法中，使用正则化技术的话，那么对于这类算法我们需要自己定义costfunction 这个我们自定义的 costFunction 的输入为向量 θ ，返回值有两项，分别是代价函数 jVal 以及 梯度gradient。 总之我们需要的就是这个自定义函数costFunction，针对Octave而言，我们可以将这个函数作为参数传入到 fminunc 系统函数中（fminunc 用来求函数的最小值，将@costFunction作为参数代进去，注意 @costFunction 类似于C语言中的函数指针），fminunc返回的是函数 costFunction 在无约束条件下的最小值，即我们提供的代价函数 jVal 的最小值，当然也会返回向量 θ 的解。 上述方法显然对正则化逻辑回归是适用的。 总结从这里开始感觉课程已经有些难度了，关于正则化我也是查阅了其他相关资料才得以明白，尝试写个关于正则化的程序吧。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则化]]></title>
    <url>%2F2018%2F05%2F09%2F%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[在机器学习中遇到了难题，就是对正则化的理解，通过查阅资料，记录下什么是正则化。 正则化模型选择的典型方法是正则化（regularization）。正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项（regularizer）或罚项（penalty term）。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大。比如，正则化项可以是模型参数向量的==范数==。 在这里我又遇到了一个问题，什么是范数，哎，高数没学好，啥也不知道了。 范数(norm)是数学中的一种基本概念。在泛函分析中，它定义在赋范线性空间中，并满足一定的条件，即①非负性；②齐次性；③三角不等式。它常常被用来度量某个向量空间（或矩阵）中的每个向量的长度或大小。 看完定义又是头大，完全不理解啊！定义和性质什么的都不重要了，这里我只需要知道范数所代表的函数意义：12341-范数：║x║1=│x1│+│x2│+…+│xn│2-范数：║x║2=（│x1│2+│x2│2+…+│xn│2）1/2∞-范数：║x║∞=max（│x1│，│x2│，…，│xn│）其中2-范数就是通常意义下的距离。 矩阵范数：1234561-范数：║A║1 = max&#123; ∑|ai1|，∑|ai2|，……，∑|ain| &#125; （列和范数，A每一列元素绝对值之和的最大值）（其中∑|ai1|第一列元素绝对值的和∑|ai1|=|a11|+|a21|+...+|an1|，其余类似）；2-范数：║A║2 = A的最大奇异值 = (max&#123; λi(AH*A) &#125;) 1/2 （谱范数，即A^H*A特征值λi中最大者λ1的平方根，其中AH为A的转置共轭矩阵）；∞-范数：║A║∞ = max&#123; ∑|a1j|，∑|a2j|,...，∑|amj| &#125; （行和范数，A每一行元素绝对值之和的最大值）（其中∑|a1j| 为第一行元素绝对值的和，其余类似）； 看了这几个例子大概理解了，若，那么继续正则化的话题，正则化主要解决的问题： 1.正则化就是对最小化经验误差函数上加约束，这样的约束可以解释为先验知识(正则化参数等价于对参数引入先验分布)。约束有引导作用，在优化误差函数的时候倾向于选择满足约束的梯度减少的方向，使最终的解倾向于符合先验知识(如一般的l-norm先验，表示原问题更可能是比较简单的，这样的优化倾向于产生参数值量级小的解，一般对应于稀疏参数的平滑解)。 2.同时，正则化解决了逆问题的不适定性，产生的解是存在，唯一同时也依赖于数据的，噪声对不适定的影响就弱，解就不会过拟合，而且如果先验(正则化)合适，则解就倾向于是符合真解(更不会过拟合了)，即使训练集中彼此间不相关的样本数很少。 正则化一般具有如下形式：其中第一项是经验风险，第二项是正则化项，λ&gt;=0为调整两者关系的系数。正则化项可以取不同的形式，例如，回归问题，损失函数是平方损失，正则化项可以使参数向量的2范类。范类的记录大概就是这么多了。 泰勒公式顺便说下泰勒公式，是在学习吴恩达的机器学习正则化时才想到的，我们的预测模型是一个多项式的和，当多项式过少会欠拟合，过多会过拟合，当多项式足够多的时候就会区分出所有的种类，这不就是泰勒公式展开式吗？ 数学中，泰勒公式是一个用函数在某点的信息描述其附近取值的公式。如果函数足够平滑的话，在已知函数在某一点的各阶导数值的情况之下，泰勒公式可以用这些导数值做系数构建一个多项式来近似函数在这一点的邻域中的值。 泰勒公式是将一个在x=x0处具有n阶导数的函数f(x)利用关于(x-x0)的n次多项式来逼近函数的方法。若函数f(x)在包含x0的某个闭区间[a,b]上具有n阶导数，且在开区间(a,b)上具有(n+1)阶导数，则对闭区间[a,b]上任意一点x，成立下式：其中,fn^(x)表示fn(x)的n阶导数，等号后的多项式称为函数f(x)在x0处的泰勒展开式，剩余的Rn(x)是泰勒公式的余项，是(x-x0)n的高阶无穷小。 参考：李航 统计学习方法]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>正则化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python实现逻辑回归]]></title>
    <url>%2F2018%2F05%2F04%2FPython%E5%AE%9E%E7%8E%B0%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[首先总结一下学习，虽然叫回归但是和回归没有任何关系，刚尝试写代码时，思考分类问题陷入了线性回归的思路，纠结了好久，已经求出weights但不会拟合直线，后来用笔画了下立刻明白思考偏离了，所以就算有了电脑还是应该用笔在纸上画一画。 写代码首先第一步是要知道做什么：我需要画一个直线，直线公式为 θ0 x0 + θ1 x1 + θ2 * x2 = 0 其中x0 = 1。想要画出这条直线我需要知道三个θ的值，通过吴大大的机器学习视频我知道的把θ的转置乘以x的带入逻辑函数g(z)就能求出预测函数h(x),然后通过梯度下降的方式更新θ，最终得到θ的近似值。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778#!/usr/bin/python# coding=utf-8from sklearn.datasets import load_irisimport matplotlib.pyplot as pltimport numpy as np# 逻辑函数(Logistic function)def gfunc(z): return 1 / (1 + np.exp(-z))# 构造训练集：引入了鸢尾花数据集来作为训练集iris = load_iris()data = iris.datatarget = iris.target# 取前一百行的第一列和第三列做特征值X = data[0:100, [0, 2]]y = target[0:100]# 画出训练集的散点图label = np.array(y)index_0 = np.where(label == 0)plt.scatter(X[index_0, 0], X[index_0, 1], marker='x', color='b', label='0', s=15)index_1 = np.where(label == 1)plt.scatter(X[index_1, 0], X[index_1, 1], marker='o', color='r', label='1', s=15)plt.xlabel('X1')plt.ylabel('X2')plt.legend(loc='upper left')######################################################### 训练集构建完成后判断边界，我猜边界是一条直线# 直线的公式：θ0 * x0 + θ1 * x1 + θ2 * x2 = 0 其中x0 = 1# 因为这个问题里是一个二维分类，所以边界是有三个θ决定的######################################################### 训练集的个数mm = 100# 重新构建了X向量 加上了x0=1x0 = np.full(m, 1.0)x0 = np.vstack(x0)x = np.column_stack((x0, X))# 随机设置三个theta值theta = np.random.randn(3)# 两种终止条件loop_max = 10000 # 最大迭代次数(防止死循环)epsilon = 1e-3error = np.zeros(3)count = 0alpha = 0.001 # 步长while count &lt; loop_max: delta = np.zeros(3) for i in range(m): delta = delta + (gfunc(np.dot(theta, x[i])) - y[i]) * x[i] theta = theta - alpha * delta # 判断是否已收敛 if np.linalg.norm(theta - error) &lt; epsilon: finish = 1 break else: error = theta count += 1print("The number of iterations = ", count)print(theta)# x0 = 1# 已经求得theta参数，给出x1的值，根据theta计算x2，画出直线x1 = np.arange(4, 7.5, 0.5)x2 = (- theta[0] - theta[1] * x1) / theta[2]plt.plot(x1, x2, color='black')plt.show() 后来我通过学习他人的逻辑回归函数，修改步长，观察损失图，发现了些有趣的事，我把代码重构了，更便于可视化 &gt;逻辑回归源代码&lt; 首先我把步长设置为0.001，然后画出loss图： 0.001的步数大概迭代2500多次达到低谷，从图中中观察到loss损失相当平滑，没有出现震荡 然后我修改了步数为0.01，只通过800次迭代就下降到低谷，但是出现震荡，如果在线性回归中出现震荡则不会收敛，但是在逻辑回归问题中，尽管出现了震荡，但最终还是收敛。但如果我把步数设置的更大0.02时，就会每1800次后出现震荡的情况，最终无法收敛。]]></content>
      <categories>
        <category>机器学习代码实现</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习（五）]]></title>
    <url>%2F2018%2F04%2F24%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%94%EF%BC%89%2F</url>
    <content type="text"><![CDATA[最近事情比较多，又懒惰了，继续学习。 分类问题什么是分类问题，例如：垃圾邮件分类，恶性肿瘤预测。在分类问题中一般结果是0和1，1称为正样本或正类，0称为负样本或负类。 首先讲解的是简单的两变量分类问题使用线性回归的方式解决分类问题如何？如果是上图这样的例子来看，使用线性回归的方式貌似可以解决分类问题，但是如果存在一个严重偏差的特征时，使用线性回归拟合分类问题就会出现严重的偏差，在分类问题中最终的结果只有0和1，但是在线性回归中会出现小于1和大于0的结果。所以使用线性回归的方式不能很好的处理分类问题，于是引出了另一种模型，逻辑回归（逻辑回归的叫法是历史原因，和回归并没有什么关系） 逻辑回归什么样的数学模型适合回归问题呢？一个只会在0与1中间震荡的函数模型： 其中： x,表示的是特征向量 g，代表逻辑函数(Logistic function)是一个常用的曲线函数(Sigmoid function),表达式为： 函数的图像就如上图所示。 h,表示的就是逻辑回归，带入到函数g中，最终得到的表达式就是 函数h表示的就是当输入特征X时，根据输入的特征计算输出变量Y=1的可能性。假设h(x)=0.7,表示的就是患有恶性肿瘤的概率为0.7 判定边界(Decision Boundary)判定边界能够让我们更好的理解逻辑回归和假设函数在计算什么 上图就是逻辑回归的函数和图像，看一下数学意义：123456当h &gt;= 0.5时,预测结果 y = 1，当h &lt; 0.5时，预测结果 y = 0,所以：当 y = 1 时，h(x) = g(z) &gt;= 0.5 那么 z &gt;= 0,也就是θtX&gt;=0;当y=0时，最后得到θtX&lt;0。 具体看下面这个例子 其中的theta的参数分别为-3,1,1存在如上图所示的数据以及表示函数,如果要预测y=1的概率，最后得到的表达式为： 最后得到的结果很明显是一个过（0,3）（3,0）的直线： 其中的方程就是一个判定边界，通过这条线就可以分辨出正样本和负样本了。 除了这种线性的判定边界之外，还有一些其他形状的判定边界，如圆形。 逻辑回归中的代价函数上面就是之前讲过的线性回归中的代价函数，这个代价函数在线性回归中能够很好地使用，但是在逻辑回归中却会出现问题，因为将逻辑回归的表达式带入到h函数中得到的是一个非凸函数的图像，那么就会存在多个局部最优解，无法像凸函数一样得到全局最优解。示例如下。 所以在逻辑回归中需要重新定义代价函数： 最后得到的函数h和Cost函数之前的关系如下： 构建一个这样的函数的好处是在于，当y=1时，h=1，如果h不为1时误差随着h的变小而增大；同样，当y=0时，h=0，如果h不为0时误差随着h的变大而增大。 代价函数中的梯度下降在上一节中的逻辑回归中的代价函数中给出了代价函数的定义 最后可以简化为: 最终的求解问题就是要求回归函数的值最小，那么同样可以使用在线性回归中所用到的梯度函数。 上图就是逻辑回归的梯度求解过程，虽然看起来和线性回归相似，但实则是完全不同的。在线性回归中，h函数为theta的转置与X的乘积，但是在逻辑回归中则不是。这样就导致了两者在运算方面和优化方面是完全不同的。但是在运行梯度下降算法之前，进行特征缩放依旧是非常重要的。 高级优化优化算法除了讲到的梯度下降算法之外，还有一些叫做共轭梯度下降算法(BFGS,L-BFGS)。使用这些共轭梯度下降算法的好处在于，不需要手动地选择学习率a，这些算法会自行尝试选择a;比梯度下降算法运算更快。一般情况下，在常见的机器学习算法库中都带有这些算法，不需要程序员手动实现这些算法。 多类别分类问题现实世界中除了二元的分类问题还有多元的分类问题，例如邮件的类型有工作，朋友，家人，爱好等多种，分类到不同的文件夹下，如对天气的分类，是晴天、多云、小雨等等天气。 多元分类问题与二元分类问题的区别如下:多元分类的思路与二元分类问题的解决思路是类似的。可以将多元问题变为两元问题，具体如下： 这样n元的分类问题，就会进行n次的机器学习的分类算法。对每一次的分类结果即为h(x)。那么经过n此分类之后，最后得到的结果为: 那么当输入新的训练集或者是变量X，只需要按照上面的思路进行分类，其中的h(x)的最大值就是对应的最后的分类结果。 总结本章写完用了一周时间，但其实视频一天就看完了，博客内容基本是照抄别人的，关于具体的代码实现查看另一篇博客。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习（四）]]></title>
    <url>%2F2018%2F04%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前四章的内容学习完毕，第五章讲了Octave这个软件的使用，类似于matlab，大学有过学习matlab经验所以这个学起来想对比较轻松，不论是在Ubuntu还是windows安装都很简单，这个的界面布局都和matlab基本一模一样。 虽然用python都可以实现，但Octave开源免费，比numpy更简单的实现算法，所以有必要学习一下。 其实关于Octave的东西并不想记录，和matlab一样，但为了这个博客的完整性还是简单的记录一下，我使用的是windows版的直接打开GUI就能使用了。 基本操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152%基本四则运算&gt;&gt; 1+2ans = 3&gt;&gt; 6-1ans = 5&gt;&gt; 5*8ans = 40&gt;&gt; 1/5ans = 0.20000&gt;&gt; 3^6ans = 729%不等号是~而不是！&gt;&gt; 1==2ans = 0&gt;&gt; 1~=2ans = 1%与 或 异或&gt;&gt; 8 &gt; 1 &amp;&amp; 0ans = 0&gt;&gt; 9 &gt; 1 || 0ans = 1&gt;&gt; xor(1, 0)ans = 1%如果你想分配一个变量，但不希望在屏幕上显示结果，你可以在命令后加一个分号，可以抑制打印输出，敲入回车后，不打印任何东西。&gt;&gt; a = 3a = 3&gt;&gt; a = 3;&gt;&gt; b = 'hello word';&gt;&gt; bb = hello word%设置A等于圆周率π，如果我要打印该值，那么只需键入A像这样就打印出来了。&gt;&gt; a = pi;&gt;&gt; pians = 3.1416&gt;&gt; aa = 3.1416&gt;&gt; disp(sprintf('2 decimals: %0.12f', a))2 decimals: 3.141592653590这是一种，旧风格的C语言语法，对于之前就学过C语言的同学来说，你可以使用这种基本的语法来将结果打印到屏幕。例如 sprintf命令的六个小数：0.6%f ,a，这应该打印π的6位小数形式。也有一些控制输出长短格式的快捷命令：&gt;&gt; format long&gt;&gt; aa = 3.14159265358979&gt;&gt; format short&gt;&gt; aa = 3.1416 简单的运算符就是这些，重点是关于矩阵的 简单矩阵的创建1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495简单矩阵的创建&gt;&gt; A = [1 2; 3 4; 5 6]A = 1 2 3 4 5 6&gt;&gt; A = [2 2;3 3;4 4]A = 2 2 3 3 4 4&gt;&gt; B = [1 2 3]B = 1 2 3&gt;&gt; B = [1; 2; 3]B = 1 2 3&gt;&gt;这个集合V是一组值，从数值1开始，增量或说是步长为0.1，直到增加到2，按照这样的方法对向量V操作，可以得到一个行向量，这是一个1行11列的矩阵，其矩阵的元素是1 1.1 1.2 1.3，依此类推，直到数值2。我也可以建立一个集合V并用命令“1:6”进行赋值，这样V就被赋值了1至6的六个整数。&gt;&gt; v = 1:6v = 1 2 3 4 5 6这里还有一些其他的方法来生成矩阵例如“ones(2,3)”，也可以用来生成矩阵：&gt;&gt; ones(2,3)ans = 1 1 1 1 1 1元素都为2，两行三列的矩阵，就可以使用这个命令：&gt;&gt; C = 2*ones(2,3)C = 2 2 2 2 2 2你可以把这个方法当成一个生成矩阵的快速方法。w为一个一行三列的零矩阵，一行三列的A矩阵里的元素全部是零：&gt;&gt; W = zeros(1,3)W = 0 0 0如果我对W进行赋值，用Rand命令建立一个一行三列的矩阵，因为使用了Rand命令，则其一行三列的元素均为随机值，如“rand(3, 3)”命令，这就生成了一个3×3的矩阵，并且其所有元素均为随机。&gt;&gt; rand(3,3)ans = 0.60790 0.22000 0.10036 0.61343 0.58981 0.17660 0.22697 0.88276 0.42049&gt;&gt;你知道什么是高斯随机变量，或者，你知道什么是正态分布的随机变量，你可以设置集合W，使其等于一个一行三列的N矩阵，并且，来自三个值，一个平均值为0的高斯分布，方差或者等于1的标准偏差。&gt;&gt; w = randn(1,3)w = -1.24688 1.87417 -0.70878并用hist命令绘制直方图。&gt;&gt; w = -9 + sqrt(10)*(randn(1, 10000));&gt;&gt; hist(w)&gt;&gt; hist(w,50)绘制单位矩阵：&gt;&gt; I = eye(6)I =Diagonal Matrix 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1对命令不清楚可以通过help命令查询 size函数123456789101112131415161718&gt;&gt; A = [1: 2; 3 4; 5 6]A = 1 2 3 4 5 6&gt;&gt; size(A) %输出[行数 列数]ans = 3 2&gt;&gt; size(A, 1) %行数ans = 3&gt;&gt; size(A, 2) %列数ans = 2&gt;&gt; length(A) %行数和列数中最大值ans = 3 导入与导出数据123456load 文件名whos %将当前的变量都显示出来clear A %将变量A删除save hello.mat A; %将变量A存入hello.mat文件save hello.txt A -ascii; %将A存为ascii 取矩阵中的值123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&gt;&gt; AA = 1 2 3 4 5 6&gt;&gt; A(3,2) %矩阵A第三行第二列的数ans = 6&gt;&gt; A(2,:) %第二行的数ans = 3 4&gt;&gt; A(:,2) %第二列的数ans = 2 4 6&gt;&gt; A([1 3],:) %第一行和第三行的数ans = 1 2 5 6&gt;&gt; A(:,2) = [10;11;12] %修改第二列的数A = 1 10 3 11 5 12&gt;&gt; A = [A,[100;200;300]] %增加一列数据A = 1 10 100 3 11 200 5 12 300&gt;&gt; A(:) %修改为一列向量ans = 1 3 5 10 11 12 100 200 300 拼接矩阵12345678910111213141516171819202122232425262728293031&gt;&gt; A = [1 2; 3 4; 5 6]A = 1 2 3 4 5 6&gt;&gt; B = [11 12; 13 14; 15 16]B = 11 12 13 14 15 16&gt;&gt; C = [A B] %将矩阵A和B并列拼接C = 1 2 11 12 3 4 13 14 5 6 15 16&gt;&gt; C = [A;B] %加分号是将B矩阵拼接到A下面C = 1 2 3 4 5 6 11 12 13 14 15 16&gt;&gt; 矩阵计算12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&gt;&gt; a = [1 2; 3 4; 5 6]a = 1 2 3 4 5 6&gt;&gt; B = [11 22; 33 44; 55 66]B = 11 22 33 44 55 66&gt;&gt; C = [1 1; 2 2]C = 1 1 2 2&gt;&gt; V = [1; 2; 3]V = 1 2 3&gt;&gt; A*C %矩阵相乘ans = 5 5 11 11 17 17&gt;&gt; A*B %相乘条件必须是A矩阵的列等于B矩阵的行，否则报错error: operator *: nonconformant arguments (op1 is 3x2, op2 is 3x2)&gt;&gt; A.*2 %矩阵中的每个元素都乘二ans = 2 4 6 8 10 12&gt;&gt; A.^2 %每个元素的平方ans = 1 4 9 16 25 36&gt;&gt; 1./V %每个元素的倒数ans = 1.00000 0.50000 0.33333 &gt;&gt; V + ones(length(V), 1) %每个元素都加一ans = 2 3 4&gt;&gt; A' %A的转置ans = 1 3 5 2 4 6&gt;&gt; 矩阵的索引123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119&gt;&gt; a = [1 15 2 0.5]a = 1.00000 15.00000 2.00000 0.50000&gt;&gt; [val,ind] = max(a) % val 矩阵中的最大元素，ind 最大值的indexval = 15ind = 2&gt;&gt; val = max(A) %矩阵每列的最大值val = 5 6&gt;&gt; a &lt; 3 %检查矩阵中比3小的元素，返回布尔型ans = 1 0 1 1&gt;&gt; find(a&lt;3) %比3小的元素的位置ans = 1 3 4&gt;&gt; A = magic(3) %创建一个幻方 （行，列，对角线相加想等）A = 8 1 6 3 5 7 4 9 2&gt;&gt; [r c] = find(A&gt;=7) % 符合A&gt;=7元素的行列坐标r = 1 3 2c = 1 2 3&gt;&gt; sum(a) %求所有元素的和ans = 18.500&gt;&gt; prod(A) %求每列的乘积ans = 96 45 84&gt;&gt; sum(A) %求每列的和ans = 15 15 15&gt;&gt; floor(a) %返回小于元素的最小整数ans = 1 15 2 0&gt;&gt; ceil(a) %返回大于元素的最大整数ans = 1 15 2 1&gt;&gt; max(rand(3), rand(3)) %比较两个矩阵返回最大值ans = 0.65329 0.32803 0.23948 0.56627 0.37716 0.64170 0.17771 0.81867 0.73937&gt;&gt; max(A, [], 1) %返回每一列的最大值ans = 8 9 7&gt;&gt; max(A, [], 2) %返回每一行的最大值ans = 8 7 9&gt;&gt; A = magic(9)A = 47 58 69 80 1 12 23 34 45 57 68 79 9 11 22 33 44 46 67 78 8 10 21 32 43 54 56 77 7 18 20 31 42 53 55 66 6 17 19 30 41 52 63 65 76 16 27 29 40 51 62 64 75 5 26 28 39 50 61 72 74 4 15 36 38 49 60 71 73 3 14 25 37 48 59 70 81 2 13 24 35&gt;&gt; sum(A,2) %行的和ans = 369 369 369 369 369 369 369 369 369&gt;&gt; sum(A,1) %列的和ans = 369 369 369 369 369 369 369 369 369&gt;&gt; sum(sum(A.* eye(9))) %对角线的和ans = 369&gt;&gt; pinv(A) %伪逆矩阵 画图123&gt; t = [0 : 0.01 : 0.98];&gt;&gt; y1 = sin(2*pi*4*t);&gt;&gt; plot(t, y1,'r') 在一个画布上画两副如图12345&gt;&gt; y1 = sin(2*pi*4*t);&gt;&gt; y2 = cos(2*pi*4*t);&gt;&gt; plot(t,y2)&gt;&gt; hold on;&gt;&gt; plot(t, y1,'r') 并列显示两个图12345&gt;&gt; subplot(1,2,1)&gt;&gt; plot(t,y1)&gt;&gt; subplot(1,2,2)&gt;&gt; plot(t,y2)&gt;&gt; axis([0.5 1 -1 1]) 绘制矩阵12345678910&gt;&gt; A = magic(5)A = 17 24 1 8 15 23 5 7 14 16 4 6 13 20 22 10 12 19 21 3 11 18 25 2 9&gt;&gt; imagesc(A) 1&gt;&gt; imagesc(A),colorbar,colormap gray; 控制语句1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556%for循环语句&gt;&gt; v = zeros(1,10)v = 0 0 0 0 0 0 0 0 0 0&gt;&gt; for i = 1: 10v(i) = 2^i;end;&gt;&gt; vv = 2 4 8 16 32 64 128 256 512 1024&gt;&gt;%while语句&gt;&gt; while i &lt; 5,v(i) = 100;i = i+1end&gt;&gt; vv = 2 4 8 16 32 64 128 256 512 1024%if break语句&gt;&gt; i = 1;&gt;&gt; while true;v(i) = 999;i = i+1if i==6,break;endendi = 2i = 3i = 4i = 5i = 6&gt;&gt; vv = 999 999 999 999 999 64 128 256 512 1024&gt;&gt;&gt;&gt; if v(1) == 1,disp('The value is one!')elseif v(1) == 2,disp('The value is two!')elsedisp('The value is not one or two!')endThe value is not one or two!&gt;&gt; 定义函数将函数定义写在文件中，并把文件名命名为‘函数名.m’，将文件放在当前路径下，或者用 addpath 将文件目录加入当前会话 本章学习结束]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>octave</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DragonBoard-410C开发环境搭建]]></title>
    <url>%2F2018%2F04%2F19%2FDragonBoard-410C%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[初识DragonBoard 410cDragonBoard 410c是一款搭载Qualcomm® Snapdragon™ 410（64位的四核处理器）的开发板，它功能齐全，具有强大的处理能力，内置 8GB eMMC (支持标准microSD卡槽)，并且还内置wifi、蓝牙、和GPS模块，具有HDMI 输出及USB 接口 (3个)。所有这些集成到只有信用卡大小的一块板子上，售价仅为75美元。所有的这一切使得DragonBoard 410c成为嵌入式计算以及物联网（IoT）产品的理想选择，例如下一代的机器人，摄像头，医疗设备，自动售货机，智能建筑，数字标牌，赌场游戏机等等。DragonBoard 410c DragonBoard 410c 兼容96Boards消费版（CE）规范，该规范由Linaro社区委员会组织维护，定位移动、嵌入及数字家庭领域。 DragonBoard 410c目前已经可以运行Android5.1、Ubuntu以及Windows 10 IoT Core等系统，并且是首批取得微软认证的设备之一，认证后可支持Azure IoT SDK，可随时用于物联网应用。 # 开发环境的搭建本次开发环境的搭建都是在windows下完成 刷为Linux系统410C开发板自带安卓系统，通过HDMI连接显示器即可显示，通过USB连接键盘鼠标进行操作。此次开发环境要求是Linux，所以要重刷系统，官方提供的是debian深度修改的系统，高通起名叫linaro。Linux内核为4.140. &gt;&gt;&gt;镜像下载连接点这里&gt;&gt;&gt;Win32DiskImager卡刷工具点这里 这里我选择了最新的18.01的卡刷img 写入Linux镜像到SD卡操作步骤如下：12345● 下载Win32DiskImager和卡刷镜像● 打开Disklmager工具● 选择镜像文件路径● 选择电脑映射的SD卡盘符● 点击 Write 把镜像写入SD卡 使开发板从SD卡启动：12345678● 在开发板上插入写好镜像的sd卡● 一个鼠标和键盘连接到410C上● 显示器通过HDMI连接到410C上● 设置启动开关S6 - 0100(从sd卡启动)● 接入电源● 开发板应该会启动并显示一个对话框,选择要安装的操作系统● 选择显示的操作系统(Linux Linaro)并点击“Install”。● 如果一切都成功进入下一步 重启开发板：1234● 拔掉电源线● 拆下sd卡● 复位启动开关调到0000● 重启后应该会引导进入新的系统 通过Visual Studio编译调试410C的程序个人更喜欢使用VS而不是Eclipse，好在这款开发板支持使用VS2013Pro交叉编译，首先安装VS2013或2012，不支持更高版本。 1234567891011121314151617181920212223242526272829303132VS2013旗舰版/专业版/高级版产品密钥Visual Studio Ultimate 2013 KEY（VS2013旗舰版密钥）：BWG7X-J98B3-W34RT-33B3R-JVYW9Visual Studio Premium 2013 KEY（VS2013高级版密钥）：FBJVC-3CMTX-D8DVP-RTQCT-92494Visual Studio Professional 2013 KEY（VS2013专业版密钥）： XDM3T-W3T3V-MGJWK-8BFVD-GVPKYTeam Foundation Server 2013 KEY（密钥）：MHG9J-HHHX9-WWPQP-D8T7H-7KCQGVS2013官方中文专业版（Visual Studio Professional 2013）安装激活方法1、下载后得到的是ISO文件，直接解压缩或用虚拟光驱加载运行都可以2、无所不藏在这里直接解压，然后双击“vs_ultimate.exe”开始安装；3、设置好安装路径后，点击“我同意许可条款和条件”点击“下一步”继续；4、选择您要安装的Visual Studio 2013选项，根据自身需要勾选安装；5、接下来就是有点漫长的安装过程，这时候就是拼电脑配置的时候了；6、成功安装后打开软件，设置好熟悉的环境启动（包括vb、vc、vf等多个开发环境）7、第一次运行软件会有点慢，再点击“帮助”–“注册软件”–可以看到软件有30天试用期，点击“更改我的产品许可证”；10、输入visual studio 2013专业版密钥【XDM3T-W3T3V-MGJWK-8BFVD-GVPKY】11、到这步就已成功激活visual studio 2013专业版了，现在您可以无限制免费使用VS2013。VS2013官方中文专业版（Visual Studio Professional 2013）下载地址Visual Studio Professional 2013 with Update 5 (x86) 官方专业版下载地址：ed2k://|file|cn_visual_studio_professional_2013_with_update_5_x86_dvd_6815749.iso|5517246464|DEA9BB85B73F6A1F23E638DFE06CEF07|/ 安装好visual studio后安装snapdragondebuggerforvsinstaller，解压后安装即可。 什么是 Snapdragon Debugger for Visual Studio？ Snapdragon Debugger for Visual Studio 是微软 Visual Studio IDE 的一款插件工具，针对目前搭载骁龙处理器的设备，可调试各种 API。 目前这款工具可用于在 Microsoft Visual Studio 环境中创建并调试 Android NDK应用。仅可调试原生 C/C++ 代码，不支持调试 Java 代码。 关于更多请看Snapdragon Debugger for Visual Studio 快速入门指南 这个插件安装需要许多东西，SDK,NDK，而我只是需要编译C，不想装太多不用的东西，但是又想用visual studio，可以用VS来远程调试Linux程序 用VS2015开发Linux程序vs2017自带Linux开发环境，可惜我只装了2015，所以尝试用2015来开发Linux需要一个插件： Visual C++ for Linux Development(VC_Linux.exe) 通过远程SSH协议既可以调试了 因为中美贸易战，高通可能不给中国供货了，所以公司改用NXP的芯片，这个项目就搁浅了，话说NXP不也被高通收购了吗 发现问题：我选择的410C刷的img是debian-283，有个问题，WiFi连接后会时不时自己断开但是桌面右下角的wifi图标显示正常，查看官网更新日志，在359已经修复这个问题，但是因为中美贸易战的原因无法从高通官网下载镜像了，所以没有验证是否还存在次BUG。]]></content>
      <categories>
        <category>嵌入式开发</category>
      </categories>
      <tags>
        <tag>嵌入式</tag>
        <tag>410C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python实现梯度下降]]></title>
    <url>%2F2018%2F04%2F19%2FPython%E5%AE%9E%E7%8E%B0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%2F</url>
    <content type="text"><![CDATA[看了Andrew Ng的关于机器学习中梯度下降的学习，用最简单粗暴的解法实现下 注意的地方就是$\theta_0,\theta_1$是同时更新的，所以用一个临时变量接了下 收敛条件的判断：可以让函数迭代指定的次数后退出，也可以认为n次迭代的结果和n-1次的结果非常接近时就代表下降到谷底，退出函数 步数alpha的设置和epsilon的选择，这个例子我尝试步数为0.0025时就会振荡无法收敛，epsilon等于于0.001时有时会产生局部最优解，建议是$10^{-4}$ 明天继续尝试最小二乘法，这个代码写得比较loser，先放这，学完再优化，记录下现在和以后的思考有什么区别12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#!/usr/bin/python# coding=utf-8import numpy as npfrom scipy import statsimport matplotlib.pyplot as pltimport math # This will import math module# 构造训练集# x 特征值# y 实际结果x = np.arange(0, 50, 1)m = len(x)y = x/2 + np.random.randn(m) -5# 终止条件loop_max = 100000 # 最大迭代次数(防止死循环)epsilon = 1e-4 # 精确度alpha = 0.002 # 步长(注意取值过大会导致振荡即不收敛,过小收敛速度变慢)count = 0 # 循环次数finish = 0 # 终止标志theta = np.random.randn(2)# 初始化theta#theta = [0.5,-0.5]temp = np.zeros(2)error = 0while count &lt; loop_max: count+=1 sum = np.zeros(2) for i in range(m): sum[0] = sum[0] + (theta[0] + theta[1] * x[i] - y[i]) temp0 = theta[0] - alpha * sum[0] / m for i in range(m): sum[1] = sum[1]+ (theta[0] + theta[1] * x[i] - y[i]) * x[i] temp1 = theta[1] - alpha * sum[1] / m theta[0] = temp0 theta[1] = temp1 # 判断是否已收敛 if abs((sum[1]+ sum[0] - error)) &lt; epsilon: finish = 1 break else: error = sum[1]+ sum[0] print('intercept = %s slope = %s' % (theta[0], theta[1]))#slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x, y)#print('intercept = %s slope = %s' % (intercept, slope))print('loop count = %d\n' % count, theta)plt.plot(x, y, 'r*')plt.plot(x, theta[1] * x + theta[0], 'g')#plt.plot(x, slope * x + intercept, 'b')plt.show() 偷懒了两天后用normal equation方法实现了，结果和stats.linregress的结果完全一样，注意矩阵需要垂直排列，记录俩函数用来修改矩阵堆叠方式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647vstack()函数 函数原型：vstack(tup) ，参数tup可以是元组，列表，或者numpy数组，返回结果为numpy的数组 作用：在垂直方向把元素堆叠起来&gt;&gt;&gt;import numpy as np&gt;&gt;&gt;a=[[1],[2],[3]]&gt;&gt;&gt;b=[[1],[2],[3]]&gt;&gt;&gt;c=[[1],[2],[3]]&gt;&gt;&gt;d=[[1],[2],[3]]&gt;&gt;&gt;print(np.vstack((a,b,c,d)))[[1] [2] [3] [1] [2] [3] [1] [2] [3] [1] [2] [3]] stack函数原型为：stack(arrays, axis=0) import numpy as npa=[[1,2,3], [4,5,6]]print("列表a如下：")print(a)print("增加一维，新维度的下标为0")c=np.stack(a,axis=0)print(c)print("增加一维，新维度的下标为1")c=np.stack(a,axis=1)print(c)输出：列表a如下：[[1, 2, 3], [4, 5, 6]]增加一维，新维度下标为0[[1 2 3] [4 5 6]]增加一维，新维度下标为1[[1 4] [2 5] [3 6]] Numpy中stack()，hstack()，vstack()函数详解 1234567891011121314151617181920212223242526272829#!/usr/bin/python# coding=utf-8import numpy as npfrom scipy import statsimport matplotlib.pyplot as plt# 构造训练集# x 特征值# y 实际结果x1 = np.arange(0, 50, 1) + np.random.randn(50) -5m = len(x1)x0 = np.full(m, 1.0)y = x1/2 + np.random.randn(m) -5target_data = np.vstack(y) # 将结果矩阵修改为垂直方向x = np.stack((x0, x1), axis=1) # 构建X矩阵#print(x,y)theta = np.dot(np.dot(np.linalg.inv(np.dot(x.T, x)), x.T), target_data)print(theta)slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x1, y)print('intercept = %s slope = %s' % (intercept, slope))"""得到的结果和stats.linregress函数完全一样，猜测这个函数也是如此实现的"""plt.plot(x1, y, '*')plt.plot(x, slope * x + intercept, 'b')plt.plot(x, theta[1] * x + theta[0], 'r')plt.show() 通过学习别人的代码和修改完成了最终版本，要注意步长和终止条件，步长alpha通过多次尝试最后选取适合值123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/usr/bin/python# coding=utf-8import numpy as npfrom scipy import statsimport matplotlib.pyplot as plt# 构造训练集# x 特征值# y 实际结果x1 = np.arange(0, 50, 1) + np.random.randn(50)m = len(x1)x0 = np.full(m, 1.0)x = np.vstack([x0, x1]).Ty = x1/2 + np.random.randn(m) -5# 两种终止条件loop_max = 10000 # 最大迭代次数(防止死循环)epsilon = 1e-4# 初始化权值np.random.seed(0)theta = np.random.randn(2)alpha = 0.002 # 步长(注意取值过大会导致振荡即不收敛,过小收敛速度变慢) 大于0.002会不收敛error = np.zeros(2)count = 0 # 循环次数while count &lt; loop_max: count += 1 delta = np.zeros(2) for i in range(m): delta = delta + (np.dot(theta, x[i]) - y[i]) * x[i]/m theta = theta - alpha * delta # 判断是否已收敛 if np.linalg.norm(theta - error) &lt; epsilon: # np.linalg.norm 求范类：平方和，开方 break else: error = theta print(theta)print(theta,count)slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x1, y)print('intercept = %s slope = %s' % (intercept, slope))plt.plot(x1, y, 'g*')plt.plot(x, theta[1] * x + theta[0], 'r')plt.plot(x, slope * x + intercept, 'b')plt.show()]]></content>
      <categories>
        <category>机器学习代码实现</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习（三）]]></title>
    <url>%2F2018%2F04%2F19%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[特征缩放 （feature scaling）确保不同的特征值在同一个范围内，这样能保证梯度下降能够更快的收敛。 例如：x1是房屋大小，非常的大（0-2000）x2是房间卧室数（1-5）参数适当的缩放，使收敛的更快建议把特征缩放到-1到1的范围，可以偏差，但不能偏差太大，特征值不需要太精确，只是希望梯度下降收敛更快。 学习速率的选取如何选择梯度下降学习速率$\alpha$，可以画出代价函数随迭代步数$J(\theta)$增加的函数曲线，观察曲线来判断梯度下降算法是否收敛，下面这个曲线中，当迭代达到三百时基本已经停止下降，所以这个曲线中三百是最佳的迭代次数一个典型例子来的判断是否收敛，比如代价函数$J(\theta)$已经小于一个ε，比如设置ε为0.001，选择一个阈值来告诉算法已经收敛。 $\alpha$过小会收敛太慢，过大会导致震荡无法收敛，通常会尝试多个α来找到最佳的学习速率，比如从0.001到1，每扩大三倍选取一个alpha来确定学习速率。 多特征值的线性回归问题和前一章一样，对每个参数θ求J的偏导数，然后把它们全部置零，然后求出θ1到θn的值，这样就能求出最小的代价函数的所有θ的值。这是一个非常复杂的微分方程，用线性代数的方法可以快速解决。 Normal Equation构建两个矩阵，矩阵X由x0（全部为1），x1，x2…xn构成，y是结果矩阵，X和y矩阵是以列排的简单的说就是: 通过计算X的转置乘以X的逆乘以X的转置乘以Y来得到θ就是这个公式，这里懒得写为什么了，这个也是最小二乘法的公式。优点是不需要选取学习速率，不需要迭代，但是当特征值大于百万级别求矩阵的逆会非常慢，这时则应该选择梯度下降而不是标准方程。 当特征值存在线性关系时，会导致矩阵不可逆，但是可以通过求伪逆来获取结果刚，对结果影响不大。 Python代码实现 Python实现梯度下降]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习（二）]]></title>
    <url>%2F2018%2F04%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[记得当年背英语单词的书永远都翻在第一页，abandon背了一遍又一遍，还是没有记住，现如今周志华老师的书买了好久没有翻，吴恩达的视频看了一遍又一遍，只能希望这是最后一次看了。 Machine Learning 学习视屏地址 什么是机器学习 不借助特定的程序使电脑学习的科学 监督学习（supervised learning）例如：房价预测（回归问题），肿瘤预测（分类问题）监督学习就是给出一组特征值，同时也给出这组特征所对应的结果。比如通过某一地区房子的面积和卧室数来预测房子的价格。 无监督学习（unsupervised learning）无监督学习则是只给出一些特征值，但是并没有这些特征所对应的结果，通过这些特征值来寻找他们之间的关系，例如聚类问题，把同一事件的新闻划分为一类。 模型表示（Model Representation）符号定义m：样本数量（training examples）x：输入值，又叫特征（input variables/features）y：输出值，又叫目标值（output variables/target variables）（x，y）：训练样本第i个训练样本：$(x^i,y^i)$ 监督学习的工作方式 预测函数的表示$h_\theta(x)=\theta_0+\theta_1x$ 关于$x$单变量的线性回归方程 代价函数（cost function）预测函数$h_\theta(x)$的线性意义：预测函数$h_\theta(x)$是关于$x$的函数,而代价函数是一个关于$(\theta_0,\theta_1)$的函数 $J(\theta_0,\theta_1) = \frac{1}{2m} \sum^m_{i=1}(h_\theta(x^i)-y^i)^2$ 优化目标：$minimize J(\theta_0,\theta_1)$教授讲的很详细，这里记一下自己的见解吧：1预测函数是根据已知特征向量和结果所描述的一个线性方程，根据改变线段的斜率来观察匹配到的特征吻合度达，当预测函数可以匹配到最多特征时则这个预测函数是最优解，如何获取最优解引入了代价函数，所谓代价函数就是预测函数的积分， 梯度下降在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度 梯度下降是迭代法的一种,可以用于求解最小二乘问题(线性和非线性都可以)。在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。 通过梯度下降的方法来寻找代价函数的最优解符号解释 ：= 赋值符，把右边的值赋值给左边 $\alpha$ 学习速度，步长，过小收敛时间过长，过大超过最小值无法收敛 $\theta_0$和$\theta_1$是同时更新的 梯度下降的缺点：靠近极小值时收敛速度减慢。直线搜索时可能会产生一些问题。可能会“之字形”地下降。会产生局部最优解而非全局。 总结这里基本简单的记录了视频前三章的内容，梳理一下知识点。在机器学习中首先需要有样本，也叫训练集，然后是一个机器学习算法，把训练集扔进这个算法中，通过迭代之类的方法计算机会发现其中的规律而给出统一的模型从而做到预测分析。 当训练集既有输入内容又有输出结果，就是监督学习（比如回归问题），当样本里没有结果时是无监督学习（比如聚类问题） 梯度下降就是寻找最佳的预测模型的方式，当我们要建立一个准确的预测模型需要不断改变参数（$\theta_0,\theta_1$）,于是建立一个关于$\theta_0,\theta_1$的方程，这个方程叫代价方程（cost function），其实这个方程就是度量预测函数的结果和实际结果的方差，当方差最小就是最佳$\theta_0,\theta_1$，方法就是计算所有预测函数的结果减实际结果和的平方，方差最小就代表拟合度最佳。可视化观察就向是在一个弯曲的山谷中寻找最低点。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习（一）]]></title>
    <url>%2F2018%2F04%2F10%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这篇博文是对Andrew Ng的 机器学习入门的学习笔记，关于机器学习已经看了一段时间了，现在开始正式总结一下这段时间所学的东西，一边学习一边记录，希望能够更完这个博文。 首先复习数学，哎，学校学的全还给老师了。。。 线性代数 线性代数是数学的一个分支，它的研究对象是向量，向量空间（或称线性空间），线性变换和有限维的线性方程组。向量空间是现代数学的一个重要课题；因而，线性代数被广泛地应用于抽象代数和泛函分析中；通过解析几何，线性代数得以被具体表示。线性代数的理论已被泛化为算子理论。由于科学研究中的非线性模型通常可以被近似为线性模型，使得线性代数被广泛地应用于自然科学和社会科学中。 矩阵定义由m$\times$n个数排列成m行n列的矩阵，简称m$\times$记作： 矩阵加法矩阵的加法满足下列运算律(A，B，C都是同型矩阵)： A + B = B + A (A + B) + C = A + (B + C) 只有行列相同的的矩阵才可以进行加法 矩阵减法 数乘矩阵的加减法和矩阵的数乘合称矩阵的线性运算 转置把矩阵A的行和列互换产生的新矩阵称之为矩阵A的转置矩阵的转置满足一下定律： $$(A^T)^T = A$$ $(\lambda A^T) = \lambda A^T$ $(AB)^T = B^TA^T$ 矩阵乘法两个矩阵能够相乘，当且仅当第一个矩阵A的列数等于第二个矩阵B的行数时才能定义，如果A是$m\times n$的矩阵B是$n\times p$的矩阵，他们的乘积C将是一个$m\times p$的矩阵$C=(C_{ij})$,它的每个元素是： $c_{i,j} = a_{i,1}b_{1,j} + a_{i,2}b_{2,j} + … + a_{i,n}b{n,j} = \sum_{r=1}^n a_{i,r}b_{r,j}$ 记作：C = AB 例如：矩阵的乘法满足以下运算律：结合律，分配律，矩阵乘法不满足交换律。 转置:$(AB)^T=B^TA^T$ 当矩阵A的列数等于矩阵B的行数时，A与B可以相乘。 矩阵C的行数等于矩阵A的行数，C的列数等于B的列数。 乘积C的第m行第n列的元素等于矩阵A的第m行的元素与矩阵B的第n列对应元素乘积之和。 导数对于机器学习不需要理解的太深入，深入的自己也没学懂&gt;_&lt;,大概就是知道且会求偏导，知道斜率的意义就够了，其他部分太复杂就不记录了。。。好后悔当初没有好好学习微积分 概率论先验概率和后验概率： 后验概率是指在得到“结果”的信息后重新修正的概率，是“执果寻因”问题中的”果”。先验概率与后验概率有不可分割的联系，后验概率的计算要以先验概率为基础。事情还没有发生，要求这件事情发生的可能性的大小，是先验概率。事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小，是后验概率。先验概率不是根据有关自然状态的全部资料测定的，而只是利用现有的材料(主要是历史资料)计算的；后验概率使用了有关自然状态更加全面的资料，既有先验概率资料，也有补充资料；先验概率的计算比较简单，没有使用贝叶斯公式；而后验概率的计算，要使用贝叶斯公式，而且在利用样本资料计算逻辑概率时，还要使用理论概率分布，需要更多的数理统计知识。 目前先知道这点就够了，具体以后再补充 数学知识复习完毕 正式开始机器学习]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随便玩玩的玩家]]></title>
    <url>%2F2018%2F04%2F07%2F%E9%9A%8F%E4%BE%BF%E7%8E%A9%E7%8E%A9%E7%9A%84%E7%8E%A9%E5%AE%B6%2F</url>
    <content type="text"><![CDATA[如果从三岁看哥哥玩马戏团算起，游戏玩了也有二十个年头了，但还是个游戏菜鸟，已经古稀之年的斯皮尔伯格却要拍一部头号玩家，这使我怀着浓厚的兴趣走进了影院。 如果说最早看过斯皮尔伯格执导的电影那应该是大白鲨和侏罗纪公园了，但那时我还太小，大概七八岁，并不知道导演什么的，但是大白鲨却给我留下怕水的和鱼的心理阴影，小时候连鱼都不敢抓，对恐龙没有太大的兴趣所以侏罗纪公园没太大印象，到初中时候英语老师给放夺宝奇兵，介绍说是一个多么棒的导演拍的，没记住导演和剧情，只是被电影中的各种探险寻宝的过程深深吸引，然后化身中二少年与小伙伴们一起开始寻宝之旅。对电影的理解还是要到高中时期，一次回家的路上，谈到了电影，好友说他圣城上下载了《拯救大兵瑞恩》超清版的，连弹道都能看到，汤姆汉克斯演的不愧是最好的二战电影，另一位好友说是斯皮尔伯格拍的，吧啦吧啦。。。什么他们在说什么，我看电影都是只记得剧情和中国的演员，而他们能把外国的导演和演员如数家珍，为了不丢人于是回家恶补，结果发现斯皮尔伯格拍的电影大都看过，但却没关注过导演(lll￢ω￢)，我正是从那时起，看电影不再是看看剧情一笑而过了，而是欣赏导演和演员的艺术，看完后会去和好友聊聊不同的见解，这仿佛为我打开了新世界的大门，从此沉迷电影无法自拔了。 周末迎着呼呼的风一大早跑去电影院，途中遇到了因为风大而改变行程的两位好友一起随我到影院，虽然我们仨都是游戏迷，但只有我是影迷，他俩看完并没有什么感想。电影开场介绍了世界观与社会背景，当主角连接绿洲时，差点使我中二之魂爆发喊出了Link Start！，刀剑第一季是我最喜欢的动漫之一，接着闪过我的世界等游戏画面，介绍各种世界时让我想到了星际特工·千星之城，一个场景和女主不错，但故事有点老套的商业电影，这是吕克·贝松唯一让我看的有点瞌睡的电影，不能怪他，只怪那是一个翻拍无数遍的剧本，当时担心斯皮尔伯格会不会陷入这种情况，还好全程无尿点，剧情非常紧凑。 第一关是老司机开车，主角的座驾有些眼熟，后来看别人说了回到未来才想起来，当主角倒着开车时，我回想了下我有没有这样玩过呢？一个晕车的小孩当然也不会喜欢赛车游戏，在游戏厅看别人玩极品飞车的我因为跑不过人家有时会生气的倒着开，反正也追不上了就随便玩玩吧，并不是没有好胜心，而是想你们看我开倒了，输了也只是因为不想玩，呵呵。。在主角倒车到达终点时，我想我也经常不按游戏设计走，会瞎跑一会但都无疾而终，有没有倒着却赢的时候呢，思绪回到了2008年QQ飞车上线时，和小伙伴愉快的玩耍，我属于技术比较菜的，漂移都是360°的，在玩月牙湾的地图时，好友都快领先我一圈了，而我漂移后找不到方向，继续向前开他却迎面向我开来把我撞偏了，说我怎么反着开，我说我也不知到啊，系统也没提示我，反正我也分不清方向就继续开，然后我就第一了，还打破了记录，大家都很惊讶，我们仔细研究下发现了BUG，在过一个弯道后反向开回终点也算完成一圈但路途近了一半，于是我们开始刷这个BUG把自己的名次刷到了全服最高，后来玩家基本都知道了，跑图时会发现一半人在反向开，那时玩的还是很开心的，再后来BUG当然是被修复了，分数也被清零了，正在我想着当年一起开车的场景时，主角已经找到了第二条线索，闪灵是我最想看但最不敢看的恐怖片了，所以至今没有正眼看过，当我们的钢铁少年拿着球遇到了俩小萝莉时，我尽管没看过但我还是知道，要发生恐怖的事情了，当血水涌出的镜头又出现时，我想起了星爷的功夫，那部电影是我第一次知道了闪灵，知道了，一部电影可以向另一部电影致敬，而不能说是抄袭，还好这是一部带有喜剧色彩的电影并没有什么恐怖的镜头，然后剧情发展有些快，第三关IOI是怎么找到的，主角的也没有根据提示获取第三关的情报，不是说好闯过一关才有第二关吗？怎么看IOI都是一直在准备第三关，导演也没有给我们留下思考这个问题的时间，于是开始打雅达利的游戏机，雅达利还是通过敖厂长的介绍才知道的，当年雅达利就是因为开发了斯皮尔伯格拍的ET外星人游戏而走向衰落的，我以为导演要打这部游戏，不是更有趣，最后主角随便玩玩找到了彩蛋，（哈哈，我可不会在我写的代码里放上我的名字，那可就要被下个接手的人骂死了）又经过了最后的考验，当主角问哈利迪死了吗的时候又让我想到了刀剑的茅场晶彦，结局皆大欢喜，住居成功的出任CEO，迎娶白富美，走上人生巅峰。 看完电影让我明白了，游戏还是随便玩玩就好，游戏无法代替现实生活，随着年龄的增长游戏中的胜负显得没那么重要了，能和好朋友开开黑就很美了，还有一个从小就明白的道理，游戏还是要有队友才最好玩。]]></content>
      <categories>
        <category>影评</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[GitHub学习笔记]]></title>
    <url>%2F2018%2F04%2F02%2FGit%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Git是一个开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。Git 是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。Git 与常用的版本控制工具 CVS, Subversion 等不同，它采用了分布式版本库的方式，不必服务器端软件支持。Git不同于SVN之前写过 SVN使用手册 ，搭建过SVN服务，现在复习一下Git 配置Git首先创建SSH key： 1ssh-keygen -t rsa -C "email@email.com" 在~/目录下生成.ssh文件夹，打开id_rsa.pub，复制里面的key。在github上，进入 Account Settings（账户配置），左边选择SSH Keys，Add SSHKey,title随便填，粘贴在电脑上生成的key。验证是否成功1ssh -T git@github.com 如果是第一次的会提示是否continue，输入yes就会看到：You’ve successfully authenticated, but GitHub does not provide shell access 。这就表示已成功连上github。接下来我们要做的就是把本地仓库传到github上去，在此之前还需要设置username和email，因为github每次commit都会记录他们。12git config --global user.name "your name"git config --global user.eamil "your_eamail@email.com" 上传远程仓库，需要添加远程地址，仓库需要在github上先建立好1git remote add origin git@github.com:yourName/yourRepo.git 检出仓库克隆本地仓库： git clone /path/to/repository 克隆远程仓库： git clone username@host:/path/to/repository 推送流程123456789101112131415$mkdir test #创建一个测试目录$cd test/ #进入test目录$echo "#git test file" &gt;&gt; README.md #给readme文件写入内容$lsREADME.md$git init #初始化init$git add README.md #添加文件[master (root-commit) 0205aab] 添加 README.md 文件 1 file changed, 1 insertion(+) create mode 100644 README.md$git commit -m "add readme.md file" #提交备注信息#提交到github$ git remote add origin git@github.com:usename/Repositoryname.git$ git push -u origin master 常用命令1234567891011121314151617181920212223242526272829303132333435363738394041424344#初始化git$git init#拷贝一个仓库到本地$git clone [url]#添加文件到缓存$git add#查看当前项目状态$git status -s#A 加入缓存 M有改动 #查看修改执行 git diff 来查看执行 git status 的结果的详细信息。git diff 命令显示已写入缓存与已修改但尚未写入缓存的改动的区别。git diff 有两个主要的应用场景。尚未缓存的改动：git diff查看已缓存的改动： git diff --cached查看已缓存的与未缓存的所有改动：git diff HEAD显示摘要而非整个 diff：git diff --stat#添加到仓库$git commit -m "描述"#取消缓存内容$git reset HEAD#删除文件如果只是简单地从工作目录中手工删除文件，运行 git status 时就会在 Changes not staged for commit 的提示。要从 Git 中移除某个文件，就必须要从已跟踪文件清单中移除，然后提交。可以用以下命令完成此项工作$git rm &lt;file&gt;如果删除之前修改过并且已经放到暂存区域的话，则必须要用强制删除选项 -f$git rm -f &lt;file&gt;如果把文件从暂存区域移除，但仍然希望保留在当前工作目录中，换句话说，仅是从跟踪清单中删除，使用 --cached 选项即可$git rm --cached &lt;file&gt;#重命名$git mv oldname newname 查看当前远程仓库：12345$ git remoteorigin$ git remote -vorigin git@github.com:Voidmort/blogs.git (fetch)origin git@github.com:Voidmort/blogs.git (push) Git 有两个命令用来提取远程仓库的更新。1、从远程仓库下载新分支与数据： git fetch 该命令执行完后需要执行git merge 远程分支到你所在的分支。2、从远端仓库提取数据并尝试合并到当前分支： git merge 该命令就是在执行 git fetch 之后紧接着执行 git merge 远程分支到你所在的任意分支。 删除1234567891011121314151617181920$ git remote -vorigin git@github.com:Voidmort/blogs.git (fetch)origin git@github.com:Voidmort/blogs.git (push)#添加仓库2$ git remote add origin2 git@github.com:Voidmort/blogs.git$ git remote -vorigin git@github.com:Voidmort/blogs.git (fetch)origin git@github.com:Voidmort/blogs.git (push)origin2 git@github.com:Voidmort/blogs.git (fetch)origin2 git@github.com:Voidmort/blogs.git (push)#删除仓库2$ git remote rm origin2$ git remote -vorigin git@github.com:Voidmort/blogs.git (fetch)origin git@github.com:Voidmort/blogs.git (push)]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客搭建笔记]]></title>
    <url>%2F2018%2F04%2F01%2F%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[搭建环境这次尝试在Ubuntu环境下搭建github+hexo博客软件需要以下四个: Ubuntu 16.04 Node.js Hexo GitHub Ubuntu安装略 Node环境安装Hexo博客系统是静态网页的形似，依赖Node.js，简单的说 Node.js 就是运行在服务端的 JavaScript。Node.js 是一个基于Chrome JavaScript 运行时建立的一个平台。（其实我并不懂这个玩意）只是Hexo需要使用npm安装，npm是依托于node的安装软件管理系统 方法一Windowns下直接下载安装，Ubuntu 我刚开始使用了apt-get install结果装完后版本过低使后面的搭建过程接连出错，直接使用编译好的文件安装，首先官网下载最新tar包然后链接为全局12345tar xf node-v5.10.1-linux-x64.tar.gz -C /usr/local/cd /usr/local/mv node-v5.10.1-linux-x64/ nodejsln -s /usr/local/nodejs/bin/node /usr/local/binln -s /usr/local/nodejs/bin/npm /usr/local/bin 方法二源码安装参考了菜鸟教程 123456789101112131415161718Node.js 源码安装以下部分我们将介绍在Ubuntu Linux下安装 Node.js 。 其他的Linux系统，如Centos等类似如下安装步骤。在 Github 上获取 Node.js 源码：$ sudo git clone https://github.com/nodejs/node.gitCloning into 'node'...修改目录权限：$ sudo chmod -R 755 node使用 ./configure 创建编译文件，并按照：$ cd node$ sudo ./configure$ sudo make$ sudo make install查看 node 版本： $ node --versionv10.0.0-pre$npm -v5.6.0 源码编译的时间比我想象中的长啊 方法三nodesource123# Using Ubuntucurl -sL https://deb.nodesource.com/setup_11.x | sudo -E bash -sudo apt-get install -y nodejs 注册一个GitHub账号关于Git的学习使看了廖雪峰老师的博客，好久之前看的都忘了，哎https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000 Github账户注册和新建项目，项目必须要遵守格式：账户名.github.io，不然接下来会有很多麻烦。并且需要勾选Initialize this repository with a README 在建好的项目右侧有个settings按钮，点击它，向下拉到GitHub Pages，你会看到那边有个网址，访问它，你将会惊奇的发现该项目已经被部署到网络上，能够通过外网来访问它。 Hexo安装找个合适的地方 sudo npm npm install hexo-cli -g 12国内上npm很慢或失败，尝试淘宝源的cnpmhttp://npm.taobao.org/ 输入hexo -v检查hexo是否安装成功 输入hexo init,初始化项目，npm国外的源有点慢啊，可以修改使用淘宝源 输入npm install， 安装所有组件 输入hexo g，创建静态网页 输入hexo s，开启服务器，访问http://localhost:4000 中途出现了好多次error，除了要加sudo，还有一些奇怪的错误，但是重复了几遍就只剩warn了。。。 总结 使用最新的软件和节点能减少出错几率 将Hexo与Github page联系起来，设置Git的user name和email（如果是第一次的话） git方面另写一篇博客 测试： 在终端 ssh -T git@github.com Hi Voidmort! You&apos;ve successfully authenticated, but GitHub does not provide shell access. 成功！ 配置Deployment，在其文件夹中，找到_config.yml文件，修改repo值（在末尾）123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: git@github.com:Voidmort/Voidmort.github.io.git branch: master repo值是github项目里的ssh（右下角） 部署到git之前要装一个扩展： npm install hexo-deployer-git –save hexo指令 123456hexo n "我的博客" == hexo new "我的博客" #新建文章hexo p == hexo publishhexo g == hexo generate#生成hexo s == hexo server #启动服务预览hexo d == hexo deploy#部署hexo clean 域名绑定我购买了阿里的域名，首先ping voidmort.github.io,查看IP地址然后直接在阿里域名管理里点新手引导写上IP地址，然后使用新域名登陆，发现上不去。。。在GitHub setting中找到Custom domain 写上刚购买的域名 OK！ 博客搭建完成 Next 主题晋级主题地址：theme-next.iissnan.com 搜索服务微搜索 由 lzlun129 贡献 npm install swig-templates TBD Local Search 由 flashlab 贡献 添加百度/谷歌/本地 自定义站点内容搜索 安装 hexo-generator-searchdb，在站点的根目录下执行以下命令： $ npm install hexo-generator-searchdb –save Problem$ hexo dERROR Deployer not found: git npm install –save hexo-deployer-git search: path: search.xml field: post format: html limit: 10000 编辑 主题配置文件，启用本地搜索功能： Local searchlocal_search: enable: true RSS： 需要先安装 hexo-generator-feed 插件。https://github.com/hexojs/hexo-generator-feed Live2D：https://www.npmjs.com/package/hexo-helper-live2d]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>hexo</tag>
        <tag>github</tag>
        <tag>node.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown学习笔记]]></title>
    <url>%2F2018%2F03%2F30%2FMarkdown%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[初次写博客，从搭建github hexo走了不少坑，稍后记录搭建博客过程，首先学习使用markdown教程来源：http://www.markdown.cn/markdown是一个HTML的转换工具，是HTML的书写格式 标题语法类Atx形式在首行插入1个到6个#，对应到标题1到6一定要# + 空格 + 标题加空格！！！！！！123# 一级标题## 二级标题###### 六级标题 区块引用段首使用 &gt; 作为引用，引用部分也支持markdown语法1234&gt;hello word!&gt;# 标题一&gt;代码提示：&gt; return Null; hello word! 标题一代码提示： return Null; 列表markdown支持有序和无序列表无序列表标记符号可以是“* + -”，有序符号是数字加英文点“1. ” 123456789* red* green* blue+ red+ green- red1.good2.name3.learn red green blue red green red good name learn 代码区块要在 Markdown 中建立代码区块很简单，只要简单地缩进 4 个空格或是 1 个制表符就可以，例如，下面的输入：这是一个普通段落： 123456789 //这是一个代码区块。#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;int main()&#123; print("Hello word!\n"); return 0;&#125; 分割线三个连续的 - _ ** 链接链接文字用[方括号]标记要建立一个内行式链接，只要在方括号后面紧接着元括号并插入网址链接 This is [an example](http://example.com/ &quot;Title&quot;) inline link. [This link](http://example.net/) has no title attribute. 百度一下This is an example inline link.This link has no title attribute. 代码标记段行内代码，用反引号包起来（’） use the &apos;printf()&apos; function Use the printf() function. 图片和链接一样12![Alt text](/path/to/img.jpg)![Alt text](/path/to/img.jpg "Optional title") Alt text 详细叙述如下： 一个惊叹号 !接着一个方括号，里面放上图片的替代文字接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上 选择性的 ‘title’ 文字。 其他反斜杠在一下符号前面加入反斜杠来插入普通符号： 123456789101112\ 反斜线` 反引号* 星号_ 底线&#123;&#125; 花括号[] 方括号() 括弧# 井字号+ 加号- 减号. 英文句点! 惊叹号 自动链接处理短链接用&lt;&gt;括住能够自动转换成链接http://www.baidu.com&#x77;&#x78;&#x6a;&#x35;&#54;&#53;&#x38;&#64;&#x68;&#x6f;&#x74;&#109;&#x61;&#x69;&#46;&#99;&#111;&#109; 编辑软件windows平台 Markdownpad 激活： 12345678注册信息邮箱地址：Soar360@live.com授权密钥：GBPduHjWfJU1mZqcPM3BikjYKF6xKhlKIys3i1MU2eJHqWGImDHzWdD6xhMNLGVpbP2M5SN6bnxn2kSE8qHqNY5QaaRxmO3YSMHxlv2EYpjdwLcPwfeTG7kUdnhKE0vVy4RidP6Y2wZ0q74f47fzsZo45JE2hfQBFi2O9Jldjp1mW8HUpTtLA2a5/sQytXJUQl/QKO0jUQY4pa5CCx20sV1ClOTZtAGngSOJtIOFXK599sBr5aIEFyH0K7H4BoNMiiDMnxt1rD8Vb/ikJdhGMMQr0R4B+L3nWU97eaVPTRKfWGDE8/eAgKzpGwrQQoDh+nzX1xoVQ8NAuH+s4UcSeQ== 在windows 10 系统下，windows10 MarkdownPad html会产生一个 渲染错误 awesomium（ This view has crashed ），此时就需要下载一个 HTML UI ENGINE（awesomium_v1.6.6_sdk_win）去解决该错误，该组件的下载地址： http://markdownpad.com/download/awesomium_v1.6.6_sdk_win.exe]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转】从机器学习谈起]]></title>
    <url>%2F2018%2F03%2F21%2F%E4%BB%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%B0%88%E8%B5%B7%2F</url>
    <content type="text"><![CDATA[从决定向AI方向转已经过了半年了，书买了好多，还没有拆封，四月了，这次是必须下决心了，从博客搭建开始，记录学习过程，首先从这里开始，是最初看了这篇博客，开始了解机器学习，感谢博主的帮助 博客原地址：https://www.cnblogs.com/subconscious/p/4107357.html#first 作者：计算机的潜意识 从机器学习谈起 在本篇文章中，我将对机器学习做个概要的介绍。本文的目的是能让即便完全不了解机器学习的人也能了解机器学习，并且上手相关的实践。这篇文档也算是EasyPR开发的番外篇，从这里开始，必须对机器学习了解才能进一步介绍EasyPR的内核。当然，本文也面对一般读者，不会对阅读有相关的前提要求。 在进入正题前，我想读者心中可能会有一个疑惑：机器学习有什么重要性，以至于要阅读完这篇非常长的文章呢？ 我并不直接回答这个问题前。相反，我想请大家看两张图，下图是图一：&nbsp;图1 机器学习界的执牛耳者与互联网界的大鳄的联姻 这幅图上上的三人是当今机器学习界的执牛耳者。中间的是Geoffrey Hinton, 加拿大多伦多大学的教授，如今被聘为&ldquo;Google大脑&rdquo;的负责人。右边的是Yann LeCun, 纽约大学教授，如今是Facebook人工智能实验室的主任。而左边的大家都很熟悉，Andrew Ng，中文名吴恩达，斯坦福大学副教授，如今也是&ldquo;百度大脑&rdquo;的负责人与百度首席科学家。这三位都是目前业界炙手可热的大牛，被互联网界大鳄求贤若渴的聘请，足见他们的重要性。而他们的研究方向，则全部都是机器学习的子类–深度学习。 下图是图二：图2 语音助手产品 这幅图上描述的是什么？Windows Phone上的语音助手Cortana，名字来源于《光环》中士官长的助手。相比其他竞争对手，微软很迟才推出这个服务。Cortana背后的核心技术是什么，为什么它能够听懂人的语音？事实上，这个技术正是机器学习。机器学习是所有语音助手产品(包括Apple的siri与Google的Now)能够跟人交互的关键技术。 通过上面两图，我相信大家可以看出机器学习似乎是一个很重要的，有很多未知特性的技术。学习它似乎是一件有趣的任务。实际上，学习机器学习不仅可以帮助我们了解互联网界最新的趋势，同时也可以知道伴随我们的便利服务的实现技术。 机器学习是什么，为什么它能有这么大的魔力，这些问题正是本文要回答的。同时，本文叫做&ldquo;从机器学习谈起&rdquo;，因此会以漫谈的形式介绍跟机器学习相关的所有内容，包括学科(如数据挖掘、计算机视觉等)，算法(神经网络，svm)等等。本文的主要目录如下： 1.一个故事说明什么是机器学习 2.机器学习的定义 3.机器学习的范围 4.机器学习的方法 5.机器学习的应用–大数据 6.机器学习的子类–深度学习 7.机器学习的父类–人工智能 8.机器学习的思考–计算机的潜意识 9.总结 10.后记1.一个故事说明什么是机器学习 机器学习这个词是让人疑惑的，首先它是英文名称Machine Learning(简称ML)的直译，在计算界Machine一般指计算机。这个名字使用了拟人的手法，说明了这门技术是让机器&ldquo;学习&rdquo;的技术。但是计算机是死的，怎么可能像人类一样&ldquo;学习&rdquo;呢？ 传统上如果我们想让计算机工作，我们给它一串指令，然后它遵照这个指令一步步执行下去。有因有果，非常明确。但这样的方式在机器学习中行不通。机器学习根本不接受你输入的指令，相反，它接受你输入的数据! 也就是说，机器学习是一种让计算机利用数据而不是指令来进行各种工作的方法。这听起来非常不可思议，但结果上却是非常可行的。&ldquo;统计&rdquo;思想将在你学习&ldquo;机器学习&rdquo;相关理念时无时无刻不伴随，相关而不是因果的概念将是支撑机器学习能够工作的核心概念。你会颠覆对你以前所有程序中建立的因果无处不在的根本理念。 下面我通过一个故事来简单地阐明什么是机器学习。这个故事比较适合用在知乎上作为一个概念的阐明。在这里，这个故事没有展开，但相关内容与核心是存在的。如果你想简单的了解一下什么是机器学习，那么看完这个故事就足够了。如果你想了解机器学习的更多知识以及与它关联紧密的当代技术，那么请你继续往下看，后面有更多的丰富的内容。 这个例子来源于我真实的生活经验，我在思考这个问题的时候突然发现它的过程可以被扩充化为一个完整的机器学习的过程，因此我决定使用这个例子作为所有介绍的开始。这个故事称为&ldquo;等人问题&rdquo;。 我相信大家都有跟别人相约，然后等人的经历。现实中不是每个人都那么守时的，于是当你碰到一些爱迟到的人，你的时间不可避免的要浪费。我就碰到过这样的一个例子。 对我的一个朋友小Y而言，他就不是那么守时，最常见的表现是他经常迟到。当有一次我跟他约好3点钟在某个麦当劳见面时，在我出门的那一刻我突然想到一个问题：我现在出发合适么？我会不会又到了地点后，花上30分钟去等他？我决定采取一个策略解决这个问题。 要想解决这个问题，有好几种方法。第一种方法是采用知识：我搜寻能够解决这个问题的知识。但很遗憾，没有人会把如何等人这个问题作为知识传授，因此我不可能找到已有的知识能够解决这个问题。第二种方法是问他人：我去询问他人获得解决这个问题的能力。但是同样的，这个问题没有人能够解答，因为可能没人碰上跟我一样的情况。第三种方法是准则法：我问自己的内心，我有否设立过什么准则去面对这个问题？例如，无论别人如何，我都会守时到达。但我不是个死板的人，我没有设立过这样的规则。 事实上，我相信有种方法比以上三种都合适。我把过往跟小Y相约的经历在脑海中重现一下，看看跟他相约的次数中，迟到占了多大的比例。而我利用这来预测他这次迟到的可能性。如果这个值超出了我心里的某个界限，那我选择等一会再出发。假设我跟小Y约过5次，他迟到的次数是1次，那么他按时到的比例为80%，我心中的阈值为70%，我认为这次小Y应该不会迟到，因此我按时出门。如果小Y在5次迟到的次数中占了4次，也就是他按时到达的比例为20%，由于这个值低于我的阈值，因此我选择推迟出门的时间。这个方法从它的利用层面来看，又称为经验法。在经验法的思考过程中，我事实上利用了以往所有相约的数据。因此也可以称之为依据数据做的判断。 依据数据所做的判断跟机器学习的思想根本上是一致的。 刚才的思考过程我只考虑&ldquo;频次&rdquo;这种属性。在真实的机器学习中，这可能都不算是一个应用。一般的机器学习模型至少考虑两个量：一个是因变量，也就是我们希望预测的结果，在这个例子里就是小Y迟到与否的判断。另一个是自变量，也就是用来预测小Y是否迟到的量。假设我把时间作为自变量，譬如我发现小Y所有迟到的日子基本都是星期五，而在非星期五情况下他基本不迟到。于是我可以建立一个模型，来模拟小Y迟到与否跟日子是否是星期五的概率。见下图：&nbsp;图3 决策树模型 这样的图就是一个最简单的机器学习模型，称之为决策树。 当我们考虑的自变量只有一个时，情况较为简单。如果把我们的自变量再增加一个。例如小Y迟到的部分情况时是在他开车过来的时候(你可以理解为他开车水平较臭，或者路较堵)。于是我可以关联考虑这些信息。建立一个更复杂的模型，这个模型包含两个自变量与一个因变量。 再更复杂一点，小Y的迟到跟天气也有一定的原因，例如下雨的时候，这时候我需要考虑三个自变量。 如果我希望能够预测小Y迟到的具体时间，我可以把他每次迟到的时间跟雨量的大小以及前面考虑的自变量统一建立一个模型。于是我的模型可以预测值，例如他大概会迟到几分钟。这样可以帮助我更好的规划我出门的时间。在这样的情况下，决策树就无法很好地支撑了，因为决策树只能预测离散值。我们可以用节2所介绍的线型回归方法建立这个模型。 如果我把这些建立模型的过程交给电脑。比如把所有的自变量和因变量输入，然后让计算机帮我生成一个模型，同时让计算机根据我当前的情况，给出我是否需要迟出门，需要迟几分钟的建议。那么计算机执行这些辅助决策的过程就是机器学习的过程。 机器学习方法是计算机利用已有的数据(经验)，得出了某种模型(迟到的规律)，并利用此模型预测未来(是否迟到)的一种方法。 通过上面的分析，可以看出机器学习与人类思考的经验过程是类似的，不过它能考虑更多的情况，执行更加复杂的计算。事实上，机器学习的一个主要目的就是把人类思考归纳经验的过程转化为计算机通过对数据的处理计算得出模型的过程。经过计算机得出的模型能够以近似于人的方式解决很多灵活复杂的问题。 下面，我会开始对机器学习的正式介绍，包括定义、范围，方法、应用等等，都有所包含。&nbsp;2.机器学习的定义 从广义上来说，机器学习是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法。但从实践的意义上来说，机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。 让我们具体看一个例子。图4 房价的例子 拿国民话题的房子来说。现在我手里有一栋房子需要售卖，我应该给它标上多大的价格？房子的面积是100平方米，价格是100万，120万，还是140万？ 很显然，我希望获得房价与面积的某种规律。那么我该如何获得这个规律？用报纸上的房价平均数据么？还是参考别人面积相似的？无论哪种，似乎都并不是太靠谱。 我现在希望获得一个合理的，并且能够最大程度的反映面积与房价关系的规律。于是我调查了周边与我房型类似的一些房子，获得一组数据。这组数据中包含了大大小小房子的面积与价格，如果我能从这组数据中找出面积与价格的规律，那么我就可以得出房子的价格。 对规律的寻找很简单，拟合出一条直线，让它&ldquo;穿过&rdquo;所有的点，并且与各个点的距离尽可能的小。 通过这条直线，我获得了一个能够最佳反映房价与面积规律的规律。这条直线同时也是一个下式所表明的函数： 房价 = 面积 a + b 上述中的a、b都是直线的参数。获得这些参数以后，我就可以计算出房子的价格。 假设a = 0.75,b = 50，则房价 = 100 0.75 + 50 = 125万。这个结果与我前面所列的100万，120万，140万都不一样。由于这条直线综合考虑了大部分的情况，因此从&ldquo;统计&rdquo;意义上来说，这是一个最合理的预测。 在求解过程中透露出了两个信息： 1.房价模型是根据拟合的函数类型决定的。如果是直线，那么拟合出的就是直线方程。如果是其他类型的线，例如抛物线，那么拟合出的就是抛物线方程。机器学习有众多算法，一些强力算法可以拟合出复杂的非线性模型，用来反映一些不是直线所能表达的情况。 2.如果我的数据越多，我的模型就越能够考虑到越多的情况，由此对于新情况的预测效果可能就越好。这是机器学习界&ldquo;数据为王&rdquo;思想的一个体现。一般来说(不是绝对)，数据越多，最后机器学习生成的模型预测的效果越好。 通过我拟合直线的过程，我们可以对机器学习过程做一个完整的回顾。首先，我们需要在计算机中存储历史的数据。接着，我们将这些 数据通过机器学习算法进行处理，这个过程在机器学习中叫做&ldquo;训练&rdquo;，处理的结果可以被我们用来对新的数据进行预测，这个结果一般称之为&ldquo;模型&rdquo;。对新数据 的预测过程在机器学习中叫做&ldquo;预测&rdquo;。&ldquo;训练&rdquo;与&ldquo;预测&rdquo;是机器学习的两个过程，&ldquo;模型&rdquo;则是过程的中间输出结果，&ldquo;训练&rdquo;产生&ldquo;模型&rdquo;，&ldquo;模型&rdquo;指导 &ldquo;预测&rdquo;。 让我们把机器学习的过程与人类对历史经验归纳的过程做个比对。图5 机器学习与人类思考的类比 人类在成长、生活过程中积累了很多的历史与经验。人类定期地对这些经验进行&ldquo;归纳&rdquo;，获得了生活的&ldquo;规律&rdquo;。当人类遇到未知的问题或者需要对未来进行&ldquo;推测&rdquo;的时候，人类使用这些&ldquo;规律&rdquo;，对未知问题与未来进行&ldquo;推测&rdquo;，从而指导自己的生活和工作。 机器学习中的&ldquo;训练&rdquo;与&ldquo;预测&rdquo;过程可以对应到人类的&ldquo;归纳&rdquo;和&ldquo;推测&rdquo;过程。通过这样的对应，我们可以发现，机器学习的思想并不复杂，仅仅是对人类在生活中学习成长的一个模拟。由于机器学习不是基于编程形成的结果，因此它的处理过程不是因果的逻辑，而是通过归纳思想得出的相关性结论。&nbsp; 这也可以联想到人类为什么要学习历史，历史实际上是人类过往经验的总结。有句话说得很好，&ldquo;历史往往不一样，但历史总是惊人的相似&rdquo;。通过学习历史，我们从历史中归纳出人生与国家的规律，从而指导我们的下一步工作，这是具有莫大价值的。当代一些人忽视了历史的本来价值，而是把其作为一种宣扬功绩的手段，这其实是对历史真实价值的一种误用。 3.机器学习的范围 上文虽然说明了机器学习是什么，但是并没有给出机器学习的范围。 其实，机器学习跟模式识别，统计学习，数据挖掘，计算机视觉，语音识别，自然语言处理等领域有着很深的联系。 从范围上来说，机器学习跟模式识别，统计学习，数据挖掘是类似的，同时，机器学习与其他领域的处理技术的结合，形成了计算机视觉、语音识别、自然语言处理等交叉学科。因此，一般说数据挖掘时，可以等同于说机器学习。同时，我们平常所说的机器学习应用，应该是通用的，不仅仅局限在结构化数据，还有图像，音频等应用。 在这节对机器学习这些相关领域的介绍有助于我们理清机器学习的应用场景与研究范围，更好的理解后面的算法与应用层次。 下图是机器学习所牵扯的一些相关范围的学科与研究领域。图6 机器学习与相关学科 模式识别 模式识别=机器学习。两者的主要区别在于前者是从工业界发展起来的概念，后者则主要源自计算机学科。在著名的《Pattern Recognition And Machine Learning》这本书中，Christopher M. Bishop在开头是这样说的&ldquo;模式识别源自工业界，而机器学习来自于计算机学科。不过，它们中的活动可以被视为同一个领域的两个方面，同时在过去的10年间，它们都有了长足的发展&rdquo;。 数据挖掘 数据挖掘=机器学习+数据库。这几年数据挖掘的概念实在是太耳熟能详。几乎等同于炒作。但凡说数据挖掘都会吹嘘数据挖掘如何如何，例如从数据中挖出金子，以及将废弃的数据转化为价值等等。但是，我尽管可能会挖出金子，但我也可能挖的是&ldquo;石头&rdquo;啊。这个说法的意思是，数据挖掘仅仅是一种思考方式，告诉我们应该尝试从数据中挖掘出知识，但不是每个数据都能挖掘出金子的，所以不要神话它。一个系统绝对不会因为上了一个数据挖掘模块就变得无所不能(这是IBM最喜欢吹嘘的)，恰恰相反，一个拥有数据挖掘思维的人员才是关键，而且他还必须对数据有深刻的认识，这样才可能从数据中导出模式指引业务的改善。大部分数据挖掘中的算法是机器学习的算法在数据库中的优化。 统计学习 统计学习近似等于机器学习。统计学习是个与机器学习高度重叠的学科。因为机器学习中的大多数方法来自统计学，甚至可以认为，统计学的发展促进机器学习的繁荣昌盛。例如著名的支持向量机算法，就是源自统计学科。但是在某种程度上两者是有分别的，这个分别在于：统计学习者重点关注的是统计模型的发展与优化，偏数学，而机器学习者更关注的是能够解决问题，偏实践，因此机器学习研究者会重点研究学习算法在计算机上执行的效率与准确性的提升。 计算机视觉 计算机视觉=图像处理+机器学习。图像处理技术用于将图像处理为适合进入机器学习模型中的输入，机器学习则负责从图像中识别出相关的模式。计算机视觉相关的应用非常的多，例如百度识图、手写字符识别、车牌识别等等应用。这个领域是应用前景非常火热的，同时也是研究的热门方向。随着机器学习的新领域深度学习的发展，大大促进了计算机图像识别的效果，因此未来计算机视觉界的发展前景不可估量。 语音识别 语音识别=语音处理+机器学习。语音识别就是音频处理技术与机器学习的结合。语音识别技术一般不会单独使用，一般会结合自然语言处理的相关技术。目前的相关应用有苹果的语音助手siri等。 自然语言处理 自然语言处理=文本处理+机器学习。自然语言处理技术主要是让机器理解人类的语言的一门领域。在自然语言处理技术中，大量使用了编译原理相关的技术，例如词法分析，语法分析等等，除此之外，在理解这个层面，则使用了语义理解，机器学习等技术。作为唯一由人类自身创造的符号，自然语言处理一直是机器学习界不断研究的方向。按照百度机器学习专家余凯的说法&ldquo;听与看，说白了就是阿猫和阿狗都会的，而只有语言才是人类独有的&rdquo;。如何利用机器学习技术进行自然语言的的深度理解，一直是工业和学术界关注的焦点。 可以看出机器学习在众多领域的外延和应用。机器学习技术的发展促使了很多智能领域的进步，改善着我们的生活。&nbsp;4.机器学习的方法 通过上节的介绍我们知晓了机器学习的大致范围，那么机器学习里面究竟有多少经典的算法呢？在这个部分我会简要介绍一下机器学习中的经典代表方法。这部分介绍的重点是这些方法内涵的思想，数学与实践细节不会在这讨论。 1、回归算法 在大部分机器学习课程中，回归算法都是介绍的第一个算法。原因有两个：一.回归算法比较简单，介绍它可以让人平滑地从统计学迁移到机器学习中。二.回归算法是后面若干强大算法的基石，如果不理解回归算法，无法学习那些强大的算法。回归算法有两个重要的子类：即线性回归和逻辑回归。 线性回归就是我们前面说过的房价求解问题。如何拟合出一条直线最佳匹配我所有的数据？一般使用&ldquo;最小二乘法&rdquo;来求解。&ldquo;最小二乘法&rdquo;的思想是这样的，假设我们拟合出的直线代表数据的真实值，而观测到的数据代表拥有误差的值。为了尽可能减小误差的影响，需要求解一条直线使所有误差的平方和最小。最小二乘法将最优问题转化为求函数极值问题。函数极值在数学上我们一般会采用求导数为0的方法。但这种做法并不适合计算机，可能求解不出来，也可能计算量太大。 计算机科学界专门有一个学科叫&ldquo;数值计算&rdquo;，专门用来提升计算机进行各类计算时的准确性和效率问题。例如，著名的&ldquo;梯度下降&rdquo;以及&ldquo;牛顿法&rdquo;就是数值计算中的经典算法，也非常适合来处理求解函数极值的问题。梯度下降法是解决回归模型中最简单且有效的方法之一。从严格意义上来说，由于后文中的神经网络和推荐算法中都有线性回归的因子，因此梯度下降法在后面的算法实现中也有应用。 逻辑回归是一种与线性回归非常类似的算法，但是，从本质上讲，线型回归处理的问题类型与逻辑回归不一致。线性回归处理的是数值问题，也就是最后预测出的结果是数字，例如房价。而逻辑回归属于分类算法，也就是说，逻辑回归预测结果是离散的分类，例如判断这封邮件是否是垃圾邮件，以及用户是否会点击此广告等等。 实现方面的话，逻辑回归只是对对线性回归的计算结果加上了一个Sigmoid函数，将数值结果转化为了0到1之间的概率(Sigmoid函数的图像一般来说并不直观，你只需要理解对数值越大，函数越逼近1，数值越小，函数越逼近0)，接着我们根据这个概率可以做预测，例如概率大于0.5，则这封邮件就是垃圾邮件，或者肿瘤是否是恶性的等等。从直观上来说，逻辑回归是画出了一条分类线，见下图。 图7 逻辑回归的直观解释 假设我们有一组肿瘤患者的数据，这些患者的肿瘤中有些是良性的(图中的蓝色点)，有些是恶性的(图中的红色点)。这里肿瘤的红蓝色可以被称作数据的&ldquo;标签&rdquo;。同时每个数据包括两个&ldquo;特征&rdquo;：患者的年龄与肿瘤的大小。我们将这两个特征与标签映射到这个二维空间上，形成了我上图的数据。 当我有一个绿色的点时，我该判断这个肿瘤是恶性的还是良性的呢？根据红蓝点我们训练出了一个逻辑回归模型，也就是图中的分类线。这时，根据绿点出现在分类线的左侧，因此我们判断它的标签应该是红色，也就是说属于恶性肿瘤。 逻辑回归算法划出的分类线基本都是线性的(也有划出非线性分类线的逻辑回归，不过那样的模型在处理数据量较大的时候效率会很低)，这意味着当两类之间的界线不是线性时，逻辑回归的表达能力就不足。下面的两个算法是机器学习界最强大且重要的算法，都可以拟合出非线性的分类线。 2、神经网络 神经网络(也称之为人工神经网络，ANN)算法是80年代机器学习界非常流行的算法，不过在90年代中途衰落。现在，携着&ldquo;深度学习&rdquo;之势，神经网络重装归来，重新成为最强大的机器学习算法之一。 神经网络的诞生起源于对大脑工作机理的研究。早期生物界学者们使用神经网络来模拟大脑。机器学习的学者们使用神经网络进行机器学习的实验，发现在视觉与语音的识别上效果都相当好。在BP算法(加速神经网络训练过程的数值算法)诞生以后，神经网络的发展进入了一个热潮。BP算法的发明人之一是前面介绍的机器学习大牛Geoffrey Hinton(图1中的中间者)。 具体说来，神经网络的学习机理是什么？简单来说，就是分解与整合。在著名的Hubel-Wiesel试验中，学者们研究猫的视觉分析机理是这样的。 &nbsp;图8 Hubel-Wiesel试验与大脑视觉机理 比方说，一个正方形，分解为四个折线进入视觉处理的下一层中。四个神经元分别处理一个折线。每个折线再继续被分解为两条直线，每条直线再被分解为黑白两个面。于是，一个复杂的图像变成了大量的细节进入神经元，神经元处理以后再进行整合，最后得出了看到的是正方形的结论。这就是大脑视觉识别的机理，也是神经网络工作的机理。 让我们看一个简单的神经网络的逻辑架构。在这个网络中，分成输入层，隐藏层，和输出层。输入层负责接收信号，隐藏层负责对数据的分解与处理，最后的结果被整合到输出层。每层中的一个圆代表一个处理单元，可以认为是模拟了一个神经元，若干个处理单元组成了一个层，若干个层再组成了一个网络，也就是”神经网络”。图9 神经网络的逻辑架构 在神经网络中，每个处理单元事实上就是一个逻辑回归模型，逻辑回归模型接收上层的输入，把模型的预测结果作为输出传输到下一个层次。通过这样的过程，神经网络可以完成非常复杂的非线性分类。 下图会演示神经网络在图像识别领域的一个著名应用，这个程序叫做LeNet，是一个基于多个隐层构建的神经网络。通过LeNet可以识别多种手写数字，并且达到很高的识别精度与拥有较好的鲁棒性。图10 LeNet的效果展示 右下方的方形中显示的是输入计算机的图像，方形上方的红色字样&ldquo;answer&rdquo;后面显示的是计算机的输出。左边的三条竖直的图像列显示的是神经网络中三个隐藏层的输出，可以看出，随着层次的不断深入，越深的层次处理的细节越低，例如层3基本处理的都已经是线的细节了。LeNet的发明人就是前文介绍过的机器学习的大牛Yann LeCun(图1右者)。 进入90年代，神经网络的发展进入了一个瓶颈期。其主要原因是尽管有BP算法的加速，神经网络的训练过程仍然很困难。因此90年代后期支持向量机(SVM)算法取代了神经网络的地位。 3、SVM（支持向量机） 支持向量机算法是诞生于统计学习界，同时在机器学习界大放光彩的经典算法。 支持向量机算法从某种意义上来说是逻辑回归算法的强化：通过给予逻辑回归算法更严格的优化条件，支持向量机算法可以获得比逻辑回归更好的分类界线。但是如果没有某类函数技术，则支持向量机算法最多算是一种更好的线性分类技术。 但是，通过跟高斯&ldquo;核&rdquo;的结合，支持向量机可以表达出非常复杂的分类界线，从而达成很好的的分类效果。&ldquo;核&rdquo;事实上就是一种特殊的函数，最典型的特征就是可以将低维的空间映射到高维的空间。 例如下图所示：&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;图11 支持向量机图例 我们如何在二维平面划分出一个圆形的分类界线？在二维平面可能会很困难，但是通过&ldquo;核&rdquo;可以将二维空间映射到三维空间，然后使用一个线性平面就可以达成类似效果。也就是说，二维平面划分出的非线性分类界线可以等价于三维平面的线性分类界线。于是，我们可以通过在三维空间中进行简单的线性划分就可以达到在二维平面中的非线性划分效果。&nbsp;图12 三维空间的切割 支持向量机是一种数学成分很浓的机器学习算法（相对的，神经网络则有生物科学成分）。在算法的核心步骤中，有一步证明，即将数据从低维映射到高维不会带来最后计算复杂性的提升。于是，通过支持向量机算法，既可以保持计算效率，又可以获得非常好的分类效果。因此支持向量机在90年代后期一直占据着机器学习中最核心的地位，基本取代了神经网络算法。直到现在神经网络借着深度学习重新兴起，两者之间才又发生了微妙的平衡转变。 4、聚类算法 前面的算法中的一个显著特征就是我的训练数据中包含了标签，训练出的模型可以对其他未知数据预测标签。在下面的算法中，训练数据都是不含标签的，而算法的目的则是通过训练，推测出这些数据的标签。这类算法有一个统称，即无监督算法(前面有标签的数据的算法则是有监督算法)。无监督算法中最典型的代表就是聚类算法。 让我们还是拿一个二维的数据来说，某一个数据包含两个特征。我希望通过聚类算法，给他们中不同的种类打上标签，我该怎么做呢？简单来说，聚类算法就是计算种群中的距离，根据距离的远近将数据划分为多个族群。 聚类算法中最典型的代表就是K-Means算法。 5、降维算法 降维算法也是一种无监督学习算法，其主要特征是将数据从高维降低到低维层次。在这里，维度其实表示的是数据的特征量的大小，例如，房价包含房子的长、宽、面积与房间数量四个特征，也就是维度为4维的数据。可以看出来，长与宽事实上与面积表示的信息重叠了，例如面积=长 &times; 宽。通过降维算法我们就可以去除冗余信息，将特征减少为面积与房间数量两个特征，即从4维的数据压缩到2维。于是我们将数据从高维降低到低维，不仅利于表示，同时在计算上也能带来加速。 刚才说的降维过程中减少的维度属于肉眼可视的层次，同时压缩也不会带来信息的损失(因为信息冗余了)。如果肉眼不可视，或者没有冗余的特征，降维算法也能工作，不过这样会带来一些信息的损失。但是，降维算法可以从数学上证明，从高维压缩到的低维中最大程度地保留了数据的信息。因此，使用降维算法仍然有很多的好处。 降维算法的主要作用是压缩数据与提升机器学习其他算法的效率。通过降维算法，可以将具有几千个特征的数据压缩至若干个特征。另外，降维算法的另一个好处是数据的可视化，例如将5维的数据压缩至2维，然后可以用二维平面来可视。降维算法的主要代表是PCA算法(即主成分分析算法)。 6、推荐算法 推荐算法是目前业界非常火的一种算法，在电商界，如亚马逊，天猫，京东等得到了广泛的运用。推荐算法的主要特征就是可以自动向用户推荐他们最感兴趣的东西，从而增加购买率，提升效益。推荐算法有两个主要的类别： 一类是基于物品内容的推荐，是将与用户购买的内容近似的物品推荐给用户，这样的前提是每个物品都得有若干个标签，因此才可以找出与用户购买物品类似的物品，这样推荐的好处是关联程度较大，但是由于每个物品都需要贴标签，因此工作量较大。 另一类是基于用户相似度的推荐，则是将与目标用户兴趣相同的其他用户购买的东西推荐给目标用户，例如小A历史上买了物品B和C，经过算法分析，发现另一个与小A近似的用户小D购买了物品E，于是将物品E推荐给小A。 两类推荐都有各自的优缺点，在一般的电商应用中，一般是两类混合使用。推荐算法中最有名的算法就是协同过滤算法。 7、其他 除了以上算法之外，机器学习界还有其他的如高斯判别，朴素贝叶斯，决策树等等算法。但是上面列的六个算法是使用最多，影响最广，种类最全的典型。机器学习界的一个特色就是算法众多，发展百花齐放。 下面做一个总结，按照训练的数据有无标签，可以将上面算法分为监督学习算法和无监督学习算法，但推荐算法较为特殊，既不属于监督学习，也不属于非监督学习，是单独的一类。 监督学习算法： 线性回归，逻辑回归，神经网络，SVM 无监督学习算法： 聚类算法，降维算法 特殊算法： 推荐算法 除了这些算法以外，有一些算法的名字在机器学习领域中也经常出现。但他们本身并不算是一个机器学习算法，而是为了解决某个子问题而诞生的。你可以理解他们为以上算法的子算法，用于大幅度提高训练过程。其中的代表有：梯度下降法，主要运用在线型回归，逻辑回归，神经网络，推荐算法中；牛顿法，主要运用在线型回归中；BP算法，主要运用在神经网络中；SMO算法，主要运用在SVM中。5.机器学习的应用–大数据 说完机器学习的方法，下面要谈一谈机器学习的应用了。无疑，在2010年以前，机器学习的应用在某些特定领域发挥了巨大的作用，如车牌识别，网络攻击防范，手写字符识别等等。但是，从2010年以后，随着大数据概念的兴起，机器学习大量的应用都与大数据高度耦合，几乎可以认为大数据是机器学习应用的最佳场景。 譬如，但凡你能找到的介绍大数据魔力的文章，都会说大数据如何准确准确预测到了某些事。例如经典的Google利用大数据预测了H1N1在美国某小镇的爆发。 图13 Google成功预测H1N1 百度预测2014年世界杯，从淘汰赛到决赛全部预测正确。图14 百度世界杯成功预测了所有比赛结果 这些实在太神奇了，那么究竟是什么原因导致大数据具有这些魔力的呢？简单来说，就是机器学习技术。正是基于机器学习技术的应用，数据才能发挥其魔力。 大数据的核心是利用数据的价值，机器学习是利用数据价值的关键技术，对于大数据而言，机器学习是不可或缺的。相反，对于机器学习而言，越多的数据会越 可能提升模型的精确性，同时，复杂的机器学习算法的计算时间也迫切需要分布式计算与内存计算这样的关键技术。因此，机器学习的兴盛也离不开大数据的帮助。 大数据与机器学习两者是互相促进，相依相存的关系。 机器学习与大数据紧密联系。但是，必须清醒的认识到，大数据并不等同于机器学习，同理，机器学习也不等同于大数据。大数据中包含有分布式计算，内存数据库，多维分析等等多种技术。单从分析方法来看，大数据也包含以下四种分析方法： 1.大数据，小分析：即数据仓库领域的OLAP分析思路，也就是多维分析思想。 2.大数据，大分析：这个代表的就是数据挖掘与机器学习分析法。 3.流式分析：这个主要指的是事件驱动架构。 4.查询分析：经典代表是NoSQL数据库。 也就是说，机器学习仅仅是大数据分析中的一种而已。尽管机器学习的一些结果具有很大的魔力，在某种场合下是大数据价值最好的说明。但这并不代表机器学习是大数据下的唯一的分析方法。 机器学习与大数据的结合产生了巨大的价值。基于机器学习技术的发展，数据能够&ldquo;预测&rdquo;。对人类而言，积累的经验越丰富，阅历也广泛，对未来的判断越准确。例如常说的&ldquo;经验丰富&rdquo;的人比&ldquo;初出茅庐&rdquo;的小伙子更有工作上的优势，就在于经验丰富的人获得的规律比他人更准确。而在机器学习领域，根据著名的一个实验，有效的证实了机器学习界一个理论：即机器学习模型的数据越多，机器学习的预测的效率就越好。见下图：图15 机器学习准确率与数据的关系 通过这张图可以看出，各种不同算法在输入的数据量达到一定级数后，都有相近的高准确度。于是诞生了机器学习界的名言：成功的机器学习应用不是拥有最好的算法，而是拥有最多的数据！ 在大数据的时代，有好多优势促使机器学习能够应用更广泛。例如随着物联网和移动设备的发展，我们拥有的数据越来越多，种类也包括图片、文本、视频等非结构化数据，这使得机器学习模型可以获得越来越多的数据。同时大数据技术中的分布式计算Map-Reduce使得机器学习的速度越来越快，可以更方便的使用。种种优势使得在大数据时代，机器学习的优势可以得到最佳的发挥。6.机器学习的子类–深度学习 近来，机器学习的发展产生了一个新的方向，即&ldquo;深度学习&rdquo;。 虽然深度学习这四字听起来颇为高大上，但其理念却非常简单，就是传统的神经网络发展到了多隐藏层的情况。 在上文介绍过，自从90年代以后，神经网络已经消寂了一段时间。但是BP算法的发明人Geoffrey Hinton一直没有放弃对神经网络的研究。由于神经网络在隐藏层扩大到两个以上，其训练速度就会非常慢，因此实用性一直低于支持向量机。2006年，Geoffrey Hinton在科学杂志《Science》上发表了一篇文章，论证了两个观点： 1.多隐层的神经网络具有优异的特征学习能力，学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类； 2.深度神经网络在训练上的难度，可以通过&ldquo;逐层初始化&rdquo; 来有效克服。图16 Geoffrey Hinton与他的学生在Science上发表文章 通过这样的发现，不仅解决了神经网络在计算上的难度，同时也说明了深层神经网络在学习上的优异性。从此，神经网络重新成为了机器学习界中的主流强大学习技术。同时，具有多个隐藏层的神经网络被称为深度神经网络，基于深度神经网络的学习研究称之为深度学习。 由于深度学习的重要性质，在各方面都取得极大的关注，按照时间轴排序，有以下四个标志性事件值得一说： 2012年6月，《纽约时报》披露了Google Brain项目，这个项目是由Andrew Ng和Map-Reduce发明人Jeff Dean共同主导，用16000个CPU Core的并行计算平台训练一种称为&ldquo;深层神经网络&rdquo;的机器学习模型，在语音识别和图像识别等领域获得了巨大的成功。Andrew Ng就是文章开始所介绍的机器学习的大牛(图1中左者)。 2012年11月，微软在中国天津的一次活动上公开演示了一个全自动的同声传译系统，讲演者用英文演讲，后台的计算机一气呵成自动完成语音识别、英中机器翻译，以及中文语音合成，效果非常流畅，其中支撑的关键技术是深度学习； 2013年1月，在百度的年会上，创始人兼CEO李彦宏高调宣布要成立百度研究院，其中第一个重点方向就是深度学习，并为此而成立深度学习研究院(IDL)。 2013年4月，《麻省理工学院技术评论》杂志将深度学习列为2013年十大突破性技术(Breakthrough Technology)之首。图17 深度学习的发展热潮 文章开头所列的三位机器学习的大牛，不仅都是机器学习界的专家，更是深度学习研究领域的先驱。因此，使他们担任各个大型互联网公司技术掌舵者的原因不仅在于他们的技术实力，更在于他们研究的领域是前景无限的深度学习技术。 目前业界许多的图像识别技术与语音识别技术的进步都源于深度学习的发展，除了本文开头所提的Cortana等语音助手，还包括一些图像识别应用，其中典型的代表就是下图的百度识图功能。图18 百度识图 深度学习属于机器学习的子类。基于深度学习的发展极大的促进了机器学习的地位提高，更进一步地，推动了业界对机器学习父类人工智能梦想的再次重视。&nbsp;7.机器学习的父类–人工智能 人工智能是机器学习的父类。深度学习则是机器学习的子类。如果把三者的关系用图来表明的话，则是下图：图19 深度学习、机器学习、人工智能三者关系 毫无疑问，人工智能(AI)是人类所能想象的科技界最突破性的发明了，某种意义上来说，人工智能就像游戏最终幻想的名字一样，是人类对于科技界的最终梦想。从50年代提出人工智能的理念以后，科技界，产业界不断在探索，研究。这段时间各种小说、电影都在以各种方式展现对于人工智能的想象。人类可以发明类似于人类的机器，这是多么伟大的一种理念！但事实上，自从50年代以后，人工智能的发展就磕磕碰碰，未有见到足够震撼的科学技术的进步。 总结起来，人工智能的发展经历了如下若干阶段，从早期的逻辑推理，到中期的专家系统，这些科研进步确实使我们离机器的智能有点接近了，但还有一大段距离。直到机器学习诞生以后，人工智能界感觉终于找对了方向。基于机器学习的图像识别和语音识别在某些垂直领域达到了跟人相媲美的程度。机器学习使人类第一次如此接近人工智能的梦想。 事实上，如果我们把人工智能相关的技术以及其他业界的技术做一个类比，就可以发现机器学习在人工智能中的重要地位不是没有理由的。 人类区别于其他物体，植物，动物的最主要区别，作者认为是&ldquo;智慧&rdquo;。而智慧的最佳体现是什么？ 是计算能力么，应该不是，心算速度快的人我们一般称之为天才。 是反应能力么，也不是，反应快的人我们称之为灵敏。 是记忆能力么，也不是，记忆好的人我们一般称之为过目不忘。 是推理能力么，这样的人我也许会称他智力很高，类似&ldquo;福尔摩斯&rdquo;，但不会称他拥有智慧。 是知识能力么，这样的人我们称之为博闻广，也不会称他拥有智慧。 想想看我们一般形容谁有大智慧？圣人，诸如庄子，老子等。智慧是对生活的感悟，是对人生的积淀与思考，这与我们机器学习的思想何其相似？通过经验获取规律，指导人生与未来。没有经验就没有智慧。&nbsp;图20 机器学习与智慧 那么，从计算机来看，以上的种种能力都有种种技术去应对。 例如计算能力我们有分布式计算，反应能力我们有事件驱动架构，检索能力我们有搜索引擎，知识存储能力我们有数据仓库，逻辑推理能力我们有专家系统，但是，唯有对应智慧中最显著特征的归纳与感悟能力，只有机器学习与之对应。这也是机器学习能力最能表征智慧的根本原因。 让我们再看一下机器人的制造，在我们具有了强大的计算，海量的存储，快速的检索，迅速的反应，优秀的逻辑推理后我们如果再配合上一个强大的智慧大脑，一个真正意义上的人工智能也许就会诞生，这也是为什么说在机器学习快速发展的现在，人工智能可能不再是梦想的原因。 人工智能的发展可能不仅取决于机器学习，更取决于前面所介绍的深度学习，深度学习技术由于深度模拟了人类大脑的构成，在视觉识别与语音识别上显著性的突破了原有机器学习技术的界限，因此极有可能是真正实现人工智能梦想的关键技术。无论是谷歌大脑还是百度大脑，都是通过海量层次的深度学习网络所构成的。也许借助于深度学习技术，在不远的将来，一个具有人类智能的计算机真的有可能实现。 最后再说一下题外话，由于人工智能借助于深度学习技术的快速发展，已经在某些地方引起了传统技术界达人的担忧。真实世界的&ldquo;钢铁侠&rdquo;，特斯拉CEO马斯克就是其中之一。最近马斯克在参加MIT讨论会时，就表达了对于人工智能的担忧。&ldquo;人工智能的研究就类似于召唤恶魔，我们必须在某些地方加强注意。&rdquo;&nbsp;图21 马斯克与人工智能 尽管马斯克的担心有些危言耸听，但是马斯克的推理不无道理。&ldquo;如果人工智能想要消除垃圾邮件的话，可能它最后的决定就是消灭人类。&rdquo;马斯克认为预防此类现象的方法是引入政府的监管。在这里作者的观点与马斯克类似，在人工智能诞生之初就给其加上若干规则限制可能有效，也就是不应该使用单纯的机器学习，而应该是机器学习与规则引擎等系统的综合能够较好的解决这类问题。因为如果学习没有限制，极有可能进入某个误区，必须要加上某些引导。正如人类社会中，法律就是一个最好的规则，杀人者死就是对于人类在探索提高生产力时不可逾越的界限。 在这里，必须提一下这里的规则与机器学习引出的规律的不同，规律不是一个严格意义的准则，其代表的更多是概率上的指导，而规则则是神圣不可侵犯，不可修改的。规律可以调整，但规则是不能改变的。有效的结合规律与规则的特点，可以引导出一个合理的，可控的学习型人工智能。&nbsp;8.机器学习的思考–计算机的潜意识 最后，作者想谈一谈关于机器学习的一些思考。主要是作者在日常生活总结出来的一些感悟。 回想一下我在节1里所说的故事，我把小Y过往跟我相约的经历做了一个罗列。但是这种罗列以往所有经历的方法只有少数人会这么做，大部分的人采用的是更直接的方法，即利用直觉。那么，直觉是什么？其实直觉也是你在潜意识状态下思考经验后得出的规律。就像你通过机器学习算法，得到了一个模型，那么你下次只要直接使用就行了。那么这个规律你是什么时候思考的？可能是在你无意识的情况下，例如睡觉，走路等情况。这种时候，大脑其实也在默默地做一些你察觉不到的工作。 这种直觉与潜意识，我把它与另一种人类思考经验的方式做了区分。如果一个人勤于思考，例如他会每天做一个小结，譬如&ldquo;吾日三省吾身&rdquo;，或者他经常与同伴讨论最近工作的得失，那么他这种训练模型的方式是直接的，明意识的思考与归纳。这样的效果很好，记忆性强，并且更能得出有效反应现实的规律。但是大部分的人可能很少做这样的总结，那么他们得出生活中规律的方法使用的就是潜意识法。 举一个作者本人关于潜意识的例子。作者本人以前没开过车，最近一段时间买了车后，天天开车上班。我每天都走固定的路线。有趣的是，在一开始的几天，我非常紧张的注意着前方的路况，而现在我已经在无意识中就把车开到了目标。这个过程中我的眼睛是注视着前方的，我的大脑是没有思考，但是我手握着的方向盘会自动的调整方向。也就是说。随着我开车次数的增多，我已经把我开车的动作交给了潜意识。这是非常有趣的一件事。在这段过程中，我的大脑将前方路况的图像记录了下来，同时大脑也记忆了我转动方向盘的动作。经过大脑自己的潜意识思考，最后生成的潜意识可以直接根据前方的图像调整我手的动作。假设我们将前方的录像交给计算机，然后让计算机记录与图像对应的驾驶员的动作。经过一段时间的学习，计算机生成的机器学习模型就可以进行自动驾驶了。这很神奇，不是么。其实包括Google、特斯拉在内的自动驾驶汽车技术的原理就是这样。 除了自动驾驶汽车以外，潜意识的思想还可以扩展到人的交际。譬如说服别人，一个最佳的方法就是给他展示一些信息，然后让他自己去归纳得出我们想要的结论。这就好比在阐述一个观点时，用一个事实，或者一个故事，比大段的道理要好很多。古往今来，但凡优秀的说客，无不采用的是这种方法。春秋战国时期，各国合纵连横，经常有各种说客去跟一国之君交流，直接告诉君主该做什么，无异于自寻死路，但是跟君主讲故事，通过这些故事让君主恍然大悟，就是一种正确的过程。这里面有许多杰出的代表，如墨子，苏秦等等。 基本上所有的交流过程，使用故事说明的效果都要远胜于阐述道义之类的效果好很多。为什么用故事的方法比道理或者其他的方法好很多，这是因为在人成长的过程，经过自己的思考，已经形成了很多规律与潜意识。如果你告诉的规律与对方的不相符，很有可能出于保护，他们会本能的拒绝你的新规律，但是如果你跟他讲一个故事，传递一些信息，输送一些数据给他，他会思考并自我改变。他的思考过程实际上就是机器学习的过程，他把新的数据纳入到他的旧有的记忆与数据中，经过重新训练。如果你给出的数据的信息量非常大，大到调整了他的模型，那么他就会按照你希望的规律去做事。有的时候，他会本能的拒绝执行这个思考过程，但是数据一旦输入，无论他希望与否，他的大脑都会在潜意识状态下思考，并且可能改变他的看法。 如果计算机也拥有潜意识(正如本博客的名称一样)，那么会怎么样？譬如让计算机在工作的过程中，逐渐产生了自身的潜意识，于是甚至可以在你不需要告诉它做什么时它就会完成那件事。这是个非常有意思的设想，这里留给各位读者去发散思考吧。9.总结 本文首先介绍了互联网界与机器学习大牛结合的趋势，以及使用机器学习的相关应用，接着以一个&ldquo;等人故事&rdquo;展开对机器学习的介绍。介绍中首先是机器学习的概念与定义，然后是机器学习的相关学科，机器学习中包含的各类学习算法，接着介绍机器学习与大数据的关系，机器学习的新子类深度学习，最后探讨了一下机器学习与人工智能发展的联系以及机器学习与潜意识的关联。经过本文的介绍，相信大家对机器学习技术有一定的了解，例如机器学习是什么，它的内核思想是什么(即统计和归纳)，通过了解机器学习与人类思考的近似联系可以知晓机器学习为什么具有智慧能力的原因等等。其次，本文漫谈了机器学习与外延学科的关系，机器学习与大数据相互促进相得益彰的联系，机器学习界最新的深度学习的迅猛发展，以及对于人类基于机器学习开发智能机器人的一种展望与思考，最后作者简单谈了一点关于让计算机拥有潜意识的设想。 机器学习是目前业界最为Amazing与火热的一项技术，从网上的每一次淘宝的购买东西，到自动驾驶汽车技术，以及网络攻击抵御系统等等，都有机器学习的因子在内，同时机器学习也是最有可能使人类完成AI dream的一项技术，各种人工智能目前的应用，如微软小冰聊天机器人，到计算机视觉技术的进步，都有机器学习努力的成分。作为一名当代的计算机领域的开发或管理人员，以及身处这个世界，使用者IT技术带来便利的人们，最好都应该了解一些机器学习的相关知识与概念，因为这可以帮你更好的理解为你带来莫大便利技术的背后原理，以及让你更好的理解当代科技的进程。10.后记 这篇文档花了作者两个月的时间，终于在2014年的最后一天的前一天基本完成。通过这篇文章，作者希望对机器学习在国内的普及做一点贡献，同时也是作者本人自己对于所学机器学习知识的一个融汇贯通，整体归纳的提高过程。作者把这么多的知识经过自己的大脑思考，训练出了一个模型，形成了这篇文档，可以说这也是一种机器学习的过程吧(笑)。 作者所在的行业会接触到大量的数据，因此对于数据的处理和分析是平常非常重要的工作，机器学习课程的思想和理念对于作者日常的工作指引作用极大，几乎导致了作者对于数据价值的重新认识。想想半年前，作者还对机器学习似懂非懂，如今也可以算是一个机器学习的Expert了(笑)。但作者始终认为，机器学习的真正应用不是通过概念或者思想的方式，而是通过实践。只有当把机器学习技术真正应用时，才可算是对机器学习的理解进入了一个层次。正所谓再&ldquo;阳春白雪&rdquo;的技术，也必须落到&ldquo;下里巴人&rdquo;的场景下运用。目前有一种风气，国内外研究机器学习的某些学者，有一种高贵的逼格，认为自己的研究是普通人无法理解的，但是这样的理念是根本错误的，没有在真正实际的地方发挥作用，凭什么证明你的研究有所价值呢？作者认为必须将高大上的技术用在改变普通人的生活上，才能发挥其根本的价值。一些简单的场景，恰恰是实践机器学习技术的最好地方。]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟机搭建Hadoop实验]]></title>
    <url>%2F2017%2F09%2F14%2F%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%90%AD%E5%BB%BAHadoop%E5%AE%9E%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[在学习大数据的过程中搭建由三台虚拟机构成的Hadoop模型 创建三台虚拟机使用软件：Vmware12 Centos7最小镜像 安装一台虚拟机，配置好网络与jdk，复制出两台同样的 方法/步骤 首先我们安装后centos7最小化系统后，并进入系统执行命令ifconfig，会发现系统提示命令未找到。具体展示效果如下图所示。 然后输入命令查看本机是否分配IP,执行命令ip addr ，可以发现系统的网卡没有分配IP地址，在此我们需要记住本机网卡的名称，用于下一步使用，本篇中我们的网卡为：eno16777736。具体效果如下图所示。 然后我们进入网卡配置文件的目录。执行命令 cd /etc/sysconfig/network-scripts/ 然后查看下面的网卡文件。具体效果如下图所示。 然后我们找到对应的网卡文件执行命令 vi ifcfg-eno16777736。进行修改网卡文件，不同机器网卡不同，本篇以自己电脑为例展示。 我们需要首先找到ONBOOT=no ，需要修改为ONBOOT=yes然后保存退出。 然后执行命令 service network restart 重启网卡服务。具体操作如下图所示。 执行完成后，我们再次执行命令 ip addr 查看是否分配到IP地址，可以看到已经分配到IP地址。具体操作如下图所示。 然后我们执行命令yum provides ifconfig 查看哪个包提供了ifconfig命令，然后可以看到net-tools包提供ifconfig包， 具体操作如下图所示。 然后我们执行命令安装net-tools包，执行命令：yum install net-tools。具体操作如下图所示。 然后我们执行命令ifconfig，可以看到可以使用了，而且展示了系统的网卡信息。具体操作如下图所示。 123456789101112131415161718192021222324252627282930313233343536373839404142安装jdk：1.查看yum库中都有哪些jdk版本(暂时只发现了openjdk)[root@localhost ~]# yum search java|grep jdkldapjdk-javadoc.x86_64 : Javadoc for ldapjdkjava-1.6.0-openjdk.x86_64 : OpenJDK Runtime Environmentjava-1.6.0-openjdk-demo.x86_64 : OpenJDK Demosjava-1.6.0-openjdk-devel.x86_64 : OpenJDK Development Environmentjava-1.6.0-openjdk-javadoc.x86_64 : OpenJDK API Documentationjava-1.6.0-openjdk-src.x86_64 : OpenJDK Source Bundlejava-1.7.0-openjdk.x86_64 : OpenJDK Runtime Environmentjava-1.7.0-openjdk-demo.x86_64 : OpenJDK Demosjava-1.7.0-openjdk-devel.x86_64 : OpenJDK Development Environmentjava-1.7.0-openjdk-javadoc.noarch : OpenJDK API Documentationjava-1.7.0-openjdk-src.x86_64 : OpenJDK Source Bundlejava-1.8.0-openjdk.x86_64 : OpenJDK Runtime Environmentjava-1.8.0-openjdk-demo.x86_64 : OpenJDK Demosjava-1.8.0-openjdk-devel.x86_64 : OpenJDK Development Environmentjava-1.8.0-openjdk-headless.x86_64 : OpenJDK Runtime Environmentjava-1.8.0-openjdk-javadoc.noarch : OpenJDK API Documentationjava-1.8.0-openjdk-src.x86_64 : OpenJDK Source Bundleldapjdk.x86_64 : The Mozilla LDAP Java SDK2.选择版本,进行安装//选择1.7版本进行安装[root@localhost ~]# yum install java-1.7.0-openjdk//安装完之后，默认的安装目录是在: /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.75.x86_643.设置环境变量 （如果已经有java命令不用设置）[root@localhost ~]# vi /etc/profile在profile文件中添加如下内容#set java environmentJAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.75.x86_64JRE_HOME=$JAVA_HOME/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport JAVA_HOME JRE_HOME CLASS_PATH PATH4.让修改生效[root@localhost java]# source /etc/profile5.查看刚安装的Java版本信息。◆输入：java -version 可查看Java版本； 配置hosts vi /etc/hosts 说明：slaver217,slaver214作为datanode节点，master204作为namenode节点。另外，各datanode节点主机上只需配置如：172.16.51.214 slaver214。]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>BigData</tag>
      </tags>
  </entry>
</search>
