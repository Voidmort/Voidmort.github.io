<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="稀疏数据和嵌入简介 将影评字符串数据转换为稀疏特征矢量 使用稀疏特征矢量实现情感分析线性模型 通过将数据投射到二维空间的嵌入来实现情感分析 DNN 模型 将嵌入可视化，以便查看模型学到的词语之间的关系  在此练习中，我们将探讨稀疏数据，并使用影评文本数据（来自 ACL 2011 IMDB 数据集）进行嵌入。这些数据已被处理成 tf.Example 格式。 设置我们导入依赖项并下载训练数据和测试数据">
<meta name="keywords" content="Tensorflow">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow 10">
<meta property="og:url" content="https://xvjie.wang/2019/02/25/TensorFlow_10/index.html">
<meta property="og:site_name" content="Voidmort">
<meta property="og:description" content="稀疏数据和嵌入简介 将影评字符串数据转换为稀疏特征矢量 使用稀疏特征矢量实现情感分析线性模型 通过将数据投射到二维空间的嵌入来实现情感分析 DNN 模型 将嵌入可视化，以便查看模型学到的词语之间的关系  在此练习中，我们将探讨稀疏数据，并使用影评文本数据（来自 ACL 2011 IMDB 数据集）进行嵌入。这些数据已被处理成 tf.Example 格式。 设置我们导入依赖项并下载训练数据和测试数据">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://xvjie.wang/2019/02/25/TensorFlow_10/TensorFlow_10_30_0.png">
<meta property="og:image" content="https://xvjie.wang/2019/02/25/TensorFlow_10/TensorFlow_10_31_0.png">
<meta property="og:updated_time" content="2019-03-19T13:59:46.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorFlow 10">
<meta name="twitter:description" content="稀疏数据和嵌入简介 将影评字符串数据转换为稀疏特征矢量 使用稀疏特征矢量实现情感分析线性模型 通过将数据投射到二维空间的嵌入来实现情感分析 DNN 模型 将嵌入可视化，以便查看模型学到的词语之间的关系  在此练习中，我们将探讨稀疏数据，并使用影评文本数据（来自 ACL 2011 IMDB 数据集）进行嵌入。这些数据已被处理成 tf.Example 格式。 设置我们导入依赖项并下载训练数据和测试数据">
<meta name="twitter:image" content="https://xvjie.wang/2019/02/25/TensorFlow_10/TensorFlow_10_30_0.png">



  <link rel="alternate" href="/atom.xml" title="Voidmort" type="application/atom+xml" />




  <link rel="canonical" href="https://xvjie.wang/2019/02/25/TensorFlow_10/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>TensorFlow 10 | Voidmort</title>
  






  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?7b96eb7c2717359e36cc8bd8b546997c";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>
<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    <a href="https://github.com/Voidmort" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Voidmort</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />分类</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-other">
    <a href="/other/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-plane"></i> <br />其它</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />关于</a>
  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://xvjie.wang/2019/02/25/TensorFlow_10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Voidmort">
      <meta itemprop="description" content="研究AI，撰写影评，热爱音乐，努力健身">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Voidmort">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">TensorFlow 10
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-02-25 16:51:00" itemprop="dateCreated datePublished" datetime="2019-02-25T16:51:00+08:00">2019-02-25</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-03-19 21:59:46" itemprop="dateModified" datetime="2019-03-19T21:59:46+08:00">2019-03-19</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Tensorflow/" itemprop="url" rel="index"><span itemprop="name">Tensorflow</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="稀疏数据和嵌入简介"><a href="#稀疏数据和嵌入简介" class="headerlink" title="稀疏数据和嵌入简介"></a>稀疏数据和嵌入简介</h1><ul>
<li>将影评字符串数据转换为稀疏特征矢量</li>
<li>使用稀疏特征矢量实现情感分析线性模型</li>
<li>通过将数据投射到二维空间的嵌入来实现情感分析 DNN 模型</li>
<li>将嵌入可视化，以便查看模型学到的词语之间的关系</li>
</ul>
<p>在此练习中，我们将探讨稀疏数据，并使用影评文本数据（来自 <a href="http://ai.stanford.edu/~amaas/data/sentiment/" target="_blank" rel="noopener">ACL 2011 IMDB 数据集</a>）进行嵌入。这些数据已被处理成 <code>tf.Example</code> 格式。</p>
<h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2><p>我们导入依赖项并下载训练数据和测试数据。<a href="https://www.tensorflow.org/api_docs/python/tf/keras" target="_blank" rel="noopener"><code>tf.keras</code></a> 中包含一个文件下载和缓存工具，我们可以用它来检索数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line">tf.logging.set_verbosity(tf.logging.ERROR)</span><br><span class="line">train_url = <span class="string">'https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/train.tfrecord'</span></span><br><span class="line">train_path = tf.keras.utils.get_file(train_url.split(<span class="string">'/'</span>)[<span class="number">-1</span>], train_url)</span><br><span class="line">test_url = <span class="string">'https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/test.tfrecord'</span></span><br><span class="line">test_path = tf.keras.utils.get_file(test_url.split(<span class="string">'/'</span>)[<span class="number">-1</span>], test_url)</span><br></pre></td></tr></table></figure>
<pre><code>Downloading data from https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/train.tfrecord
41631744/41625533 [==============================] - 0s 0us/step
Downloading data from https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/test.tfrecord
40689664/40688441 [==============================] - 0s 0us/step
</code></pre><h2 id="构建情感分析模型"><a href="#构建情感分析模型" class="headerlink" title="构建情感分析模型"></a>构建情感分析模型</h2><p>  我们根据这些数据训练一个情感分析模型，以预测某条评价总体上是<em>好评</em>（标签为 1）还是<em>差评</em>（标签为 0）。</p>
<p>为此，我们会使用<em>词汇表</em>（即我们预计将在数据中看到的每个术语的列表），将字符串值 <code>terms</code> 转换为特征矢量。在本练习中，我们创建了侧重于一组有限术语的小型词汇表。其中的大多数术语明确表示是<em>好评</em>或<em>差评</em>，但有些只是因为有趣而被添加进来。</p>
<p>词汇表中的每个术语都与特征矢量中的一个坐标相对应。为了将样本的字符串值 <code>terms</code> 转换为这种矢量格式，我们按以下方式处理字符串值：如果该术语没有出现在样本字符串中，则坐标值将为 0；如果出现在样本字符串中，则值为 1。未出现在该词汇表中的样本中的术语将被弃用。</p>
<p> <strong>注意</strong>：<em>我们当然可以使用更大的词汇表，而且有创建此类词汇表的专用工具。此外，我们可以添加少量的 OOV（未收录词汇）分桶，您可以在其中对词汇表中未包含的术语进行哈希处理，而不仅仅是弃用这些术语。我们还可以使用<strong>特征哈希</strong>法对每个术语进行哈希处理，而不是创建显式词汇表。这在实践中很有效，但却不具备可解读性（这对本练习非常实用）。如需了解处理此类词汇表的工具，请参阅 tf.feature_column 模块。</em></p>
<h2 id="构建输入管道"><a href="#构建输入管道" class="headerlink" title="构建输入管道"></a>构建输入管道</h2><p>  首先，我们来配置输入管道，以将数据导入 TensorFlow 模型中。我们可以使用以下函数来解析训练数据和测试数据（格式为 <a href="https://www.tensorflow.org/guide/datasets#consuming_tfrecord_data" target="_blank" rel="noopener">TFRecord</a>），然后返回一个由特征和相应标签组成的字典。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_parse_function</span><span class="params">(record)</span>:</span></span><br><span class="line">  <span class="string">"""Extracts features and labels.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    record: File path to a TFRecord file    </span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    A `tuple` `(labels, features)`:</span></span><br><span class="line"><span class="string">      features: A dict of tensors representing the features</span></span><br><span class="line"><span class="string">      labels: A tensor with the corresponding labels.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  features = &#123;</span><br><span class="line">    <span class="string">"terms"</span>: tf.VarLenFeature(dtype=tf.string), <span class="comment"># terms are strings of varying lengths</span></span><br><span class="line">    <span class="string">"labels"</span>: tf.FixedLenFeature(shape=[<span class="number">1</span>], dtype=tf.float32) <span class="comment"># labels are 0 or 1</span></span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  parsed_features = tf.parse_single_example(record, features)</span><br><span class="line">  </span><br><span class="line">  terms = parsed_features[<span class="string">'terms'</span>].values</span><br><span class="line">  labels = parsed_features[<span class="string">'labels'</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span>  &#123;<span class="string">'terms'</span>:terms&#125;, labels</span><br></pre></td></tr></table></figure>
<p> 为了确认函数是否能正常运行，我们为训练数据构建一个 <code>TFRecordDataset</code>，并使用上述函数将数据映射到特征和标签。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create the Dataset object.</span></span><br><span class="line">ds = tf.data.TFRecordDataset(train_path)</span><br><span class="line"><span class="comment"># Map features and labels with the parse function.</span></span><br><span class="line">ds = ds.map(_parse_function)</span><br><span class="line"></span><br><span class="line">ds</span><br></pre></td></tr></table></figure>
<pre><code>&lt;DatasetV1Adapter shapes: ({terms: (?,)}, (1,)), types: ({terms: tf.string}, tf.float32)&gt;
</code></pre><p> 运行以下单元，以从训练数据集中获取第一个样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n = ds.make_one_shot_iterator().get_next()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(n)</span><br></pre></td></tr></table></figure>
<pre><code>({&apos;terms&apos;: array([b&apos;but&apos;, b&apos;it&apos;, b&apos;does&apos;, b&apos;have&apos;, b&apos;some&apos;, b&apos;good&apos;, b&apos;action&apos;,
         b&apos;and&apos;, b&apos;a&apos;, b&apos;plot&apos;, b&apos;that&apos;, b&apos;is&apos;, b&apos;somewhat&apos;, b&apos;interesting&apos;,
         b&apos;.&apos;, b&apos;nevsky&apos;, b&apos;acts&apos;, b&apos;like&apos;, b&apos;a&apos;, b&apos;body&apos;, b&apos;builder&apos;,
         b&apos;and&apos;, b&apos;he&apos;, b&apos;isn&apos;, b&quot;&apos;&quot;, b&apos;t&apos;, b&apos;all&apos;, b&apos;that&apos;, b&apos;attractive&apos;,
         b&apos;,&apos;, b&apos;in&apos;, b&apos;fact&apos;, b&apos;,&apos;, b&apos;imo&apos;, b&apos;,&apos;, b&apos;he&apos;, b&apos;is&apos;, b&apos;ugly&apos;,
         b&apos;.&apos;, b&apos;(&apos;, b&apos;his&apos;, b&apos;acting&apos;, b&apos;skills&apos;, b&apos;lack&apos;, b&apos;everything&apos;,
         b&apos;!&apos;, b&apos;)&apos;, b&apos;sascha&apos;, b&apos;is&apos;, b&apos;played&apos;, b&apos;very&apos;, b&apos;well&apos;, b&apos;by&apos;,
         b&apos;joanna&apos;, b&apos;pacula&apos;, b&apos;,&apos;, b&apos;but&apos;, b&apos;she&apos;, b&apos;needed&apos;, b&apos;more&apos;,
         b&apos;lines&apos;, b&apos;than&apos;, b&apos;she&apos;, b&apos;was&apos;, b&apos;given&apos;, b&apos;,&apos;, b&apos;her&apos;,
         b&apos;character&apos;, b&apos;needed&apos;, b&apos;to&apos;, b&apos;be&apos;, b&apos;developed&apos;, b&apos;.&apos;,
         b&apos;there&apos;, b&apos;are&apos;, b&apos;way&apos;, b&apos;too&apos;, b&apos;many&apos;, b&apos;men&apos;, b&apos;in&apos;, b&apos;this&apos;,
         b&apos;story&apos;, b&apos;,&apos;, b&apos;there&apos;, b&apos;is&apos;, b&apos;zero&apos;, b&apos;romance&apos;, b&apos;,&apos;, b&apos;too&apos;,
         b&apos;much&apos;, b&apos;action&apos;, b&apos;,&apos;, b&apos;and&apos;, b&apos;way&apos;, b&apos;too&apos;, b&apos;dumb&apos;, b&apos;of&apos;,
         b&apos;an&apos;, b&apos;ending&apos;, b&apos;.&apos;, b&apos;it&apos;, b&apos;is&apos;, b&apos;very&apos;, b&apos;violent&apos;, b&apos;.&apos;,
         b&apos;i&apos;, b&apos;did&apos;, b&apos;however&apos;, b&apos;love&apos;, b&apos;the&apos;, b&apos;scenery&apos;, b&apos;,&apos;,
         b&apos;this&apos;, b&apos;movie&apos;, b&apos;takes&apos;, b&apos;you&apos;, b&apos;all&apos;, b&apos;over&apos;, b&apos;the&apos;,
         b&apos;world&apos;, b&apos;,&apos;, b&apos;and&apos;, b&apos;that&apos;, b&apos;is&apos;, b&apos;a&apos;, b&apos;bonus&apos;, b&apos;.&apos;, b&apos;i&apos;,
         b&apos;also&apos;, b&apos;liked&apos;, b&apos;how&apos;, b&apos;it&apos;, b&apos;had&apos;, b&apos;some&apos;, b&apos;stuff&apos;,
         b&apos;about&apos;, b&apos;the&apos;, b&apos;mafia&apos;, b&apos;in&apos;, b&apos;it&apos;, b&apos;,&apos;, b&apos;not&apos;, b&apos;too&apos;,
         b&apos;much&apos;, b&apos;or&apos;, b&apos;too&apos;, b&apos;little&apos;, b&apos;,&apos;, b&apos;but&apos;, b&apos;enough&apos;,
         b&apos;that&apos;, b&apos;it&apos;, b&apos;got&apos;, b&apos;my&apos;, b&apos;attention&apos;, b&apos;.&apos;, b&apos;the&apos;,
         b&apos;actors&apos;, b&apos;needed&apos;, b&apos;to&apos;, b&apos;be&apos;, b&apos;more&apos;, b&apos;handsome&apos;, b&apos;.&apos;,
         b&apos;.&apos;, b&apos;.&apos;, b&apos;the&apos;, b&apos;biggest&apos;, b&apos;problem&apos;, b&apos;i&apos;, b&apos;had&apos;, b&apos;was&apos;,
         b&apos;that&apos;, b&apos;nevsky&apos;, b&apos;was&apos;, b&apos;just&apos;, b&apos;too&apos;, b&apos;normal&apos;, b&apos;,&apos;,
         b&apos;not&apos;, b&apos;sexy&apos;, b&apos;enough&apos;, b&apos;.&apos;, b&apos;i&apos;, b&apos;think&apos;, b&apos;for&apos;, b&apos;most&apos;,
         b&apos;guys&apos;, b&apos;,&apos;, b&apos;sascha&apos;, b&apos;will&apos;, b&apos;be&apos;, b&apos;hot&apos;, b&apos;enough&apos;, b&apos;,&apos;,
         b&apos;but&apos;, b&apos;for&apos;, b&apos;us&apos;, b&apos;ladies&apos;, b&apos;that&apos;, b&apos;are&apos;, b&apos;fans&apos;, b&apos;of&apos;,
         b&apos;action&apos;, b&apos;,&apos;, b&apos;nevsky&apos;, b&apos;just&apos;, b&apos;doesn&apos;, b&quot;&apos;&quot;, b&apos;t&apos;, b&apos;cut&apos;,
         b&apos;it&apos;, b&apos;.&apos;, b&apos;overall&apos;, b&apos;,&apos;, b&apos;this&apos;, b&apos;movie&apos;, b&apos;was&apos;, b&apos;fine&apos;,
         b&apos;,&apos;, b&apos;i&apos;, b&apos;didn&apos;, b&quot;&apos;&quot;, b&apos;t&apos;, b&apos;love&apos;, b&apos;it&apos;, b&apos;nor&apos;, b&apos;did&apos;,
         b&apos;i&apos;, b&apos;hate&apos;, b&apos;it&apos;, b&apos;,&apos;, b&apos;just&apos;, b&apos;found&apos;, b&apos;it&apos;, b&apos;to&apos;, b&apos;be&apos;,
         b&apos;another&apos;, b&apos;normal&apos;, b&apos;action&apos;, b&apos;flick&apos;, b&apos;.&apos;], dtype=object)},
 array([0.], dtype=float32))
</code></pre><p> 现在，我们构建一个正式的输入函数，可以将其传递给 TensorFlow Estimator 对象的 <code>train()</code> 方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create an input_fn that parses the tf.Examples from the given files,</span></span><br><span class="line"><span class="comment"># and split them into features and targets.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_input_fn</span><span class="params">(input_filenames, num_epochs=None, shuffle=True)</span>:</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Same code as above; create a dataset and map features and labels.</span></span><br><span class="line">  ds = tf.data.TFRecordDataset(input_filenames)</span><br><span class="line">  ds = ds.map(_parse_function)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> shuffle:</span><br><span class="line">    ds = ds.shuffle(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Our feature data is variable-length, so we pad and batch</span></span><br><span class="line">  <span class="comment"># each field of the dataset structure to whatever size is necessary.     </span></span><br><span class="line">  ds = ds.padded_batch(<span class="number">25</span>, ds.output_shapes)</span><br><span class="line">  </span><br><span class="line">  ds = ds.repeat(num_epochs)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Return the next batch of data.</span></span><br><span class="line">  features, labels = ds.make_one_shot_iterator().get_next()</span><br><span class="line">  <span class="keyword">return</span> features, labels</span><br></pre></td></tr></table></figure>
<h2 id="使用具有稀疏输入和显式词汇表的线性模型"><a href="#使用具有稀疏输入和显式词汇表的线性模型" class="headerlink" title="使用具有稀疏输入和显式词汇表的线性模型"></a>使用具有稀疏输入和显式词汇表的线性模型</h2><p>对于我们的第一个模型，我们将使用 50 个信息性术语来构建 <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/LinearClassifier" target="_blank" rel="noopener"><code>LinearClassifier</code></a> 模型；始终从简单入手！</p>
<p>以下代码将为我们的术语构建特征列。<a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list" target="_blank" rel="noopener"><code>categorical_column_with_vocabulary_list</code></a> 函数可使用“字符串-特征矢量”映射来创建特征列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 50 informative terms that compose our model vocabulary. </span></span><br><span class="line">informative_terms = (<span class="string">"bad"</span>, <span class="string">"great"</span>, <span class="string">"best"</span>, <span class="string">"worst"</span>, <span class="string">"fun"</span>, <span class="string">"beautiful"</span>,</span><br><span class="line">                     <span class="string">"excellent"</span>, <span class="string">"poor"</span>, <span class="string">"boring"</span>, <span class="string">"awful"</span>, <span class="string">"terrible"</span>,</span><br><span class="line">                     <span class="string">"definitely"</span>, <span class="string">"perfect"</span>, <span class="string">"liked"</span>, <span class="string">"worse"</span>, <span class="string">"waste"</span>,</span><br><span class="line">                     <span class="string">"entertaining"</span>, <span class="string">"loved"</span>, <span class="string">"unfortunately"</span>, <span class="string">"amazing"</span>,</span><br><span class="line">                     <span class="string">"enjoyed"</span>, <span class="string">"favorite"</span>, <span class="string">"horrible"</span>, <span class="string">"brilliant"</span>, <span class="string">"highly"</span>,</span><br><span class="line">                     <span class="string">"simple"</span>, <span class="string">"annoying"</span>, <span class="string">"today"</span>, <span class="string">"hilarious"</span>, <span class="string">"enjoyable"</span>,</span><br><span class="line">                     <span class="string">"dull"</span>, <span class="string">"fantastic"</span>, <span class="string">"poorly"</span>, <span class="string">"fails"</span>, <span class="string">"disappointing"</span>,</span><br><span class="line">                     <span class="string">"disappointment"</span>, <span class="string">"not"</span>, <span class="string">"him"</span>, <span class="string">"her"</span>, <span class="string">"good"</span>, <span class="string">"time"</span>,</span><br><span class="line">                     <span class="string">"?"</span>, <span class="string">"."</span>, <span class="string">"!"</span>, <span class="string">"movie"</span>, <span class="string">"film"</span>, <span class="string">"action"</span>, <span class="string">"comedy"</span>,</span><br><span class="line">                     <span class="string">"drama"</span>, <span class="string">"family"</span>)</span><br><span class="line"></span><br><span class="line">terms_feature_column = tf.feature_column.categorical_column_with_vocabulary_list(key=<span class="string">"terms"</span>, vocabulary_list=informative_terms)</span><br></pre></td></tr></table></figure>
<p> 接下来，我们将构建 <code>LinearClassifier</code>，在训练集中训练该模型，并在评估集中对其进行评估。阅读上述代码后，运行该模型以了解其效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">my_optimizer = tf.train.AdagradOptimizer(learning_rate=<span class="number">0.1</span>)</span><br><span class="line">my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line"></span><br><span class="line">feature_columns = [ terms_feature_column ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">classifier = tf.estimator.LinearClassifier(</span><br><span class="line">  feature_columns=feature_columns,</span><br><span class="line">  optimizer=my_optimizer,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">classifier.train(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([train_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">evaluation_metrics = classifier.evaluate(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([train_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line">print(<span class="string">"Training set metrics:"</span>)</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> evaluation_metrics:</span><br><span class="line">  print(m, evaluation_metrics[m])</span><br><span class="line">print(<span class="string">"---"</span>)</span><br><span class="line"></span><br><span class="line">evaluation_metrics = classifier.evaluate(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([test_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test set metrics:"</span>)</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> evaluation_metrics:</span><br><span class="line">  print(m, evaluation_metrics[m])</span><br><span class="line">print(<span class="string">"---"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Training set metrics:
accuracy 0.78928
accuracy_baseline 0.5
auc 0.87206453
auc_precision_recall 0.8640158
average_loss 0.45088252
label/mean 0.5
loss 11.272063
precision 0.77057767
prediction/mean 0.4956976
recall 0.82384
global_step 1000
---
Test set metrics:
accuracy 0.78504
accuracy_baseline 0.5
auc 0.86939275
auc_precision_recall 0.8610384
average_loss 0.4532239
label/mean 0.5
loss 11.330598
precision 0.7680963
prediction/mean 0.49426404
recall 0.81664
global_step 1000
---
</code></pre><h2 id="使用深度神经网络-DNN-模型"><a href="#使用深度神经网络-DNN-模型" class="headerlink" title="使用深度神经网络 (DNN) 模型"></a>使用深度神经网络 (DNN) 模型</h2><p>上述模型是一个线性模型，效果非常好。但是，我们可以使用 DNN 模型实现更好的效果吗？</p>
<p>我们将 <code>LinearClassifier</code> 切换为 <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier" target="_blank" rel="noopener"><code>DNNClassifier</code></a>。运行以下单元，看看您的模型效果如何。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##################### Here's what we changed ##################################</span></span><br><span class="line">classifier = tf.estimator.DNNClassifier(                                      <span class="comment">#</span></span><br><span class="line">  feature_columns=[tf.feature_column.indicator_column(terms_feature_column)], <span class="comment">#</span></span><br><span class="line">  hidden_units=[<span class="number">20</span>,<span class="number">20</span>],                                                       <span class="comment">#</span></span><br><span class="line">  optimizer=my_optimizer,                                                     <span class="comment">#</span></span><br><span class="line">)                                                                             <span class="comment">#</span></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  classifier.train(</span><br><span class="line">    input_fn=<span class="keyword">lambda</span>: _input_fn([train_path]),</span><br><span class="line">    steps=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">  evaluation_metrics = classifier.evaluate(</span><br><span class="line">    input_fn=<span class="keyword">lambda</span>: _input_fn([train_path]),</span><br><span class="line">    steps=<span class="number">1</span>)</span><br><span class="line">  print(<span class="string">"Training set metrics:"</span>)</span><br><span class="line">  <span class="keyword">for</span> m <span class="keyword">in</span> evaluation_metrics:</span><br><span class="line">    print(m, evaluation_metrics[m])</span><br><span class="line">  print(<span class="string">"---"</span>)</span><br><span class="line"></span><br><span class="line">  evaluation_metrics = classifier.evaluate(</span><br><span class="line">    input_fn=<span class="keyword">lambda</span>: _input_fn([test_path]),</span><br><span class="line">    steps=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  print(<span class="string">"Test set metrics:"</span>)</span><br><span class="line">  <span class="keyword">for</span> m <span class="keyword">in</span> evaluation_metrics:</span><br><span class="line">    print(m, evaluation_metrics[m])</span><br><span class="line">  print(<span class="string">"---"</span>)</span><br><span class="line"><span class="keyword">except</span> ValueError <span class="keyword">as</span> err:</span><br><span class="line">  print(err)</span><br></pre></td></tr></table></figure>
<pre><code>Training set metrics:
accuracy 0.92
accuracy_baseline 0.68
auc 0.9705881
auc_precision_recall 0.9885154
average_loss 0.33181748
label/mean 0.68
loss 8.295437
precision 1.0
prediction/mean 0.52073544
recall 0.88235295
global_step 1000
---
Test set metrics:
accuracy 0.8
accuracy_baseline 0.56
auc 0.75974023
auc_precision_recall 0.66889143
average_loss 0.7257034
label/mean 0.56
loss 18.142586
precision 0.84615386
prediction/mean 0.4270074
recall 0.78571427
global_step 1000
---
</code></pre><h2 id="在-DNN-模型中使用嵌入"><a href="#在-DNN-模型中使用嵌入" class="headerlink" title="在 DNN 模型中使用嵌入"></a>在 DNN 模型中使用嵌入</h2><p>在此任务中，我们将使用嵌入列来实现 DNN 模型。嵌入列会将稀疏数据作为输入，并返回一个低维度密集矢量作为输出。</p>
<p> <strong>注意</strong>：<em>从计算方面而言，embedding_column 通常是用于在稀疏数据中训练模型最有效的选项。在此练习末尾的<a href="#scrollTo=XDMlGgRfKSVz">可选部分</a>，我们将更深入地讨论使用 <code>embedding_column</code> 与 <code>indicator_column</code> 之间的实现差异，以及如何在这两者之间做出权衡。</em></p>
<p> 在下面的代码中，执行以下操作：</p>
<ul>
<li>通过将数据投射到二维空间的 <code>embedding_column</code> 来为模型定义特征列（如需详细了解 <code>embedding_column</code> 的函数签名，请参阅相关 <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column" target="_blank" rel="noopener">TF 文档</a>）。</li>
<li><p>定义符合以下规范的 <code>DNNClassifier</code>：</p>
<ul>
<li>具有两个隐藏层，每个包含 20 个单元</li>
<li>采用学习速率为 0.1 的 AdaGrad 优化方法</li>
<li><code>gradient_clip_norm 值为 5.0</code></li>
</ul>
<p><strong>注意</strong>：<em>在实践中，我们可能会将数据投射到 2 维以上（比如 50 或 100）的空间中。但就目前而言，2 维是比较容易可视化的维数。</em></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">########################## SOLUTION CODE ########################################</span></span><br><span class="line">terms_embedding_column = tf.feature_column.embedding_column(terms_feature_column, dimension=<span class="number">2</span>)</span><br><span class="line">feature_columns = [ terms_embedding_column ]</span><br><span class="line"></span><br><span class="line">my_optimizer = tf.train.AdagradOptimizer(learning_rate=<span class="number">0.1</span>)</span><br><span class="line">my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line"></span><br><span class="line">classifier = tf.estimator.DNNClassifier(</span><br><span class="line">  feature_columns=feature_columns,</span><br><span class="line">  hidden_units=[<span class="number">20</span>,<span class="number">20</span>],</span><br><span class="line">  optimizer=my_optimizer</span><br><span class="line">)</span><br><span class="line"><span class="comment">#################################################################################</span></span><br><span class="line"></span><br><span class="line">classifier.train(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([train_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">evaluation_metrics = classifier.evaluate(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([train_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line">print(<span class="string">"Training set metrics:"</span>)</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> evaluation_metrics:</span><br><span class="line">  print(m, evaluation_metrics[m])</span><br><span class="line">print(<span class="string">"---"</span>)</span><br><span class="line"></span><br><span class="line">evaluation_metrics = classifier.evaluate(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([test_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test set metrics:"</span>)</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> evaluation_metrics:</span><br><span class="line">  print(m, evaluation_metrics[m])</span><br><span class="line">print(<span class="string">"---"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Training set metrics:
accuracy 0.78516
accuracy_baseline 0.5
auc 0.8685013
auc_precision_recall 0.8568284
average_loss 0.45557868
label/mean 0.5
loss 11.389467
precision 0.7566789
prediction/mean 0.52443045
recall 0.84064
global_step 1000
---
Test set metrics:
accuracy 0.78168
accuracy_baseline 0.5
auc 0.8668425
auc_precision_recall 0.85428405
average_loss 0.45733798
label/mean 0.5
loss 11.433449
precision 0.7556637
prediction/mean 0.52328736
recall 0.83256
global_step 1000
---
</code></pre><h2 id="确信模型中确实存在嵌入"><a href="#确信模型中确实存在嵌入" class="headerlink" title="确信模型中确实存在嵌入"></a>确信模型中确实存在嵌入</h2><p>上述模型使用了 <code>embedding_column</code>，而且似乎很有效，但这并没有让我们了解到内部发生的情形。我们如何检查该模型确实在内部使用了嵌入？</p>
<p>首先，我们来看看该模型中的张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classifier.get_variable_names()</span><br></pre></td></tr></table></figure>
<pre><code>[&apos;dnn/hiddenlayer_0/bias&apos;,
 &apos;dnn/hiddenlayer_0/bias/t_0/Adagrad&apos;,
 &apos;dnn/hiddenlayer_0/kernel&apos;,
 &apos;dnn/hiddenlayer_0/kernel/t_0/Adagrad&apos;,
 &apos;dnn/hiddenlayer_1/bias&apos;,
 &apos;dnn/hiddenlayer_1/bias/t_0/Adagrad&apos;,
 &apos;dnn/hiddenlayer_1/kernel&apos;,
 &apos;dnn/hiddenlayer_1/kernel/t_0/Adagrad&apos;,
 &apos;dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights&apos;,
 &apos;dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights/t_0/Adagrad&apos;,
 &apos;dnn/logits/bias&apos;,
 &apos;dnn/logits/bias/t_0/Adagrad&apos;,
 &apos;dnn/logits/kernel&apos;,
 &apos;dnn/logits/kernel/t_0/Adagrad&apos;,
 &apos;global_step&apos;]
</code></pre><p> 好的，我们可以看到这里有一个嵌入层：<code>&#39;dnn/input_from_feature_columns/input_layer/terms_embedding/...&#39;</code>。（顺便说一下，有趣的是，该层可以与模型的其他层一起训练，就像所有隐藏层一样。）</p>
<p>嵌入层的形状是否正确？请运行以下代码来查明。</p>
<p> <strong>注意</strong>：<em>在我们的示例中，嵌入是一个矩阵，可让我们将一个 50 维矢量投射到 2 维空间。</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> neure <span class="keyword">in</span> classifier.get_variable_names():</span><br><span class="line">    print(classifier.get_variable_value(neure).shape, <span class="string">": "</span> + neure)</span><br></pre></td></tr></table></figure>
<pre><code>(20,) : dnn/hiddenlayer_0/bias
(20,) : dnn/hiddenlayer_0/bias/t_0/Adagrad
(2, 20) : dnn/hiddenlayer_0/kernel
(2, 20) : dnn/hiddenlayer_0/kernel/t_0/Adagrad
(20,) : dnn/hiddenlayer_1/bias
(20,) : dnn/hiddenlayer_1/bias/t_0/Adagrad
(20, 20) : dnn/hiddenlayer_1/kernel
(20, 20) : dnn/hiddenlayer_1/kernel/t_0/Adagrad
(50, 2) : dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights
(50, 2) : dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights/t_0/Adagrad
(1,) : dnn/logits/bias
(1,) : dnn/logits/bias/t_0/Adagrad
(20, 1) : dnn/logits/kernel
(20, 1) : dnn/logits/kernel/t_0/Adagrad
() : global_step
</code></pre><p>花些时间来手动检查各个层及其形状，以确保一切都按照您预期的方式互相连接。</p>
<h2 id="检查嵌入"><a href="#检查嵌入" class="headerlink" title="检查嵌入"></a>检查嵌入</h2><p>现在，我们来看看实际嵌入空间，并了解术语最终所在的位置。请执行以下操作：</p>
<ol>
<li><p>运行以下代码来查看我们在训练的嵌入。一切最终是否如您所预期的那样？</p>
</li>
<li><p>重新运行<strong>在 DNN 模型中使用嵌 </strong> 中的代码来重新训练该模型，然后再次运行下面的嵌入可视化。哪些保持不变？哪些发生了变化？</p>
</li>
<li><p>最后，仅使用 10 步来重新训练该模型（这将产生一个糟糕的模型）。再次运行下面的嵌入可视化。您现在看到了什么？为什么？</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">embedding_matrix = classifier.get_variable_value(<span class="string">'dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> term_index <span class="keyword">in</span> range(len(informative_terms)):</span><br><span class="line">  <span class="comment"># Create a one-hot encoding for our term.  It has 0s everywhere, except for</span></span><br><span class="line">  <span class="comment"># a single 1 in the coordinate that corresponds to that term.</span></span><br><span class="line">  term_vector = np.zeros(len(informative_terms))</span><br><span class="line">  term_vector[term_index] = <span class="number">1</span></span><br><span class="line">  <span class="comment"># We'll now project that one-hot vector into the embedding space.</span></span><br><span class="line">  embedding_xy = np.matmul(term_vector, embedding_matrix)</span><br><span class="line">  plt.text(embedding_xy[<span class="number">0</span>],</span><br><span class="line">           embedding_xy[<span class="number">1</span>],</span><br><span class="line">           informative_terms[term_index])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do a little setup to make sure the plot displays nicely.</span></span><br><span class="line">plt.rcParams[<span class="string">"figure.figsize"</span>] = (<span class="number">15</span>, <span class="number">15</span>)</span><br><span class="line">plt.xlim(<span class="number">1.2</span> * embedding_matrix.min(), <span class="number">1.2</span> * embedding_matrix.max())</span><br><span class="line">plt.ylim(<span class="number">1.2</span> * embedding_matrix.min(), <span class="number">1.2</span> * embedding_matrix.max())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="TensorFlow_10_30_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">embedding_matrix = classifier.get_variable_value(<span class="string">'dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> term_index <span class="keyword">in</span> range(len(informative_terms)):</span><br><span class="line">  <span class="comment"># Create a one-hot encoding for our term.  It has 0s everywhere, except for</span></span><br><span class="line">  <span class="comment"># a single 1 in the coordinate that corresponds to that term.</span></span><br><span class="line">  term_vector = np.zeros(len(informative_terms))</span><br><span class="line">  term_vector[term_index] = <span class="number">1</span></span><br><span class="line">  <span class="comment"># We'll now project that one-hot vector into the embedding space.</span></span><br><span class="line">  embedding_xy = np.matmul(term_vector, embedding_matrix)</span><br><span class="line">  plt.text(embedding_xy[<span class="number">0</span>],</span><br><span class="line">           embedding_xy[<span class="number">1</span>],</span><br><span class="line">           informative_terms[term_index])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do a little setup to make sure the plot displays nicely.</span></span><br><span class="line">plt.rcParams[<span class="string">"figure.figsize"</span>] = (<span class="number">15</span>, <span class="number">15</span>)</span><br><span class="line">plt.xlim(<span class="number">1.2</span> * embedding_matrix.min(), <span class="number">1.2</span> * embedding_matrix.max())</span><br><span class="line">plt.ylim(<span class="number">1.2</span> * embedding_matrix.min(), <span class="number">1.2</span> * embedding_matrix.max())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="TensorFlow_10_31_0.png" alt="png"></p>
<h2 id="任务-6：尝试改进模型的效果"><a href="#任务-6：尝试改进模型的效果" class="headerlink" title="任务 6：尝试改进模型的效果"></a>任务 6：尝试改进模型的效果</h2><p>看看您能否优化该模型以改进其效果。您可以尝试以下几种做法：</p>
<ul>
<li><strong>更改超参数</strong>或<strong>使用其他优化工具</strong>，比如 Adam（通过遵循这些策略，您的准确率可能只会提高一两个百分点）。</li>
<li><strong>向 <code>informative_terms</code> 中添加其他术语。</strong>此数据集有一个完整的词汇表文件，其中包含 30716 个术语，您可以在以下位置找到该文件：<a href="https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/terms.txt" target="_blank" rel="noopener">https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/terms.txt</a> 您可以从该词汇表文件中挑选出其他术语，也可以通过 <code>categorical_column_with_vocabulary_file</code> 特征列使用整个词汇表文件。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Download the vocabulary file.</span></span><br><span class="line">terms_url = <span class="string">'https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/terms.txt'</span></span><br><span class="line">terms_path = tf.keras.utils.get_file(terms_url.split(<span class="string">'/'</span>)[<span class="number">-1</span>], terms_url)</span><br></pre></td></tr></table></figure>
<pre><code>Downloading data from https://download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/terms.txt
253952/253538 [==============================] - 0s 0us/step
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a feature column from "terms", using a full vocabulary file.</span></span><br><span class="line">informative_terms = <span class="keyword">None</span></span><br><span class="line"><span class="keyword">with</span> io.open(terms_path, <span class="string">'r'</span>, encoding=<span class="string">'utf8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">  <span class="comment"># Convert it to a set first to remove duplicates.</span></span><br><span class="line">  informative_terms = list(set(f.read().split()))</span><br><span class="line">  </span><br><span class="line">terms_feature_column = tf.feature_column.categorical_column_with_vocabulary_list(key=<span class="string">"terms"</span>, </span><br><span class="line">                                                                                 vocabulary_list=informative_terms)</span><br><span class="line"></span><br><span class="line">terms_embedding_column = tf.feature_column.embedding_column(terms_feature_column, dimension=<span class="number">2</span>)</span><br><span class="line">feature_columns = [ terms_embedding_column ]</span><br><span class="line"></span><br><span class="line">my_optimizer = tf.train.AdagradOptimizer(learning_rate=<span class="number">0.1</span>)</span><br><span class="line">my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, <span class="number">5.0</span>)</span><br><span class="line"></span><br><span class="line">classifier = tf.estimator.DNNClassifier(</span><br><span class="line">  feature_columns=feature_columns,</span><br><span class="line">  hidden_units=[<span class="number">10</span>,<span class="number">10</span>],</span><br><span class="line">  optimizer=my_optimizer</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">classifier.train(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([train_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">evaluation_metrics = classifier.evaluate(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([train_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line">print(<span class="string">"Training set metrics:"</span>)</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> evaluation_metrics:</span><br><span class="line">  print(m, evaluation_metrics[m])</span><br><span class="line">print(<span class="string">"---"</span>)</span><br><span class="line"></span><br><span class="line">evaluation_metrics = classifier.evaluate(</span><br><span class="line">  input_fn=<span class="keyword">lambda</span>: _input_fn([test_path]),</span><br><span class="line">  steps=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test set metrics:"</span>)</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> evaluation_metrics:</span><br><span class="line">  print(m, evaluation_metrics[m])</span><br><span class="line">print(<span class="string">"---"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Training set metrics:
accuracy 0.82
accuracy_baseline 0.5
auc 0.89789164
auc_precision_recall 0.8937925
average_loss 0.4075714
label/mean 0.5
loss 10.189285
precision 0.83664364
prediction/mean 0.4748372
recall 0.79528
global_step 1000
---
Test set metrics:
accuracy 0.8048
accuracy_baseline 0.5
auc 0.88663244
auc_precision_recall 0.8821298
average_loss 0.42734343
label/mean 0.5
loss 10.683586
precision 0.8235394
prediction/mean 0.47395515
recall 0.77584
global_step 1000
---
</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我们可能获得了比我们原来的线性模型更好且具有嵌入的 DNN 解决方案，但线性模型也相当不错，而且训练速度快得多。线性模型的训练速度之所以更快，是因为它们没有太多要更新的参数或要反向传播的层。</p>
<p>在有些应用中，线性模型的速度可能非常关键，或者从质量的角度来看，线性模型可能完全够用。在其他领域，DNN 提供的额外模型复杂性和能力可能更重要。在定义模型架构时，请记得要充分探讨您的问题，以便知道自己所处的情形。</p>
<h3 id="可选内容：在-embedding-column-与-indicator-column-之间进行权衡"><a href="#可选内容：在-embedding-column-与-indicator-column-之间进行权衡" class="headerlink" title="可选内容：在 embedding_column 与 indicator_column 之间进行权衡"></a><em>可选内容：</em>在 <code>embedding_column</code> 与 <code>indicator_column</code> 之间进行权衡</h3><p>从概念上讲，在训练 <code>LinearClassifier</code> 或 <code>DNNClassifier</code> 时，需要根据实际情况使用稀疏列。TF 提供了两个选项：<code>embedding_column</code> 或 <code>indicator_column</code>。</p>
<p>在训练 LinearClassifier（如<strong>使用具有稀疏输入和显式词汇表的线性模型</strong> 中所示）时，系统在后台使用了 <code>embedding_column</code>。正如<strong>使用深度神经网络 (DNN) 模型</strong> 中所示，在训练 <code>DNNClassifier</code> 时，您必须明确选择 <code>embedding_column</code> 或 <code>indicator_column</code>。本部分通过一个简单的示例讨论了这两者之间的区别，以及如何在二者之间进行权衡。</p>
<p> 假设我们的稀疏数据包含 <code>&quot;great&quot;</code>、<code>&quot;beautiful&quot;</code> 和 <code>&quot;excellent&quot;</code> 这几个值。由于我们在此处使用的词汇表大小为 $V = 50$，因此第一层中的每个单元（神经元）的权重将为 50。我们用 $s$ 表示稀疏输入中的项数。对于此示例稀疏数据，$s = 3$。对于具有 $V$ 个可能值的输入层，带有 $d$ 个单元的隐藏层需要运行一次“矢量 - 矩阵”乘法运算：$(1 \times V) <em> (V \times d)$。此运算会产生 $O(V </em> d)$ 的计算成本。请注意，此成本与隐藏层中的权重数成正比，而与 $s$ 无关。</p>
<p>如果输入使用 <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/indicator_column" target="_blank" rel="noopener"><code>indicator_column</code></a> 进行了独热编码（长度为 $V$ 的布尔型矢量，存在用 1 表示，其余则为 0），这表示很多零进行了相乘和相加运算。</p>
<p> 当我们通过使用大小为 $d$ 的 <a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column" target="_blank" rel="noopener"><code>embedding_column</code></a> 获得完全相同的结果时，我们将仅查询与示例输入中存在的 3 个特征 <code>&quot;great&quot;</code>、<code>&quot;beautiful&quot;</code> 和 <code>&quot;excellent&quot;</code> 相对应的嵌入并将这三个嵌入相加：$(1 \times d) + (1 \times d) + (1 \times d)$。由于不存在的特征的权重在“矢量-矩阵”乘法中与 0 相乘，因此对结果没有任何影响；而存在的特征的权重在“矢量-矩阵”乘法中与 1 相乘。因此，将通过嵌入查询获得的权重相加会获得与“矢量-矩阵”乘法相同的结果。</p>
<p>当使用嵌入时，计算嵌入查询是一个 $O(s <em> d)$ 计算；从计算方面而言，它比稀疏数据中的 <code>indicator_column</code> 的 $O(V </em> d)$ 更具成本效益，因为 $s$ 远远小于 $V$。（请注意，这些嵌入是临时学习的结果。在任何指定的训练迭代中，都是当前查询的权重。</p>
<p> 正如我们在<strong>在 DNN 模型中使用嵌入</strong> 中看到的，通过在训练 <code>DNNClassifier</code> 过程中使用 <code>embedding_column</code>，我们的模型学习了特征的低维度表示法，其中点积定义了一个针对目标任务的相似性指标。在本例中，影评中使用的相似术语（例如 <code>&quot;great&quot;</code> 和 <code>&quot;excellent&quot;</code>）在嵌入空间中彼此之间距离较近（即具有较大的点积），而相异的术语（例如 <code>&quot;great&quot;</code> 和 <code>&quot;bad&quot;</code>）在嵌入空间中彼此之间距离较远（即具有较小的点积）。</p>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Tensorflow/" rel="tag"><i class="fa fa-tag"></i> Tensorflow</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/02/21/TensorFlow_9/" rel="next" title="TensorFlow 9">
                <i class="fa fa-chevron-left"></i> TensorFlow 9
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/03/27/激活函数（Activation functions）/" rel="prev" title="激活函数（Activation functions）">
                激活函数（Activation functions） <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Voidmort" />
            
              <p class="site-author-name" itemprop="name">Voidmort</p>
              <p class="site-description motion-element" itemprop="description">研究AI，撰写影评，热爱音乐，努力健身</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">72</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">16</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">35</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/Voidmort" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:wxj5658@hotmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="http://weibo.com/u/1749266905" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
                  
                </span>
              
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://jusanliusha.xyz" title="Jusanliusha" target="_blank">Jusanliusha</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#稀疏数据和嵌入简介"><span class="nav-number">1.</span> <span class="nav-text">稀疏数据和嵌入简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#设置"><span class="nav-number">1.1.</span> <span class="nav-text">设置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#构建情感分析模型"><span class="nav-number">1.2.</span> <span class="nav-text">构建情感分析模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#构建输入管道"><span class="nav-number">1.3.</span> <span class="nav-text">构建输入管道</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用具有稀疏输入和显式词汇表的线性模型"><span class="nav-number">1.4.</span> <span class="nav-text">使用具有稀疏输入和显式词汇表的线性模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用深度神经网络-DNN-模型"><span class="nav-number">1.5.</span> <span class="nav-text">使用深度神经网络 (DNN) 模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#在-DNN-模型中使用嵌入"><span class="nav-number">1.6.</span> <span class="nav-text">在 DNN 模型中使用嵌入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#确信模型中确实存在嵌入"><span class="nav-number">1.7.</span> <span class="nav-text">确信模型中确实存在嵌入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#检查嵌入"><span class="nav-number">1.8.</span> <span class="nav-text">检查嵌入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#任务-6：尝试改进模型的效果"><span class="nav-number">1.9.</span> <span class="nav-text">任务 6：尝试改进模型的效果</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">1.10.</span> <span class="nav-text">总结</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#可选内容：在-embedding-column-与-indicator-column-之间进行权衡"><span class="nav-number">1.10.1.</span> <span class="nav-text">可选内容：在 embedding_column 与 indicator_column 之间进行权衡</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Voidmort</span>

  

  
</div>





        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



	





  





  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  

  

  

  

<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/kesyoban.model.json"},"display":{"position":"right","width":130,"height":260},"mobile":{"show":false},"log":false,"tagMode":false});</script></body>
</html>
